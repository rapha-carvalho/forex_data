[2021-05-14 19:55:38,639] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:54:47.557892+00:00 [queued]>
[2021-05-14 19:55:38,645] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:54:47.557892+00:00 [queued]>
[2021-05-14 19:55:38,646] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:55:38,646] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 19:55:38,646] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:55:38,650] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T22:54:47.557892+00:00
[2021-05-14 19:55:38,653] {standard_task_runner.py:52} INFO - Started process 26027 to run task
[2021-05-14 19:55:38,660] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T22:54:47.557892+00:00', '--job-id', '582', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpeafs8uh4', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpsvwkmpst']
[2021-05-14 19:55:38,662] {standard_task_runner.py:77} INFO - Job 582: Subtask run_spark_job
[2021-05-14 19:55:38,694] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T22:54:47.557892+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-14 19:55:38,715] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T22:54:47.557892+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T22:54:47.557892+00:00
[2021-05-14 19:55:38,718] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 19:55:42,232] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 19:55:42,236] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-14 19:55:42,237] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 19:55:42,240] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 19:55:44,217] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 19:55:44,770] {docker.py:276} INFO - 21/05/14 22:55:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 19:55:46,942] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 19:55:46,943] {docker.py:276} INFO - 
[2021-05-14 19:55:46,979] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 19:55:47,044] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceUtils: ==============================================================
[2021-05-14 19:55:47,044] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 19:55:47,045] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceUtils: ==============================================================
[2021-05-14 19:55:47,046] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkContext: Submitted application: spark.py
[2021-05-14 19:55:47,081] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 19:55:47,096] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 19:55:47,097] {docker.py:276} INFO - 21/05/14 22:55:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 19:55:47,166] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 19:55:47,166] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 19:55:47,167] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 19:55:47,167] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 19:55:47,167] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 19:55:47,510] {docker.py:276} INFO - 21/05/14 22:55:47 INFO Utils: Successfully started service 'sparkDriver' on port 32847.
[2021-05-14 19:55:47,548] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 19:55:47,593] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 19:55:47,625] {docker.py:276} INFO - 21/05/14 22:55:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 19:55:47,626] {docker.py:276} INFO - 21/05/14 22:55:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 19:55:47,633] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 19:55:47,650] {docker.py:276} INFO - 21/05/14 22:55:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-654d346c-7fe4-49d9-8002-e27c4d21cb90
[2021-05-14 19:55:47,677] {docker.py:276} INFO - 21/05/14 22:55:47 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 19:55:47,700] {docker.py:276} INFO - 21/05/14 22:55:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 19:55:47,992] {docker.py:276} INFO - 21/05/14 22:55:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 19:55:48,083] {docker.py:276} INFO - 21/05/14 22:55:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://5c14c3e96bf4:4040
[2021-05-14 19:55:48,331] {docker.py:276} INFO - 21/05/14 22:55:48 INFO Executor: Starting executor ID driver on host 5c14c3e96bf4
[2021-05-14 19:55:48,370] {docker.py:276} INFO - 21/05/14 22:55:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33219.
21/05/14 22:55:48 INFO NettyBlockTransferService: Server created on 5c14c3e96bf4:33219
[2021-05-14 19:55:48,372] {docker.py:276} INFO - 21/05/14 22:55:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 19:55:48,393] {docker.py:276} INFO - 21/05/14 22:55:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5c14c3e96bf4, 33219, None)
[2021-05-14 19:55:48,401] {docker.py:276} INFO - 21/05/14 22:55:48 INFO BlockManagerMasterEndpoint: Registering block manager 5c14c3e96bf4:33219 with 934.4 MiB RAM, BlockManagerId(driver, 5c14c3e96bf4, 33219, None)
[2021-05-14 19:55:48,404] {docker.py:276} INFO - 21/05/14 22:55:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5c14c3e96bf4, 33219, None)
[2021-05-14 19:55:48,406] {docker.py:276} INFO - 21/05/14 22:55:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5c14c3e96bf4, 33219, None)
[2021-05-14 19:55:48,967] {docker.py:276} INFO - 21/05/14 22:55:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 19:55:48,968] {docker.py:276} INFO - 21/05/14 22:55:48 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 19:55:49,986] {docker.py:276} INFO - 21/05/14 22:55:49 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 19:55:50,033] {docker.py:276} INFO - 21/05/14 22:55:50 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/14 22:55:50 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 19:55:55,789] {docker.py:276} INFO - 21/05/14 22:55:55 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620946501_to_1620948301.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620948301_to_1620950101.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620950101_to_1620951901.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620951901_to_1620953701.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620953701_to_1620955501.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620955501_to_1620957301.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620957301_to_1620959101.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620959101_to_1620960901.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620960901_to_1620962701.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620962701_to_1620964501.csv.
[2021-05-14 19:55:56,311] {docker.py:276} INFO - 21/05/14 22:55:56 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:55:56,340] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-14 19:55:56,341] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 19:55:56,342] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:55:56,345] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:55:56,353] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:55:56,459] {docker.py:276} INFO - 21/05/14 22:55:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-14 19:55:56,527] {docker.py:276} INFO - 21/05/14 22:55:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-14 19:55:56,532] {docker.py:276} INFO - 21/05/14 22:55:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5c14c3e96bf4:33219 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:55:56,538] {docker.py:276} INFO - 21/05/14 22:55:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:55:56,571] {docker.py:276} INFO - 21/05/14 22:55:56 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 19:55:56,578] {docker.py:276} INFO - 21/05/14 22:55:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-14 19:55:56,678] {docker.py:276} INFO - 21/05/14 22:55:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5c14c3e96bf4, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:56,683] {docker.py:276} INFO - 21/05/14 22:55:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (5c14c3e96bf4, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:56,685] {docker.py:276} INFO - 21/05/14 22:55:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (5c14c3e96bf4, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:56,690] {docker.py:276} INFO - 21/05/14 22:55:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (5c14c3e96bf4, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:56,712] {docker.py:276} INFO - 21/05/14 22:55:56 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
21/05/14 22:55:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 19:55:56,713] {docker.py:276} INFO - 21/05/14 22:55:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-14 19:55:56,713] {docker.py:276} INFO - 21/05/14 22:55:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-14 19:55:57,179] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1929 bytes result sent to driver
[2021-05-14 19:55:57,187] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (5c14c3e96bf4, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,191] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-14 19:55:57,203] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 561 ms on 5c14c3e96bf4 (executor driver) (1/141)
[2021-05-14 19:55:57,375] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-14 19:55:57,380] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (5c14c3e96bf4, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,381] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-14 19:55:57,382] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 196 ms on 5c14c3e96bf4 (executor driver) (2/141)
[2021-05-14 19:55:57,565] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-14 19:55:57,568] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (5c14c3e96bf4, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,570] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-14 19:55:57,572] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 195 ms on 5c14c3e96bf4 (executor driver) (3/141)
[2021-05-14 19:55:57,669] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-14 19:55:57,670] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-14 19:55:57,672] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (5c14c3e96bf4, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,674] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-14 19:55:57,674] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-14 19:55:57,675] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 993 ms on 5c14c3e96bf4 (executor driver) (4/141)
[2021-05-14 19:55:57,677] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 993 ms on 5c14c3e96bf4 (executor driver) (5/141)
[2021-05-14 19:55:57,679] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (5c14c3e96bf4, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,680] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-14 19:55:57,682] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (5c14c3e96bf4, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,684] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-14 19:55:57,685] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1000 ms on 5c14c3e96bf4 (executor driver) (6/141)
[2021-05-14 19:55:57,763] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-14 19:55:57,766] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (5c14c3e96bf4, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,768] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 200 ms on 5c14c3e96bf4 (executor driver) (7/141)
[2021-05-14 19:55:57,769] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-14 19:55:57,864] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-14 19:55:57,866] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (5c14c3e96bf4, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,868] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-14 19:55:57,869] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 188 ms on 5c14c3e96bf4 (executor driver) (8/141)
[2021-05-14 19:55:57,876] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-14 19:55:57,877] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 199 ms on 5c14c3e96bf4 (executor driver) (9/141)
[2021-05-14 19:55:57,880] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (5c14c3e96bf4, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,882] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-14 19:55:57,969] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1886 bytes result sent to driver
[2021-05-14 19:55:57,970] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (5c14c3e96bf4, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:57,971] {docker.py:276} INFO - 21/05/14 22:55:57 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-14 19:55:57,972] {docker.py:276} INFO - 21/05/14 22:55:57 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 208 ms on 5c14c3e96bf4 (executor driver) (10/141)
[2021-05-14 19:55:58,061] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1886 bytes result sent to driver
[2021-05-14 19:55:58,063] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (5c14c3e96bf4, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,064] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 199 ms on 5c14c3e96bf4 (executor driver) (11/141)
[2021-05-14 19:55:58,065] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-14 19:55:58,083] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1886 bytes result sent to driver
[2021-05-14 19:55:58,086] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (5c14c3e96bf4, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,087] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 208 ms on 5c14c3e96bf4 (executor driver) (12/141)
[2021-05-14 19:55:58,087] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-14 19:55:58,148] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-14 19:55:58,149] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (5c14c3e96bf4, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,151] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 182 ms on 5c14c3e96bf4 (executor driver) (13/141)
[2021-05-14 19:55:58,151] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-14 19:55:58,181] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1886 bytes result sent to driver
[2021-05-14 19:55:58,182] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (5c14c3e96bf4, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,184] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-14 19:55:58,184] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 513 ms on 5c14c3e96bf4 (executor driver) (14/141)
[2021-05-14 19:55:58,254] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-14 19:55:58,257] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (5c14c3e96bf4, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,259] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-14 19:55:58,260] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 197 ms on 5c14c3e96bf4 (executor driver) (15/141)
[2021-05-14 19:55:58,285] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1843 bytes result sent to driver
[2021-05-14 19:55:58,287] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (5c14c3e96bf4, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,289] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-14 19:55:58,290] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 205 ms on 5c14c3e96bf4 (executor driver) (16/141)
[2021-05-14 19:55:58,331] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-14 19:55:58,336] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (5c14c3e96bf4, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,338] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 189 ms on 5c14c3e96bf4 (executor driver) (17/141)
[2021-05-14 19:55:58,340] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-14 19:55:58,369] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-14 19:55:58,371] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (5c14c3e96bf4, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,372] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 190 ms on 5c14c3e96bf4 (executor driver) (18/141)
[2021-05-14 19:55:58,373] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-14 19:55:58,459] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-14 19:55:58,461] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (5c14c3e96bf4, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,463] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 206 ms on 5c14c3e96bf4 (executor driver) (19/141)
[2021-05-14 19:55:58,463] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-14 19:55:58,506] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-14 19:55:58,509] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (5c14c3e96bf4, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,510] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-14 19:55:58,511] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 225 ms on 5c14c3e96bf4 (executor driver) (20/141)
[2021-05-14 19:55:58,521] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1886 bytes result sent to driver
[2021-05-14 19:55:58,522] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (5c14c3e96bf4, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,523] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 189 ms on 5c14c3e96bf4 (executor driver) (21/141)
[2021-05-14 19:55:58,527] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-14 19:55:58,555] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1886 bytes result sent to driver
[2021-05-14 19:55:58,556] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 187 ms on 5c14c3e96bf4 (executor driver) (22/141)
[2021-05-14 19:55:58,559] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (5c14c3e96bf4, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,559] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-14 19:55:58,649] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1886 bytes result sent to driver
[2021-05-14 19:55:58,651] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (5c14c3e96bf4, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,652] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 191 ms on 5c14c3e96bf4 (executor driver) (23/141)
[2021-05-14 19:55:58,652] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-14 19:55:58,691] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1886 bytes result sent to driver
[2021-05-14 19:55:58,692] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 184 ms on 5c14c3e96bf4 (executor driver) (24/141)
[2021-05-14 19:55:58,693] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (5c14c3e96bf4, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,694] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-14 19:55:58,698] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-14 19:55:58,700] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (5c14c3e96bf4, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,701] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-14 19:55:58,702] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 180 ms on 5c14c3e96bf4 (executor driver) (25/141)
[2021-05-14 19:55:58,739] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-14 19:55:58,740] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (5c14c3e96bf4, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,741] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
21/05/14 22:55:58 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 184 ms on 5c14c3e96bf4 (executor driver) (26/141)
[2021-05-14 19:55:58,835] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-14 19:55:58,836] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (5c14c3e96bf4, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,838] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 187 ms on 5c14c3e96bf4 (executor driver) (27/141)
[2021-05-14 19:55:58,838] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-14 19:55:58,870] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
[2021-05-14 19:55:58,872] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 179 ms on 5c14c3e96bf4 (executor driver) (28/141)
[2021-05-14 19:55:58,874] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (5c14c3e96bf4, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,876] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-14 19:55:58,877] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-14 19:55:58,879] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (5c14c3e96bf4, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,880] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-14 19:55:58,885] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 181 ms on 5c14c3e96bf4 (executor driver) (29/141)
[2021-05-14 19:55:58,918] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-14 19:55:58,919] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (5c14c3e96bf4, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:58,920] {docker.py:276} INFO - 21/05/14 22:55:58 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-14 19:55:58,921] {docker.py:276} INFO - 21/05/14 22:55:58 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 181 ms on 5c14c3e96bf4 (executor driver) (30/141)
[2021-05-14 19:55:59,016] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1886 bytes result sent to driver
[2021-05-14 19:55:59,017] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (5c14c3e96bf4, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,017] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 182 ms on 5c14c3e96bf4 (executor driver) (31/141)
[2021-05-14 19:55:59,018] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-14 19:55:59,048] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1886 bytes result sent to driver
[2021-05-14 19:55:59,052] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (5c14c3e96bf4, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,053] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 180 ms on 5c14c3e96bf4 (executor driver) (32/141)
[2021-05-14 19:55:59,054] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-14 19:55:59,057] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1886 bytes result sent to driver
[2021-05-14 19:55:59,063] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (5c14c3e96bf4, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,064] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 181 ms on 5c14c3e96bf4 (executor driver) (33/141)
[2021-05-14 19:55:59,064] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-14 19:55:59,103] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1886 bytes result sent to driver
[2021-05-14 19:55:59,105] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (5c14c3e96bf4, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,105] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-14 19:55:59,107] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 187 ms on 5c14c3e96bf4 (executor driver) (34/141)
[2021-05-14 19:55:59,202] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-14 19:55:59,204] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (5c14c3e96bf4, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,206] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-14 19:55:59,207] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 189 ms on 5c14c3e96bf4 (executor driver) (35/141)
[2021-05-14 19:55:59,227] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-14 19:55:59,229] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (5c14c3e96bf4, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,231] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 179 ms on 5c14c3e96bf4 (executor driver) (36/141)
[2021-05-14 19:55:59,231] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1800 bytes result sent to driver
[2021-05-14 19:55:59,232] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-14 19:55:59,233] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (5c14c3e96bf4, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,234] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 177 ms on 5c14c3e96bf4 (executor driver) (37/141)
[2021-05-14 19:55:59,235] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-14 19:55:59,285] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-14 19:55:59,286] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (5c14c3e96bf4, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,287] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
21/05/14 22:55:59 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 184 ms on 5c14c3e96bf4 (executor driver) (38/141)
[2021-05-14 19:55:59,387] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-14 19:55:59,389] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (5c14c3e96bf4, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,390] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 187 ms on 5c14c3e96bf4 (executor driver) (39/141)
[2021-05-14 19:55:59,390] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-14 19:55:59,406] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-14 19:55:59,410] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (5c14c3e96bf4, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,411] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 181 ms on 5c14c3e96bf4 (executor driver) (40/141)
21/05/14 22:55:59 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-14 19:55:59,412] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-14 19:55:59,412] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (5c14c3e96bf4, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,413] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 179 ms on 5c14c3e96bf4 (executor driver) (41/141)
[2021-05-14 19:55:59,413] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-14 19:55:59,466] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1886 bytes result sent to driver
[2021-05-14 19:55:59,468] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (5c14c3e96bf4, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,469] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
21/05/14 22:55:59 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 183 ms on 5c14c3e96bf4 (executor driver) (42/141)
[2021-05-14 19:55:59,570] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1886 bytes result sent to driver
[2021-05-14 19:55:59,572] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (5c14c3e96bf4, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,574] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 185 ms on 5c14c3e96bf4 (executor driver) (43/141)
[2021-05-14 19:55:59,575] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-14 19:55:59,590] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1886 bytes result sent to driver
21/05/14 22:55:59 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1886 bytes result sent to driver
[2021-05-14 19:55:59,592] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (5c14c3e96bf4, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,593] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-14 19:55:59,594] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (5c14c3e96bf4, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,595] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 186 ms on 5c14c3e96bf4 (executor driver) (44/141)
[2021-05-14 19:55:59,595] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 185 ms on 5c14c3e96bf4 (executor driver) (45/141)
[2021-05-14 19:55:59,596] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-14 19:55:59,655] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-14 19:55:59,657] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (5c14c3e96bf4, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,658] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 191 ms on 5c14c3e96bf4 (executor driver) (46/141)
21/05/14 22:55:59 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-14 19:55:59,756] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-14 19:55:59,757] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (5c14c3e96bf4, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,758] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 187 ms on 5c14c3e96bf4 (executor driver) (47/141)
21/05/14 22:55:59 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-14 19:55:59,766] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-14 19:55:59,767] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (5c14c3e96bf4, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,768] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-14 19:55:59,769] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-14 19:55:59,770] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (5c14c3e96bf4, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,771] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-14 19:55:59,772] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 178 ms on 5c14c3e96bf4 (executor driver) (48/141)
[2021-05-14 19:55:59,772] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 181 ms on 5c14c3e96bf4 (executor driver) (49/141)
[2021-05-14 19:55:59,839] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-14 19:55:59,841] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (5c14c3e96bf4, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,842] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 186 ms on 5c14c3e96bf4 (executor driver) (50/141)
[2021-05-14 19:55:59,843] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-14 19:55:59,940] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-14 19:55:59,944] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (5c14c3e96bf4, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,946] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-14 19:55:59,947] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 188 ms on 5c14c3e96bf4 (executor driver) (51/141)
21/05/14 22:55:59 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-14 19:55:59,948] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-14 19:55:59,949] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (5c14c3e96bf4, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,951] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 181 ms on 5c14c3e96bf4 (executor driver) (52/141)
[2021-05-14 19:55:59,952] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-14 19:55:59,953] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 185 ms on 5c14c3e96bf4 (executor driver) (53/141)
[2021-05-14 19:55:59,954] {docker.py:276} INFO - 21/05/14 22:55:59 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (5c14c3e96bf4, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:55:59,955] {docker.py:276} INFO - 21/05/14 22:55:59 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-14 19:56:00,021] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1886 bytes result sent to driver
[2021-05-14 19:56:00,022] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (5c14c3e96bf4, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,023] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 183 ms on 5c14c3e96bf4 (executor driver) (54/141)
[2021-05-14 19:56:00,024] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-14 19:56:00,130] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-14 19:56:00,131] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (5c14c3e96bf4, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,132] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
21/05/14 22:56:00 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 191 ms on 5c14c3e96bf4 (executor driver) (55/141)
[2021-05-14 19:56:00,139] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1886 bytes result sent to driver
[2021-05-14 19:56:00,140] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (5c14c3e96bf4, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,141] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 192 ms on 5c14c3e96bf4 (executor driver) (56/141)
[2021-05-14 19:56:00,141] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-14 19:56:00,146] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1886 bytes result sent to driver
[2021-05-14 19:56:00,147] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (5c14c3e96bf4, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,148] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-14 19:56:00,149] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 195 ms on 5c14c3e96bf4 (executor driver) (57/141)
[2021-05-14 19:56:00,204] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-14 19:56:00,205] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (5c14c3e96bf4, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,206] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 184 ms on 5c14c3e96bf4 (executor driver) (58/141)
[2021-05-14 19:56:00,207] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-14 19:56:00,313] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-14 19:56:00,315] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (5c14c3e96bf4, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,317] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 185 ms on 5c14c3e96bf4 (executor driver) (59/141)
[2021-05-14 19:56:00,318] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-14 19:56:00,320] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
21/05/14 22:56:00 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-14 19:56:00,321] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (5c14c3e96bf4, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,322] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-14 19:56:00,323] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (5c14c3e96bf4, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,324] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 177 ms on 5c14c3e96bf4 (executor driver) (60/141)
[2021-05-14 19:56:00,325] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-14 19:56:00,326] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 185 ms on 5c14c3e96bf4 (executor driver) (61/141)
[2021-05-14 19:56:00,394] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-14 19:56:00,396] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (5c14c3e96bf4, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,397] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 192 ms on 5c14c3e96bf4 (executor driver) (62/141)
[2021-05-14 19:56:00,398] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-14 19:56:00,503] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-14 19:56:00,505] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
21/05/14 22:56:00 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (5c14c3e96bf4, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,507] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (5c14c3e96bf4, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,508] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-14 19:56:00,509] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 187 ms on 5c14c3e96bf4 (executor driver) (63/141)
[2021-05-14 19:56:00,510] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-14 19:56:00,511] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 187 ms on 5c14c3e96bf4 (executor driver) (64/141)
[2021-05-14 19:56:00,512] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-14 19:56:00,513] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (5c14c3e96bf4, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,514] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
21/05/14 22:56:00 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 200 ms on 5c14c3e96bf4 (executor driver) (65/141)
[2021-05-14 19:56:00,582] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1886 bytes result sent to driver
[2021-05-14 19:56:00,583] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (5c14c3e96bf4, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,584] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-14 19:56:00,585] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 190 ms on 5c14c3e96bf4 (executor driver) (66/141)
[2021-05-14 19:56:00,694] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1886 bytes result sent to driver
[2021-05-14 19:56:00,696] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (5c14c3e96bf4, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,697] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1886 bytes result sent to driver
[2021-05-14 19:56:00,698] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-14 19:56:00,699] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (5c14c3e96bf4, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,700] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1886 bytes result sent to driver
[2021-05-14 19:56:00,701] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 194 ms on 5c14c3e96bf4 (executor driver) (67/141)
[2021-05-14 19:56:00,702] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-14 19:56:00,703] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 189 ms on 5c14c3e96bf4 (executor driver) (68/141)
[2021-05-14 19:56:00,704] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (5c14c3e96bf4, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,705] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 201 ms on 5c14c3e96bf4 (executor driver) (69/141)
21/05/14 22:56:00 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-14 19:56:00,766] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-14 19:56:00,768] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (5c14c3e96bf4, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,769] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 188 ms on 5c14c3e96bf4 (executor driver) (70/141)
21/05/14 22:56:00 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-14 19:56:00,895] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-14 19:56:00,896] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-14 19:56:00,897] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-14 19:56:00,898] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (5c14c3e96bf4, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,900] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-14 19:56:00,901] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (5c14c3e96bf4, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,902] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 206 ms on 5c14c3e96bf4 (executor driver) (71/141)
[2021-05-14 19:56:00,902] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-14 19:56:00,903] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (5c14c3e96bf4, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,904] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 201 ms on 5c14c3e96bf4 (executor driver) (72/141)
[2021-05-14 19:56:00,905] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 206 ms on 5c14c3e96bf4 (executor driver) (73/141)
[2021-05-14 19:56:00,906] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-14 19:56:00,959] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-14 19:56:00,962] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (5c14c3e96bf4, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:00,963] {docker.py:276} INFO - 21/05/14 22:56:00 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 195 ms on 5c14c3e96bf4 (executor driver) (74/141)
[2021-05-14 19:56:00,964] {docker.py:276} INFO - 21/05/14 22:56:00 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-14 19:56:01,087] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1886 bytes result sent to driver
[2021-05-14 19:56:01,088] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1886 bytes result sent to driver
[2021-05-14 19:56:01,090] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1886 bytes result sent to driver
21/05/14 22:56:01 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (5c14c3e96bf4, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,091] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
21/05/14 22:56:01 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (5c14c3e96bf4, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,095] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (5c14c3e96bf4, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,096] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 194 ms on 5c14c3e96bf4 (executor driver) (75/141)
[2021-05-14 19:56:01,096] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 199 ms on 5c14c3e96bf4 (executor driver) (76/141)
[2021-05-14 19:56:01,097] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 192 ms on 5c14c3e96bf4 (executor driver) (77/141)
[2021-05-14 19:56:01,097] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-14 19:56:01,097] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-14 19:56:01,153] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1886 bytes result sent to driver
[2021-05-14 19:56:01,155] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (5c14c3e96bf4, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,156] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 195 ms on 5c14c3e96bf4 (executor driver) (78/141)
[2021-05-14 19:56:01,157] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-14 19:56:01,275] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-14 19:56:01,276] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-14 19:56:01,278] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 184 ms on 5c14c3e96bf4 (executor driver) (79/141)
[2021-05-14 19:56:01,279] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (5c14c3e96bf4, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,281] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (5c14c3e96bf4, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,282] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-14 19:56:01,283] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-14 19:56:01,284] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 192 ms on 5c14c3e96bf4 (executor driver) (80/141)
[2021-05-14 19:56:01,285] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-14 19:56:01,285] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (5c14c3e96bf4, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,286] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 199 ms on 5c14c3e96bf4 (executor driver) (81/141)
[2021-05-14 19:56:01,289] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-14 19:56:01,343] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-14 19:56:01,345] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (5c14c3e96bf4, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,347] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 193 ms on 5c14c3e96bf4 (executor driver) (82/141)
[2021-05-14 19:56:01,349] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-14 19:56:01,459] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-14 19:56:01,461] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (5c14c3e96bf4, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,463] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-14 19:56:01,464] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 185 ms on 5c14c3e96bf4 (executor driver) (83/141)
[2021-05-14 19:56:01,466] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-14 19:56:01,467] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-14 19:56:01,468] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (5c14c3e96bf4, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,468] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-14 19:56:01,469] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (5c14c3e96bf4, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,470] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-14 19:56:01,471] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 186 ms on 5c14c3e96bf4 (executor driver) (84/141)
[2021-05-14 19:56:01,472] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 191 ms on 5c14c3e96bf4 (executor driver) (85/141)
[2021-05-14 19:56:01,542] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1886 bytes result sent to driver
[2021-05-14 19:56:01,543] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (5c14c3e96bf4, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,544] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 200 ms on 5c14c3e96bf4 (executor driver) (86/141)
[2021-05-14 19:56:01,545] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-14 19:56:01,651] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1886 bytes result sent to driver
[2021-05-14 19:56:01,652] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1886 bytes result sent to driver
[2021-05-14 19:56:01,654] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (5c14c3e96bf4, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,654] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 188 ms on 5c14c3e96bf4 (executor driver) (87/141)
[2021-05-14 19:56:01,657] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-14 19:56:01,658] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1886 bytes result sent to driver
[2021-05-14 19:56:01,658] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (5c14c3e96bf4, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,659] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-14 19:56:01,660] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 200 ms on 5c14c3e96bf4 (executor driver) (88/141)
[2021-05-14 19:56:01,662] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (5c14c3e96bf4, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,663] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 194 ms on 5c14c3e96bf4 (executor driver) (89/141)
[2021-05-14 19:56:01,664] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-14 19:56:01,743] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-14 19:56:01,745] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (5c14c3e96bf4, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,747] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-14 19:56:01,748] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 204 ms on 5c14c3e96bf4 (executor driver) (90/141)
[2021-05-14 19:56:01,836] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1843 bytes result sent to driver
[2021-05-14 19:56:01,837] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (5c14c3e96bf4, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,839] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 186 ms on 5c14c3e96bf4 (executor driver) (91/141)
[2021-05-14 19:56:01,840] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-14 19:56:01,842] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1843 bytes result sent to driver
[2021-05-14 19:56:01,845] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (5c14c3e96bf4, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,846] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-14 19:56:01,846] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 184 ms on 5c14c3e96bf4 (executor driver) (92/141)
[2021-05-14 19:56:01,847] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1843 bytes result sent to driver
[2021-05-14 19:56:01,849] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (5c14c3e96bf4, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,850] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-14 19:56:01,851] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 194 ms on 5c14c3e96bf4 (executor driver) (93/141)
[2021-05-14 19:56:01,925] {docker.py:276} INFO - 21/05/14 22:56:01 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1843 bytes result sent to driver
[2021-05-14 19:56:01,926] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (5c14c3e96bf4, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:01,927] {docker.py:276} INFO - 21/05/14 22:56:01 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 183 ms on 5c14c3e96bf4 (executor driver) (94/141)
21/05/14 22:56:01 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-14 19:56:02,022] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
[2021-05-14 19:56:02,023] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (5c14c3e96bf4, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,023] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-14 19:56:02,024] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 175 ms on 5c14c3e96bf4 (executor driver) (95/141)
[2021-05-14 19:56:02,025] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-14 19:56:02,026] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (5c14c3e96bf4, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,027] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 183 ms on 5c14c3e96bf4 (executor driver) (96/141)
[2021-05-14 19:56:02,029] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-14 19:56:02,038] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1886 bytes result sent to driver
[2021-05-14 19:56:02,040] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (5c14c3e96bf4, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,041] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 204 ms on 5c14c3e96bf4 (executor driver) (97/141)
[2021-05-14 19:56:02,042] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-14 19:56:02,105] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1886 bytes result sent to driver
[2021-05-14 19:56:02,107] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (5c14c3e96bf4, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,108] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-14 19:56:02,108] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 182 ms on 5c14c3e96bf4 (executor driver) (98/141)
[2021-05-14 19:56:02,204] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1886 bytes result sent to driver
[2021-05-14 19:56:02,204] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (5c14c3e96bf4, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,205] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-14 19:56:02,206] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 184 ms on 5c14c3e96bf4 (executor driver) (99/141)
[2021-05-14 19:56:02,215] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
[2021-05-14 19:56:02,216] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (5c14c3e96bf4, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,217] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 178 ms on 5c14c3e96bf4 (executor driver) (100/141)
[2021-05-14 19:56:02,217] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1886 bytes result sent to driver
[2021-05-14 19:56:02,218] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-14 19:56:02,219] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (5c14c3e96bf4, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,220] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-14 19:56:02,220] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 194 ms on 5c14c3e96bf4 (executor driver) (101/141)
[2021-05-14 19:56:02,381] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1843 bytes result sent to driver
[2021-05-14 19:56:02,382] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (5c14c3e96bf4, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,384] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
21/05/14 22:56:02 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 179 ms on 5c14c3e96bf4 (executor driver) (102/141)
[2021-05-14 19:56:02,394] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1843 bytes result sent to driver
[2021-05-14 19:56:02,395] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-14 19:56:02,395] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (5c14c3e96bf4, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,396] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-14 19:56:02,397] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (5c14c3e96bf4, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,398] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 292 ms on 5c14c3e96bf4 (executor driver) (103/141)
[2021-05-14 19:56:02,398] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 182 ms on 5c14c3e96bf4 (executor driver) (104/141)
[2021-05-14 19:56:02,400] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1843 bytes result sent to driver
[2021-05-14 19:56:02,401] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-14 19:56:02,402] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (5c14c3e96bf4, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,403] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 185 ms on 5c14c3e96bf4 (executor driver) (105/141)
[2021-05-14 19:56:02,404] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-14 19:56:02,555] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
[2021-05-14 19:56:02,557] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (5c14c3e96bf4, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,558] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-14 19:56:02,559] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 177 ms on 5c14c3e96bf4 (executor driver) (106/141)
[2021-05-14 19:56:02,575] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-14 19:56:02,576] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (5c14c3e96bf4, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,578] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-14 19:56:02,579] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 177 ms on 5c14c3e96bf4 (executor driver) (107/141)
[2021-05-14 19:56:02,580] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-14 19:56:02,587] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (5c14c3e96bf4, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,588] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 194 ms on 5c14c3e96bf4 (executor driver) (108/141)
[2021-05-14 19:56:02,589] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-14 19:56:02,589] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1886 bytes result sent to driver
[2021-05-14 19:56:02,592] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (5c14c3e96bf4, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,593] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 197 ms on 5c14c3e96bf4 (executor driver) (109/141)
21/05/14 22:56:02 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-14 19:56:02,739] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1886 bytes result sent to driver
[2021-05-14 19:56:02,741] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (5c14c3e96bf4, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,742] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 186 ms on 5c14c3e96bf4 (executor driver) (110/141)
21/05/14 22:56:02 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-14 19:56:02,764] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1886 bytes result sent to driver
[2021-05-14 19:56:02,766] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (5c14c3e96bf4, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,767] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1843 bytes result sent to driver
[2021-05-14 19:56:02,769] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
21/05/14 22:56:02 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (5c14c3e96bf4, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,770] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-14 19:56:02,770] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 194 ms on 5c14c3e96bf4 (executor driver) (111/141)
[2021-05-14 19:56:02,771] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 191 ms on 5c14c3e96bf4 (executor driver) (112/141)
[2021-05-14 19:56:02,773] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1843 bytes result sent to driver
[2021-05-14 19:56:02,774] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (5c14c3e96bf4, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,776] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-14 19:56:02,776] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 184 ms on 5c14c3e96bf4 (executor driver) (113/141)
[2021-05-14 19:56:02,930] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-14 19:56:02,932] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (5c14c3e96bf4, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,933] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-14 19:56:02,934] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 193 ms on 5c14c3e96bf4 (executor driver) (114/141)
[2021-05-14 19:56:02,946] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-14 19:56:02,948] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (5c14c3e96bf4, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,949] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-14 19:56:02,950] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 182 ms on 5c14c3e96bf4 (executor driver) (115/141)
[2021-05-14 19:56:02,953] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-14 19:56:02,954] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (5c14c3e96bf4, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,955] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-14 19:56:02,956] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 182 ms on 5c14c3e96bf4 (executor driver) (116/141)
[2021-05-14 19:56:02,957] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-14 19:56:02,958] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (5c14c3e96bf4, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:02,959] {docker.py:276} INFO - 21/05/14 22:56:02 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 193 ms on 5c14c3e96bf4 (executor driver) (117/141)
[2021-05-14 19:56:02,960] {docker.py:276} INFO - 21/05/14 22:56:02 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-14 19:56:03,110] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1843 bytes result sent to driver
[2021-05-14 19:56:03,113] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (5c14c3e96bf4, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,113] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
21/05/14 22:56:03 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 182 ms on 5c14c3e96bf4 (executor driver) (118/141)
[2021-05-14 19:56:03,134] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1843 bytes result sent to driver
[2021-05-14 19:56:03,141] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (5c14c3e96bf4, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,142] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
21/05/14 22:56:03 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 185 ms on 5c14c3e96bf4 (executor driver) (119/141)
[2021-05-14 19:56:03,143] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1886 bytes result sent to driver
[2021-05-14 19:56:03,144] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (5c14c3e96bf4, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,145] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 192 ms on 5c14c3e96bf4 (executor driver) (120/141)
[2021-05-14 19:56:03,146] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-14 19:56:03,168] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1886 bytes result sent to driver
[2021-05-14 19:56:03,170] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (5c14c3e96bf4, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,170] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 223 ms on 5c14c3e96bf4 (executor driver) (121/141)
[2021-05-14 19:56:03,171] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-14 19:56:03,288] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1886 bytes result sent to driver
[2021-05-14 19:56:03,289] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (5c14c3e96bf4, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,291] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 181 ms on 5c14c3e96bf4 (executor driver) (122/141)
[2021-05-14 19:56:03,292] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-14 19:56:03,319] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-14 19:56:03,320] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (5c14c3e96bf4, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,321] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 177 ms on 5c14c3e96bf4 (executor driver) (123/141)
[2021-05-14 19:56:03,322] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-14 19:56:03,328] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1843 bytes result sent to driver
[2021-05-14 19:56:03,330] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (5c14c3e96bf4, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,331] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
21/05/14 22:56:03 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 190 ms on 5c14c3e96bf4 (executor driver) (124/141)
[2021-05-14 19:56:03,350] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-14 19:56:03,351] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 182 ms on 5c14c3e96bf4 (executor driver) (125/141)
[2021-05-14 19:56:03,351] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (5c14c3e96bf4, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,352] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-14 19:56:03,467] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1843 bytes result sent to driver
[2021-05-14 19:56:03,468] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (5c14c3e96bf4, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,469] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 180 ms on 5c14c3e96bf4 (executor driver) (126/141)
[2021-05-14 19:56:03,470] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-14 19:56:03,507] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1843 bytes result sent to driver
[2021-05-14 19:56:03,509] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (5c14c3e96bf4, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,511] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-14 19:56:03,512] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 183 ms on 5c14c3e96bf4 (executor driver) (127/141)
[2021-05-14 19:56:03,513] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-14 19:56:03,514] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (5c14c3e96bf4, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,515] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 195 ms on 5c14c3e96bf4 (executor driver) (128/141)
[2021-05-14 19:56:03,516] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-14 19:56:03,531] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1843 bytes result sent to driver
[2021-05-14 19:56:03,532] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (5c14c3e96bf4, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,533] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 182 ms on 5c14c3e96bf4 (executor driver) (129/141)
[2021-05-14 19:56:03,534] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-14 19:56:03,661] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1843 bytes result sent to driver
[2021-05-14 19:56:03,664] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (5c14c3e96bf4, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,665] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-14 19:56:03,665] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 197 ms on 5c14c3e96bf4 (executor driver) (130/141)
[2021-05-14 19:56:03,687] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1886 bytes result sent to driver
[2021-05-14 19:56:03,688] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (5c14c3e96bf4, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,689] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 175 ms on 5c14c3e96bf4 (executor driver) (131/141)
[2021-05-14 19:56:03,690] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-14 19:56:03,694] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1886 bytes result sent to driver
[2021-05-14 19:56:03,695] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (5c14c3e96bf4, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,696] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 188 ms on 5c14c3e96bf4 (executor driver) (132/141)
[2021-05-14 19:56:03,697] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-14 19:56:03,710] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1886 bytes result sent to driver
[2021-05-14 19:56:03,711] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (5c14c3e96bf4, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,712] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-14 19:56:03,712] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 181 ms on 5c14c3e96bf4 (executor driver) (133/141)
[2021-05-14 19:56:03,845] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1886 bytes result sent to driver
[2021-05-14 19:56:03,846] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (5c14c3e96bf4, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,849] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 186 ms on 5c14c3e96bf4 (executor driver) (134/141)
21/05/14 22:56:03 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-14 19:56:03,864] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1843 bytes result sent to driver
[2021-05-14 19:56:03,865] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (5c14c3e96bf4, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,865] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-14 19:56:03,866] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 171 ms on 5c14c3e96bf4 (executor driver) (135/141)
[2021-05-14 19:56:03,869] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
[2021-05-14 19:56:03,870] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (5c14c3e96bf4, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,871] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 182 ms on 5c14c3e96bf4 (executor driver) (136/141)
[2021-05-14 19:56:03,872] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-14 19:56:03,896] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-14 19:56:03,897] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (5c14c3e96bf4, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:03,898] {docker.py:276} INFO - 21/05/14 22:56:03 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-14 19:56:03,899] {docker.py:276} INFO - 21/05/14 22:56:03 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 188 ms on 5c14c3e96bf4 (executor driver) (137/141)
[2021-05-14 19:56:04,024] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1843 bytes result sent to driver
[2021-05-14 19:56:04,033] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 187 ms on 5c14c3e96bf4 (executor driver) (138/141)
[2021-05-14 19:56:04,044] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1843 bytes result sent to driver
[2021-05-14 19:56:04,045] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 175 ms on 5c14c3e96bf4 (executor driver) (139/141)
[2021-05-14 19:56:04,047] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1843 bytes result sent to driver
[2021-05-14 19:56:04,048] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 183 ms on 5c14c3e96bf4 (executor driver) (140/141)
[2021-05-14 19:56:04,082] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1843 bytes result sent to driver
[2021-05-14 19:56:04,084] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 187 ms on 5c14c3e96bf4 (executor driver) (141/141)
[2021-05-14 19:56:04,088] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 19:56:04,089] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.713 s
[2021-05-14 19:56:04,097] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 19:56:04,098] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 19:56:04,101] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.798171 s
[2021-05-14 19:56:04,141] {docker.py:276} INFO - 21/05/14 22:56:04 INFO InMemoryFileIndex: It took 8375 ms to list leaf files for 141 paths.
[2021-05-14 19:56:04,257] {docker.py:276} INFO - 21/05/14 22:56:04 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620946501_to_1620948301.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620948301_to_1620950101.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620950101_to_1620951901.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620951901_to_1620953701.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620953701_to_1620955501.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620955501_to_1620957301.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620957301_to_1620959101.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620959101_to_1620960901.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620960901_to_1620962701.csv, s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620962701_to_1620964501.csv.
[2021-05-14 19:56:04,301] {docker.py:276} INFO - 21/05/14 22:56:04 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:04,304] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/14 22:56:04 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:56:04 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:56:04,305] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:56:04,306] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:56:04,331] {docker.py:276} INFO - 21/05/14 22:56:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-14 19:56:04,342] {docker.py:276} INFO - 21/05/14 22:56:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-14 19:56:04,343] {docker.py:276} INFO - 21/05/14 22:56:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5c14c3e96bf4:33219 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:56:04,346] {docker.py:276} INFO - 21/05/14 22:56:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5c14c3e96bf4:33219 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:56:04,347] {docker.py:276} INFO - 21/05/14 22:56:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:56:04,351] {docker.py:276} INFO - 21/05/14 22:56:04 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 19:56:04,352] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-14 19:56:04,358] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (5c14c3e96bf4, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,358] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (5c14c3e96bf4, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,360] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (5c14c3e96bf4, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,360] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (5c14c3e96bf4, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,362] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-14 19:56:04,363] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-14 19:56:04,363] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-14 19:56:04,364] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-14 19:56:04,541] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-14 19:56:04,541] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-14 19:56:04,542] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (5c14c3e96bf4, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,543] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 185 ms on 5c14c3e96bf4 (executor driver) (1/141)
[2021-05-14 19:56:04,545] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (5c14c3e96bf4, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,546] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-14 19:56:04,547] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 189 ms on 5c14c3e96bf4 (executor driver) (2/141)
[2021-05-14 19:56:04,548] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-14 19:56:04,549] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-14 19:56:04,550] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (5c14c3e96bf4, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,551] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-14 19:56:04,551] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-14 19:56:04,552] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (5c14c3e96bf4, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,555] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 195 ms on 5c14c3e96bf4 (executor driver) (3/141)
[2021-05-14 19:56:04,555] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-14 19:56:04,555] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 196 ms on 5c14c3e96bf4 (executor driver) (4/141)
[2021-05-14 19:56:04,723] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-14 19:56:04,725] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (5c14c3e96bf4, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,725] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 182 ms on 5c14c3e96bf4 (executor driver) (5/141)
[2021-05-14 19:56:04,726] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-14 19:56:04,740] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1886 bytes result sent to driver
[2021-05-14 19:56:04,741] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (5c14c3e96bf4, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,741] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-14 19:56:04,742] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 194 ms on 5c14c3e96bf4 (executor driver) (6/141)
[2021-05-14 19:56:04,744] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1886 bytes result sent to driver
[2021-05-14 19:56:04,746] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (5c14c3e96bf4, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,746] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1886 bytes result sent to driver
[2021-05-14 19:56:04,747] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 195 ms on 5c14c3e96bf4 (executor driver) (7/141)
[2021-05-14 19:56:04,747] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-14 19:56:04,748] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (5c14c3e96bf4, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,749] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 208 ms on 5c14c3e96bf4 (executor driver) (8/141)
[2021-05-14 19:56:04,750] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-14 19:56:04,909] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1886 bytes result sent to driver
[2021-05-14 19:56:04,910] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (5c14c3e96bf4, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,911] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
21/05/14 22:56:04 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 187 ms on 5c14c3e96bf4 (executor driver) (9/141)
[2021-05-14 19:56:04,917] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
[2021-05-14 19:56:04,918] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 177 ms on 5c14c3e96bf4 (executor driver) (10/141)
[2021-05-14 19:56:04,918] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (5c14c3e96bf4, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,920] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-14 19:56:04,923] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-14 19:56:04,924] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (5c14c3e96bf4, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,925] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-14 19:56:04,926] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 177 ms on 5c14c3e96bf4 (executor driver) (11/141)
[2021-05-14 19:56:04,931] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-14 19:56:04,932] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (5c14c3e96bf4, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:04,933] {docker.py:276} INFO - 21/05/14 22:56:04 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 188 ms on 5c14c3e96bf4 (executor driver) (12/141)
[2021-05-14 19:56:04,933] {docker.py:276} INFO - 21/05/14 22:56:04 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-14 19:56:05,082] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1843 bytes result sent to driver
[2021-05-14 19:56:05,084] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (5c14c3e96bf4, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,085] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
21/05/14 22:56:05 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 176 ms on 5c14c3e96bf4 (executor driver) (13/141)
[2021-05-14 19:56:05,099] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1843 bytes result sent to driver
[2021-05-14 19:56:05,101] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (5c14c3e96bf4, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,102] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-14 19:56:05,103] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 184 ms on 5c14c3e96bf4 (executor driver) (14/141)
[2021-05-14 19:56:05,104] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1843 bytes result sent to driver
[2021-05-14 19:56:05,107] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (5c14c3e96bf4, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,107] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 174 ms on 5c14c3e96bf4 (executor driver) (15/141)
[2021-05-14 19:56:05,108] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1843 bytes result sent to driver
[2021-05-14 19:56:05,109] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-14 19:56:05,109] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (5c14c3e96bf4, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,121] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 193 ms on 5c14c3e96bf4 (executor driver) (16/141)
[2021-05-14 19:56:05,121] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-14 19:56:05,260] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1886 bytes result sent to driver
[2021-05-14 19:56:05,263] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (5c14c3e96bf4, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:56:05 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 180 ms on 5c14c3e96bf4 (executor driver) (17/141)
21/05/14 22:56:05 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-14 19:56:05,287] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1886 bytes result sent to driver
[2021-05-14 19:56:05,289] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (5c14c3e96bf4, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,290] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-14 19:56:05,291] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 187 ms on 5c14c3e96bf4 (executor driver) (18/141)
[2021-05-14 19:56:05,294] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1886 bytes result sent to driver
[2021-05-14 19:56:05,295] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (5c14c3e96bf4, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,297] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 197 ms on 5c14c3e96bf4 (executor driver) (19/141)
21/05/14 22:56:05 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-14 19:56:05,299] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-14 19:56:05,301] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (5c14c3e96bf4, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,302] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 194 ms on 5c14c3e96bf4 (executor driver) (20/141)
[2021-05-14 19:56:05,303] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-14 19:56:05,435] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-14 19:56:05,437] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (5c14c3e96bf4, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,439] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
21/05/14 22:56:05 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 176 ms on 5c14c3e96bf4 (executor driver) (21/141)
[2021-05-14 19:56:05,469] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-14 19:56:05,470] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (5c14c3e96bf4, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,472] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 184 ms on 5c14c3e96bf4 (executor driver) (22/141)
[2021-05-14 19:56:05,473] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-14 19:56:05,474] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-14 19:56:05,476] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (5c14c3e96bf4, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,477] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 176 ms on 5c14c3e96bf4 (executor driver) (23/141)
[2021-05-14 19:56:05,479] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-14 19:56:05,485] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-14 19:56:05,491] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (5c14c3e96bf4, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,492] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 197 ms on 5c14c3e96bf4 (executor driver) (24/141)
[2021-05-14 19:56:05,492] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-14 19:56:05,614] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1886 bytes result sent to driver
[2021-05-14 19:56:05,616] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (5c14c3e96bf4, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,617] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 181 ms on 5c14c3e96bf4 (executor driver) (25/141)
[2021-05-14 19:56:05,619] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-14 19:56:05,650] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1886 bytes result sent to driver
[2021-05-14 19:56:05,653] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (5c14c3e96bf4, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,654] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 184 ms on 5c14c3e96bf4 (executor driver) (26/141)
21/05/14 22:56:05 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-14 19:56:05,660] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1886 bytes result sent to driver
[2021-05-14 19:56:05,661] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (5c14c3e96bf4, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,663] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 187 ms on 5c14c3e96bf4 (executor driver) (27/141)
[2021-05-14 19:56:05,664] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-14 19:56:05,665] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1843 bytes result sent to driver
[2021-05-14 19:56:05,666] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (5c14c3e96bf4, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,668] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 177 ms on 5c14c3e96bf4 (executor driver) (28/141)
[2021-05-14 19:56:05,668] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-14 19:56:05,792] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-14 19:56:05,794] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (5c14c3e96bf4, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,796] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 180 ms on 5c14c3e96bf4 (executor driver) (29/141)
21/05/14 22:56:05 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-14 19:56:05,837] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-14 19:56:05,838] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
[2021-05-14 19:56:05,839] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-14 19:56:05,840] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 187 ms on 5c14c3e96bf4 (executor driver) (30/141)
[2021-05-14 19:56:05,840] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (5c14c3e96bf4, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,842] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-14 19:56:05,843] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (5c14c3e96bf4, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,844] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
21/05/14 22:56:05 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 182 ms on 5c14c3e96bf4 (executor driver) (31/141)
[2021-05-14 19:56:05,845] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (5c14c3e96bf4, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,846] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-14 19:56:05,847] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 181 ms on 5c14c3e96bf4 (executor driver) (32/141)
[2021-05-14 19:56:05,966] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-14 19:56:05,968] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (5c14c3e96bf4, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:05,969] {docker.py:276} INFO - 21/05/14 22:56:05 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 175 ms on 5c14c3e96bf4 (executor driver) (33/141)
[2021-05-14 19:56:05,969] {docker.py:276} INFO - 21/05/14 22:56:05 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-14 19:56:06,014] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1886 bytes result sent to driver
[2021-05-14 19:56:06,015] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (5c14c3e96bf4, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,016] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 175 ms on 5c14c3e96bf4 (executor driver) (34/141)
[2021-05-14 19:56:06,016] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-14 19:56:06,035] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1886 bytes result sent to driver
[2021-05-14 19:56:06,037] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1886 bytes result sent to driver
[2021-05-14 19:56:06,039] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (5c14c3e96bf4, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,040] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-14 19:56:06,040] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (5c14c3e96bf4, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,041] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 197 ms on 5c14c3e96bf4 (executor driver) (35/141)
[2021-05-14 19:56:06,042] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 201 ms on 5c14c3e96bf4 (executor driver) (36/141)
[2021-05-14 19:56:06,042] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-14 19:56:06,148] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1886 bytes result sent to driver
[2021-05-14 19:56:06,150] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (5c14c3e96bf4, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,151] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 184 ms on 5c14c3e96bf4 (executor driver) (37/141)
[2021-05-14 19:56:06,152] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-14 19:56:06,217] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1843 bytes result sent to driver
[2021-05-14 19:56:06,218] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (5c14c3e96bf4, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,219] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 179 ms on 5c14c3e96bf4 (executor driver) (38/141)
[2021-05-14 19:56:06,221] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-14 19:56:06,221] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1843 bytes result sent to driver
[2021-05-14 19:56:06,222] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (5c14c3e96bf4, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,223] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 208 ms on 5c14c3e96bf4 (executor driver) (39/141)
[2021-05-14 19:56:06,224] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-14 19:56:06,226] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
[2021-05-14 19:56:06,227] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (5c14c3e96bf4, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,228] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 192 ms on 5c14c3e96bf4 (executor driver) (40/141)
[2021-05-14 19:56:06,228] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-14 19:56:06,391] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1843 bytes result sent to driver
[2021-05-14 19:56:06,392] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (5c14c3e96bf4, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,394] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 245 ms on 5c14c3e96bf4 (executor driver) (41/141)
[2021-05-14 19:56:06,394] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-14 19:56:06,404] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-14 19:56:06,405] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-14 19:56:06,406] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1843 bytes result sent to driver
[2021-05-14 19:56:06,406] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (5c14c3e96bf4, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,407] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-14 19:56:06,408] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (5c14c3e96bf4, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,409] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-14 19:56:06,410] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (5c14c3e96bf4, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,410] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 184 ms on 5c14c3e96bf4 (executor driver) (42/141)
[2021-05-14 19:56:06,417] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-14 19:56:06,418] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 200 ms on 5c14c3e96bf4 (executor driver) (43/141)
21/05/14 22:56:06 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 196 ms on 5c14c3e96bf4 (executor driver) (44/141)
[2021-05-14 19:56:06,570] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1886 bytes result sent to driver
[2021-05-14 19:56:06,571] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (5c14c3e96bf4, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,572] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 180 ms on 5c14c3e96bf4 (executor driver) (45/141)
[2021-05-14 19:56:06,572] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-14 19:56:06,584] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1843 bytes result sent to driver
[2021-05-14 19:56:06,585] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (5c14c3e96bf4, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,586] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 177 ms on 5c14c3e96bf4 (executor driver) (46/141)
[2021-05-14 19:56:06,587] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-14 19:56:06,590] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1886 bytes result sent to driver
[2021-05-14 19:56:06,591] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (5c14c3e96bf4, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,592] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 186 ms on 5c14c3e96bf4 (executor driver) (47/141)
[2021-05-14 19:56:06,592] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-14 19:56:06,596] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1886 bytes result sent to driver
[2021-05-14 19:56:06,597] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (5c14c3e96bf4, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,598] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 190 ms on 5c14c3e96bf4 (executor driver) (48/141)
[2021-05-14 19:56:06,599] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-14 19:56:06,745] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-14 19:56:06,747] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (5c14c3e96bf4, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,749] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 177 ms on 5c14c3e96bf4 (executor driver) (49/141)
[2021-05-14 19:56:06,750] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-14 19:56:06,765] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-14 19:56:06,767] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
21/05/14 22:56:06 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (5c14c3e96bf4, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,768] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-14 19:56:06,769] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (5c14c3e96bf4, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,770] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-14 19:56:06,771] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 186 ms on 5c14c3e96bf4 (executor driver) (50/141)
[2021-05-14 19:56:06,771] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-14 19:56:06,772] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (5c14c3e96bf4, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,773] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 176 ms on 5c14c3e96bf4 (executor driver) (51/141)
[2021-05-14 19:56:06,774] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 183 ms on 5c14c3e96bf4 (executor driver) (52/141)
21/05/14 22:56:06 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-14 19:56:06,920] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1886 bytes result sent to driver
[2021-05-14 19:56:06,922] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (5c14c3e96bf4, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,923] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 176 ms on 5c14c3e96bf4 (executor driver) (53/141)
[2021-05-14 19:56:06,924] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-14 19:56:06,937] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1886 bytes result sent to driver
[2021-05-14 19:56:06,938] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (5c14c3e96bf4, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,939] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-14 19:56:06,939] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 173 ms on 5c14c3e96bf4 (executor driver) (54/141)
[2021-05-14 19:56:06,965] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1886 bytes result sent to driver
[2021-05-14 19:56:06,966] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1886 bytes result sent to driver
[2021-05-14 19:56:06,967] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (5c14c3e96bf4, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,968] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 198 ms on 5c14c3e96bf4 (executor driver) (55/141)
[2021-05-14 19:56:06,969] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-14 19:56:06,970] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (5c14c3e96bf4, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:06,971] {docker.py:276} INFO - 21/05/14 22:56:06 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 199 ms on 5c14c3e96bf4 (executor driver) (56/141)
[2021-05-14 19:56:06,973] {docker.py:276} INFO - 21/05/14 22:56:06 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-14 19:56:07,096] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1843 bytes result sent to driver
[2021-05-14 19:56:07,098] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (5c14c3e96bf4, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,099] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 177 ms on 5c14c3e96bf4 (executor driver) (57/141)
[2021-05-14 19:56:07,100] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-14 19:56:07,109] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1843 bytes result sent to driver
[2021-05-14 19:56:07,110] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (5c14c3e96bf4, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,111] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 173 ms on 5c14c3e96bf4 (executor driver) (58/141)
[2021-05-14 19:56:07,112] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-14 19:56:07,154] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
21/05/14 22:56:07 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1843 bytes result sent to driver
[2021-05-14 19:56:07,155] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (5c14c3e96bf4, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,156] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-14 19:56:07,158] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (5c14c3e96bf4, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,159] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 188 ms on 5c14c3e96bf4 (executor driver) (59/141)
[2021-05-14 19:56:07,159] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-14 19:56:07,160] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 192 ms on 5c14c3e96bf4 (executor driver) (60/141)
[2021-05-14 19:56:07,268] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-14 19:56:07,269] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (5c14c3e96bf4, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:56:07 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 173 ms on 5c14c3e96bf4 (executor driver) (61/141)
[2021-05-14 19:56:07,270] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-14 19:56:07,279] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1843 bytes result sent to driver
[2021-05-14 19:56:07,279] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 170 ms on 5c14c3e96bf4 (executor driver) (62/141)
[2021-05-14 19:56:07,280] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (5c14c3e96bf4, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,281] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-14 19:56:07,336] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1886 bytes result sent to driver
[2021-05-14 19:56:07,337] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (5c14c3e96bf4, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,337] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 182 ms on 5c14c3e96bf4 (executor driver) (63/141)
[2021-05-14 19:56:07,338] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-14 19:56:07,341] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1886 bytes result sent to driver
[2021-05-14 19:56:07,341] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (5c14c3e96bf4, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,343] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
21/05/14 22:56:07 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 189 ms on 5c14c3e96bf4 (executor driver) (64/141)
[2021-05-14 19:56:07,437] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1886 bytes result sent to driver
[2021-05-14 19:56:07,438] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (5c14c3e96bf4, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,438] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 170 ms on 5c14c3e96bf4 (executor driver) (65/141)
[2021-05-14 19:56:07,439] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-14 19:56:07,452] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1886 bytes result sent to driver
[2021-05-14 19:56:07,453] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (5c14c3e96bf4, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,454] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-14 19:56:07,455] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 174 ms on 5c14c3e96bf4 (executor driver) (66/141)
[2021-05-14 19:56:07,516] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
[2021-05-14 19:56:07,517] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (5c14c3e96bf4, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,518] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 182 ms on 5c14c3e96bf4 (executor driver) (67/141)
[2021-05-14 19:56:07,518] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-14 19:56:07,519] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (5c14c3e96bf4, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,520] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 179 ms on 5c14c3e96bf4 (executor driver) (68/141)
[2021-05-14 19:56:07,521] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-14 19:56:07,527] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-14 19:56:07,609] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-14 19:56:07,610] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (5c14c3e96bf4, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,610] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 173 ms on 5c14c3e96bf4 (executor driver) (69/141)
[2021-05-14 19:56:07,610] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-14 19:56:07,622] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-14 19:56:07,623] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (5c14c3e96bf4, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,623] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-14 19:56:07,624] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 171 ms on 5c14c3e96bf4 (executor driver) (70/141)
[2021-05-14 19:56:07,701] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-14 19:56:07,703] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (5c14c3e96bf4, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,704] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
21/05/14 22:56:07 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 184 ms on 5c14c3e96bf4 (executor driver) (71/141)
[2021-05-14 19:56:07,704] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-14 19:56:07,706] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (5c14c3e96bf4, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,707] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 189 ms on 5c14c3e96bf4 (executor driver) (72/141)
[2021-05-14 19:56:07,708] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-14 19:56:07,785] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1886 bytes result sent to driver
[2021-05-14 19:56:07,786] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (5c14c3e96bf4, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,787] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-14 19:56:07,787] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 178 ms on 5c14c3e96bf4 (executor driver) (73/141)
[2021-05-14 19:56:07,791] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1886 bytes result sent to driver
[2021-05-14 19:56:07,791] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (5c14c3e96bf4, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,792] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
21/05/14 22:56:07 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 170 ms on 5c14c3e96bf4 (executor driver) (74/141)
[2021-05-14 19:56:07,905] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1886 bytes result sent to driver
21/05/14 22:56:07 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1886 bytes result sent to driver
[2021-05-14 19:56:07,907] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (5c14c3e96bf4, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,908] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-14 19:56:07,909] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 206 ms on 5c14c3e96bf4 (executor driver) (75/141)
[2021-05-14 19:56:07,910] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (5c14c3e96bf4, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,910] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 206 ms on 5c14c3e96bf4 (executor driver) (76/141)
[2021-05-14 19:56:07,911] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
[2021-05-14 19:56:07,964] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1843 bytes result sent to driver
[2021-05-14 19:56:07,964] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1843 bytes result sent to driver
[2021-05-14 19:56:07,965] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (5c14c3e96bf4, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,967] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (5c14c3e96bf4, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:07,969] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-14 19:56:07,969] {docker.py:276} INFO - 21/05/14 22:56:07 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-14 19:56:07,970] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 181 ms on 5c14c3e96bf4 (executor driver) (77/141)
[2021-05-14 19:56:07,970] {docker.py:276} INFO - 21/05/14 22:56:07 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 177 ms on 5c14c3e96bf4 (executor driver) (78/141)
[2021-05-14 19:56:08,091] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
[2021-05-14 19:56:08,091] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1843 bytes result sent to driver
[2021-05-14 19:56:08,093] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (5c14c3e96bf4, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,094] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 188 ms on 5c14c3e96bf4 (executor driver) (79/141)
21/05/14 22:56:08 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-14 19:56:08,096] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (5c14c3e96bf4, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,097] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 188 ms on 5c14c3e96bf4 (executor driver) (80/141)
[2021-05-14 19:56:08,098] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-14 19:56:08,136] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1843 bytes result sent to driver
[2021-05-14 19:56:08,138] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (5c14c3e96bf4, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,138] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 173 ms on 5c14c3e96bf4 (executor driver) (81/141)
[2021-05-14 19:56:08,139] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-14 19:56:08,140] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-14 19:56:08,141] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (5c14c3e96bf4, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,142] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 178 ms on 5c14c3e96bf4 (executor driver) (82/141)
[2021-05-14 19:56:08,142] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-14 19:56:08,274] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1886 bytes result sent to driver
[2021-05-14 19:56:08,275] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1886 bytes result sent to driver
[2021-05-14 19:56:08,276] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (5c14c3e96bf4, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,278] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-14 19:56:08,279] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (5c14c3e96bf4, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,280] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 184 ms on 5c14c3e96bf4 (executor driver) (83/141)
[2021-05-14 19:56:08,281] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 188 ms on 5c14c3e96bf4 (executor driver) (84/141)
[2021-05-14 19:56:08,282] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-14 19:56:08,315] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1886 bytes result sent to driver
21/05/14 22:56:08 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1886 bytes result sent to driver
[2021-05-14 19:56:08,317] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (5c14c3e96bf4, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,319] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (5c14c3e96bf4, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,320] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
21/05/14 22:56:08 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 182 ms on 5c14c3e96bf4 (executor driver) (85/141)
[2021-05-14 19:56:08,321] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 180 ms on 5c14c3e96bf4 (executor driver) (86/141)
[2021-05-14 19:56:08,322] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-14 19:56:08,456] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-14 19:56:08,459] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (5c14c3e96bf4, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,460] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 184 ms on 5c14c3e96bf4 (executor driver) (87/141)
[2021-05-14 19:56:08,460] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-14 19:56:08,493] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1843 bytes result sent to driver
[2021-05-14 19:56:08,494] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-14 19:56:08,496] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (5c14c3e96bf4, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,497] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-14 19:56:08,498] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (5c14c3e96bf4, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,499] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 180 ms on 5c14c3e96bf4 (executor driver) (88/141)
[2021-05-14 19:56:08,500] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-14 19:56:08,501] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 184 ms on 5c14c3e96bf4 (executor driver) (89/141)
[2021-05-14 19:56:08,529] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-14 19:56:08,531] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (5c14c3e96bf4, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,532] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 254 ms on 5c14c3e96bf4 (executor driver) (90/141)
[2021-05-14 19:56:08,533] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-14 19:56:08,637] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1843 bytes result sent to driver
[2021-05-14 19:56:08,639] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (5c14c3e96bf4, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,641] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 183 ms on 5c14c3e96bf4 (executor driver) (91/141)
[2021-05-14 19:56:08,642] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-14 19:56:08,670] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1843 bytes result sent to driver
[2021-05-14 19:56:08,672] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (5c14c3e96bf4, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,673] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 176 ms on 5c14c3e96bf4 (executor driver) (92/141)
[2021-05-14 19:56:08,674] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-14 19:56:08,676] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1843 bytes result sent to driver
[2021-05-14 19:56:08,677] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (5c14c3e96bf4, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,678] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 183 ms on 5c14c3e96bf4 (executor driver) (93/141)
[2021-05-14 19:56:08,679] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-14 19:56:08,716] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1886 bytes result sent to driver
[2021-05-14 19:56:08,716] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (5c14c3e96bf4, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,717] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
21/05/14 22:56:08 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 187 ms on 5c14c3e96bf4 (executor driver) (94/141)
[2021-05-14 19:56:08,822] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1886 bytes result sent to driver
[2021-05-14 19:56:08,824] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (5c14c3e96bf4, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,826] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 188 ms on 5c14c3e96bf4 (executor driver) (95/141)
[2021-05-14 19:56:08,827] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-14 19:56:08,854] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1886 bytes result sent to driver
[2021-05-14 19:56:08,855] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (5c14c3e96bf4, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,856] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 179 ms on 5c14c3e96bf4 (executor driver) (96/141)
[2021-05-14 19:56:08,857] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1886 bytes result sent to driver
[2021-05-14 19:56:08,858] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-14 19:56:08,858] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (5c14c3e96bf4, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,859] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 187 ms on 5c14c3e96bf4 (executor driver) (97/141)
[2021-05-14 19:56:08,859] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-14 19:56:08,906] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-14 19:56:08,908] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (5c14c3e96bf4, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:08,909] {docker.py:276} INFO - 21/05/14 22:56:08 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 193 ms on 5c14c3e96bf4 (executor driver) (98/141)
[2021-05-14 19:56:08,910] {docker.py:276} INFO - 21/05/14 22:56:08 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
[2021-05-14 19:56:09,015] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1843 bytes result sent to driver
[2021-05-14 19:56:09,016] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (5c14c3e96bf4, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,017] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-14 19:56:09,018] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 194 ms on 5c14c3e96bf4 (executor driver) (99/141)
[2021-05-14 19:56:09,025] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-14 19:56:09,026] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-14 19:56:09,027] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (5c14c3e96bf4, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,028] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (5c14c3e96bf4, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,029] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 174 ms on 5c14c3e96bf4 (executor driver) (100/141)
[2021-05-14 19:56:09,030] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-14 19:56:09,031] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 173 ms on 5c14c3e96bf4 (executor driver) (101/141)
[2021-05-14 19:56:09,031] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-14 19:56:09,090] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-14 19:56:09,092] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (5c14c3e96bf4, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,094] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 185 ms on 5c14c3e96bf4 (executor driver) (102/141)
[2021-05-14 19:56:09,094] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-14 19:56:09,192] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-14 19:56:09,193] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (5c14c3e96bf4, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,194] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-14 19:56:09,194] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 178 ms on 5c14c3e96bf4 (executor driver) (103/141)
[2021-05-14 19:56:09,204] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1886 bytes result sent to driver
[2021-05-14 19:56:09,205] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1886 bytes result sent to driver
[2021-05-14 19:56:09,206] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (5c14c3e96bf4, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,206] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-14 19:56:09,207] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (5c14c3e96bf4, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,207] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 179 ms on 5c14c3e96bf4 (executor driver) (104/141)
[2021-05-14 19:56:09,208] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 181 ms on 5c14c3e96bf4 (executor driver) (105/141)
[2021-05-14 19:56:09,209] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-14 19:56:09,269] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1886 bytes result sent to driver
[2021-05-14 19:56:09,270] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (5c14c3e96bf4, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,271] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 180 ms on 5c14c3e96bf4 (executor driver) (106/141)
[2021-05-14 19:56:09,272] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-14 19:56:09,368] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1886 bytes result sent to driver
[2021-05-14 19:56:09,369] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (5c14c3e96bf4, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,370] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-14 19:56:09,370] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 178 ms on 5c14c3e96bf4 (executor driver) (107/141)
[2021-05-14 19:56:09,383] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1843 bytes result sent to driver
[2021-05-14 19:56:09,384] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (5c14c3e96bf4, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,385] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 179 ms on 5c14c3e96bf4 (executor driver) (108/141)
[2021-05-14 19:56:09,386] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-14 19:56:09,387] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1843 bytes result sent to driver
[2021-05-14 19:56:09,388] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (5c14c3e96bf4, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,389] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 184 ms on 5c14c3e96bf4 (executor driver) (109/141)
21/05/14 22:56:09 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-14 19:56:09,451] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1843 bytes result sent to driver
[2021-05-14 19:56:09,453] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (5c14c3e96bf4, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,454] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 184 ms on 5c14c3e96bf4 (executor driver) (110/141)
[2021-05-14 19:56:09,456] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-14 19:56:09,541] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1843 bytes result sent to driver
[2021-05-14 19:56:09,543] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (5c14c3e96bf4, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,544] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 175 ms on 5c14c3e96bf4 (executor driver) (111/141)
21/05/14 22:56:09 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-14 19:56:09,555] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-14 19:56:09,556] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (5c14c3e96bf4, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,557] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 169 ms on 5c14c3e96bf4 (executor driver) (112/141)
[2021-05-14 19:56:09,558] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-14 19:56:09,559] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1843 bytes result sent to driver
[2021-05-14 19:56:09,560] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (5c14c3e96bf4, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,561] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 177 ms on 5c14c3e96bf4 (executor driver) (113/141)
[2021-05-14 19:56:09,562] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-14 19:56:09,633] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1886 bytes result sent to driver
[2021-05-14 19:56:09,633] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (5c14c3e96bf4, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,634] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
21/05/14 22:56:09 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 181 ms on 5c14c3e96bf4 (executor driver) (114/141)
[2021-05-14 19:56:09,720] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1886 bytes result sent to driver
[2021-05-14 19:56:09,721] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (5c14c3e96bf4, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,722] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 180 ms on 5c14c3e96bf4 (executor driver) (115/141)
[2021-05-14 19:56:09,724] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-14 19:56:09,747] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1886 bytes result sent to driver
[2021-05-14 19:56:09,749] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (5c14c3e96bf4, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,750] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 195 ms on 5c14c3e96bf4 (executor driver) (116/141)
[2021-05-14 19:56:09,751] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-14 19:56:09,753] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1929 bytes result sent to driver
[2021-05-14 19:56:09,754] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (5c14c3e96bf4, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,755] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 194 ms on 5c14c3e96bf4 (executor driver) (117/141)
[2021-05-14 19:56:09,755] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-14 19:56:09,820] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-14 19:56:09,821] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (5c14c3e96bf4, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,823] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
21/05/14 22:56:09 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 190 ms on 5c14c3e96bf4 (executor driver) (118/141)
[2021-05-14 19:56:09,895] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1843 bytes result sent to driver
[2021-05-14 19:56:09,896] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (5c14c3e96bf4, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,897] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 177 ms on 5c14c3e96bf4 (executor driver) (119/141)
21/05/14 22:56:09 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-14 19:56:09,921] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1843 bytes result sent to driver
[2021-05-14 19:56:09,922] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (5c14c3e96bf4, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,923] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-14 19:56:09,924] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 176 ms on 5c14c3e96bf4 (executor driver) (120/141)
[2021-05-14 19:56:09,931] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1843 bytes result sent to driver
[2021-05-14 19:56:09,932] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (5c14c3e96bf4, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:09,933] {docker.py:276} INFO - 21/05/14 22:56:09 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 180 ms on 5c14c3e96bf4 (executor driver) (121/141)
[2021-05-14 19:56:09,934] {docker.py:276} INFO - 21/05/14 22:56:09 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-14 19:56:10,000] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1843 bytes result sent to driver
[2021-05-14 19:56:10,001] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (5c14c3e96bf4, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,001] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 182 ms on 5c14c3e96bf4 (executor driver) (122/141)
[2021-05-14 19:56:10,002] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-14 19:56:10,066] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
[2021-05-14 19:56:10,067] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (5c14c3e96bf4, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,069] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 173 ms on 5c14c3e96bf4 (executor driver) (123/141)
[2021-05-14 19:56:10,069] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-14 19:56:10,093] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1886 bytes result sent to driver
[2021-05-14 19:56:10,093] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (5c14c3e96bf4, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,094] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 172 ms on 5c14c3e96bf4 (executor driver) (124/141)
[2021-05-14 19:56:10,095] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-14 19:56:10,108] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1886 bytes result sent to driver
[2021-05-14 19:56:10,109] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (5c14c3e96bf4, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,109] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 178 ms on 5c14c3e96bf4 (executor driver) (125/141)
[2021-05-14 19:56:10,110] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-14 19:56:10,181] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1886 bytes result sent to driver
[2021-05-14 19:56:10,182] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (5c14c3e96bf4, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,183] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 183 ms on 5c14c3e96bf4 (executor driver) (126/141)
[2021-05-14 19:56:10,184] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-14 19:56:10,246] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1886 bytes result sent to driver
[2021-05-14 19:56:10,246] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (5c14c3e96bf4, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,247] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 181 ms on 5c14c3e96bf4 (executor driver) (127/141)
[2021-05-14 19:56:10,248] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-14 19:56:10,270] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1843 bytes result sent to driver
[2021-05-14 19:56:10,271] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (5c14c3e96bf4, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,272] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 179 ms on 5c14c3e96bf4 (executor driver) (128/141)
[2021-05-14 19:56:10,273] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-14 19:56:10,285] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1843 bytes result sent to driver
[2021-05-14 19:56:10,286] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 176 ms on 5c14c3e96bf4 (executor driver) (129/141)
[2021-05-14 19:56:10,287] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (5c14c3e96bf4, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,288] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-14 19:56:10,374] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1843 bytes result sent to driver
[2021-05-14 19:56:10,375] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (5c14c3e96bf4, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,377] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 194 ms on 5c14c3e96bf4 (executor driver) (130/141)
[2021-05-14 19:56:10,377] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-14 19:56:10,415] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-14 19:56:10,416] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (5c14c3e96bf4, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,417] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
21/05/14 22:56:10 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 170 ms on 5c14c3e96bf4 (executor driver) (131/141)
[2021-05-14 19:56:10,490] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1843 bytes result sent to driver
[2021-05-14 19:56:10,492] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (5c14c3e96bf4, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,493] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 223 ms on 5c14c3e96bf4 (executor driver) (132/141)
[2021-05-14 19:56:10,494] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-14 19:56:10,531] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1843 bytes result sent to driver
[2021-05-14 19:56:10,548] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (5c14c3e96bf4, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,549] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 247 ms on 5c14c3e96bf4 (executor driver) (133/141)
[2021-05-14 19:56:10,549] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-14 19:56:10,554] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1886 bytes result sent to driver
[2021-05-14 19:56:10,555] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (5c14c3e96bf4, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,555] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-14 19:56:10,556] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 182 ms on 5c14c3e96bf4 (executor driver) (134/141)
[2021-05-14 19:56:10,585] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1886 bytes result sent to driver
[2021-05-14 19:56:10,586] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (5c14c3e96bf4, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,587] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
21/05/14 22:56:10 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 171 ms on 5c14c3e96bf4 (executor driver) (135/141)
[2021-05-14 19:56:10,665] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1886 bytes result sent to driver
[2021-05-14 19:56:10,667] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (5c14c3e96bf4, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,668] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 176 ms on 5c14c3e96bf4 (executor driver) (136/141)
[2021-05-14 19:56:10,669] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-14 19:56:10,721] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1886 bytes result sent to driver
[2021-05-14 19:56:10,722] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (5c14c3e96bf4, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:10,724] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 192 ms on 5c14c3e96bf4 (executor driver) (137/141)
[2021-05-14 19:56:10,724] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-14 19:56:10,730] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1843 bytes result sent to driver
[2021-05-14 19:56:10,731] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 177 ms on 5c14c3e96bf4 (executor driver) (138/141)
[2021-05-14 19:56:10,760] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-14 19:56:10,761] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 175 ms on 5c14c3e96bf4 (executor driver) (139/141)
[2021-05-14 19:56:10,837] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
21/05/14 22:56:10 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 171 ms on 5c14c3e96bf4 (executor driver) (140/141)
[2021-05-14 19:56:10,907] {docker.py:276} INFO - 21/05/14 22:56:10 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-14 19:56:10,908] {docker.py:276} INFO - 21/05/14 22:56:10 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 185 ms on 5c14c3e96bf4 (executor driver) (141/141)
21/05/14 22:56:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 19:56:10,910] {docker.py:276} INFO - 21/05/14 22:56:10 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.602 s
21/05/14 22:56:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 22:56:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/05/14 22:56:10 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.616082 s
[2021-05-14 19:56:10,927] {docker.py:276} INFO - 21/05/14 22:56:10 INFO InMemoryFileIndex: It took 6685 ms to list leaf files for 141 paths.
[2021-05-14 19:56:11,128] {docker.py:276} INFO - 21/05/14 22:56:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5c14c3e96bf4:33219 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:56:13,428] {docker.py:276} INFO - 21/05/14 22:56:13 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:56:13,433] {docker.py:276} INFO - 21/05/14 22:56:13 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 19:56:13,438] {docker.py:276} INFO - 21/05/14 22:56:13 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:56:13,917] {docker.py:276} INFO - 21/05/14 22:56:13 INFO CodeGenerator: Code generated in 268.593 ms
[2021-05-14 19:56:13,930] {docker.py:276} INFO - 21/05/14 22:56:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-14 19:56:13,941] {docker.py:276} INFO - 21/05/14 22:56:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 19:56:13,942] {docker.py:276} INFO - 21/05/14 22:56:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5c14c3e96bf4:33219 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:56:13,944] {docker.py:276} INFO - 21/05/14 22:56:13 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:13,970] {docker.py:276} INFO - 21/05/14 22:56:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:56:14,059] {docker.py:276} INFO - 21/05/14 22:56:14 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:14,061] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/14 22:56:14 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:56:14 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:56:14,062] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:56:14,063] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:56:14,098] {docker.py:276} INFO - 21/05/14 22:56:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 19:56:14,111] {docker.py:276} INFO - 21/05/14 22:56:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 19:56:14,112] {docker.py:276} INFO - 21/05/14 22:56:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5c14c3e96bf4:33219 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 19:56:14,113] {docker.py:276} INFO - 21/05/14 22:56:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:56:14,114] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/14 22:56:14 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-14 19:56:14,119] {docker.py:276} INFO - 21/05/14 22:56:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (5c14c3e96bf4, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:14,120] {docker.py:276} INFO - 21/05/14 22:56:14 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-14 19:56:14,215] {docker.py:276} INFO - 21/05/14 22:56:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621027501_to_1621029301.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:56:14,241] {docker.py:276} INFO - 21/05/14 22:56:14 INFO CodeGenerator: Code generated in 18.4313 ms
[2021-05-14 19:56:14,832] {docker.py:276} INFO - 21/05/14 22:56:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-14 19:56:14,833] {docker.py:276} INFO - 21/05/14 22:56:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 718 ms on 5c14c3e96bf4 (executor driver) (1/1)
21/05/14 22:56:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 19:56:14,837] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.767 s
[2021-05-14 19:56:14,838] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 19:56:14,838] {docker.py:276} INFO - 21/05/14 22:56:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 19:56:14,838] {docker.py:276} INFO - 21/05/14 22:56:14 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.777467 s
[2021-05-14 19:56:14,868] {docker.py:276} INFO - 21/05/14 22:56:14 INFO CodeGenerator: Code generated in 14.8934 ms
[2021-05-14 19:56:14,939] {docker.py:276} INFO - 21/05/14 22:56:14 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 22:56:14 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:56:14,939] {docker.py:276} INFO - 21/05/14 22:56:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:56:14,945] {docker.py:276} INFO - 21/05/14 22:56:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-14 19:56:14,971] {docker.py:276} INFO - 21/05/14 22:56:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5c14c3e96bf4:33219 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 19:56:14,977] {docker.py:276} INFO - 21/05/14 22:56:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 19:56:14,978] {docker.py:276} INFO - 21/05/14 22:56:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5c14c3e96bf4:33219 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:56:14,979] {docker.py:276} INFO - 21/05/14 22:56:15 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:14,984] {docker.py:276} INFO - 21/05/14 22:56:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:56:15,547] {docker.py:276} INFO - 21/05/14 22:56:15 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:56:15,548] {docker.py:276} INFO - 21/05/14 22:56:15 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:56:15,548] {docker.py:276} INFO - 21/05/14 22:56:15 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 19:56:16,170] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:16,174] {docker.py:276} INFO - 21/05/14 22:56:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:16,174] {docker.py:276} INFO - 21/05/14 22:56:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167691469056325385884_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167691469056325385884_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167691469056325385884_0000}; taskId=attempt_202105142256167691469056325385884_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@656aee1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:16,175] {docker.py:276} INFO - 21/05/14 22:56:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:16,209] {docker.py:276} INFO - 21/05/14 22:56:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:56:16,298] {docker.py:276} INFO - 21/05/14 22:56:16 INFO CodeGenerator: Code generated in 58.6773 ms
[2021-05-14 19:56:16,301] {docker.py:276} INFO - 21/05/14 22:56:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:56:16,343] {docker.py:276} INFO - 21/05/14 22:56:16 INFO CodeGenerator: Code generated in 35.5379 ms
[2021-05-14 19:56:16,347] {docker.py:276} INFO - 21/05/14 22:56:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-14 19:56:16,373] {docker.py:276} INFO - 21/05/14 22:56:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-14 19:56:16,374] {docker.py:276} INFO - 21/05/14 22:56:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5c14c3e96bf4:33219 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:56:16,376] {docker.py:276} INFO - 21/05/14 22:56:16 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:16,384] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:56:16,445] {docker.py:276} INFO - 21/05/14 22:56:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5c14c3e96bf4:33219 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:56:16,515] {docker.py:276} INFO - 21/05/14 22:56:16 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:56:16,519] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 19:56:16,522] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2021-05-14 19:56:16,522] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 19:56:16,522] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-14 19:56:16,525] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-14 19:56:16,528] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:56:16,541] {docker.py:276} INFO - 21/05/14 22:56:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-14 19:56:16,553] {docker.py:276} INFO - 21/05/14 22:56:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-14 19:56:16,554] {docker.py:276} INFO - 21/05/14 22:56:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5c14c3e96bf4:33219 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-14 19:56:16,554] {docker.py:276} INFO - 21/05/14 22:56:16 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:56:16,557] {docker.py:276} INFO - 21/05/14 22:56:16 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/14 22:56:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-14 19:56:16,559] {docker.py:276} INFO - 21/05/14 22:56:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (5c14c3e96bf4, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:16,560] {docker.py:276} INFO - 21/05/14 22:56:16 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (5c14c3e96bf4, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:16,561] {docker.py:276} INFO - 21/05/14 22:56:16 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (5c14c3e96bf4, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:16,561] {docker.py:276} INFO - 21/05/14 22:56:16 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (5c14c3e96bf4, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:16,562] {docker.py:276} INFO - 21/05/14 22:56:16 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-14 19:56:16,562] {docker.py:276} INFO - 21/05/14 22:56:16 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-14 19:56:16,563] {docker.py:276} INFO - 21/05/14 22:56:16 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-14 19:56:16,563] {docker.py:276} INFO - 21/05/14 22:56:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-14 19:56:16,672] {docker.py:276} INFO - 21/05/14 22:56:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5c14c3e96bf4:33219 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:56:16,686] {docker.py:276} INFO - 21/05/14 22:56:16 INFO CodeGenerator: Code generated in 44.3035 ms
[2021-05-14 19:56:16,716] {docker.py:276} INFO - 21/05/14 22:56:16 INFO CodeGenerator: Code generated in 12.0496 ms
[2021-05-14 19:56:16,749] {docker.py:276} INFO - 21/05/14 22:56:16 INFO CodeGenerator: Code generated in 22.6801 ms
[2021-05-14 19:56:16,769] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620969901_to_1620971701.csv, range: 0-104272, partition values: [empty row]
[2021-05-14 19:56:16,774] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621027501_to_1621029301.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:56:16,780] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620953701_to_1620955501.csv, range: 0-103996, partition values: [empty row]
[2021-05-14 19:56:16,788] {docker.py:276} INFO - 21/05/14 22:56:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620995101_to_1620996901.csv, range: 0-104206, partition values: [empty row]
[2021-05-14 19:56:17,599] {docker.py:276} INFO - 21/05/14 22:56:17 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621020301_to_1621022101.csv, range: 0-103995, partition values: [empty row]
[2021-05-14 19:56:17,987] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621009501_to_1621011301.csv, range: 0-103986, partition values: [empty row]
[2021-05-14 19:56:18,099] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620969901_to_1620971701.csv, range: 0-104267, partition values: [empty row]
[2021-05-14 19:56:18,101] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620991501_to_1620993301.csv, range: 0-104206, partition values: [empty row]
[2021-05-14 19:56:18,118] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621029301_to_1621031101.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:56:18,478] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620959101_to_1620960901.csv, range: 0-103980, partition values: [empty row]
[2021-05-14 19:56:18,485] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620980701_to_1620982501.csv, range: 0-104203, partition values: [empty row]
[2021-05-14 19:56:18,494] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621025701_to_1621027501.csv, range: 0-111701, partition values: [empty row]
[2021-05-14 19:56:18,603] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621014901_to_1621016701.csv, range: 0-104259, partition values: [empty row]
[2021-05-14 19:56:18,847] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620995101_to_1620996901.csv, range: 0-103962, partition values: [empty row]
[2021-05-14 19:56:18,861] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621023901_to_1621025701.csv, range: 0-111661, partition values: [empty row]
[2021-05-14 19:56:18,959] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620973501_to_1620975301.csv, range: 0-104259, partition values: [empty row]
[2021-05-14 19:56:18,978] {docker.py:276} INFO - 21/05/14 22:56:18 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620989701_to_1620991501.csv, range: 0-104202, partition values: [empty row]
[2021-05-14 19:56:19,210] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620982501_to_1620984301.csv, range: 0-103959, partition values: [empty row]
[2021-05-14 19:56:19,237] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621022101_to_1621023901.csv, range: 0-110426, partition values: [empty row]
[2021-05-14 19:56:19,315] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621018501_to_1621020301.csv, range: 0-104258, partition values: [empty row]
[2021-05-14 19:56:19,401] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620978901_to_1620980701.csv, range: 0-104201, partition values: [empty row]
[2021-05-14 19:56:19,596] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620951901_to_1620953701.csv, range: 0-103941, partition values: [empty row]
[2021-05-14 19:56:19,713] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621007701_to_1621009501.csv, range: 0-104257, partition values: [empty row]
[2021-05-14 19:56:19,743] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620946501_to_1620948301.csv, range: 0-105494, partition values: [empty row]
[2021-05-14 19:56:19,766] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620987901_to_1620989701.csv, range: 0-104191, partition values: [empty row]
[2021-05-14 19:56:19,955] {docker.py:276} INFO - 21/05/14 22:56:19 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621004101_to_1621005901.csv, range: 0-103939, partition values: [empty row]
[2021-05-14 19:56:20,082] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620971701_to_1620973501.csv, range: 0-104256, partition values: [empty row]
[2021-05-14 19:56:20,110] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621027501_to_1621029301.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:56:20,189] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620968101_to_1620969901.csv, range: 0-104190, partition values: [empty row]
[2021-05-14 19:56:20,303] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620969901_to_1620971701.csv, range: 0-103929, partition values: [empty row]
[2021-05-14 19:56:20,431] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620996901_to_1620998701.csv, range: 0-104253, partition values: [empty row]
[2021-05-14 19:56:20,461] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621029301_to_1621031101.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:56:20,554] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620991501_to_1620993301.csv, range: 0-104184, partition values: [empty row]
[2021-05-14 19:56:20,649] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620971701_to_1620973501.csv, range: 0-103920, partition values: [empty row]
[2021-05-14 19:56:20,787] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620971701_to_1620973501.csv, range: 0-104246, partition values: [empty row]
[2021-05-14 19:56:20,828] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621027501_to_1621029301.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:56:20,930] {docker.py:276} INFO - 21/05/14 22:56:20 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620977101_to_1620978901.csv, range: 0-104174, partition values: [empty row]
[2021-05-14 19:56:21,004] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620989701_to_1620991501.csv, range: 0-103898, partition values: [empty row]
[2021-05-14 19:56:21,174] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620957301_to_1620959101.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 19:56:21,181] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621029301_to_1621031101.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:56:21,272] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621011301_to_1621013101.csv, range: 0-104172, partition values: [empty row]
[2021-05-14 19:56:21,350] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621013101_to_1621014901.csv, range: 0-103897, partition values: [empty row]
[2021-05-14 19:56:21,529] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620962701_to_1620964501.csv, range: 0-104243, partition values: [empty row]
[2021-05-14 19:56:21,535] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621025701_to_1621027501.csv, range: 0-104473, partition values: [empty row]
[2021-05-14 19:56:21,625] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620960901_to_1620962701.csv, range: 0-104171, partition values: [empty row]
[2021-05-14 19:56:21,712] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621011301_to_1621013101.csv, range: 0-103894, partition values: [empty row]
[2021-05-14 19:56:21,898] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620955501_to_1620957301.csv, range: 0-104382, partition values: [empty row]
[2021-05-14 19:56:21,946] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621002301_to_1621004101.csv, range: 0-104240, partition values: [empty row]
[2021-05-14 19:56:21,970] {docker.py:276} INFO - 21/05/14 22:56:21 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620975301_to_1620977101.csv, range: 0-104171, partition values: [empty row]
[2021-05-14 19:56:22,064] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620987901_to_1620989701.csv, range: 0-103890, partition values: [empty row]
[2021-05-14 19:56:22,311] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620973501_to_1620975301.csv, range: 0-104167, partition values: [empty row]
[2021-05-14 19:56:22,326] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621018501_to_1621020301.csv, range: 0-104237, partition values: [empty row]
[2021-05-14 19:56:22,412] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620957301_to_1620959101.csv, range: 0-103886, partition values: [empty row]
[2021-05-14 19:56:22,673] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620975301_to_1620977101.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 19:56:22,677] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620998701_to_1621000501.csv, range: 0-104165, partition values: [empty row]
[2021-05-14 19:56:22,761] {docker.py:276} INFO - 21/05/14 22:56:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620955501_to_1620957301.csv, range: 0-103856, partition values: [empty row]
[2021-05-14 19:56:23,023] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621005901_to_1621007701.csv, range: 0-104165, partition values: [empty row]
[2021-05-14 19:56:23,042] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620978901_to_1620980701.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 19:56:23,104] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620953701_to_1620955501.csv, range: 0-103852, partition values: [empty row]
[2021-05-14 19:56:23,215] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621025701_to_1621027501.csv, range: 0-104360, partition values: [empty row]
[2021-05-14 19:56:23,387] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621022101_to_1621023901.csv, range: 0-104235, partition values: [empty row]
[2021-05-14 19:56:23,389] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620987901_to_1620989701.csv, range: 0-104163, partition values: [empty row]
[2021-05-14 19:56:23,471] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621002301_to_1621004101.csv, range: 0-103851, partition values: [empty row]
[2021-05-14 19:56:23,704] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621023901_to_1621025701.csv, range: 0-104351, partition values: [empty row]
[2021-05-14 19:56:23,750] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620989701_to_1620991501.csv, range: 0-104162, partition values: [empty row]
[2021-05-14 19:56:23,753] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621009501_to_1621011301.csv, range: 0-104235, partition values: [empty row]
[2021-05-14 19:56:23,822] {docker.py:276} INFO - 21/05/14 22:56:23 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621000501_to_1621002301.csv, range: 0-103840, partition values: [empty row]
[2021-05-14 19:56:24,055] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620959101_to_1620960901.csv, range: 0-104340, partition values: [empty row]
[2021-05-14 19:56:24,101] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621013101_to_1621014901.csv, range: 0-104161, partition values: [empty row]
[2021-05-14 19:56:24,172] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620986101_to_1620987901.csv, range: 0-104231, partition values: [empty row]
[2021-05-14 19:56:24,176] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620975301_to_1620977101.csv, range: 0-103838, partition values: [empty row]
[2021-05-14 19:56:24,405] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620993301_to_1620995101.csv, range: 0-104322, partition values: [empty row]
[2021-05-14 19:56:24,504] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620966301_to_1620968101.csv, range: 0-104158, partition values: [empty row]
[2021-05-14 19:56:24,522] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621002301_to_1621004101.csv, range: 0-104230, partition values: [empty row]
[2021-05-14 19:56:24,524] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620991501_to_1620993301.csv, range: 0-103835, partition values: [empty row]
[2021-05-14 19:56:24,811] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620948301_to_1620950101.csv, range: 0-104316, partition values: [empty row]
[2021-05-14 19:56:24,855] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620966301_to_1620968101.csv, range: 0-104157, partition values: [empty row]
[2021-05-14 19:56:24,883] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621007701_to_1621009501.csv, range: 0-103833, partition values: [empty row]
[2021-05-14 19:56:24,895] {docker.py:276} INFO - 21/05/14 22:56:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621007701_to_1621009501.csv, range: 0-104228, partition values: [empty row]
[2021-05-14 19:56:25,172] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620966301_to_1620968101.csv, range: 0-104311, partition values: [empty row]
[2021-05-14 19:56:25,235] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621000501_to_1621002301.csv, range: 0-104153, partition values: [empty row]
[2021-05-14 19:56:25,241] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620948301_to_1620950101.csv, range: 0-103831, partition values: [empty row]
[2021-05-14 19:56:25,254] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621011301_to_1621013101.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 19:56:25,591] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620993301_to_1620995101.csv, range: 0-103822, partition values: [empty row]
[2021-05-14 19:56:25,595] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620984301_to_1620986101.csv, range: 0-104222, partition values: [empty row]
[2021-05-14 19:56:25,627] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621004101_to_1621005901.csv, range: 0-104145, partition values: [empty row]
[2021-05-14 19:56:25,663] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620946501_to_1620948301.csv, range: 0-104310, partition values: [empty row]
[2021-05-14 19:56:25,947] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621000501_to_1621002301.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 19:56:25,950] {docker.py:276} INFO - 21/05/14 22:56:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620964501_to_1620966301.csv, range: 0-103819, partition values: [empty row]
[2021-05-14 19:56:26,016] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620960901_to_1620962701.csv, range: 0-104300, partition values: [empty row]
[2021-05-14 19:56:26,050] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620950101_to_1620951901.csv, range: 0-104120, partition values: [empty row]
[2021-05-14 19:56:26,309] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620962701_to_1620964501.csv, range: 0-103805, partition values: [empty row]
[2021-05-14 19:56:26,310] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620980701_to_1620982501.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 19:56:26,350] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621014901_to_1621016701.csv, range: 0-104298, partition values: [empty row]
[2021-05-14 19:56:26,395] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621009501_to_1621011301.csv, range: 0-104119, partition values: [empty row]
[2021-05-14 19:56:26,651] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620968101_to_1620969901.csv, range: 0-103798, partition values: [empty row]
[2021-05-14 19:56:26,653] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620977101_to_1620978901.csv, range: 0-104218, partition values: [empty row]
[2021-05-14 19:56:26,692] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620986101_to_1620987901.csv, range: 0-104296, partition values: [empty row]
[2021-05-14 19:56:26,786] {docker.py:276} INFO - 21/05/14 22:56:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621005901_to_1621007701.csv, range: 0-104117, partition values: [empty row]
[2021-05-14 19:56:26,999] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620973501_to_1620975301.csv, range: 0-103796, partition values: [empty row]
[2021-05-14 19:56:27,016] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621016701_to_1621018501.csv, range: 0-104218, partition values: [empty row]
[2021-05-14 19:56:27,039] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621016701_to_1621018501.csv, range: 0-104294, partition values: [empty row]
[2021-05-14 19:56:27,345] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620960901_to_1620962701.csv, range: 0-103792, partition values: [empty row]
[2021-05-14 19:56:27,356] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620980701_to_1620982501.csv, range: 0-104110, partition values: [empty row]
[2021-05-14 19:56:27,359] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620982501_to_1620984301.csv, range: 0-104217, partition values: [empty row]
[2021-05-14 19:56:27,388] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1621013101_to_1621014901.csv, range: 0-104294, partition values: [empty row]
[2021-05-14 19:56:27,701] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620977101_to_1620978901.csv, range: 0-103784, partition values: [empty row]
[2021-05-14 19:56:27,702] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620995101_to_1620996901.csv, range: 0-104216, partition values: [empty row]
[2021-05-14 19:56:27,734] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620984301_to_1620986101.csv, range: 0-104293, partition values: [empty row]
[2021-05-14 19:56:27,767] {docker.py:276} INFO - 21/05/14 22:56:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620951901_to_1620953701.csv, range: 0-104109, partition values: [empty row]
[2021-05-14 19:56:28,051] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620964501_to_1620966301.csv, range: 0-104212, partition values: [empty row]
[2021-05-14 19:56:28,055] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620950101_to_1620951901.csv, range: 0-103783, partition values: [empty row]
[2021-05-14 19:56:28,070] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620982501_to_1620984301.csv, range: 0-104291, partition values: [empty row]
[2021-05-14 19:56:28,117] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620951901_to_1620953701.csv, range: 0-104109, partition values: [empty row]
[2021-05-14 19:56:28,396] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620959101_to_1620960901.csv, range: 0-104211, partition values: [empty row]
[2021-05-14 19:56:28,398] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620984301_to_1620986101.csv, range: 0-103776, partition values: [empty row]
[2021-05-14 19:56:28,457] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620996901_to_1620998701.csv, range: 0-104103, partition values: [empty row]
[2021-05-14 19:56:28,573] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620953701_to_1620955501.csv, range: 0-104288, partition values: [empty row]
[2021-05-14 19:56:28,749] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620998701_to_1621000501.csv, range: 0-104211, partition values: [empty row]
[2021-05-14 19:56:28,923] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_55_01/from_1620968101_to_1620969901.csv, range: 0-104288, partition values: [empty row]
[2021-05-14 19:56:28,968] {docker.py:276} INFO - 21/05/14 22:56:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620955501_to_1620957301.csv, range: 0-104102, partition values: [empty row]
[2021-05-14 19:56:29,082] {docker.py:276} INFO - 21/05/14 22:56:29 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2722 bytes result sent to driver
[2021-05-14 19:56:29,083] {docker.py:276} INFO - 21/05/14 22:56:29 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (5c14c3e96bf4, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:29,084] {docker.py:276} INFO - 21/05/14 22:56:29 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-14 19:56:29,086] {docker.py:276} INFO - 21/05/14 22:56:29 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 12505 ms on 5c14c3e96bf4 (executor driver) (1/5)
[2021-05-14 19:56:29,097] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620950101_to_1620951901.csv, range: 0-103771, partition values: [empty row]
[2021-05-14 19:56:29,161] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620957301_to_1620959101.csv, range: 0-104206, partition values: [empty row]
[2021-05-14 19:56:29,282] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620962701_to_1620964501.csv, range: 0-104285, partition values: [empty row]
[2021-05-14 19:56:29,328] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620964501_to_1620966301.csv, range: 0-104063, partition values: [empty row]
[2021-05-14 19:56:29,445] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620998701_to_1621000501.csv, range: 0-103768, partition values: [empty row]
[2021-05-14 19:56:29,635] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620993301_to_1620995101.csv, range: 0-104283, partition values: [empty row]
[2021-05-14 19:56:29,698] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620978901_to_1620980701.csv, range: 0-104045, partition values: [empty row]
[2021-05-14 19:56:29,701] {docker.py:276} INFO - 21/05/14 22:56:29 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-14 19:56:29,702] {docker.py:276} INFO - 21/05/14 22:56:29 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 13124 ms on 5c14c3e96bf4 (executor driver) (2/5)
[2021-05-14 19:56:29,817] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620986101_to_1620987901.csv, range: 0-103752, partition values: [empty row]
[2021-05-14 19:56:29,985] {docker.py:276} INFO - 21/05/14 22:56:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621020301_to_1621022101.csv, range: 0-104281, partition values: [empty row]
[2021-05-14 19:56:30,186] {docker.py:276} INFO - 21/05/14 22:56:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620948301_to_1620950101.csv, range: 0-103752, partition values: [empty row]
[2021-05-14 19:56:30,252] {docker.py:276} INFO - 21/05/14 22:56:30 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-14 19:56:30,253] {docker.py:276} INFO - 21/05/14 22:56:30 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 13674 ms on 5c14c3e96bf4 (executor driver) (3/5)
[2021-05-14 19:56:30,348] {docker.py:276} INFO - 21/05/14 22:56:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621004101_to_1621005901.csv, range: 0-104279, partition values: [empty row]
[2021-05-14 19:56:30,553] {docker.py:276} INFO - 21/05/14 22:56:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621014901_to_1621016701.csv, range: 0-103748, partition values: [empty row]
[2021-05-14 19:56:30,855] {docker.py:276} INFO - 21/05/14 22:56:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2679 bytes result sent to driver
[2021-05-14 19:56:30,855] {docker.py:276} INFO - 21/05/14 22:56:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 14280 ms on 5c14c3e96bf4 (executor driver) (4/5)
[2021-05-14 19:56:30,899] {docker.py:276} INFO - 21/05/14 22:56:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621005901_to_1621007701.csv, range: 0-103739, partition values: [empty row]
[2021-05-14 19:56:31,264] {docker.py:276} INFO - 21/05/14 22:56:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621020301_to_1621022101.csv, range: 0-103716, partition values: [empty row]
[2021-05-14 19:56:31,611] {docker.py:276} INFO - 21/05/14 22:56:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621016701_to_1621018501.csv, range: 0-103705, partition values: [empty row]
[2021-05-14 19:56:31,987] {docker.py:276} INFO - 21/05/14 22:56:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1621018501_to_1621020301.csv, range: 0-103704, partition values: [empty row]
[2021-05-14 19:56:32,340] {docker.py:276} INFO - 21/05/14 22:56:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_55_01/from_1620996901_to_1620998701.csv, range: 0-103665, partition values: [empty row]
[2021-05-14 19:56:32,683] {docker.py:276} INFO - 21/05/14 22:56:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621023901_to_1621025701.csv, range: 0-103617, partition values: [empty row]
[2021-05-14 19:56:33,071] {docker.py:276} INFO - 21/05/14 22:56:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1621022101_to_1621023901.csv, range: 0-103614, partition values: [empty row]
[2021-05-14 19:56:33,438] {docker.py:276} INFO - 21/05/14 22:56:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_55_01/from_1620946501_to_1620948301.csv, range: 0-103247, partition values: [empty row]
[2021-05-14 19:56:33,882] {docker.py:276} INFO - 21/05/14 22:56:33 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-14 19:56:33,883] {docker.py:276} INFO - 21/05/14 22:56:33 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4806 ms on 5c14c3e96bf4 (executor driver) (5/5)
21/05/14 22:56:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-14 19:56:33,884] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 17.336 s
[2021-05-14 19:56:33,885] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 19:56:33,886] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: running: Set()
[2021-05-14 19:56:33,887] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-14 19:56:33,888] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: failed: Set()
[2021-05-14 19:56:33,893] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:56:33,931] {docker.py:276} INFO - 21/05/14 22:56:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-14 19:56:33,933] {docker.py:276} INFO - 21/05/14 22:56:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-14 19:56:33,934] {docker.py:276} INFO - 21/05/14 22:56:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5c14c3e96bf4:33219 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-14 19:56:33,935] {docker.py:276} INFO - 21/05/14 22:56:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:56:33,936] {docker.py:276} INFO - 21/05/14 22:56:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 22:56:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-14 19:56:33,946] {docker.py:276} INFO - 21/05/14 22:56:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (5c14c3e96bf4, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:33,947] {docker.py:276} INFO - 21/05/14 22:56:33 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (5c14c3e96bf4, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:33,951] {docker.py:276} INFO - 21/05/14 22:56:33 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (5c14c3e96bf4, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:33,952] {docker.py:276} INFO - 21/05/14 22:56:33 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (5c14c3e96bf4, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:33,952] {docker.py:276} INFO - 21/05/14 22:56:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-14 19:56:33,952] {docker.py:276} INFO - 21/05/14 22:56:33 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
21/05/14 22:56:33 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-14 19:56:33,953] {docker.py:276} INFO - 21/05/14 22:56:33 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-14 19:56:34,022] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:34,024] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Getting 5 (39.5 KiB) non-empty blocks including 5 (39.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:34,024] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:34,025] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Getting 5 (39.6 KiB) non-empty blocks including 5 (39.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:34,026] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-14 19:56:34,026] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-14 19:56:34,027] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-14 19:56:34,027] {docker.py:276} INFO - 21/05/14 22:56:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 19:56:34,047] {docker.py:276} INFO - 21/05/14 22:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:34,048] {docker.py:276} INFO - 21/05/14 22:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:34,048] {docker.py:276} INFO - 21/05/14 22:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:34,049] {docker.py:276} INFO - 21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:34,051] {docker.py:276} INFO - 21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163355653553296268076_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163355653553296268076_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163355653553296268076_0004}; taskId=attempt_202105142256163355653553296268076_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64e24851}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:34,051] {docker.py:276} INFO - 21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166393519440381944974_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166393519440381944974_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166393519440381944974_0004}; taskId=attempt_202105142256166393519440381944974_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c4c8427}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:34,051] {docker.py:276} INFO - 21/05/14 22:56:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:34,052] {docker.py:276} INFO - 21/05/14 22:56:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:34,052] {docker.py:276} INFO - 21/05/14 22:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:34,053] {docker.py:276} INFO - 21/05/14 22:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164620521653785484044_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164620521653785484044_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164620521653785484044_0004}; taskId=attempt_202105142256164620521653785484044_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e5cf338}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:34,053] {docker.py:276} INFO - 21/05/14 22:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:34,054] {docker.py:276} INFO - 21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:34,054] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256163355653553296268076_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163355653553296268076_0004_m_000003_291 
21/05/14 22:56:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166331696919024632765_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166331696919024632765_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166331696919024632765_0004}; taskId=attempt_202105142256166331696919024632765_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bd80ed3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:34,056] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256164620521653785484044_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164620521653785484044_0004_m_000002_290
[2021-05-14 19:56:34,057] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256166393519440381944974_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166393519440381944974_0004_m_000000_288
[2021-05-14 19:56:34,058] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256166331696919024632765_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166331696919024632765_0004_m_000001_289
[2021-05-14 19:56:34,083] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Task committer attempt_202105142256163355653553296268076_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163355653553296268076_0004_m_000003_291 : duration 0:00.029s
[2021-05-14 19:56:34,085] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Task committer attempt_202105142256164620521653785484044_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164620521653785484044_0004_m_000002_290 : duration 0:00.028s
[2021-05-14 19:56:34,090] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Task committer attempt_202105142256166393519440381944974_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166393519440381944974_0004_m_000000_288 : duration 0:00.036s
[2021-05-14 19:56:34,093] {docker.py:276} INFO - 21/05/14 22:56:34 INFO StagingCommitter: Task committer attempt_202105142256166331696919024632765_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166331696919024632765_0004_m_000001_289 : duration 0:00.036s
[2021-05-14 19:56:36,102] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256164620521653785484044_0004_m_000002_290: needsTaskCommit() Task attempt_202105142256164620521653785484044_0004_m_000002_290
[2021-05-14 19:56:36,103] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256166393519440381944974_0004_m_000000_288: needsTaskCommit() Task attempt_202105142256166393519440381944974_0004_m_000000_288
[2021-05-14 19:56:36,104] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256164620521653785484044_0004_m_000002_290: needsTaskCommit() Task attempt_202105142256164620521653785484044_0004_m_000002_290: duration 0:00.001s
[2021-05-14 19:56:36,104] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256166393519440381944974_0004_m_000000_288: needsTaskCommit() Task attempt_202105142256166393519440381944974_0004_m_000000_288: duration 0:00.000s
[2021-05-14 19:56:36,104] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164620521653785484044_0004_m_000002_290
[2021-05-14 19:56:36,106] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166393519440381944974_0004_m_000000_288
[2021-05-14 19:56:36,112] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4630 bytes result sent to driver
[2021-05-14 19:56:36,113] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (5c14c3e96bf4, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:36,114] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4630 bytes result sent to driver
[2021-05-14 19:56:36,115] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-14 19:56:36,116] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (5c14c3e96bf4, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:36,116] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-14 19:56:36,117] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 2172 ms on 5c14c3e96bf4 (executor driver) (1/200)
[2021-05-14 19:56:36,117] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2176 ms on 5c14c3e96bf4 (executor driver) (2/200)
[2021-05-14 19:56:36,125] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Getting 5 (39.9 KiB) non-empty blocks including 5 (39.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:36,127] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:36,128] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:36,129] {docker.py:276} INFO - 21/05/14 22:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,129] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168362397285462507533_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168362397285462507533_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168362397285462507533_0004}; taskId=attempt_202105142256168362397285462507533_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63a3fb9d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,129] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256168362397285462507533_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168362397285462507533_0004_m_000004_292
[2021-05-14 19:56:36,130] {docker.py:276} INFO - 21/05/14 22:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:36,130] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,131] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163173201951157988683_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163173201951157988683_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163173201951157988683_0004}; taskId=attempt_202105142256163173201951157988683_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@247c81c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,131] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:36,131] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256163173201951157988683_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163173201951157988683_0004_m_000005_293
[2021-05-14 19:56:36,136] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256168362397285462507533_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168362397285462507533_0004_m_000004_292 : duration 0:00.007s
[2021-05-14 19:56:36,140] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256163173201951157988683_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163173201951157988683_0004_m_000005_293 : duration 0:00.010s
[2021-05-14 19:56:36,489] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256166331696919024632765_0004_m_000001_289: needsTaskCommit() Task attempt_202105142256166331696919024632765_0004_m_000001_289
[2021-05-14 19:56:36,489] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256163355653553296268076_0004_m_000003_291: needsTaskCommit() Task attempt_202105142256163355653553296268076_0004_m_000003_291
[2021-05-14 19:56:36,489] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256163355653553296268076_0004_m_000003_291: needsTaskCommit() Task attempt_202105142256163355653553296268076_0004_m_000003_291: duration 0:00.000s
[2021-05-14 19:56:36,490] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163355653553296268076_0004_m_000003_291
[2021-05-14 19:56:36,490] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256166331696919024632765_0004_m_000001_289: needsTaskCommit() Task attempt_202105142256166331696919024632765_0004_m_000001_289: duration 0:00.002s
21/05/14 22:56:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166331696919024632765_0004_m_000001_289
[2021-05-14 19:56:36,491] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
21/05/14 22:56:36 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-14 19:56:36,492] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (5c14c3e96bf4, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:36,492] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (5c14c3e96bf4, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:36,493] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 2545 ms on 5c14c3e96bf4 (executor driver) (3/200)
[2021-05-14 19:56:36,493] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-14 19:56:36,494] {docker.py:276} INFO - 21/05/14 22:56:36 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 2550 ms on 5c14c3e96bf4 (executor driver) (4/200)
[2021-05-14 19:56:36,495] {docker.py:276} INFO - 21/05/14 22:56:36 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-14 19:56:36,509] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:36,511] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:36,512] {docker.py:276} INFO - 21/05/14 22:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:56:36,513] {docker.py:276} INFO - 21/05/14 22:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:36,514] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,514] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161981809860499965949_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161981809860499965949_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161981809860499965949_0004}; taskId=attempt_202105142256161981809860499965949_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5301151d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,514] {docker.py:276} INFO - 21/05/14 22:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256161981809860499965949_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161981809860499965949_0004_m_000007_295
[2021-05-14 19:56:36,516] {docker.py:276} INFO - 21/05/14 22:56:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:36,516] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:36,517] {docker.py:276} INFO - 21/05/14 22:56:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616563947029790528506_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616563947029790528506_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616563947029790528506_0004}; taskId=attempt_20210514225616563947029790528506_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68a0ab43}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:36,517] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Starting: Task committer attempt_20210514225616563947029790528506_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616563947029790528506_0004_m_000006_294
[2021-05-14 19:56:36,522] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_202105142256161981809860499965949_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161981809860499965949_0004_m_000007_295 : duration 0:00.008s
[2021-05-14 19:56:36,530] {docker.py:276} INFO - 21/05/14 22:56:36 INFO StagingCommitter: Task committer attempt_20210514225616563947029790528506_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616563947029790528506_0004_m_000006_294 : duration 0:00.013s
[2021-05-14 19:56:38,860] {docker.py:276} INFO - 21/05/14 22:56:38 INFO StagingCommitter: Starting: Task committer attempt_202105142256168362397285462507533_0004_m_000004_292: needsTaskCommit() Task attempt_202105142256168362397285462507533_0004_m_000004_292
[2021-05-14 19:56:38,861] {docker.py:276} INFO - 21/05/14 22:56:38 INFO StagingCommitter: Task committer attempt_202105142256168362397285462507533_0004_m_000004_292: needsTaskCommit() Task attempt_202105142256168362397285462507533_0004_m_000004_292: duration 0:00.001s
21/05/14 22:56:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168362397285462507533_0004_m_000004_292
[2021-05-14 19:56:38,863] {docker.py:276} INFO - 21/05/14 22:56:38 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-14 19:56:38,865] {docker.py:276} INFO - 21/05/14 22:56:38 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (5c14c3e96bf4, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:38,866] {docker.py:276} INFO - 21/05/14 22:56:38 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 2756 ms on 5c14c3e96bf4 (executor driver) (5/200)
[2021-05-14 19:56:38,866] {docker.py:276} INFO - 21/05/14 22:56:38 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-14 19:56:38,885] {docker.py:276} INFO - 21/05/14 22:56:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:38,887] {docker.py:276} INFO - 21/05/14 22:56:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:38,888] {docker.py:276} INFO - 21/05/14 22:56:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161872330697945435588_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161872330697945435588_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161872330697945435588_0004}; taskId=attempt_202105142256161872330697945435588_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1938aea6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:38 INFO StagingCommitter: Starting: Task committer attempt_202105142256161872330697945435588_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161872330697945435588_0004_m_000008_296
[2021-05-14 19:56:38,893] {docker.py:276} INFO - 21/05/14 22:56:38 INFO StagingCommitter: Task committer attempt_202105142256161872330697945435588_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161872330697945435588_0004_m_000008_296 : duration 0:00.005s
[2021-05-14 19:56:39,117] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256163173201951157988683_0004_m_000005_293: needsTaskCommit() Task attempt_202105142256163173201951157988683_0004_m_000005_293
[2021-05-14 19:56:39,118] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_202105142256163173201951157988683_0004_m_000005_293: needsTaskCommit() Task attempt_202105142256163173201951157988683_0004_m_000005_293: duration 0:00.001s
[2021-05-14 19:56:39,118] {docker.py:276} INFO - 21/05/14 22:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163173201951157988683_0004_m_000005_293
[2021-05-14 19:56:39,119] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4587 bytes result sent to driver
[2021-05-14 19:56:39,121] {docker.py:276} INFO - 21/05/14 22:56:39 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (5c14c3e96bf4, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:39,122] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-14 19:56:39,122] {docker.py:276} INFO - 21/05/14 22:56:39 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 3009 ms on 5c14c3e96bf4 (executor driver) (6/200)
[2021-05-14 19:56:39,133] {docker.py:276} INFO - 21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:39,136] {docker.py:276} INFO - 21/05/14 22:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:39,136] {docker.py:276} INFO - 21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163336424041794486240_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163336424041794486240_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163336424041794486240_0004}; taskId=attempt_202105142256163336424041794486240_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@430550d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:39,137] {docker.py:276} INFO - 21/05/14 22:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:39,137] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256163336424041794486240_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163336424041794486240_0004_m_000009_297
[2021-05-14 19:56:39,144] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_202105142256163336424041794486240_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163336424041794486240_0004_m_000009_297 : duration 0:00.008s
[2021-05-14 19:56:39,154] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_20210514225616563947029790528506_0004_m_000006_294: needsTaskCommit() Task attempt_20210514225616563947029790528506_0004_m_000006_294
[2021-05-14 19:56:39,155] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_20210514225616563947029790528506_0004_m_000006_294: needsTaskCommit() Task attempt_20210514225616563947029790528506_0004_m_000006_294: duration 0:00.001s
[2021-05-14 19:56:39,155] {docker.py:276} INFO - 21/05/14 22:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616563947029790528506_0004_m_000006_294
[2021-05-14 19:56:39,158] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4587 bytes result sent to driver
[2021-05-14 19:56:39,159] {docker.py:276} INFO - 21/05/14 22:56:39 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (5c14c3e96bf4, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:39,160] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
21/05/14 22:56:39 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 2672 ms on 5c14c3e96bf4 (executor driver) (7/200)
[2021-05-14 19:56:39,170] {docker.py:276} INFO - 21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:39,170] {docker.py:276} INFO - 21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:39,172] {docker.py:276} INFO - 21/05/14 22:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:39,173] {docker.py:276} INFO - 21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163230924651541010345_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163230924651541010345_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163230924651541010345_0004}; taskId=attempt_202105142256163230924651541010345_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d4ef4ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:39,173] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256163230924651541010345_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163230924651541010345_0004_m_000010_298
[2021-05-14 19:56:39,177] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_202105142256163230924651541010345_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163230924651541010345_0004_m_000010_298 : duration 0:00.004s
[2021-05-14 19:56:39,301] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256161981809860499965949_0004_m_000007_295: needsTaskCommit() Task attempt_202105142256161981809860499965949_0004_m_000007_295
[2021-05-14 19:56:39,301] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_202105142256161981809860499965949_0004_m_000007_295: needsTaskCommit() Task attempt_202105142256161981809860499965949_0004_m_000007_295: duration 0:00.001s
21/05/14 22:56:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161981809860499965949_0004_m_000007_295
[2021-05-14 19:56:39,302] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4587 bytes result sent to driver
[2021-05-14 19:56:39,303] {docker.py:276} INFO - 21/05/14 22:56:39 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (5c14c3e96bf4, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:39,304] {docker.py:276} INFO - 21/05/14 22:56:39 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2815 ms on 5c14c3e96bf4 (executor driver) (8/200)
[2021-05-14 19:56:39,305] {docker.py:276} INFO - 21/05/14 22:56:39 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-14 19:56:39,313] {docker.py:276} INFO - 21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:39,315] {docker.py:276} INFO - 21/05/14 22:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:39,316] {docker.py:276} INFO - 21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616682077150003180567_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616682077150003180567_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616682077150003180567_0004}; taskId=attempt_20210514225616682077150003180567_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15be6677}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:39 INFO StagingCommitter: Starting: Task committer attempt_20210514225616682077150003180567_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616682077150003180567_0004_m_000011_299
[2021-05-14 19:56:39,320] {docker.py:276} INFO - 21/05/14 22:56:39 INFO StagingCommitter: Task committer attempt_20210514225616682077150003180567_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616682077150003180567_0004_m_000011_299 : duration 0:00.004s
[2021-05-14 19:56:41,619] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161872330697945435588_0004_m_000008_296: needsTaskCommit() Task attempt_202105142256161872330697945435588_0004_m_000008_296
[2021-05-14 19:56:41,620] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_202105142256161872330697945435588_0004_m_000008_296: needsTaskCommit() Task attempt_202105142256161872330697945435588_0004_m_000008_296: duration 0:00.002s
21/05/14 22:56:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161872330697945435588_0004_m_000008_296
[2021-05-14 19:56:41,622] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-14 19:56:41,624] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (5c14c3e96bf4, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:41,625] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2765 ms on 5c14c3e96bf4 (executor driver) (9/200)
[2021-05-14 19:56:41,627] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-14 19:56:41,636] {docker.py:276} INFO - 21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:41,639] {docker.py:276} INFO - 21/05/14 22:56:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:41,639] {docker.py:276} INFO - 21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161185909271520686285_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161185909271520686285_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161185909271520686285_0004}; taskId=attempt_202105142256161185909271520686285_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43f69af3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161185909271520686285_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161185909271520686285_0004_m_000012_300
[2021-05-14 19:56:41,644] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_202105142256161185909271520686285_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161185909271520686285_0004_m_000012_300 : duration 0:00.004s
[2021-05-14 19:56:41,848] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256163230924651541010345_0004_m_000010_298: needsTaskCommit() Task attempt_202105142256163230924651541010345_0004_m_000010_298
[2021-05-14 19:56:41,849] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_202105142256163230924651541010345_0004_m_000010_298: needsTaskCommit() Task attempt_202105142256163230924651541010345_0004_m_000010_298: duration 0:00.002s
21/05/14 22:56:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163230924651541010345_0004_m_000010_298
[2021-05-14 19:56:41,851] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-14 19:56:41,852] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (5c14c3e96bf4, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:41,853] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2698 ms on 5c14c3e96bf4 (executor driver) (10/200)
[2021-05-14 19:56:41,854] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-14 19:56:41,856] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256163336424041794486240_0004_m_000009_297: needsTaskCommit() Task attempt_202105142256163336424041794486240_0004_m_000009_297
[2021-05-14 19:56:41,857] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_202105142256163336424041794486240_0004_m_000009_297: needsTaskCommit() Task attempt_202105142256163336424041794486240_0004_m_000009_297: duration 0:00.000s
[2021-05-14 19:56:41,857] {docker.py:276} INFO - 21/05/14 22:56:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163336424041794486240_0004_m_000009_297
[2021-05-14 19:56:41,858] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-14 19:56:41,859] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (5c14c3e96bf4, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:41,860] {docker.py:276} INFO - 21/05/14 22:56:41 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-14 19:56:41,861] {docker.py:276} INFO - 21/05/14 22:56:41 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 2744 ms on 5c14c3e96bf4 (executor driver) (11/200)
[2021-05-14 19:56:41,876] {docker.py:276} INFO - 21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:41,876] {docker.py:276} INFO - 21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:41,876] {docker.py:276} INFO - 21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:41,877] {docker.py:276} INFO - 21/05/14 22:56:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:56:41,879] {docker.py:276} INFO - 21/05/14 22:56:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:41,880] {docker.py:276} INFO - 21/05/14 22:56:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:41,880] {docker.py:276} INFO - 21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:41,881] {docker.py:276} INFO - 21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164021028528480325954_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164021028528480325954_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164021028528480325954_0004}; taskId=attempt_202105142256164021028528480325954_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ca8de2c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:41,881] {docker.py:276} INFO - 21/05/14 22:56:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:41,881] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256164021028528480325954_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164021028528480325954_0004_m_000013_301
[2021-05-14 19:56:41,883] {docker.py:276} INFO - 21/05/14 22:56:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:41,883] {docker.py:276} INFO - 21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616244920328564241984_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616244920328564241984_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616244920328564241984_0004}; taskId=attempt_20210514225616244920328564241984_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1901801d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:41,883] {docker.py:276} INFO - 21/05/14 22:56:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:41,884] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Starting: Task committer attempt_20210514225616244920328564241984_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616244920328564241984_0004_m_000014_302
[2021-05-14 19:56:41,887] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_202105142256164021028528480325954_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164021028528480325954_0004_m_000013_301 : duration 0:00.006s
[2021-05-14 19:56:41,891] {docker.py:276} INFO - 21/05/14 22:56:41 INFO StagingCommitter: Task committer attempt_20210514225616244920328564241984_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616244920328564241984_0004_m_000014_302 : duration 0:00.007s
[2021-05-14 19:56:42,009] {docker.py:276} INFO - 21/05/14 22:56:42 INFO StagingCommitter: Starting: Task committer attempt_20210514225616682077150003180567_0004_m_000011_299: needsTaskCommit() Task attempt_20210514225616682077150003180567_0004_m_000011_299
[2021-05-14 19:56:42,010] {docker.py:276} INFO - 21/05/14 22:56:42 INFO StagingCommitter: Task committer attempt_20210514225616682077150003180567_0004_m_000011_299: needsTaskCommit() Task attempt_20210514225616682077150003180567_0004_m_000011_299: duration 0:00.001s
21/05/14 22:56:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616682077150003180567_0004_m_000011_299
[2021-05-14 19:56:42,012] {docker.py:276} INFO - 21/05/14 22:56:42 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4587 bytes result sent to driver
[2021-05-14 19:56:42,013] {docker.py:276} INFO - 21/05/14 22:56:42 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (5c14c3e96bf4, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:42,015] {docker.py:276} INFO - 21/05/14 22:56:42 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2714 ms on 5c14c3e96bf4 (executor driver) (12/200)
21/05/14 22:56:42 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-14 19:56:42,027] {docker.py:276} INFO - 21/05/14 22:56:42 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:42,027] {docker.py:276} INFO - 21/05/14 22:56:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:42,029] {docker.py:276} INFO - 21/05/14 22:56:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:42,030] {docker.py:276} INFO - 21/05/14 22:56:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165735428651692415750_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165735428651692415750_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165735428651692415750_0004}; taskId=attempt_202105142256165735428651692415750_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11606c23}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:42,030] {docker.py:276} INFO - 21/05/14 22:56:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:42,030] {docker.py:276} INFO - 21/05/14 22:56:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256165735428651692415750_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165735428651692415750_0004_m_000015_303
[2021-05-14 19:56:42,034] {docker.py:276} INFO - 21/05/14 22:56:42 INFO StagingCommitter: Task committer attempt_202105142256165735428651692415750_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165735428651692415750_0004_m_000015_303 : duration 0:00.004s
[2021-05-14 19:56:44,422] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256161185909271520686285_0004_m_000012_300: needsTaskCommit() Task attempt_202105142256161185909271520686285_0004_m_000012_300
[2021-05-14 19:56:44,423] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256161185909271520686285_0004_m_000012_300: needsTaskCommit() Task attempt_202105142256161185909271520686285_0004_m_000012_300: duration 0:00.004s
[2021-05-14 19:56:44,424] {docker.py:276} INFO - 21/05/14 22:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161185909271520686285_0004_m_000012_300
[2021-05-14 19:56:44,426] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4587 bytes result sent to driver
[2021-05-14 19:56:44,429] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (5c14c3e96bf4, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:44,430] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2808 ms on 5c14c3e96bf4 (executor driver) (13/200)
21/05/14 22:56:44 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-14 19:56:44,440] {docker.py:276} INFO - 21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:44,442] {docker.py:276} INFO - 21/05/14 22:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:44,442] {docker.py:276} INFO - 21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163611623418495692316_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163611623418495692316_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163611623418495692316_0004}; taskId=attempt_202105142256163611623418495692316_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38d8c07c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256163611623418495692316_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163611623418495692316_0004_m_000016_304
[2021-05-14 19:56:44,447] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256163611623418495692316_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163611623418495692316_0004_m_000016_304 : duration 0:00.004s
[2021-05-14 19:56:44,607] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256164021028528480325954_0004_m_000013_301: needsTaskCommit() Task attempt_202105142256164021028528480325954_0004_m_000013_301
21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256164021028528480325954_0004_m_000013_301: needsTaskCommit() Task attempt_202105142256164021028528480325954_0004_m_000013_301: duration 0:00.002s
[2021-05-14 19:56:44,608] {docker.py:276} INFO - 21/05/14 22:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164021028528480325954_0004_m_000013_301
[2021-05-14 19:56:44,609] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4587 bytes result sent to driver
[2021-05-14 19:56:44,610] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (5c14c3e96bf4, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:44,611] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
[2021-05-14 19:56:44,611] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2762 ms on 5c14c3e96bf4 (executor driver) (14/200)
[2021-05-14 19:56:44,619] {docker.py:276} INFO - 21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:44,621] {docker.py:276} INFO - 21/05/14 22:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:44,622] {docker.py:276} INFO - 21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166255758252228637721_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166255758252228637721_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166255758252228637721_0004}; taskId=attempt_202105142256166255758252228637721_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b96d484}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:44,622] {docker.py:276} INFO - 21/05/14 22:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256166255758252228637721_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166255758252228637721_0004_m_000017_305
[2021-05-14 19:56:44,628] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256166255758252228637721_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166255758252228637721_0004_m_000017_305 : duration 0:00.006s
[2021-05-14 19:56:44,788] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_20210514225616244920328564241984_0004_m_000014_302: needsTaskCommit() Task attempt_20210514225616244920328564241984_0004_m_000014_302
[2021-05-14 19:56:44,789] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_20210514225616244920328564241984_0004_m_000014_302: needsTaskCommit() Task attempt_20210514225616244920328564241984_0004_m_000014_302: duration 0:00.002s
[2021-05-14 19:56:44,790] {docker.py:276} INFO - 21/05/14 22:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616244920328564241984_0004_m_000014_302
[2021-05-14 19:56:44,791] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4587 bytes result sent to driver
[2021-05-14 19:56:44,793] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (5c14c3e96bf4, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:44,794] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 2938 ms on 5c14c3e96bf4 (executor driver) (15/200)
[2021-05-14 19:56:44,795] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-14 19:56:44,805] {docker.py:276} INFO - 21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:44,808] {docker.py:276} INFO - 21/05/14 22:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164939606057560401049_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164939606057560401049_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164939606057560401049_0004}; taskId=attempt_202105142256164939606057560401049_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37a359d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:44,808] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256164939606057560401049_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164939606057560401049_0004_m_000018_306
[2021-05-14 19:56:44,814] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256164939606057560401049_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164939606057560401049_0004_m_000018_306 : duration 0:00.006s
[2021-05-14 19:56:44,937] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256165735428651692415750_0004_m_000015_303: needsTaskCommit() Task attempt_202105142256165735428651692415750_0004_m_000015_303
[2021-05-14 19:56:44,937] {docker.py:276} INFO - 21/05/14 22:56:44 INFO StagingCommitter: Task committer attempt_202105142256165735428651692415750_0004_m_000015_303: needsTaskCommit() Task attempt_202105142256165735428651692415750_0004_m_000015_303: duration 0:00.003s
21/05/14 22:56:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165735428651692415750_0004_m_000015_303
[2021-05-14 19:56:44,939] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4587 bytes result sent to driver
[2021-05-14 19:56:44,942] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (5c14c3e96bf4, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:44,943] {docker.py:276} INFO - 21/05/14 22:56:44 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-14 19:56:44,945] {docker.py:276} INFO - 21/05/14 22:56:44 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2935 ms on 5c14c3e96bf4 (executor driver) (16/200)
[2021-05-14 19:56:44,966] {docker.py:276} INFO - 21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:44,967] {docker.py:276} INFO - 21/05/14 22:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 19:56:44,968] {docker.py:276} INFO - 21/05/14 22:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:44,969] {docker.py:276} INFO - 21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616393117396970162374_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616393117396970162374_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616393117396970162374_0004}; taskId=attempt_20210514225616393117396970162374_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@335795a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:44,969] {docker.py:276} INFO - 21/05/14 22:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:44 INFO StagingCommitter: Starting: Task committer attempt_20210514225616393117396970162374_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616393117396970162374_0004_m_000019_307
[2021-05-14 19:56:44,974] {docker.py:276} INFO - 21/05/14 22:56:45 INFO StagingCommitter: Task committer attempt_20210514225616393117396970162374_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616393117396970162374_0004_m_000019_307 : duration 0:00.004s
[2021-05-14 19:56:47,110] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256163611623418495692316_0004_m_000016_304: needsTaskCommit() Task attempt_202105142256163611623418495692316_0004_m_000016_304
21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256163611623418495692316_0004_m_000016_304: needsTaskCommit() Task attempt_202105142256163611623418495692316_0004_m_000016_304: duration 0:00.003s
21/05/14 22:56:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163611623418495692316_0004_m_000016_304
[2021-05-14 19:56:47,112] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-14 19:56:47,114] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (5c14c3e96bf4, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:47,115] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2691 ms on 5c14c3e96bf4 (executor driver) (17/200)
[2021-05-14 19:56:47,117] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-14 19:56:47,127] {docker.py:276} INFO - 21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:47,129] {docker.py:276} INFO - 21/05/14 22:56:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:47,130] {docker.py:276} INFO - 21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168035559737830109573_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168035559737830109573_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168035559737830109573_0004}; taskId=attempt_202105142256168035559737830109573_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1602ce0d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:47,130] {docker.py:276} INFO - 21/05/14 22:56:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:47,130] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256168035559737830109573_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168035559737830109573_0004_m_000020_308
[2021-05-14 19:56:47,134] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256168035559737830109573_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168035559737830109573_0004_m_000020_308 : duration 0:00.005s
[2021-05-14 19:56:47,292] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256166255758252228637721_0004_m_000017_305: needsTaskCommit() Task attempt_202105142256166255758252228637721_0004_m_000017_305
[2021-05-14 19:56:47,294] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256166255758252228637721_0004_m_000017_305: needsTaskCommit() Task attempt_202105142256166255758252228637721_0004_m_000017_305: duration 0:00.004s
21/05/14 22:56:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166255758252228637721_0004_m_000017_305
[2021-05-14 19:56:47,296] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-14 19:56:47,297] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (5c14c3e96bf4, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:47,299] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2691 ms on 5c14c3e96bf4 (executor driver) (18/200)
21/05/14 22:56:47 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-14 19:56:47,310] {docker.py:276} INFO - 21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Getting 5 (39.2 KiB) non-empty blocks including 5 (39.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:47,313] {docker.py:276} INFO - 21/05/14 22:56:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:47,313] {docker.py:276} INFO - 21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:47,313] {docker.py:276} INFO - 21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167061083749728888997_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167061083749728888997_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167061083749728888997_0004}; taskId=attempt_202105142256167061083749728888997_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44accea4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:47,314] {docker.py:276} INFO - 21/05/14 22:56:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:47,314] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256167061083749728888997_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167061083749728888997_0004_m_000021_309
[2021-05-14 19:56:47,319] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256167061083749728888997_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167061083749728888997_0004_m_000021_309 : duration 0:00.005s
[2021-05-14 19:56:47,678] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256164939606057560401049_0004_m_000018_306: needsTaskCommit() Task attempt_202105142256164939606057560401049_0004_m_000018_306
[2021-05-14 19:56:47,678] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256164939606057560401049_0004_m_000018_306: needsTaskCommit() Task attempt_202105142256164939606057560401049_0004_m_000018_306: duration 0:00.002s
[2021-05-14 19:56:47,679] {docker.py:276} INFO - 21/05/14 22:56:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164939606057560401049_0004_m_000018_306
[2021-05-14 19:56:47,680] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-14 19:56:47,681] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (5c14c3e96bf4, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:47,682] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2894 ms on 5c14c3e96bf4 (executor driver) (19/200)
[2021-05-14 19:56:47,683] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-14 19:56:47,692] {docker.py:276} INFO - 21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:47,694] {docker.py:276} INFO - 21/05/14 22:56:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166790358755420255558_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166790358755420255558_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166790358755420255558_0004}; taskId=attempt_202105142256166790358755420255558_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23c0f814}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:47,695] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256166790358755420255558_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166790358755420255558_0004_m_000022_310
[2021-05-14 19:56:47,701] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256166790358755420255558_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166790358755420255558_0004_m_000022_310 : duration 0:00.007s
[2021-05-14 19:56:47,824] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_20210514225616393117396970162374_0004_m_000019_307: needsTaskCommit() Task attempt_20210514225616393117396970162374_0004_m_000019_307
[2021-05-14 19:56:47,825] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_20210514225616393117396970162374_0004_m_000019_307: needsTaskCommit() Task attempt_20210514225616393117396970162374_0004_m_000019_307: duration 0:00.003s
[2021-05-14 19:56:47,826] {docker.py:276} INFO - 21/05/14 22:56:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616393117396970162374_0004_m_000019_307
[2021-05-14 19:56:47,828] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-14 19:56:47,829] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (5c14c3e96bf4, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:47,830] {docker.py:276} INFO - 21/05/14 22:56:47 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2893 ms on 5c14c3e96bf4 (executor driver) (20/200)
[2021-05-14 19:56:47,831] {docker.py:276} INFO - 21/05/14 22:56:47 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-14 19:56:47,852] {docker.py:276} INFO - 21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:47,852] {docker.py:276} INFO - 21/05/14 22:56:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:47,854] {docker.py:276} INFO - 21/05/14 22:56:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:47,854] {docker.py:276} INFO - 21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164714943620901990789_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164714943620901990789_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164714943620901990789_0004}; taskId=attempt_202105142256164714943620901990789_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f16183c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:47,855] {docker.py:276} INFO - 21/05/14 22:56:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:47,855] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256164714943620901990789_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164714943620901990789_0004_m_000023_311
[2021-05-14 19:56:47,860] {docker.py:276} INFO - 21/05/14 22:56:47 INFO StagingCommitter: Task committer attempt_202105142256164714943620901990789_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164714943620901990789_0004_m_000023_311 : duration 0:00.004s
[2021-05-14 19:56:49,825] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105142256168035559737830109573_0004_m_000020_308: needsTaskCommit() Task attempt_202105142256168035559737830109573_0004_m_000020_308
[2021-05-14 19:56:49,826] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Task committer attempt_202105142256168035559737830109573_0004_m_000020_308: needsTaskCommit() Task attempt_202105142256168035559737830109573_0004_m_000020_308: duration 0:00.001s
[2021-05-14 19:56:49,826] {docker.py:276} INFO - 21/05/14 22:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168035559737830109573_0004_m_000020_308
[2021-05-14 19:56:49,827] {docker.py:276} INFO - 21/05/14 22:56:49 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4587 bytes result sent to driver
[2021-05-14 19:56:49,829] {docker.py:276} INFO - 21/05/14 22:56:49 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (5c14c3e96bf4, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:49,830] {docker.py:276} INFO - 21/05/14 22:56:49 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2685 ms on 5c14c3e96bf4 (executor driver) (21/200)
[2021-05-14 19:56:49,831] {docker.py:276} INFO - 21/05/14 22:56:49 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-14 19:56:49,839] {docker.py:276} INFO - 21/05/14 22:56:49 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:49,839] {docker.py:276} INFO - 21/05/14 22:56:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:49,841] {docker.py:276} INFO - 21/05/14 22:56:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:49,842] {docker.py:276} INFO - 21/05/14 22:56:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:49,842] {docker.py:276} INFO - 21/05/14 22:56:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:49,843] {docker.py:276} INFO - 21/05/14 22:56:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161703809419133936159_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161703809419133936159_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161703809419133936159_0004}; taskId=attempt_202105142256161703809419133936159_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d9cebda}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:49,843] {docker.py:276} INFO - 21/05/14 22:56:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:49,843] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105142256161703809419133936159_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161703809419133936159_0004_m_000024_312
[2021-05-14 19:56:49,847] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Task committer attempt_202105142256161703809419133936159_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161703809419133936159_0004_m_000024_312 : duration 0:00.005s
[2021-05-14 19:56:49,983] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Starting: Task committer attempt_202105142256167061083749728888997_0004_m_000021_309: needsTaskCommit() Task attempt_202105142256167061083749728888997_0004_m_000021_309
[2021-05-14 19:56:49,983] {docker.py:276} INFO - 21/05/14 22:56:49 INFO StagingCommitter: Task committer attempt_202105142256167061083749728888997_0004_m_000021_309: needsTaskCommit() Task attempt_202105142256167061083749728888997_0004_m_000021_309: duration 0:00.001s
21/05/14 22:56:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167061083749728888997_0004_m_000021_309
[2021-05-14 19:56:49,984] {docker.py:276} INFO - 21/05/14 22:56:49 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4587 bytes result sent to driver
[2021-05-14 19:56:49,992] {docker.py:276} INFO - 21/05/14 22:56:49 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (5c14c3e96bf4, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:49,993] {docker.py:276} INFO - 21/05/14 22:56:49 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-14 19:56:49,993] {docker.py:276} INFO - 21/05/14 22:56:49 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 2665 ms on 5c14c3e96bf4 (executor driver) (22/200)
[2021-05-14 19:56:50,001] {docker.py:276} INFO - 21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:50,003] {docker.py:276} INFO - 21/05/14 22:56:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:50,003] {docker.py:276} INFO - 21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162295776479086913734_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162295776479086913734_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162295776479086913734_0004}; taskId=attempt_202105142256162295776479086913734_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e7af284}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:50,004] {docker.py:276} INFO - 21/05/14 22:56:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256162295776479086913734_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162295776479086913734_0004_m_000025_313
[2021-05-14 19:56:50,007] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Task committer attempt_202105142256162295776479086913734_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162295776479086913734_0004_m_000025_313 : duration 0:00.004s
[2021-05-14 19:56:50,382] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256166790358755420255558_0004_m_000022_310: needsTaskCommit() Task attempt_202105142256166790358755420255558_0004_m_000022_310
[2021-05-14 19:56:50,383] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Task committer attempt_202105142256166790358755420255558_0004_m_000022_310: needsTaskCommit() Task attempt_202105142256166790358755420255558_0004_m_000022_310: duration 0:00.002s
[2021-05-14 19:56:50,383] {docker.py:276} INFO - 21/05/14 22:56:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166790358755420255558_0004_m_000022_310
[2021-05-14 19:56:50,385] {docker.py:276} INFO - 21/05/14 22:56:50 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4587 bytes result sent to driver
[2021-05-14 19:56:50,386] {docker.py:276} INFO - 21/05/14 22:56:50 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (5c14c3e96bf4, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:50,387] {docker.py:276} INFO - 21/05/14 22:56:50 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-14 19:56:50,388] {docker.py:276} INFO - 21/05/14 22:56:50 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2674 ms on 5c14c3e96bf4 (executor driver) (23/200)
[2021-05-14 19:56:50,396] {docker.py:276} INFO - 21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:50,397] {docker.py:276} INFO - 21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:50,399] {docker.py:276} INFO - 21/05/14 22:56:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:50,400] {docker.py:276} INFO - 21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:50,400] {docker.py:276} INFO - 21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164818624105359010382_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164818624105359010382_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164818624105359010382_0004}; taskId=attempt_202105142256164818624105359010382_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46b3ebca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:50,400] {docker.py:276} INFO - 21/05/14 22:56:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:50,401] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256164818624105359010382_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164818624105359010382_0004_m_000026_314
[2021-05-14 19:56:50,402] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Task committer attempt_202105142256164818624105359010382_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164818624105359010382_0004_m_000026_314 : duration 0:00.003s
[2021-05-14 19:56:50,597] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256164714943620901990789_0004_m_000023_311: needsTaskCommit() Task attempt_202105142256164714943620901990789_0004_m_000023_311
[2021-05-14 19:56:50,597] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Task committer attempt_202105142256164714943620901990789_0004_m_000023_311: needsTaskCommit() Task attempt_202105142256164714943620901990789_0004_m_000023_311: duration 0:00.001s
21/05/14 22:56:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164714943620901990789_0004_m_000023_311
[2021-05-14 19:56:50,599] {docker.py:276} INFO - 21/05/14 22:56:50 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4587 bytes result sent to driver
[2021-05-14 19:56:50,601] {docker.py:276} INFO - 21/05/14 22:56:50 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (5c14c3e96bf4, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:50,602] {docker.py:276} INFO - 21/05/14 22:56:50 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-14 19:56:50,603] {docker.py:276} INFO - 21/05/14 22:56:50 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2743 ms on 5c14c3e96bf4 (executor driver) (24/200)
[2021-05-14 19:56:50,621] {docker.py:276} INFO - 21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:50,621] {docker.py:276} INFO - 21/05/14 22:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:50,623] {docker.py:276} INFO - 21/05/14 22:56:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:50,623] {docker.py:276} INFO - 21/05/14 22:56:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165737703917503817066_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165737703917503817066_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165737703917503817066_0004}; taskId=attempt_202105142256165737703917503817066_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23720ef7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:50,623] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256165737703917503817066_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165737703917503817066_0004_m_000027_315
[2021-05-14 19:56:50,625] {docker.py:276} INFO - 21/05/14 22:56:50 INFO StagingCommitter: Task committer attempt_202105142256165737703917503817066_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165737703917503817066_0004_m_000027_315 : duration 0:00.002s
[2021-05-14 19:56:52,181] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256161703809419133936159_0004_m_000024_312: needsTaskCommit() Task attempt_202105142256161703809419133936159_0004_m_000024_312
[2021-05-14 19:56:52,182] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256161703809419133936159_0004_m_000024_312: needsTaskCommit() Task attempt_202105142256161703809419133936159_0004_m_000024_312: duration 0:00.001s
[2021-05-14 19:56:52,183] {docker.py:276} INFO - 21/05/14 22:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161703809419133936159_0004_m_000024_312
[2021-05-14 19:56:52,184] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4587 bytes result sent to driver
[2021-05-14 19:56:52,185] {docker.py:276} INFO - 21/05/14 22:56:52 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (5c14c3e96bf4, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:52,186] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-14 19:56:52,187] {docker.py:276} INFO - 21/05/14 22:56:52 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2361 ms on 5c14c3e96bf4 (executor driver) (25/200)
[2021-05-14 19:56:52,196] {docker.py:276} INFO - 21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:56:52,198] {docker.py:276} INFO - 21/05/14 22:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:52,199] {docker.py:276} INFO - 21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164566025414385121595_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164566025414385121595_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164566025414385121595_0004}; taskId=attempt_202105142256164566025414385121595_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4054985c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:52,199] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256164566025414385121595_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164566025414385121595_0004_m_000028_316
[2021-05-14 19:56:52,202] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256164566025414385121595_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164566025414385121595_0004_m_000028_316 : duration 0:00.003s
[2021-05-14 19:56:52,686] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256162295776479086913734_0004_m_000025_313: needsTaskCommit() Task attempt_202105142256162295776479086913734_0004_m_000025_313
[2021-05-14 19:56:52,687] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256162295776479086913734_0004_m_000025_313: needsTaskCommit() Task attempt_202105142256162295776479086913734_0004_m_000025_313: duration 0:00.001s
21/05/14 22:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162295776479086913734_0004_m_000025_313
[2021-05-14 19:56:52,689] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4587 bytes result sent to driver
[2021-05-14 19:56:52,709] {docker.py:276} INFO - 21/05/14 22:56:52 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (5c14c3e96bf4, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:52,710] {docker.py:276} INFO - 21/05/14 22:56:52 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2710 ms on 5c14c3e96bf4 (executor driver) (26/200)
[2021-05-14 19:56:52,710] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-14 19:56:52,710] {docker.py:276} INFO - 21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:52,711] {docker.py:276} INFO - 21/05/14 22:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:52,711] {docker.py:276} INFO - 21/05/14 22:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:52,711] {docker.py:276} INFO - 21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:52,712] {docker.py:276} INFO - 21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165694200666093521210_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165694200666093521210_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165694200666093521210_0004}; taskId=attempt_202105142256165694200666093521210_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ba3846b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:52,712] {docker.py:276} INFO - 21/05/14 22:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:52,712] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256165694200666093521210_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165694200666093521210_0004_m_000029_317
[2021-05-14 19:56:52,715] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256165694200666093521210_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165694200666093521210_0004_m_000029_317 : duration 0:00.004s
[2021-05-14 19:56:52,874] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256165737703917503817066_0004_m_000027_315: needsTaskCommit() Task attempt_202105142256165737703917503817066_0004_m_000027_315
[2021-05-14 19:56:52,875] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256165737703917503817066_0004_m_000027_315: needsTaskCommit() Task attempt_202105142256165737703917503817066_0004_m_000027_315: duration 0:00.004s
[2021-05-14 19:56:52,876] {docker.py:276} INFO - 21/05/14 22:56:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165737703917503817066_0004_m_000027_315
[2021-05-14 19:56:52,880] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4587 bytes result sent to driver
[2021-05-14 19:56:52,881] {docker.py:276} INFO - 21/05/14 22:56:52 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (5c14c3e96bf4, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:52,906] {docker.py:276} INFO - 21/05/14 22:56:52 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
21/05/14 22:56:52 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2285 ms on 5c14c3e96bf4 (executor driver) (27/200)
[2021-05-14 19:56:52,906] {docker.py:276} INFO - 21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Getting 5 (41.9 KiB) non-empty blocks including 5 (41.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:52,906] {docker.py:276} INFO - 21/05/14 22:56:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:52,907] {docker.py:276} INFO - 21/05/14 22:56:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169065972820948595896_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169065972820948595896_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169065972820948595896_0004}; taskId=attempt_202105142256169065972820948595896_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@526e116d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256169065972820948595896_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169065972820948595896_0004_m_000030_318
[2021-05-14 19:56:52,907] {docker.py:276} INFO - 21/05/14 22:56:52 INFO StagingCommitter: Task committer attempt_202105142256169065972820948595896_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169065972820948595896_0004_m_000030_318 : duration 0:00.002s
[2021-05-14 19:56:53,066] {docker.py:276} INFO - 21/05/14 22:56:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256164818624105359010382_0004_m_000026_314: needsTaskCommit() Task attempt_202105142256164818624105359010382_0004_m_000026_314
[2021-05-14 19:56:53,067] {docker.py:276} INFO - 21/05/14 22:56:53 INFO StagingCommitter: Task committer attempt_202105142256164818624105359010382_0004_m_000026_314: needsTaskCommit() Task attempt_202105142256164818624105359010382_0004_m_000026_314: duration 0:00.001s
21/05/14 22:56:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164818624105359010382_0004_m_000026_314
[2021-05-14 19:56:53,069] {docker.py:276} INFO - 21/05/14 22:56:53 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4587 bytes result sent to driver
[2021-05-14 19:56:53,070] {docker.py:276} INFO - 21/05/14 22:56:53 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (5c14c3e96bf4, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:53,071] {docker.py:276} INFO - 21/05/14 22:56:53 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-14 19:56:53,071] {docker.py:276} INFO - 21/05/14 22:56:53 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2688 ms on 5c14c3e96bf4 (executor driver) (28/200)
[2021-05-14 19:56:53,080] {docker.py:276} INFO - 21/05/14 22:56:53 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:53,080] {docker.py:276} INFO - 21/05/14 22:56:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:53,081] {docker.py:276} INFO - 21/05/14 22:56:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:53,082] {docker.py:276} INFO - 21/05/14 22:56:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:53,082] {docker.py:276} INFO - 21/05/14 22:56:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169206327127489036539_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169206327127489036539_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169206327127489036539_0004}; taskId=attempt_202105142256169206327127489036539_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fc28d3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:53,083] {docker.py:276} INFO - 21/05/14 22:56:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:53,084] {docker.py:276} INFO - 21/05/14 22:56:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256169206327127489036539_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169206327127489036539_0004_m_000031_319
[2021-05-14 19:56:53,086] {docker.py:276} INFO - 21/05/14 22:56:53 INFO StagingCommitter: Task committer attempt_202105142256169206327127489036539_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169206327127489036539_0004_m_000031_319 : duration 0:00.004s
[2021-05-14 19:56:54,849] {docker.py:276} INFO - 21/05/14 22:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105142256164566025414385121595_0004_m_000028_316: needsTaskCommit() Task attempt_202105142256164566025414385121595_0004_m_000028_316
21/05/14 22:56:54 INFO StagingCommitter: Task committer attempt_202105142256164566025414385121595_0004_m_000028_316: needsTaskCommit() Task attempt_202105142256164566025414385121595_0004_m_000028_316: duration 0:00.003s
21/05/14 22:56:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164566025414385121595_0004_m_000028_316
[2021-05-14 19:56:54,851] {docker.py:276} INFO - 21/05/14 22:56:54 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
[2021-05-14 19:56:54,853] {docker.py:276} INFO - 21/05/14 22:56:54 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (5c14c3e96bf4, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:54,854] {docker.py:276} INFO - 21/05/14 22:56:54 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2672 ms on 5c14c3e96bf4 (executor driver) (29/200)
21/05/14 22:56:54 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-14 19:56:54,864] {docker.py:276} INFO - 21/05/14 22:56:54 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:54,865] {docker.py:276} INFO - 21/05/14 22:56:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:56:54,867] {docker.py:276} INFO - 21/05/14 22:56:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:54,868] {docker.py:276} INFO - 21/05/14 22:56:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163285557330898729622_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163285557330898729622_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163285557330898729622_0004}; taskId=attempt_202105142256163285557330898729622_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@df148a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:54,868] {docker.py:276} INFO - 21/05/14 22:56:54 INFO StagingCommitter: Starting: Task committer attempt_202105142256163285557330898729622_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163285557330898729622_0004_m_000032_320
[2021-05-14 19:56:54,870] {docker.py:276} INFO - 21/05/14 22:56:54 INFO StagingCommitter: Task committer attempt_202105142256163285557330898729622_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163285557330898729622_0004_m_000032_320 : duration 0:00.002s
[2021-05-14 19:56:55,630] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256169065972820948595896_0004_m_000030_318: needsTaskCommit() Task attempt_202105142256169065972820948595896_0004_m_000030_318
[2021-05-14 19:56:55,632] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Task committer attempt_202105142256169065972820948595896_0004_m_000030_318: needsTaskCommit() Task attempt_202105142256169065972820948595896_0004_m_000030_318: duration 0:00.002s
21/05/14 22:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169065972820948595896_0004_m_000030_318
[2021-05-14 19:56:55,633] {docker.py:276} INFO - 21/05/14 22:56:55 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4587 bytes result sent to driver
[2021-05-14 19:56:55,642] {docker.py:276} INFO - 21/05/14 22:56:55 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (5c14c3e96bf4, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:55,643] {docker.py:276} INFO - 21/05/14 22:56:55 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-14 19:56:55,644] {docker.py:276} INFO - 21/05/14 22:56:55 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2765 ms on 5c14c3e96bf4 (executor driver) (30/200)
[2021-05-14 19:56:55,652] {docker.py:276} INFO - 21/05/14 22:56:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:55,652] {docker.py:276} INFO - 21/05/14 22:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:55,654] {docker.py:276} INFO - 21/05/14 22:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:55,654] {docker.py:276} INFO - 21/05/14 22:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162538584457489981876_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162538584457489981876_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162538584457489981876_0004}; taskId=attempt_202105142256162538584457489981876_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@192426af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256162538584457489981876_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162538584457489981876_0004_m_000033_321
[2021-05-14 19:56:55,658] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Task committer attempt_202105142256162538584457489981876_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162538584457489981876_0004_m_000033_321 : duration 0:00.003s
[2021-05-14 19:56:55,809] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256169206327127489036539_0004_m_000031_319: needsTaskCommit() Task attempt_202105142256169206327127489036539_0004_m_000031_319
[2021-05-14 19:56:55,811] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Task committer attempt_202105142256169206327127489036539_0004_m_000031_319: needsTaskCommit() Task attempt_202105142256169206327127489036539_0004_m_000031_319: duration 0:00.002s
21/05/14 22:56:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169206327127489036539_0004_m_000031_319
[2021-05-14 19:56:55,812] {docker.py:276} INFO - 21/05/14 22:56:55 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4587 bytes result sent to driver
[2021-05-14 19:56:55,813] {docker.py:276} INFO - 21/05/14 22:56:55 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (5c14c3e96bf4, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:55,815] {docker.py:276} INFO - 21/05/14 22:56:55 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2747 ms on 5c14c3e96bf4 (executor driver) (31/200)
[2021-05-14 19:56:55,815] {docker.py:276} INFO - 21/05/14 22:56:55 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-14 19:56:55,825] {docker.py:276} INFO - 21/05/14 22:56:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:55,828] {docker.py:276} INFO - 21/05/14 22:56:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165343131032139411451_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165343131032139411451_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165343131032139411451_0004}; taskId=attempt_202105142256165343131032139411451_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e1ae1cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:55,828] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256165343131032139411451_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165343131032139411451_0004_m_000034_322
[2021-05-14 19:56:55,831] {docker.py:276} INFO - 21/05/14 22:56:55 INFO StagingCommitter: Task committer attempt_202105142256165343131032139411451_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165343131032139411451_0004_m_000034_322 : duration 0:00.003s
[2021-05-14 19:56:56,016] {docker.py:276} INFO - 21/05/14 22:56:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256165694200666093521210_0004_m_000029_317: needsTaskCommit() Task attempt_202105142256165694200666093521210_0004_m_000029_317
[2021-05-14 19:56:56,030] {docker.py:276} INFO - 21/05/14 22:56:56 INFO StagingCommitter: Task committer attempt_202105142256165694200666093521210_0004_m_000029_317: needsTaskCommit() Task attempt_202105142256165694200666093521210_0004_m_000029_317: duration 0:00.003s
[2021-05-14 19:56:56,031] {docker.py:276} INFO - 21/05/14 22:56:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165694200666093521210_0004_m_000029_317
[2021-05-14 19:56:56,031] {docker.py:276} INFO - 21/05/14 22:56:56 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4587 bytes result sent to driver
[2021-05-14 19:56:56,031] {docker.py:276} INFO - 21/05/14 22:56:56 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (5c14c3e96bf4, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:56,032] {docker.py:276} INFO - 21/05/14 22:56:56 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 3335 ms on 5c14c3e96bf4 (executor driver) (32/200)
[2021-05-14 19:56:56,032] {docker.py:276} INFO - 21/05/14 22:56:56 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
[2021-05-14 19:56:56,032] {docker.py:276} INFO - 21/05/14 22:56:56 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:56,033] {docker.py:276} INFO - 21/05/14 22:56:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:56:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:56,034] {docker.py:276} INFO - 21/05/14 22:56:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162861859255597429722_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162861859255597429722_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162861859255597429722_0004}; taskId=attempt_202105142256162861859255597429722_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@301f848d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:56,034] {docker.py:276} INFO - 21/05/14 22:56:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256162861859255597429722_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162861859255597429722_0004_m_000035_323
[2021-05-14 19:56:56,037] {docker.py:276} INFO - 21/05/14 22:56:56 INFO StagingCommitter: Task committer attempt_202105142256162861859255597429722_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162861859255597429722_0004_m_000035_323 : duration 0:00.003s
[2021-05-14 19:56:57,513] {docker.py:276} INFO - 21/05/14 22:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105142256163285557330898729622_0004_m_000032_320: needsTaskCommit() Task attempt_202105142256163285557330898729622_0004_m_000032_320
[2021-05-14 19:56:57,514] {docker.py:276} INFO - 21/05/14 22:56:57 INFO StagingCommitter: Task committer attempt_202105142256163285557330898729622_0004_m_000032_320: needsTaskCommit() Task attempt_202105142256163285557330898729622_0004_m_000032_320: duration 0:00.001s
[2021-05-14 19:56:57,515] {docker.py:276} INFO - 21/05/14 22:56:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163285557330898729622_0004_m_000032_320
[2021-05-14 19:56:57,517] {docker.py:276} INFO - 21/05/14 22:56:57 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4587 bytes result sent to driver
[2021-05-14 19:56:57,518] {docker.py:276} INFO - 21/05/14 22:56:57 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (5c14c3e96bf4, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:57,519] {docker.py:276} INFO - 21/05/14 22:56:57 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2670 ms on 5c14c3e96bf4 (executor driver) (33/200)
[2021-05-14 19:56:57,520] {docker.py:276} INFO - 21/05/14 22:56:57 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-14 19:56:57,529] {docker.py:276} INFO - 21/05/14 22:56:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:57,530] {docker.py:276} INFO - 21/05/14 22:56:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:57,532] {docker.py:276} INFO - 21/05/14 22:56:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:57,533] {docker.py:276} INFO - 21/05/14 22:56:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:57,533] {docker.py:276} INFO - 21/05/14 22:56:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:57,534] {docker.py:276} INFO - 21/05/14 22:56:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162608253059627397494_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162608253059627397494_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162608253059627397494_0004}; taskId=attempt_202105142256162608253059627397494_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b23e0fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:57,534] {docker.py:276} INFO - 21/05/14 22:56:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:57,534] {docker.py:276} INFO - 21/05/14 22:56:57 INFO StagingCommitter: Starting: Task committer attempt_202105142256162608253059627397494_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162608253059627397494_0004_m_000036_324
[2021-05-14 19:56:57,537] {docker.py:276} INFO - 21/05/14 22:56:57 INFO StagingCommitter: Task committer attempt_202105142256162608253059627397494_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162608253059627397494_0004_m_000036_324 : duration 0:00.003s
[2021-05-14 19:56:58,364] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105142256162538584457489981876_0004_m_000033_321: needsTaskCommit() Task attempt_202105142256162538584457489981876_0004_m_000033_321
[2021-05-14 19:56:58,365] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_202105142256162538584457489981876_0004_m_000033_321: needsTaskCommit() Task attempt_202105142256162538584457489981876_0004_m_000033_321: duration 0:00.001s
[2021-05-14 19:56:58,365] {docker.py:276} INFO - 21/05/14 22:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162538584457489981876_0004_m_000033_321
[2021-05-14 19:56:58,366] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4587 bytes result sent to driver
[2021-05-14 19:56:58,366] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (5c14c3e96bf4, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:58,368] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2729 ms on 5c14c3e96bf4 (executor driver) (34/200)
[2021-05-14 19:56:58,369] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-14 19:56:58,378] {docker.py:276} INFO - 21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:58,379] {docker.py:276} INFO - 21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:58,381] {docker.py:276} INFO - 21/05/14 22:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:56:58,382] {docker.py:276} INFO - 21/05/14 22:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:58,382] {docker.py:276} INFO - 21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168890762541943982795_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168890762541943982795_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168890762541943982795_0004}; taskId=attempt_202105142256168890762541943982795_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@266348a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:56:58,383] {docker.py:276} INFO - 21/05/14 22:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:56:58,383] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105142256168890762541943982795_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168890762541943982795_0004_m_000037_325
[2021-05-14 19:56:58,386] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_202105142256168890762541943982795_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168890762541943982795_0004_m_000037_325 : duration 0:00.004s
[2021-05-14 19:56:58,460] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105142256165343131032139411451_0004_m_000034_322: needsTaskCommit() Task attempt_202105142256165343131032139411451_0004_m_000034_322
[2021-05-14 19:56:58,462] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_202105142256165343131032139411451_0004_m_000034_322: needsTaskCommit() Task attempt_202105142256165343131032139411451_0004_m_000034_322: duration 0:00.001s
21/05/14 22:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165343131032139411451_0004_m_000034_322
[2021-05-14 19:56:58,463] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4587 bytes result sent to driver
[2021-05-14 19:56:58,465] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (5c14c3e96bf4, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:58,465] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2656 ms on 5c14c3e96bf4 (executor driver) (35/200)
[2021-05-14 19:56:58,468] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-14 19:56:58,483] {docker.py:276} INFO - 21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Getting 5 (40.0 KiB) non-empty blocks including 5 (40.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:58,485] {docker.py:276} INFO - 21/05/14 22:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:58,486] {docker.py:276} INFO - 21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616971130246491094719_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616971130246491094719_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616971130246491094719_0004}; taskId=attempt_20210514225616971130246491094719_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a8a7935}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_20210514225616971130246491094719_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616971130246491094719_0004_m_000038_326
[2021-05-14 19:56:58,488] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_20210514225616971130246491094719_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616971130246491094719_0004_m_000038_326 : duration 0:00.003s
[2021-05-14 19:56:58,737] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105142256162861859255597429722_0004_m_000035_323: needsTaskCommit() Task attempt_202105142256162861859255597429722_0004_m_000035_323
[2021-05-14 19:56:58,737] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_202105142256162861859255597429722_0004_m_000035_323: needsTaskCommit() Task attempt_202105142256162861859255597429722_0004_m_000035_323: duration 0:00.001s
21/05/14 22:56:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162861859255597429722_0004_m_000035_323
[2021-05-14 19:56:58,739] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4587 bytes result sent to driver
[2021-05-14 19:56:58,741] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (5c14c3e96bf4, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:56:58,742] {docker.py:276} INFO - 21/05/14 22:56:58 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-14 19:56:58,742] {docker.py:276} INFO - 21/05/14 22:56:58 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2726 ms on 5c14c3e96bf4 (executor driver) (36/200)
[2021-05-14 19:56:58,758] {docker.py:276} INFO - 21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:56:58,758] {docker.py:276} INFO - 21/05/14 22:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:56:58,760] {docker.py:276} INFO - 21/05/14 22:56:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:56:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:56:58,760] {docker.py:276} INFO - 21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:56:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163831597844899793667_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163831597844899793667_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163831597844899793667_0004}; taskId=attempt_202105142256163831597844899793667_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@314a643f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:56:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:56:58 INFO StagingCommitter: Starting: Task committer attempt_202105142256163831597844899793667_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163831597844899793667_0004_m_000039_327
[2021-05-14 19:56:58,763] {docker.py:276} INFO - 21/05/14 22:56:58 INFO StagingCommitter: Task committer attempt_202105142256163831597844899793667_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163831597844899793667_0004_m_000039_327 : duration 0:00.002s
[2021-05-14 19:57:00,324] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256162608253059627397494_0004_m_000036_324: needsTaskCommit() Task attempt_202105142256162608253059627397494_0004_m_000036_324
[2021-05-14 19:57:00,324] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_202105142256162608253059627397494_0004_m_000036_324: needsTaskCommit() Task attempt_202105142256162608253059627397494_0004_m_000036_324: duration 0:00.001s
21/05/14 22:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162608253059627397494_0004_m_000036_324
[2021-05-14 19:57:00,326] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4587 bytes result sent to driver
[2021-05-14 19:57:00,327] {docker.py:276} INFO - 21/05/14 22:57:00 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (5c14c3e96bf4, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:00,328] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
21/05/14 22:57:00 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2813 ms on 5c14c3e96bf4 (executor driver) (37/200)
[2021-05-14 19:57:00,348] {docker.py:276} INFO - 21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:00,348] {docker.py:276} INFO - 21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:00,350] {docker.py:276} INFO - 21/05/14 22:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:00,351] {docker.py:276} INFO - 21/05/14 22:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:00,351] {docker.py:276} INFO - 21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:00,353] {docker.py:276} INFO - 21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164152114290888625291_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164152114290888625291_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164152114290888625291_0004}; taskId=attempt_202105142256164152114290888625291_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@333b062f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:00,353] {docker.py:276} INFO - 21/05/14 22:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:00,353] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256164152114290888625291_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164152114290888625291_0004_m_000040_328
[2021-05-14 19:57:00,356] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_202105142256164152114290888625291_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164152114290888625291_0004_m_000040_328 : duration 0:00.004s
[2021-05-14 19:57:00,506] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256168890762541943982795_0004_m_000037_325: needsTaskCommit() Task attempt_202105142256168890762541943982795_0004_m_000037_325
21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_202105142256168890762541943982795_0004_m_000037_325: needsTaskCommit() Task attempt_202105142256168890762541943982795_0004_m_000037_325: duration 0:00.001s
[2021-05-14 19:57:00,508] {docker.py:276} INFO - 21/05/14 22:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168890762541943982795_0004_m_000037_325
[2021-05-14 19:57:00,510] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4587 bytes result sent to driver
[2021-05-14 19:57:00,511] {docker.py:276} INFO - 21/05/14 22:57:00 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (5c14c3e96bf4, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:00,512] {docker.py:276} INFO - 21/05/14 22:57:00 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2148 ms on 5c14c3e96bf4 (executor driver) (38/200)
[2021-05-14 19:57:00,513] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-14 19:57:00,527] {docker.py:276} INFO - 21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:00,529] {docker.py:276} INFO - 21/05/14 22:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:00,529] {docker.py:276} INFO - 21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162456430076449577932_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162456430076449577932_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162456430076449577932_0004}; taskId=attempt_202105142256162456430076449577932_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30d084bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:00,529] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256162456430076449577932_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162456430076449577932_0004_m_000041_329
[2021-05-14 19:57:00,531] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_202105142256162456430076449577932_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162456430076449577932_0004_m_000041_329 : duration 0:00.002s
[2021-05-14 19:57:00,680] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_20210514225616971130246491094719_0004_m_000038_326: needsTaskCommit() Task attempt_20210514225616971130246491094719_0004_m_000038_326
[2021-05-14 19:57:00,681] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_20210514225616971130246491094719_0004_m_000038_326: needsTaskCommit() Task attempt_20210514225616971130246491094719_0004_m_000038_326: duration 0:00.001s
21/05/14 22:57:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616971130246491094719_0004_m_000038_326
[2021-05-14 19:57:00,683] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4587 bytes result sent to driver
[2021-05-14 19:57:00,684] {docker.py:276} INFO - 21/05/14 22:57:00 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (5c14c3e96bf4, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:00,684] {docker.py:276} INFO - 21/05/14 22:57:00 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
21/05/14 22:57:00 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 2223 ms on 5c14c3e96bf4 (executor driver) (39/200)
[2021-05-14 19:57:00,699] {docker.py:276} INFO - 21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Getting 5 (39.2 KiB) non-empty blocks including 5 (39.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:00,708] {docker.py:276} INFO - 21/05/14 22:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:00,709] {docker.py:276} INFO - 21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:00,709] {docker.py:276} INFO - 21/05/14 22:57:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166775401458342481907_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166775401458342481907_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166775401458342481907_0004}; taskId=attempt_202105142256166775401458342481907_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2db80c00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:00,709] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256166775401458342481907_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166775401458342481907_0004_m_000042_330
[2021-05-14 19:57:00,709] {docker.py:276} INFO - 21/05/14 22:57:00 INFO StagingCommitter: Task committer attempt_202105142256166775401458342481907_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166775401458342481907_0004_m_000042_330 : duration 0:00.003s
[2021-05-14 19:57:01,382] {docker.py:276} INFO - 21/05/14 22:57:01 INFO StagingCommitter: Starting: Task committer attempt_202105142256163831597844899793667_0004_m_000039_327: needsTaskCommit() Task attempt_202105142256163831597844899793667_0004_m_000039_327
[2021-05-14 19:57:01,383] {docker.py:276} INFO - 21/05/14 22:57:01 INFO StagingCommitter: Task committer attempt_202105142256163831597844899793667_0004_m_000039_327: needsTaskCommit() Task attempt_202105142256163831597844899793667_0004_m_000039_327: duration 0:00.001s
[2021-05-14 19:57:01,384] {docker.py:276} INFO - 21/05/14 22:57:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163831597844899793667_0004_m_000039_327
[2021-05-14 19:57:01,387] {docker.py:276} INFO - 21/05/14 22:57:01 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4587 bytes result sent to driver
[2021-05-14 19:57:01,388] {docker.py:276} INFO - 21/05/14 22:57:01 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (5c14c3e96bf4, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:01,388] {docker.py:276} INFO - 21/05/14 22:57:01 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2651 ms on 5c14c3e96bf4 (executor driver) (40/200)
[2021-05-14 19:57:01,389] {docker.py:276} INFO - 21/05/14 22:57:01 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-14 19:57:01,403] {docker.py:276} INFO - 21/05/14 22:57:01 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:01,405] {docker.py:276} INFO - 21/05/14 22:57:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:01,406] {docker.py:276} INFO - 21/05/14 22:57:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167394207737335523667_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167394207737335523667_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167394207737335523667_0004}; taskId=attempt_202105142256167394207737335523667_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25c58db3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:01,406] {docker.py:276} INFO - 21/05/14 22:57:01 INFO StagingCommitter: Starting: Task committer attempt_202105142256167394207737335523667_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167394207737335523667_0004_m_000043_331
[2021-05-14 19:57:01,409] {docker.py:276} INFO - 21/05/14 22:57:01 INFO StagingCommitter: Task committer attempt_202105142256167394207737335523667_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167394207737335523667_0004_m_000043_331 : duration 0:00.003s
[2021-05-14 19:57:03,283] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256166775401458342481907_0004_m_000042_330: needsTaskCommit() Task attempt_202105142256166775401458342481907_0004_m_000042_330
[2021-05-14 19:57:03,284] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256166775401458342481907_0004_m_000042_330: needsTaskCommit() Task attempt_202105142256166775401458342481907_0004_m_000042_330: duration 0:00.002s
[2021-05-14 19:57:03,285] {docker.py:276} INFO - 21/05/14 22:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166775401458342481907_0004_m_000042_330
[2021-05-14 19:57:03,286] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4587 bytes result sent to driver
[2021-05-14 19:57:03,287] {docker.py:276} INFO - 21/05/14 22:57:03 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (5c14c3e96bf4, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:03,288] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-14 19:57:03,289] {docker.py:276} INFO - 21/05/14 22:57:03 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2608 ms on 5c14c3e96bf4 (executor driver) (41/200)
[2021-05-14 19:57:03,303] {docker.py:276} INFO - 21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:03,305] {docker.py:276} INFO - 21/05/14 22:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:03,305] {docker.py:276} INFO - 21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162013883047108525760_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162013883047108525760_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162013883047108525760_0004}; taskId=attempt_202105142256162013883047108525760_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bd48981}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256162013883047108525760_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162013883047108525760_0004_m_000044_332
[2021-05-14 19:57:03,307] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256162013883047108525760_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162013883047108525760_0004_m_000044_332 : duration 0:00.002s
[2021-05-14 19:57:03,585] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256164152114290888625291_0004_m_000040_328: needsTaskCommit() Task attempt_202105142256164152114290888625291_0004_m_000040_328
21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256164152114290888625291_0004_m_000040_328: needsTaskCommit() Task attempt_202105142256164152114290888625291_0004_m_000040_328: duration 0:00.000s
[2021-05-14 19:57:03,585] {docker.py:276} INFO - 21/05/14 22:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164152114290888625291_0004_m_000040_328
[2021-05-14 19:57:03,588] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4587 bytes result sent to driver
[2021-05-14 19:57:03,590] {docker.py:276} INFO - 21/05/14 22:57:03 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (5c14c3e96bf4, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:03,590] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-14 19:57:03,591] {docker.py:276} INFO - 21/05/14 22:57:03 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 3268 ms on 5c14c3e96bf4 (executor driver) (42/200)
[2021-05-14 19:57:03,600] {docker.py:276} INFO - 21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:03,603] {docker.py:276} INFO - 21/05/14 22:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:03,603] {docker.py:276} INFO - 21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165248652409431514174_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165248652409431514174_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165248652409431514174_0004}; taskId=attempt_202105142256165248652409431514174_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76b09f47}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:03,603] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256165248652409431514174_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165248652409431514174_0004_m_000045_333
[2021-05-14 19:57:03,606] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256165248652409431514174_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165248652409431514174_0004_m_000045_333 : duration 0:00.003s
[2021-05-14 19:57:03,863] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256162456430076449577932_0004_m_000041_329: needsTaskCommit() Task attempt_202105142256162456430076449577932_0004_m_000041_329
[2021-05-14 19:57:03,864] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256162456430076449577932_0004_m_000041_329: needsTaskCommit() Task attempt_202105142256162456430076449577932_0004_m_000041_329: duration 0:00.000s
21/05/14 22:57:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162456430076449577932_0004_m_000041_329
[2021-05-14 19:57:03,866] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-14 19:57:03,867] {docker.py:276} INFO - 21/05/14 22:57:03 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (5c14c3e96bf4, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:03,869] {docker.py:276} INFO - 21/05/14 22:57:03 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
21/05/14 22:57:03 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 3361 ms on 5c14c3e96bf4 (executor driver) (43/200)
[2021-05-14 19:57:03,883] {docker.py:276} INFO - 21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:03,885] {docker.py:276} INFO - 21/05/14 22:57:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:03,886] {docker.py:276} INFO - 21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168705389329825048163_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168705389329825048163_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168705389329825048163_0004}; taskId=attempt_202105142256168705389329825048163_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@422215cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:03,886] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Starting: Task committer attempt_202105142256168705389329825048163_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168705389329825048163_0004_m_000046_334
[2021-05-14 19:57:03,889] {docker.py:276} INFO - 21/05/14 22:57:03 INFO StagingCommitter: Task committer attempt_202105142256168705389329825048163_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168705389329825048163_0004_m_000046_334 : duration 0:00.003s
[2021-05-14 19:57:04,105] {docker.py:276} INFO - 21/05/14 22:57:04 INFO StagingCommitter: Starting: Task committer attempt_202105142256167394207737335523667_0004_m_000043_331: needsTaskCommit() Task attempt_202105142256167394207737335523667_0004_m_000043_331
[2021-05-14 19:57:04,107] {docker.py:276} INFO - 21/05/14 22:57:04 INFO StagingCommitter: Task committer attempt_202105142256167394207737335523667_0004_m_000043_331: needsTaskCommit() Task attempt_202105142256167394207737335523667_0004_m_000043_331: duration 0:00.001s
21/05/14 22:57:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167394207737335523667_0004_m_000043_331
[2021-05-14 19:57:04,110] {docker.py:276} INFO - 21/05/14 22:57:04 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4587 bytes result sent to driver
[2021-05-14 19:57:04,111] {docker.py:276} INFO - 21/05/14 22:57:04 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (5c14c3e96bf4, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:04,112] {docker.py:276} INFO - 21/05/14 22:57:04 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-14 19:57:04,112] {docker.py:276} INFO - 21/05/14 22:57:04 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2729 ms on 5c14c3e96bf4 (executor driver) (44/200)
[2021-05-14 19:57:04,127] {docker.py:276} INFO - 21/05/14 22:57:04 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:04,128] {docker.py:276} INFO - 21/05/14 22:57:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:04,129] {docker.py:276} INFO - 21/05/14 22:57:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:04,129] {docker.py:276} INFO - 21/05/14 22:57:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164737135850889943405_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164737135850889943405_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164737135850889943405_0004}; taskId=attempt_202105142256164737135850889943405_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@776cc059}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:04,129] {docker.py:276} INFO - 21/05/14 22:57:04 INFO StagingCommitter: Starting: Task committer attempt_202105142256164737135850889943405_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164737135850889943405_0004_m_000047_335
[2021-05-14 19:57:04,132] {docker.py:276} INFO - 21/05/14 22:57:04 INFO StagingCommitter: Task committer attempt_202105142256164737135850889943405_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164737135850889943405_0004_m_000047_335 : duration 0:00.003s
[2021-05-14 19:57:06,019] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256162013883047108525760_0004_m_000044_332: needsTaskCommit() Task attempt_202105142256162013883047108525760_0004_m_000044_332
[2021-05-14 19:57:06,020] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256162013883047108525760_0004_m_000044_332: needsTaskCommit() Task attempt_202105142256162013883047108525760_0004_m_000044_332: duration 0:00.002s
21/05/14 22:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162013883047108525760_0004_m_000044_332
[2021-05-14 19:57:06,022] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4587 bytes result sent to driver
[2021-05-14 19:57:06,023] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (5c14c3e96bf4, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:06,025] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
21/05/14 22:57:06 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2741 ms on 5c14c3e96bf4 (executor driver) (45/200)
[2021-05-14 19:57:06,039] {docker.py:276} INFO - 21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:06,041] {docker.py:276} INFO - 21/05/14 22:57:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164458636929258372795_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164458636929258372795_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164458636929258372795_0004}; taskId=attempt_202105142256164458636929258372795_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ea3d5df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:06,042] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256164458636929258372795_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164458636929258372795_0004_m_000048_336
[2021-05-14 19:57:06,044] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256164458636929258372795_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164458636929258372795_0004_m_000048_336 : duration 0:00.002s
[2021-05-14 19:57:06,226] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256165248652409431514174_0004_m_000045_333: needsTaskCommit() Task attempt_202105142256165248652409431514174_0004_m_000045_333
[2021-05-14 19:57:06,227] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256165248652409431514174_0004_m_000045_333: needsTaskCommit() Task attempt_202105142256165248652409431514174_0004_m_000045_333: duration 0:00.001s
21/05/14 22:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165248652409431514174_0004_m_000045_333
[2021-05-14 19:57:06,228] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4587 bytes result sent to driver
[2021-05-14 19:57:06,230] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (5c14c3e96bf4, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:06,231] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2645 ms on 5c14c3e96bf4 (executor driver) (46/200)
[2021-05-14 19:57:06,232] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-14 19:57:06,247] {docker.py:276} INFO - 21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Getting 5 (44.5 KiB) non-empty blocks including 5 (44.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:06,249] {docker.py:276} INFO - 21/05/14 22:57:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:06,249] {docker.py:276} INFO - 21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164311775204901316436_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164311775204901316436_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164311775204901316436_0004}; taskId=attempt_202105142256164311775204901316436_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f740a50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256164311775204901316436_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164311775204901316436_0004_m_000049_337
[2021-05-14 19:57:06,252] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256164311775204901316436_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164311775204901316436_0004_m_000049_337 : duration 0:00.002s
[2021-05-14 19:57:06,738] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256168705389329825048163_0004_m_000046_334: needsTaskCommit() Task attempt_202105142256168705389329825048163_0004_m_000046_334
[2021-05-14 19:57:06,739] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256168705389329825048163_0004_m_000046_334: needsTaskCommit() Task attempt_202105142256168705389329825048163_0004_m_000046_334: duration 0:00.002s
21/05/14 22:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168705389329825048163_0004_m_000046_334
[2021-05-14 19:57:06,741] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4587 bytes result sent to driver
[2021-05-14 19:57:06,743] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (5c14c3e96bf4, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:06,744] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2880 ms on 5c14c3e96bf4 (executor driver) (47/200)
[2021-05-14 19:57:06,745] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-14 19:57:06,755] {docker.py:276} INFO - 21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:06,757] {docker.py:276} INFO - 21/05/14 22:57:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163223975067260873410_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163223975067260873410_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163223975067260873410_0004}; taskId=attempt_202105142256163223975067260873410_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ddfa080}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256163223975067260873410_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163223975067260873410_0004_m_000050_338
[2021-05-14 19:57:06,761] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256163223975067260873410_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163223975067260873410_0004_m_000050_338 : duration 0:00.005s
[2021-05-14 19:57:06,843] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256164737135850889943405_0004_m_000047_335: needsTaskCommit() Task attempt_202105142256164737135850889943405_0004_m_000047_335
[2021-05-14 19:57:06,844] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256164737135850889943405_0004_m_000047_335: needsTaskCommit() Task attempt_202105142256164737135850889943405_0004_m_000047_335: duration 0:00.001s
21/05/14 22:57:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164737135850889943405_0004_m_000047_335
[2021-05-14 19:57:06,846] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4587 bytes result sent to driver
[2021-05-14 19:57:06,848] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (5c14c3e96bf4, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:06,849] {docker.py:276} INFO - 21/05/14 22:57:06 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2741 ms on 5c14c3e96bf4 (executor driver) (48/200)
[2021-05-14 19:57:06,850] {docker.py:276} INFO - 21/05/14 22:57:06 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-14 19:57:06,860] {docker.py:276} INFO - 21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Getting 5 (41.0 KiB) non-empty blocks including 5 (41.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:06,862] {docker.py:276} INFO - 21/05/14 22:57:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:06,863] {docker.py:276} INFO - 21/05/14 22:57:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164585064779906858494_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164585064779906858494_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164585064779906858494_0004}; taskId=attempt_202105142256164585064779906858494_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d345809}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:06,863] {docker.py:276} INFO - 21/05/14 22:57:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:06,863] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Starting: Task committer attempt_202105142256164585064779906858494_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164585064779906858494_0004_m_000051_339
[2021-05-14 19:57:06,867] {docker.py:276} INFO - 21/05/14 22:57:06 INFO StagingCommitter: Task committer attempt_202105142256164585064779906858494_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164585064779906858494_0004_m_000051_339 : duration 0:00.005s
[2021-05-14 19:57:08,779] {docker.py:276} INFO - 21/05/14 22:57:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256164458636929258372795_0004_m_000048_336: needsTaskCommit() Task attempt_202105142256164458636929258372795_0004_m_000048_336
[2021-05-14 19:57:08,779] {docker.py:276} INFO - 21/05/14 22:57:08 INFO StagingCommitter: Task committer attempt_202105142256164458636929258372795_0004_m_000048_336: needsTaskCommit() Task attempt_202105142256164458636929258372795_0004_m_000048_336: duration 0:00.001s
21/05/14 22:57:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164458636929258372795_0004_m_000048_336
[2021-05-14 19:57:08,781] {docker.py:276} INFO - 21/05/14 22:57:08 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4587 bytes result sent to driver
[2021-05-14 19:57:08,787] {docker.py:276} INFO - 21/05/14 22:57:08 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (5c14c3e96bf4, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:08,787] {docker.py:276} INFO - 21/05/14 22:57:08 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-14 19:57:08,788] {docker.py:276} INFO - 21/05/14 22:57:08 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 2768 ms on 5c14c3e96bf4 (executor driver) (49/200)
[2021-05-14 19:57:08,795] {docker.py:276} INFO - 21/05/14 22:57:08 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:08,797] {docker.py:276} INFO - 21/05/14 22:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164182991241050986509_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164182991241050986509_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164182991241050986509_0004}; taskId=attempt_202105142256164182991241050986509_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f196ff6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256164182991241050986509_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164182991241050986509_0004_m_000052_340
[2021-05-14 19:57:08,800] {docker.py:276} INFO - 21/05/14 22:57:08 INFO StagingCommitter: Task committer attempt_202105142256164182991241050986509_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164182991241050986509_0004_m_000052_340 : duration 0:00.003s
[2021-05-14 19:57:09,005] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_202105142256164311775204901316436_0004_m_000049_337: needsTaskCommit() Task attempt_202105142256164311775204901316436_0004_m_000049_337
[2021-05-14 19:57:09,005] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_202105142256164311775204901316436_0004_m_000049_337: needsTaskCommit() Task attempt_202105142256164311775204901316436_0004_m_000049_337: duration 0:00.000s
21/05/14 22:57:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164311775204901316436_0004_m_000049_337
[2021-05-14 19:57:09,007] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4587 bytes result sent to driver
[2021-05-14 19:57:09,018] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (5c14c3e96bf4, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:09,018] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-14 19:57:09,019] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2793 ms on 5c14c3e96bf4 (executor driver) (50/200)
[2021-05-14 19:57:09,028] {docker.py:276} INFO - 21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:09,031] {docker.py:276} INFO - 21/05/14 22:57:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:09,031] {docker.py:276} INFO - 21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616638871201451710592_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616638871201451710592_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616638871201451710592_0004}; taskId=attempt_20210514225616638871201451710592_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@233c3d21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_20210514225616638871201451710592_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616638871201451710592_0004_m_000053_341
[2021-05-14 19:57:09,034] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_20210514225616638871201451710592_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616638871201451710592_0004_m_000053_341 : duration 0:00.003s
[2021-05-14 19:57:09,114] {docker.py:276} INFO - 21/05/14 22:57:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 5c14c3e96bf4:33219 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-14 19:57:09,523] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_202105142256163223975067260873410_0004_m_000050_338: needsTaskCommit() Task attempt_202105142256163223975067260873410_0004_m_000050_338
[2021-05-14 19:57:09,523] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_202105142256163223975067260873410_0004_m_000050_338: needsTaskCommit() Task attempt_202105142256163223975067260873410_0004_m_000050_338: duration 0:00.000s
21/05/14 22:57:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163223975067260873410_0004_m_000050_338
[2021-05-14 19:57:09,524] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4587 bytes result sent to driver
[2021-05-14 19:57:09,525] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (5c14c3e96bf4, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:09,528] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2789 ms on 5c14c3e96bf4 (executor driver) (51/200)
[2021-05-14 19:57:09,529] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-14 19:57:09,543] {docker.py:276} INFO - 21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Getting 5 (40.8 KiB) non-empty blocks including 5 (40.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:09,545] {docker.py:276} INFO - 21/05/14 22:57:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:09,546] {docker.py:276} INFO - 21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166167450964571044143_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166167450964571044143_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166167450964571044143_0004}; taskId=attempt_202105142256166167450964571044143_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42d5d4dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:09,546] {docker.py:276} INFO - 21/05/14 22:57:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:09,546] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_202105142256166167450964571044143_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166167450964571044143_0004_m_000054_342
[2021-05-14 19:57:09,548] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_202105142256166167450964571044143_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166167450964571044143_0004_m_000054_342 : duration 0:00.003s
[2021-05-14 19:57:09,694] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_202105142256164585064779906858494_0004_m_000051_339: needsTaskCommit() Task attempt_202105142256164585064779906858494_0004_m_000051_339
[2021-05-14 19:57:09,695] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_202105142256164585064779906858494_0004_m_000051_339: needsTaskCommit() Task attempt_202105142256164585064779906858494_0004_m_000051_339: duration 0:00.001s
[2021-05-14 19:57:09,696] {docker.py:276} INFO - 21/05/14 22:57:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164585064779906858494_0004_m_000051_339
[2021-05-14 19:57:09,697] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4587 bytes result sent to driver
[2021-05-14 19:57:09,701] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (5c14c3e96bf4, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:09,702] {docker.py:276} INFO - 21/05/14 22:57:09 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2857 ms on 5c14c3e96bf4 (executor driver) (52/200)
[2021-05-14 19:57:09,703] {docker.py:276} INFO - 21/05/14 22:57:09 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-14 19:57:09,719] {docker.py:276} INFO - 21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:09,721] {docker.py:276} INFO - 21/05/14 22:57:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:09,721] {docker.py:276} INFO - 21/05/14 22:57:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166168993110583151509_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166168993110583151509_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166168993110583151509_0004}; taskId=attempt_202105142256166168993110583151509_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@503d3879}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:09,721] {docker.py:276} INFO - 21/05/14 22:57:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:09 INFO StagingCommitter: Starting: Task committer attempt_202105142256166168993110583151509_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166168993110583151509_0004_m_000055_343
[2021-05-14 19:57:09,724] {docker.py:276} INFO - 21/05/14 22:57:09 INFO StagingCommitter: Task committer attempt_202105142256166168993110583151509_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166168993110583151509_0004_m_000055_343 : duration 0:00.003s
[2021-05-14 19:57:11,427] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Starting: Task committer attempt_202105142256164182991241050986509_0004_m_000052_340: needsTaskCommit() Task attempt_202105142256164182991241050986509_0004_m_000052_340
[2021-05-14 19:57:11,428] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Task committer attempt_202105142256164182991241050986509_0004_m_000052_340: needsTaskCommit() Task attempt_202105142256164182991241050986509_0004_m_000052_340: duration 0:00.001s
21/05/14 22:57:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164182991241050986509_0004_m_000052_340
[2021-05-14 19:57:11,430] {docker.py:276} INFO - 21/05/14 22:57:11 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4587 bytes result sent to driver
[2021-05-14 19:57:11,432] {docker.py:276} INFO - 21/05/14 22:57:11 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (5c14c3e96bf4, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:11,433] {docker.py:276} INFO - 21/05/14 22:57:11 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
21/05/14 22:57:11 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2649 ms on 5c14c3e96bf4 (executor driver) (53/200)
[2021-05-14 19:57:11,449] {docker.py:276} INFO - 21/05/14 22:57:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:11,451] {docker.py:276} INFO - 21/05/14 22:57:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169116537777152874930_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169116537777152874930_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169116537777152874930_0004}; taskId=attempt_202105142256169116537777152874930_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d5595a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:11,452] {docker.py:276} INFO - 21/05/14 22:57:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:11 INFO StagingCommitter: Starting: Task committer attempt_202105142256169116537777152874930_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169116537777152874930_0004_m_000056_344
[2021-05-14 19:57:11,455] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Task committer attempt_202105142256169116537777152874930_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169116537777152874930_0004_m_000056_344 : duration 0:00.003s
[2021-05-14 19:57:11,792] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Starting: Task committer attempt_20210514225616638871201451710592_0004_m_000053_341: needsTaskCommit() Task attempt_20210514225616638871201451710592_0004_m_000053_341
[2021-05-14 19:57:11,793] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Task committer attempt_20210514225616638871201451710592_0004_m_000053_341: needsTaskCommit() Task attempt_20210514225616638871201451710592_0004_m_000053_341: duration 0:00.002s
21/05/14 22:57:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616638871201451710592_0004_m_000053_341
[2021-05-14 19:57:11,795] {docker.py:276} INFO - 21/05/14 22:57:11 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4587 bytes result sent to driver
[2021-05-14 19:57:11,796] {docker.py:276} INFO - 21/05/14 22:57:11 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (5c14c3e96bf4, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:11,797] {docker.py:276} INFO - 21/05/14 22:57:11 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2793 ms on 5c14c3e96bf4 (executor driver) (54/200)
[2021-05-14 19:57:11,798] {docker.py:276} INFO - 21/05/14 22:57:11 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-14 19:57:11,812] {docker.py:276} INFO - 21/05/14 22:57:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:11,814] {docker.py:276} INFO - 21/05/14 22:57:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:11,815] {docker.py:276} INFO - 21/05/14 22:57:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161373738964053591012_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161373738964053591012_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161373738964053591012_0004}; taskId=attempt_202105142256161373738964053591012_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2548d7b2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:11 INFO StagingCommitter: Starting: Task committer attempt_202105142256161373738964053591012_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161373738964053591012_0004_m_000057_345
[2021-05-14 19:57:11,817] {docker.py:276} INFO - 21/05/14 22:57:11 INFO StagingCommitter: Task committer attempt_202105142256161373738964053591012_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161373738964053591012_0004_m_000057_345 : duration 0:00.003s
[2021-05-14 19:57:12,246] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Starting: Task committer attempt_202105142256166167450964571044143_0004_m_000054_342: needsTaskCommit() Task attempt_202105142256166167450964571044143_0004_m_000054_342
[2021-05-14 19:57:12,247] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Task committer attempt_202105142256166167450964571044143_0004_m_000054_342: needsTaskCommit() Task attempt_202105142256166167450964571044143_0004_m_000054_342: duration 0:00.000s
21/05/14 22:57:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166167450964571044143_0004_m_000054_342
[2021-05-14 19:57:12,249] {docker.py:276} INFO - 21/05/14 22:57:12 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4587 bytes result sent to driver
[2021-05-14 19:57:12,251] {docker.py:276} INFO - 21/05/14 22:57:12 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (5c14c3e96bf4, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:12,252] {docker.py:276} INFO - 21/05/14 22:57:12 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-14 19:57:12,252] {docker.py:276} INFO - 21/05/14 22:57:12 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2730 ms on 5c14c3e96bf4 (executor driver) (55/200)
[2021-05-14 19:57:12,262] {docker.py:276} INFO - 21/05/14 22:57:12 INFO ShuffleBlockFetcherIterator: Getting 5 (40.2 KiB) non-empty blocks including 5 (40.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:12,264] {docker.py:276} INFO - 21/05/14 22:57:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162346983277071753934_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162346983277071753934_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162346983277071753934_0004}; taskId=attempt_202105142256162346983277071753934_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3548fdce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:12,265] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Starting: Task committer attempt_202105142256162346983277071753934_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162346983277071753934_0004_m_000058_346
[2021-05-14 19:57:12,267] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Task committer attempt_202105142256162346983277071753934_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162346983277071753934_0004_m_000058_346 : duration 0:00.004s
[2021-05-14 19:57:12,317] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Starting: Task committer attempt_202105142256166168993110583151509_0004_m_000055_343: needsTaskCommit() Task attempt_202105142256166168993110583151509_0004_m_000055_343
[2021-05-14 19:57:12,318] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Task committer attempt_202105142256166168993110583151509_0004_m_000055_343: needsTaskCommit() Task attempt_202105142256166168993110583151509_0004_m_000055_343: duration 0:00.000s
21/05/14 22:57:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166168993110583151509_0004_m_000055_343
[2021-05-14 19:57:12,321] {docker.py:276} INFO - 21/05/14 22:57:12 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4587 bytes result sent to driver
[2021-05-14 19:57:12,322] {docker.py:276} INFO - 21/05/14 22:57:12 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (5c14c3e96bf4, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:12,323] {docker.py:276} INFO - 21/05/14 22:57:12 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2627 ms on 5c14c3e96bf4 (executor driver) (56/200)
[2021-05-14 19:57:12,324] {docker.py:276} INFO - 21/05/14 22:57:12 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-14 19:57:12,333] {docker.py:276} INFO - 21/05/14 22:57:12 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:12,334] {docker.py:276} INFO - 21/05/14 22:57:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:12,336] {docker.py:276} INFO - 21/05/14 22:57:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:12,337] {docker.py:276} INFO - 21/05/14 22:57:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616406009165629602009_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616406009165629602009_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616406009165629602009_0004}; taskId=attempt_20210514225616406009165629602009_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b44b6ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:12 INFO StagingCommitter: Starting: Task committer attempt_20210514225616406009165629602009_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616406009165629602009_0004_m_000059_347
[2021-05-14 19:57:12,340] {docker.py:276} INFO - 21/05/14 22:57:12 INFO StagingCommitter: Task committer attempt_20210514225616406009165629602009_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616406009165629602009_0004_m_000059_347 : duration 0:00.003s
[2021-05-14 19:57:14,244] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Starting: Task committer attempt_202105142256169116537777152874930_0004_m_000056_344: needsTaskCommit() Task attempt_202105142256169116537777152874930_0004_m_000056_344
[2021-05-14 19:57:14,244] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Task committer attempt_202105142256169116537777152874930_0004_m_000056_344: needsTaskCommit() Task attempt_202105142256169116537777152874930_0004_m_000056_344: duration 0:00.000s
[2021-05-14 19:57:14,245] {docker.py:276} INFO - 21/05/14 22:57:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169116537777152874930_0004_m_000056_344
[2021-05-14 19:57:14,245] {docker.py:276} INFO - 21/05/14 22:57:14 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-14 19:57:14,246] {docker.py:276} INFO - 21/05/14 22:57:14 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (5c14c3e96bf4, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:14,246] {docker.py:276} INFO - 21/05/14 22:57:14 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2818 ms on 5c14c3e96bf4 (executor driver) (57/200)
[2021-05-14 19:57:14,248] {docker.py:276} INFO - 21/05/14 22:57:14 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
[2021-05-14 19:57:14,256] {docker.py:276} INFO - 21/05/14 22:57:14 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:14,258] {docker.py:276} INFO - 21/05/14 22:57:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616884429284423855355_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616884429284423855355_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616884429284423855355_0004}; taskId=attempt_20210514225616884429284423855355_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6637e8ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:14,259] {docker.py:276} INFO - 21/05/14 22:57:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:14 INFO StagingCommitter: Starting: Task committer attempt_20210514225616884429284423855355_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616884429284423855355_0004_m_000060_348
[2021-05-14 19:57:14,261] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Task committer attempt_20210514225616884429284423855355_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616884429284423855355_0004_m_000060_348 : duration 0:00.003s
[2021-05-14 19:57:14,492] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Starting: Task committer attempt_202105142256161373738964053591012_0004_m_000057_345: needsTaskCommit() Task attempt_202105142256161373738964053591012_0004_m_000057_345
[2021-05-14 19:57:14,493] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Task committer attempt_202105142256161373738964053591012_0004_m_000057_345: needsTaskCommit() Task attempt_202105142256161373738964053591012_0004_m_000057_345: duration 0:00.001s
21/05/14 22:57:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161373738964053591012_0004_m_000057_345
[2021-05-14 19:57:14,494] {docker.py:276} INFO - 21/05/14 22:57:14 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4587 bytes result sent to driver
[2021-05-14 19:57:14,496] {docker.py:276} INFO - 21/05/14 22:57:14 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (5c14c3e96bf4, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:14,497] {docker.py:276} INFO - 21/05/14 22:57:14 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-14 19:57:14,498] {docker.py:276} INFO - 21/05/14 22:57:14 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2703 ms on 5c14c3e96bf4 (executor driver) (58/200)
[2021-05-14 19:57:14,508] {docker.py:276} INFO - 21/05/14 22:57:14 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:14,508] {docker.py:276} INFO - 21/05/14 22:57:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:57:14,510] {docker.py:276} INFO - 21/05/14 22:57:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:14,510] {docker.py:276} INFO - 21/05/14 22:57:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161491104756286550081_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161491104756286550081_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161491104756286550081_0004}; taskId=attempt_202105142256161491104756286550081_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44ae90f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:14,510] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Starting: Task committer attempt_202105142256161491104756286550081_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161491104756286550081_0004_m_000061_349
[2021-05-14 19:57:14,513] {docker.py:276} INFO - 21/05/14 22:57:14 INFO StagingCommitter: Task committer attempt_202105142256161491104756286550081_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161491104756286550081_0004_m_000061_349 : duration 0:00.003s
[2021-05-14 19:57:15,093] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Starting: Task committer attempt_202105142256162346983277071753934_0004_m_000058_346: needsTaskCommit() Task attempt_202105142256162346983277071753934_0004_m_000058_346
[2021-05-14 19:57:15,094] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Task committer attempt_202105142256162346983277071753934_0004_m_000058_346: needsTaskCommit() Task attempt_202105142256162346983277071753934_0004_m_000058_346: duration 0:00.001s
[2021-05-14 19:57:15,094] {docker.py:276} INFO - 21/05/14 22:57:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162346983277071753934_0004_m_000058_346
[2021-05-14 19:57:15,096] {docker.py:276} INFO - 21/05/14 22:57:15 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-14 19:57:15,098] {docker.py:276} INFO - 21/05/14 22:57:15 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (5c14c3e96bf4, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:15,099] {docker.py:276} INFO - 21/05/14 22:57:15 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-14 19:57:15,100] {docker.py:276} INFO - 21/05/14 22:57:15 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2852 ms on 5c14c3e96bf4 (executor driver) (59/200)
[2021-05-14 19:57:15,102] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Starting: Task committer attempt_20210514225616406009165629602009_0004_m_000059_347: needsTaskCommit() Task attempt_20210514225616406009165629602009_0004_m_000059_347
[2021-05-14 19:57:15,103] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Task committer attempt_20210514225616406009165629602009_0004_m_000059_347: needsTaskCommit() Task attempt_20210514225616406009165629602009_0004_m_000059_347: duration 0:00.000s
21/05/14 22:57:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616406009165629602009_0004_m_000059_347
[2021-05-14 19:57:15,104] {docker.py:276} INFO - 21/05/14 22:57:15 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-14 19:57:15,106] {docker.py:276} INFO - 21/05/14 22:57:15 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (5c14c3e96bf4, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:15,106] {docker.py:276} INFO - 21/05/14 22:57:15 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2788 ms on 5c14c3e96bf4 (executor driver) (60/200)
[2021-05-14 19:57:15,107] {docker.py:276} INFO - 21/05/14 22:57:15 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-14 19:57:15,112] {docker.py:276} INFO - 21/05/14 22:57:15 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:15,113] {docker.py:276} INFO - 21/05/14 22:57:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:15,115] {docker.py:276} INFO - 21/05/14 22:57:15 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:15,117] {docker.py:276} INFO - 21/05/14 22:57:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:15,118] {docker.py:276} INFO - 21/05/14 22:57:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163099349772253241767_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163099349772253241767_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163099349772253241767_0004}; taskId=attempt_202105142256163099349772253241767_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3aae138b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:15,118] {docker.py:276} INFO - 21/05/14 22:57:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:15,118] {docker.py:276} INFO - 21/05/14 22:57:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:15,119] {docker.py:276} INFO - 21/05/14 22:57:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169182003803375539800_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169182003803375539800_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169182003803375539800_0004}; taskId=attempt_202105142256169182003803375539800_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e5ea6b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:15 INFO StagingCommitter: Starting: Task committer attempt_202105142256163099349772253241767_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163099349772253241767_0004_m_000062_350
[2021-05-14 19:57:15,119] {docker.py:276} INFO - 21/05/14 22:57:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:15,119] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Starting: Task committer attempt_202105142256169182003803375539800_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169182003803375539800_0004_m_000063_351
[2021-05-14 19:57:15,122] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Task committer attempt_202105142256169182003803375539800_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169182003803375539800_0004_m_000063_351 : duration 0:00.004s
[2021-05-14 19:57:15,123] {docker.py:276} INFO - 21/05/14 22:57:15 INFO StagingCommitter: Task committer attempt_202105142256163099349772253241767_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163099349772253241767_0004_m_000062_350 : duration 0:00.005s
[2021-05-14 19:57:16,949] {docker.py:276} INFO - 21/05/14 22:57:16 INFO StagingCommitter: Starting: Task committer attempt_20210514225616884429284423855355_0004_m_000060_348: needsTaskCommit() Task attempt_20210514225616884429284423855355_0004_m_000060_348
21/05/14 22:57:16 INFO StagingCommitter: Task committer attempt_20210514225616884429284423855355_0004_m_000060_348: needsTaskCommit() Task attempt_20210514225616884429284423855355_0004_m_000060_348: duration 0:00.001s
21/05/14 22:57:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616884429284423855355_0004_m_000060_348
[2021-05-14 19:57:16,951] {docker.py:276} INFO - 21/05/14 22:57:16 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-14 19:57:16,953] {docker.py:276} INFO - 21/05/14 22:57:16 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (5c14c3e96bf4, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:16,953] {docker.py:276} INFO - 21/05/14 22:57:16 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 2711 ms on 5c14c3e96bf4 (executor driver) (61/200)
[2021-05-14 19:57:16,955] {docker.py:276} INFO - 21/05/14 22:57:16 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-14 19:57:16,963] {docker.py:276} INFO - 21/05/14 22:57:16 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:16,965] {docker.py:276} INFO - 21/05/14 22:57:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:16,966] {docker.py:276} INFO - 21/05/14 22:57:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166583370752887455087_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166583370752887455087_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166583370752887455087_0004}; taskId=attempt_202105142256166583370752887455087_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c444636}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:16 INFO StagingCommitter: Starting: Task committer attempt_202105142256166583370752887455087_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166583370752887455087_0004_m_000064_352
[2021-05-14 19:57:16,969] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256166583370752887455087_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166583370752887455087_0004_m_000064_352 : duration 0:00.003s
[2021-05-14 19:57:17,395] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_202105142256161491104756286550081_0004_m_000061_349: needsTaskCommit() Task attempt_202105142256161491104756286550081_0004_m_000061_349
[2021-05-14 19:57:17,397] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256161491104756286550081_0004_m_000061_349: needsTaskCommit() Task attempt_202105142256161491104756286550081_0004_m_000061_349: duration 0:00.001s
[2021-05-14 19:57:17,397] {docker.py:276} INFO - 21/05/14 22:57:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161491104756286550081_0004_m_000061_349
[2021-05-14 19:57:17,399] {docker.py:276} INFO - 21/05/14 22:57:17 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4587 bytes result sent to driver
[2021-05-14 19:57:17,400] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (5c14c3e96bf4, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:17,402] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2909 ms on 5c14c3e96bf4 (executor driver) (62/200)
21/05/14 22:57:17 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-14 19:57:17,411] {docker.py:276} INFO - 21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:17,413] {docker.py:276} INFO - 21/05/14 22:57:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168986695664205718642_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168986695664205718642_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168986695664205718642_0004}; taskId=attempt_202105142256168986695664205718642_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f95018e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_202105142256168986695664205718642_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168986695664205718642_0004_m_000065_353
[2021-05-14 19:57:17,416] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256168986695664205718642_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168986695664205718642_0004_m_000065_353 : duration 0:00.003s
[2021-05-14 19:57:17,759] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_202105142256169182003803375539800_0004_m_000063_351: needsTaskCommit() Task attempt_202105142256169182003803375539800_0004_m_000063_351
[2021-05-14 19:57:17,760] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256169182003803375539800_0004_m_000063_351: needsTaskCommit() Task attempt_202105142256169182003803375539800_0004_m_000063_351: duration 0:00.002s
[2021-05-14 19:57:17,761] {docker.py:276} INFO - 21/05/14 22:57:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169182003803375539800_0004_m_000063_351
[2021-05-14 19:57:17,763] {docker.py:276} INFO - 21/05/14 22:57:17 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4587 bytes result sent to driver
[2021-05-14 19:57:17,764] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (5c14c3e96bf4, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:17,765] {docker.py:276} INFO - 21/05/14 22:57:17 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-14 19:57:17,765] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2663 ms on 5c14c3e96bf4 (executor driver) (63/200)
[2021-05-14 19:57:17,773] {docker.py:276} INFO - 21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:17,775] {docker.py:276} INFO - 21/05/14 22:57:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162751286055950351542_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162751286055950351542_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162751286055950351542_0004}; taskId=attempt_202105142256162751286055950351542_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fea31f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_202105142256162751286055950351542_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162751286055950351542_0004_m_000066_354
[2021-05-14 19:57:17,778] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256162751286055950351542_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162751286055950351542_0004_m_000066_354 : duration 0:00.002s
[2021-05-14 19:57:17,780] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_202105142256163099349772253241767_0004_m_000062_350: needsTaskCommit() Task attempt_202105142256163099349772253241767_0004_m_000062_350
[2021-05-14 19:57:17,780] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_202105142256163099349772253241767_0004_m_000062_350: needsTaskCommit() Task attempt_202105142256163099349772253241767_0004_m_000062_350: duration 0:00.000s
[2021-05-14 19:57:17,781] {docker.py:276} INFO - 21/05/14 22:57:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163099349772253241767_0004_m_000062_350
[2021-05-14 19:57:17,781] {docker.py:276} INFO - 21/05/14 22:57:17 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4587 bytes result sent to driver
[2021-05-14 19:57:17,782] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (5c14c3e96bf4, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:17,783] {docker.py:276} INFO - 21/05/14 22:57:17 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-14 19:57:17,783] {docker.py:276} INFO - 21/05/14 22:57:17 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2689 ms on 5c14c3e96bf4 (executor driver) (64/200)
[2021-05-14 19:57:17,799] {docker.py:276} INFO - 21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:17,801] {docker.py:276} INFO - 21/05/14 22:57:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616616108614883306427_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616616108614883306427_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616616108614883306427_0004}; taskId=attempt_20210514225616616108614883306427_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12b48920}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:17,801] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Starting: Task committer attempt_20210514225616616108614883306427_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616616108614883306427_0004_m_000067_355
[2021-05-14 19:57:17,804] {docker.py:276} INFO - 21/05/14 22:57:17 INFO StagingCommitter: Task committer attempt_20210514225616616108614883306427_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616616108614883306427_0004_m_000067_355 : duration 0:00.003s
[2021-05-14 19:57:19,635] {docker.py:276} INFO - 21/05/14 22:57:19 INFO StagingCommitter: Starting: Task committer attempt_202105142256166583370752887455087_0004_m_000064_352: needsTaskCommit() Task attempt_202105142256166583370752887455087_0004_m_000064_352
[2021-05-14 19:57:19,636] {docker.py:276} INFO - 21/05/14 22:57:19 INFO StagingCommitter: Task committer attempt_202105142256166583370752887455087_0004_m_000064_352: needsTaskCommit() Task attempt_202105142256166583370752887455087_0004_m_000064_352: duration 0:00.002s
[2021-05-14 19:57:19,637] {docker.py:276} INFO - 21/05/14 22:57:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166583370752887455087_0004_m_000064_352
[2021-05-14 19:57:19,639] {docker.py:276} INFO - 21/05/14 22:57:19 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4587 bytes result sent to driver
[2021-05-14 19:57:19,641] {docker.py:276} INFO - 21/05/14 22:57:19 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (5c14c3e96bf4, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:19,643] {docker.py:276} INFO - 21/05/14 22:57:19 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2658 ms on 5c14c3e96bf4 (executor driver) (65/200)
21/05/14 22:57:19 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-14 19:57:19,652] {docker.py:276} INFO - 21/05/14 22:57:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:19,654] {docker.py:276} INFO - 21/05/14 22:57:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166595148347030453871_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166595148347030453871_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166595148347030453871_0004}; taskId=attempt_202105142256166595148347030453871_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b7096d5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:19 INFO StagingCommitter: Starting: Task committer attempt_202105142256166595148347030453871_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166595148347030453871_0004_m_000068_356
[2021-05-14 19:57:19,657] {docker.py:276} INFO - 21/05/14 22:57:19 INFO StagingCommitter: Task committer attempt_202105142256166595148347030453871_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166595148347030453871_0004_m_000068_356 : duration 0:00.003s
[2021-05-14 19:57:20,004] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_202105142256168986695664205718642_0004_m_000065_353: needsTaskCommit() Task attempt_202105142256168986695664205718642_0004_m_000065_353
[2021-05-14 19:57:20,005] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_202105142256168986695664205718642_0004_m_000065_353: needsTaskCommit() Task attempt_202105142256168986695664205718642_0004_m_000065_353: duration 0:00.001s
21/05/14 22:57:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168986695664205718642_0004_m_000065_353
[2021-05-14 19:57:20,006] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4587 bytes result sent to driver
[2021-05-14 19:57:20,009] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (5c14c3e96bf4, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:20,009] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-14 19:57:20,010] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2578 ms on 5c14c3e96bf4 (executor driver) (66/200)
[2021-05-14 19:57:20,019] {docker.py:276} INFO - 21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:20,021] {docker.py:276} INFO - 21/05/14 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616100182121033399312_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616100182121033399312_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616100182121033399312_0004}; taskId=attempt_20210514225616100182121033399312_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74aec3fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_20210514225616100182121033399312_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616100182121033399312_0004_m_000069_357
[2021-05-14 19:57:20,024] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_20210514225616100182121033399312_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616100182121033399312_0004_m_000069_357 : duration 0:00.003s
[2021-05-14 19:57:20,178] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_20210514225616616108614883306427_0004_m_000067_355: needsTaskCommit() Task attempt_20210514225616616108614883306427_0004_m_000067_355
[2021-05-14 19:57:20,180] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_20210514225616616108614883306427_0004_m_000067_355: needsTaskCommit() Task attempt_20210514225616616108614883306427_0004_m_000067_355: duration 0:00.001s
21/05/14 22:57:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616616108614883306427_0004_m_000067_355
[2021-05-14 19:57:20,182] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4587 bytes result sent to driver
[2021-05-14 19:57:20,183] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (5c14c3e96bf4, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:20,184] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2369 ms on 5c14c3e96bf4 (executor driver) (67/200)
[2021-05-14 19:57:20,185] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-14 19:57:20,202] {docker.py:276} INFO - 21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Getting 5 (39.6 KiB) non-empty blocks including 5 (39.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:20,204] {docker.py:276} INFO - 21/05/14 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168419321952483196253_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168419321952483196253_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168419321952483196253_0004}; taskId=attempt_202105142256168419321952483196253_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52559c4f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_202105142256168419321952483196253_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168419321952483196253_0004_m_000070_358
[2021-05-14 19:57:20,207] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_202105142256168419321952483196253_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168419321952483196253_0004_m_000070_358 : duration 0:00.002s
[2021-05-14 19:57:20,444] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_202105142256162751286055950351542_0004_m_000066_354: needsTaskCommit() Task attempt_202105142256162751286055950351542_0004_m_000066_354
[2021-05-14 19:57:20,445] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_202105142256162751286055950351542_0004_m_000066_354: needsTaskCommit() Task attempt_202105142256162751286055950351542_0004_m_000066_354: duration 0:00.001s
[2021-05-14 19:57:20,446] {docker.py:276} INFO - 21/05/14 22:57:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162751286055950351542_0004_m_000066_354
[2021-05-14 19:57:20,447] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4587 bytes result sent to driver
[2021-05-14 19:57:20,448] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (5c14c3e96bf4, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:20,449] {docker.py:276} INFO - 21/05/14 22:57:20 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2653 ms on 5c14c3e96bf4 (executor driver) (68/200)
[2021-05-14 19:57:20,450] {docker.py:276} INFO - 21/05/14 22:57:20 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-14 19:57:20,461] {docker.py:276} INFO - 21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:20,463] {docker.py:276} INFO - 21/05/14 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163871886692775794218_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163871886692775794218_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163871886692775794218_0004}; taskId=attempt_202105142256163871886692775794218_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f1bb438}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:20 INFO StagingCommitter: Starting: Task committer attempt_202105142256163871886692775794218_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163871886692775794218_0004_m_000071_359
[2021-05-14 19:57:20,466] {docker.py:276} INFO - 21/05/14 22:57:20 INFO StagingCommitter: Task committer attempt_202105142256163871886692775794218_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163871886692775794218_0004_m_000071_359 : duration 0:00.003s
[2021-05-14 19:57:22,396] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_202105142256166595148347030453871_0004_m_000068_356: needsTaskCommit() Task attempt_202105142256166595148347030453871_0004_m_000068_356
[2021-05-14 19:57:22,397] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_202105142256166595148347030453871_0004_m_000068_356: needsTaskCommit() Task attempt_202105142256166595148347030453871_0004_m_000068_356: duration 0:00.000s
21/05/14 22:57:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166595148347030453871_0004_m_000068_356
[2021-05-14 19:57:22,399] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4587 bytes result sent to driver
[2021-05-14 19:57:22,400] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (5c14c3e96bf4, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:22,402] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-14 19:57:22,402] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2764 ms on 5c14c3e96bf4 (executor driver) (69/200)
[2021-05-14 19:57:22,412] {docker.py:276} INFO - 21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:22,414] {docker.py:276} INFO - 21/05/14 22:57:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169144523009373385550_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169144523009373385550_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169144523009373385550_0004}; taskId=attempt_202105142256169144523009373385550_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7270476d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:22,414] {docker.py:276} INFO - 21/05/14 22:57:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_202105142256169144523009373385550_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169144523009373385550_0004_m_000072_360
[2021-05-14 19:57:22,417] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_202105142256169144523009373385550_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169144523009373385550_0004_m_000072_360 : duration 0:00.003s
[2021-05-14 19:57:22,730] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_202105142256168419321952483196253_0004_m_000070_358: needsTaskCommit() Task attempt_202105142256168419321952483196253_0004_m_000070_358
[2021-05-14 19:57:22,732] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_202105142256168419321952483196253_0004_m_000070_358: needsTaskCommit() Task attempt_202105142256168419321952483196253_0004_m_000070_358: duration 0:00.001s
21/05/14 22:57:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168419321952483196253_0004_m_000070_358
[2021-05-14 19:57:22,734] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-14 19:57:22,735] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (5c14c3e96bf4, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:22,737] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2557 ms on 5c14c3e96bf4 (executor driver) (70/200)
[2021-05-14 19:57:22,737] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-14 19:57:22,747] {docker.py:276} INFO - 21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:22,747] {docker.py:276} INFO - 21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:22,749] {docker.py:276} INFO - 21/05/14 22:57:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:22,749] {docker.py:276} INFO - 21/05/14 22:57:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:22,750] {docker.py:276} INFO - 21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:22,750] {docker.py:276} INFO - 21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161639117003141920077_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161639117003141920077_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161639117003141920077_0004}; taskId=attempt_202105142256161639117003141920077_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ef35c01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:22,750] {docker.py:276} INFO - 21/05/14 22:57:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:22,750] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_202105142256161639117003141920077_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161639117003141920077_0004_m_000073_361
[2021-05-14 19:57:22,754] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_202105142256161639117003141920077_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161639117003141920077_0004_m_000073_361 : duration 0:00.003s
[2021-05-14 19:57:22,899] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_20210514225616100182121033399312_0004_m_000069_357: needsTaskCommit() Task attempt_20210514225616100182121033399312_0004_m_000069_357
[2021-05-14 19:57:22,900] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_20210514225616100182121033399312_0004_m_000069_357: needsTaskCommit() Task attempt_20210514225616100182121033399312_0004_m_000069_357: duration 0:00.001s
21/05/14 22:57:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616100182121033399312_0004_m_000069_357
[2021-05-14 19:57:22,902] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4587 bytes result sent to driver
[2021-05-14 19:57:22,911] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (5c14c3e96bf4, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:22,911] {docker.py:276} INFO - 21/05/14 22:57:22 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2907 ms on 5c14c3e96bf4 (executor driver) (71/200)
[2021-05-14 19:57:22,912] {docker.py:276} INFO - 21/05/14 22:57:22 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-14 19:57:22,920] {docker.py:276} INFO - 21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:22,920] {docker.py:276} INFO - 21/05/14 22:57:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:22,922] {docker.py:276} INFO - 21/05/14 22:57:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:22,922] {docker.py:276} INFO - 21/05/14 22:57:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:22,922] {docker.py:276} INFO - 21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:22,923] {docker.py:276} INFO - 21/05/14 22:57:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166557689042684069608_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166557689042684069608_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166557689042684069608_0004}; taskId=attempt_202105142256166557689042684069608_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34052561}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:22,923] {docker.py:276} INFO - 21/05/14 22:57:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:22,923] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Starting: Task committer attempt_202105142256166557689042684069608_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166557689042684069608_0004_m_000074_362
[2021-05-14 19:57:22,925] {docker.py:276} INFO - 21/05/14 22:57:22 INFO StagingCommitter: Task committer attempt_202105142256166557689042684069608_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166557689042684069608_0004_m_000074_362 : duration 0:00.002s
[2021-05-14 19:57:23,380] {docker.py:276} INFO - 21/05/14 22:57:23 INFO StagingCommitter: Starting: Task committer attempt_202105142256163871886692775794218_0004_m_000071_359: needsTaskCommit() Task attempt_202105142256163871886692775794218_0004_m_000071_359
[2021-05-14 19:57:23,381] {docker.py:276} INFO - 21/05/14 22:57:23 INFO StagingCommitter: Task committer attempt_202105142256163871886692775794218_0004_m_000071_359: needsTaskCommit() Task attempt_202105142256163871886692775794218_0004_m_000071_359: duration 0:00.001s
21/05/14 22:57:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163871886692775794218_0004_m_000071_359
[2021-05-14 19:57:23,383] {docker.py:276} INFO - 21/05/14 22:57:23 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-14 19:57:23,385] {docker.py:276} INFO - 21/05/14 22:57:23 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (5c14c3e96bf4, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:23,386] {docker.py:276} INFO - 21/05/14 22:57:23 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-14 19:57:23,387] {docker.py:276} INFO - 21/05/14 22:57:23 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2942 ms on 5c14c3e96bf4 (executor driver) (72/200)
[2021-05-14 19:57:23,396] {docker.py:276} INFO - 21/05/14 22:57:23 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:23,398] {docker.py:276} INFO - 21/05/14 22:57:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164161606445070247707_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164161606445070247707_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164161606445070247707_0004}; taskId=attempt_202105142256164161606445070247707_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b00bed8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:23,399] {docker.py:276} INFO - 21/05/14 22:57:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:23,399] {docker.py:276} INFO - 21/05/14 22:57:23 INFO StagingCommitter: Starting: Task committer attempt_202105142256164161606445070247707_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164161606445070247707_0004_m_000075_363
[2021-05-14 19:57:23,402] {docker.py:276} INFO - 21/05/14 22:57:23 INFO StagingCommitter: Task committer attempt_202105142256164161606445070247707_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164161606445070247707_0004_m_000075_363 : duration 0:00.003s
[2021-05-14 19:57:25,361] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256169144523009373385550_0004_m_000072_360: needsTaskCommit() Task attempt_202105142256169144523009373385550_0004_m_000072_360
21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256169144523009373385550_0004_m_000072_360: needsTaskCommit() Task attempt_202105142256169144523009373385550_0004_m_000072_360: duration 0:00.000s
[2021-05-14 19:57:25,362] {docker.py:276} INFO - 21/05/14 22:57:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169144523009373385550_0004_m_000072_360
[2021-05-14 19:57:25,363] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4587 bytes result sent to driver
[2021-05-14 19:57:25,364] {docker.py:276} INFO - 21/05/14 22:57:25 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (5c14c3e96bf4, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:25,365] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
21/05/14 22:57:25 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 2970 ms on 5c14c3e96bf4 (executor driver) (73/200)
[2021-05-14 19:57:25,373] {docker.py:276} INFO - 21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:25,375] {docker.py:276} INFO - 21/05/14 22:57:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:25,376] {docker.py:276} INFO - 21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161071297445710954972_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161071297445710954972_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161071297445710954972_0004}; taskId=attempt_202105142256161071297445710954972_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10bb2013}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256161071297445710954972_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161071297445710954972_0004_m_000076_364
[2021-05-14 19:57:25,379] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256161071297445710954972_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161071297445710954972_0004_m_000076_364 : duration 0:00.003s
[2021-05-14 19:57:25,484] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256161639117003141920077_0004_m_000073_361: needsTaskCommit() Task attempt_202105142256161639117003141920077_0004_m_000073_361
[2021-05-14 19:57:25,485] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256161639117003141920077_0004_m_000073_361: needsTaskCommit() Task attempt_202105142256161639117003141920077_0004_m_000073_361: duration 0:00.001s
21/05/14 22:57:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161639117003141920077_0004_m_000073_361
[2021-05-14 19:57:25,486] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4587 bytes result sent to driver
[2021-05-14 19:57:25,488] {docker.py:276} INFO - 21/05/14 22:57:25 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (5c14c3e96bf4, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:25,489] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
21/05/14 22:57:25 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2758 ms on 5c14c3e96bf4 (executor driver) (74/200)
[2021-05-14 19:57:25,499] {docker.py:276} INFO - 21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:25,501] {docker.py:276} INFO - 21/05/14 22:57:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:25,501] {docker.py:276} INFO - 21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161651606786986109371_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161651606786986109371_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161651606786986109371_0004}; taskId=attempt_202105142256161651606786986109371_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ac79660}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256161651606786986109371_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161651606786986109371_0004_m_000077_365
[2021-05-14 19:57:25,503] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256161651606786986109371_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161651606786986109371_0004_m_000077_365 : duration 0:00.002s
[2021-05-14 19:57:25,952] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256166557689042684069608_0004_m_000074_362: needsTaskCommit() Task attempt_202105142256166557689042684069608_0004_m_000074_362
[2021-05-14 19:57:25,953] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256166557689042684069608_0004_m_000074_362: needsTaskCommit() Task attempt_202105142256166557689042684069608_0004_m_000074_362: duration 0:00.001s
[2021-05-14 19:57:25,953] {docker.py:276} INFO - 21/05/14 22:57:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166557689042684069608_0004_m_000074_362
[2021-05-14 19:57:25,954] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4587 bytes result sent to driver
[2021-05-14 19:57:25,955] {docker.py:276} INFO - 21/05/14 22:57:25 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (5c14c3e96bf4, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:25,956] {docker.py:276} INFO - 21/05/14 22:57:25 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 3049 ms on 5c14c3e96bf4 (executor driver) (75/200)
[2021-05-14 19:57:25,957] {docker.py:276} INFO - 21/05/14 22:57:25 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-14 19:57:25,964] {docker.py:276} INFO - 21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Getting 5 (39.8 KiB) non-empty blocks including 5 (39.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:25,965] {docker.py:276} INFO - 21/05/14 22:57:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:25,966] {docker.py:276} INFO - 21/05/14 22:57:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:25,967] {docker.py:276} INFO - 21/05/14 22:57:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161197878710053515789_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161197878710053515789_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161197878710053515789_0004}; taskId=attempt_202105142256161197878710053515789_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19a4befa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:25,967] {docker.py:276} INFO - 21/05/14 22:57:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:25,967] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Starting: Task committer attempt_202105142256161197878710053515789_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161197878710053515789_0004_m_000078_366
[2021-05-14 19:57:25,969] {docker.py:276} INFO - 21/05/14 22:57:25 INFO StagingCommitter: Task committer attempt_202105142256161197878710053515789_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161197878710053515789_0004_m_000078_366 : duration 0:00.003s
[2021-05-14 19:57:26,357] {docker.py:276} INFO - 21/05/14 22:57:26 INFO StagingCommitter: Starting: Task committer attempt_202105142256164161606445070247707_0004_m_000075_363: needsTaskCommit() Task attempt_202105142256164161606445070247707_0004_m_000075_363
[2021-05-14 19:57:26,358] {docker.py:276} INFO - 21/05/14 22:57:26 INFO StagingCommitter: Task committer attempt_202105142256164161606445070247707_0004_m_000075_363: needsTaskCommit() Task attempt_202105142256164161606445070247707_0004_m_000075_363: duration 0:00.001s
21/05/14 22:57:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164161606445070247707_0004_m_000075_363
[2021-05-14 19:57:26,360] {docker.py:276} INFO - 21/05/14 22:57:26 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4587 bytes result sent to driver
[2021-05-14 19:57:26,363] {docker.py:276} INFO - 21/05/14 22:57:26 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (5c14c3e96bf4, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:57:26 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2981 ms on 5c14c3e96bf4 (executor driver) (76/200)
[2021-05-14 19:57:26,363] {docker.py:276} INFO - 21/05/14 22:57:26 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-14 19:57:26,371] {docker.py:276} INFO - 21/05/14 22:57:26 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:26,372] {docker.py:276} INFO - 21/05/14 22:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:26,374] {docker.py:276} INFO - 21/05/14 22:57:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:26,374] {docker.py:276} INFO - 21/05/14 22:57:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:26,374] {docker.py:276} INFO - 21/05/14 22:57:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165908493491949747258_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165908493491949747258_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165908493491949747258_0004}; taskId=attempt_202105142256165908493491949747258_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5053b17a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:26 INFO StagingCommitter: Starting: Task committer attempt_202105142256165908493491949747258_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165908493491949747258_0004_m_000079_367
[2021-05-14 19:57:26,377] {docker.py:276} INFO - 21/05/14 22:57:26 INFO StagingCommitter: Task committer attempt_202105142256165908493491949747258_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165908493491949747258_0004_m_000079_367 : duration 0:00.003s
[2021-05-14 19:57:28,096] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256161071297445710954972_0004_m_000076_364: needsTaskCommit() Task attempt_202105142256161071297445710954972_0004_m_000076_364
[2021-05-14 19:57:28,097] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256161071297445710954972_0004_m_000076_364: needsTaskCommit() Task attempt_202105142256161071297445710954972_0004_m_000076_364: duration 0:00.001s
[2021-05-14 19:57:28,097] {docker.py:276} INFO - 21/05/14 22:57:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161071297445710954972_0004_m_000076_364
[2021-05-14 19:57:28,098] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-14 19:57:28,099] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (5c14c3e96bf4, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:28,101] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2739 ms on 5c14c3e96bf4 (executor driver) (77/200)
[2021-05-14 19:57:28,101] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-14 19:57:28,109] {docker.py:276} INFO - 21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:28,110] {docker.py:276} INFO - 21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:28,111] {docker.py:276} INFO - 21/05/14 22:57:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165610857483167289762_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165610857483167289762_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165610857483167289762_0004}; taskId=attempt_202105142256165610857483167289762_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2863ed91}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:28,111] {docker.py:276} INFO - 21/05/14 22:57:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256165610857483167289762_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165610857483167289762_0004_m_000080_368
[2021-05-14 19:57:28,114] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256165610857483167289762_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165610857483167289762_0004_m_000080_368 : duration 0:00.003s
[2021-05-14 19:57:28,461] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256161651606786986109371_0004_m_000077_365: needsTaskCommit() Task attempt_202105142256161651606786986109371_0004_m_000077_365
[2021-05-14 19:57:28,462] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256161651606786986109371_0004_m_000077_365: needsTaskCommit() Task attempt_202105142256161651606786986109371_0004_m_000077_365: duration 0:00.001s
21/05/14 22:57:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161651606786986109371_0004_m_000077_365
[2021-05-14 19:57:28,464] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4587 bytes result sent to driver
21/05/14 22:57:28 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (5c14c3e96bf4, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:28,466] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-14 19:57:28,466] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2982 ms on 5c14c3e96bf4 (executor driver) (78/200)
[2021-05-14 19:57:28,476] {docker.py:276} INFO - 21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:28,477] {docker.py:276} INFO - 21/05/14 22:57:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:28,478] {docker.py:276} INFO - 21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616468319792217369467_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616468319792217369467_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616468319792217369467_0004}; taskId=attempt_20210514225616468319792217369467_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@85d5f92}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_20210514225616468319792217369467_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616468319792217369467_0004_m_000081_369
[2021-05-14 19:57:28,481] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_20210514225616468319792217369467_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616468319792217369467_0004_m_000081_369 : duration 0:00.003s
[2021-05-14 19:57:28,707] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256165908493491949747258_0004_m_000079_367: needsTaskCommit() Task attempt_202105142256165908493491949747258_0004_m_000079_367
[2021-05-14 19:57:28,708] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256165908493491949747258_0004_m_000079_367: needsTaskCommit() Task attempt_202105142256165908493491949747258_0004_m_000079_367: duration 0:00.001s
21/05/14 22:57:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165908493491949747258_0004_m_000079_367
[2021-05-14 19:57:28,710] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4587 bytes result sent to driver
[2021-05-14 19:57:28,711] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (5c14c3e96bf4, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:28,713] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-14 19:57:28,714] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2355 ms on 5c14c3e96bf4 (executor driver) (79/200)
[2021-05-14 19:57:28,722] {docker.py:276} INFO - 21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:28,724] {docker.py:276} INFO - 21/05/14 22:57:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168806931116768538209_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168806931116768538209_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168806931116768538209_0004}; taskId=attempt_202105142256168806931116768538209_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7028fec2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256168806931116768538209_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168806931116768538209_0004_m_000082_370
[2021-05-14 19:57:28,727] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256168806931116768538209_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168806931116768538209_0004_m_000082_370 : duration 0:00.002s
[2021-05-14 19:57:28,747] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256161197878710053515789_0004_m_000078_366: needsTaskCommit() Task attempt_202105142256161197878710053515789_0004_m_000078_366
[2021-05-14 19:57:28,748] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256161197878710053515789_0004_m_000078_366: needsTaskCommit() Task attempt_202105142256161197878710053515789_0004_m_000078_366: duration 0:00.001s
[2021-05-14 19:57:28,748] {docker.py:276} INFO - 21/05/14 22:57:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161197878710053515789_0004_m_000078_366
[2021-05-14 19:57:28,749] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4587 bytes result sent to driver
[2021-05-14 19:57:28,751] {docker.py:276} INFO - 21/05/14 22:57:28 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (5c14c3e96bf4, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:28,752] {docker.py:276} INFO - 21/05/14 22:57:28 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
21/05/14 22:57:28 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2799 ms on 5c14c3e96bf4 (executor driver) (80/200)
[2021-05-14 19:57:28,760] {docker.py:276} INFO - 21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:28,762] {docker.py:276} INFO - 21/05/14 22:57:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:28,762] {docker.py:276} INFO - 21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:28,762] {docker.py:276} INFO - 21/05/14 22:57:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164613616977561325652_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164613616977561325652_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164613616977561325652_0004}; taskId=attempt_202105142256164613616977561325652_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@296afab5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:28,762] {docker.py:276} INFO - 21/05/14 22:57:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:28,763] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256164613616977561325652_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164613616977561325652_0004_m_000083_371
[2021-05-14 19:57:28,765] {docker.py:276} INFO - 21/05/14 22:57:28 INFO StagingCommitter: Task committer attempt_202105142256164613616977561325652_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164613616977561325652_0004_m_000083_371 : duration 0:00.003s
[2021-05-14 19:57:30,838] {docker.py:276} INFO - 21/05/14 22:57:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256165610857483167289762_0004_m_000080_368: needsTaskCommit() Task attempt_202105142256165610857483167289762_0004_m_000080_368
[2021-05-14 19:57:30,839] {docker.py:276} INFO - 21/05/14 22:57:30 INFO StagingCommitter: Task committer attempt_202105142256165610857483167289762_0004_m_000080_368: needsTaskCommit() Task attempt_202105142256165610857483167289762_0004_m_000080_368: duration 0:00.000s
21/05/14 22:57:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165610857483167289762_0004_m_000080_368
[2021-05-14 19:57:30,841] {docker.py:276} INFO - 21/05/14 22:57:30 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4587 bytes result sent to driver
[2021-05-14 19:57:30,843] {docker.py:276} INFO - 21/05/14 22:57:30 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (5c14c3e96bf4, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:30,844] {docker.py:276} INFO - 21/05/14 22:57:30 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
21/05/14 22:57:30 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2747 ms on 5c14c3e96bf4 (executor driver) (81/200)
[2021-05-14 19:57:30,853] {docker.py:276} INFO - 21/05/14 22:57:30 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:30,855] {docker.py:276} INFO - 21/05/14 22:57:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:30,855] {docker.py:276} INFO - 21/05/14 22:57:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161734653118541885876_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161734653118541885876_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161734653118541885876_0004}; taskId=attempt_202105142256161734653118541885876_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11fb1f48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256161734653118541885876_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161734653118541885876_0004_m_000084_372
[2021-05-14 19:57:30,859] {docker.py:276} INFO - 21/05/14 22:57:30 INFO StagingCommitter: Task committer attempt_202105142256161734653118541885876_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161734653118541885876_0004_m_000084_372 : duration 0:00.004s
[2021-05-14 19:57:31,031] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256164613616977561325652_0004_m_000083_371: needsTaskCommit() Task attempt_202105142256164613616977561325652_0004_m_000083_371
[2021-05-14 19:57:31,032] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_202105142256164613616977561325652_0004_m_000083_371: needsTaskCommit() Task attempt_202105142256164613616977561325652_0004_m_000083_371: duration 0:00.001s
21/05/14 22:57:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164613616977561325652_0004_m_000083_371
[2021-05-14 19:57:31,035] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-14 19:57:31,036] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (5c14c3e96bf4, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:31,037] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2289 ms on 5c14c3e96bf4 (executor driver) (82/200)
[2021-05-14 19:57:31,038] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-14 19:57:31,045] {docker.py:276} INFO - 21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:31,047] {docker.py:276} INFO - 21/05/14 22:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:31,048] {docker.py:276} INFO - 21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162580682037089135981_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162580682037089135981_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162580682037089135981_0004}; taskId=attempt_202105142256162580682037089135981_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36546425}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:31,048] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256162580682037089135981_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162580682037089135981_0004_m_000085_373
[2021-05-14 19:57:31,051] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_202105142256162580682037089135981_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162580682037089135981_0004_m_000085_373 : duration 0:00.003s
[2021-05-14 19:57:31,193] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_20210514225616468319792217369467_0004_m_000081_369: needsTaskCommit() Task attempt_20210514225616468319792217369467_0004_m_000081_369
21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_20210514225616468319792217369467_0004_m_000081_369: needsTaskCommit() Task attempt_20210514225616468319792217369467_0004_m_000081_369: duration 0:00.001s
21/05/14 22:57:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616468319792217369467_0004_m_000081_369
[2021-05-14 19:57:31,197] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4587 bytes result sent to driver
[2021-05-14 19:57:31,198] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (5c14c3e96bf4, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:31,201] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2739 ms on 5c14c3e96bf4 (executor driver) (83/200)
[2021-05-14 19:57:31,201] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-14 19:57:31,218] {docker.py:276} INFO - 21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:31,218] {docker.py:276} INFO - 21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:31,220] {docker.py:276} INFO - 21/05/14 22:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:31,220] {docker.py:276} INFO - 21/05/14 22:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:31,221] {docker.py:276} INFO - 21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:31,221] {docker.py:276} INFO - 21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163836896557457741805_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163836896557457741805_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163836896557457741805_0004}; taskId=attempt_202105142256163836896557457741805_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10ee15f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:31,221] {docker.py:276} INFO - 21/05/14 22:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256163836896557457741805_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163836896557457741805_0004_m_000086_374
[2021-05-14 19:57:31,224] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_202105142256163836896557457741805_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163836896557457741805_0004_m_000086_374 : duration 0:00.002s
[2021-05-14 19:57:31,563] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256168806931116768538209_0004_m_000082_370: needsTaskCommit() Task attempt_202105142256168806931116768538209_0004_m_000082_370
21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_202105142256168806931116768538209_0004_m_000082_370: needsTaskCommit() Task attempt_202105142256168806931116768538209_0004_m_000082_370: duration 0:00.000s
21/05/14 22:57:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168806931116768538209_0004_m_000082_370
[2021-05-14 19:57:31,567] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4587 bytes result sent to driver
[2021-05-14 19:57:31,568] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (5c14c3e96bf4, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:31,569] {docker.py:276} INFO - 21/05/14 22:57:31 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2860 ms on 5c14c3e96bf4 (executor driver) (84/200)
[2021-05-14 19:57:31,569] {docker.py:276} INFO - 21/05/14 22:57:31 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-14 19:57:31,577] {docker.py:276} INFO - 21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:31,579] {docker.py:276} INFO - 21/05/14 22:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164014464768098045113_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164014464768098045113_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164014464768098045113_0004}; taskId=attempt_202105142256164014464768098045113_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63940983}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:31,579] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256164014464768098045113_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164014464768098045113_0004_m_000087_375
[2021-05-14 19:57:31,582] {docker.py:276} INFO - 21/05/14 22:57:31 INFO StagingCommitter: Task committer attempt_202105142256164014464768098045113_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164014464768098045113_0004_m_000087_375 : duration 0:00.003s
[2021-05-14 19:57:33,588] {docker.py:276} INFO - 21/05/14 22:57:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256162580682037089135981_0004_m_000085_373: needsTaskCommit() Task attempt_202105142256162580682037089135981_0004_m_000085_373
21/05/14 22:57:33 INFO StagingCommitter: Task committer attempt_202105142256162580682037089135981_0004_m_000085_373: needsTaskCommit() Task attempt_202105142256162580682037089135981_0004_m_000085_373: duration 0:00.000s
21/05/14 22:57:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162580682037089135981_0004_m_000085_373
[2021-05-14 19:57:33,590] {docker.py:276} INFO - 21/05/14 22:57:33 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4587 bytes result sent to driver
[2021-05-14 19:57:33,592] {docker.py:276} INFO - 21/05/14 22:57:33 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (5c14c3e96bf4, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:33,593] {docker.py:276} INFO - 21/05/14 22:57:33 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2560 ms on 5c14c3e96bf4 (executor driver) (85/200)
21/05/14 22:57:33 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-14 19:57:33,602] {docker.py:276} INFO - 21/05/14 22:57:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.8 KiB) non-empty blocks including 5 (42.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:33,604] {docker.py:276} INFO - 21/05/14 22:57:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164208057389782572803_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164208057389782572803_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164208057389782572803_0004}; taskId=attempt_202105142256164208057389782572803_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43112811}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:33,604] {docker.py:276} INFO - 21/05/14 22:57:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256164208057389782572803_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164208057389782572803_0004_m_000088_376
[2021-05-14 19:57:33,606] {docker.py:276} INFO - 21/05/14 22:57:33 INFO StagingCommitter: Task committer attempt_202105142256164208057389782572803_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164208057389782572803_0004_m_000088_376 : duration 0:00.003s
[2021-05-14 19:57:34,028] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256163836896557457741805_0004_m_000086_374: needsTaskCommit() Task attempt_202105142256163836896557457741805_0004_m_000086_374
[2021-05-14 19:57:34,030] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_202105142256163836896557457741805_0004_m_000086_374: needsTaskCommit() Task attempt_202105142256163836896557457741805_0004_m_000086_374: duration 0:00.002s
21/05/14 22:57:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163836896557457741805_0004_m_000086_374
[2021-05-14 19:57:34,032] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4587 bytes result sent to driver
[2021-05-14 19:57:34,033] {docker.py:276} INFO - 21/05/14 22:57:34 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (5c14c3e96bf4, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:34,034] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
21/05/14 22:57:34 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2840 ms on 5c14c3e96bf4 (executor driver) (86/200)
[2021-05-14 19:57:34,043] {docker.py:276} INFO - 21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:34,044] {docker.py:276} INFO - 21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:34,045] {docker.py:276} INFO - 21/05/14 22:57:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161437468557429358104_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161437468557429358104_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161437468557429358104_0004}; taskId=attempt_202105142256161437468557429358104_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c0d9faf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:34,046] {docker.py:276} INFO - 21/05/14 22:57:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256161437468557429358104_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161437468557429358104_0004_m_000089_377
[2021-05-14 19:57:34,049] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_202105142256161437468557429358104_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161437468557429358104_0004_m_000089_377 : duration 0:00.003s
[2021-05-14 19:57:34,228] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256164014464768098045113_0004_m_000087_375: needsTaskCommit() Task attempt_202105142256164014464768098045113_0004_m_000087_375
[2021-05-14 19:57:34,230] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_202105142256164014464768098045113_0004_m_000087_375: needsTaskCommit() Task attempt_202105142256164014464768098045113_0004_m_000087_375: duration 0:00.003s
21/05/14 22:57:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164014464768098045113_0004_m_000087_375
[2021-05-14 19:57:34,231] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-14 19:57:34,233] {docker.py:276} INFO - 21/05/14 22:57:34 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (5c14c3e96bf4, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:34,234] {docker.py:276} INFO - 21/05/14 22:57:34 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2671 ms on 5c14c3e96bf4 (executor driver) (87/200)
[2021-05-14 19:57:34,236] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-14 19:57:34,255] {docker.py:276} INFO - 21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:34,256] {docker.py:276} INFO - 21/05/14 22:57:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167103963383061322486_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167103963383061322486_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167103963383061322486_0004}; taskId=attempt_202105142256167103963383061322486_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@629fe6b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256167103963383061322486_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167103963383061322486_0004_m_000090_378
[2021-05-14 19:57:34,259] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_202105142256167103963383061322486_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167103963383061322486_0004_m_000090_378 : duration 0:00.003s
[2021-05-14 19:57:34,332] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256161734653118541885876_0004_m_000084_372: needsTaskCommit() Task attempt_202105142256161734653118541885876_0004_m_000084_372
[2021-05-14 19:57:34,333] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_202105142256161734653118541885876_0004_m_000084_372: needsTaskCommit() Task attempt_202105142256161734653118541885876_0004_m_000084_372: duration 0:00.001s
21/05/14 22:57:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161734653118541885876_0004_m_000084_372
[2021-05-14 19:57:34,334] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4587 bytes result sent to driver
[2021-05-14 19:57:34,336] {docker.py:276} INFO - 21/05/14 22:57:34 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (5c14c3e96bf4, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:34,337] {docker.py:276} INFO - 21/05/14 22:57:34 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-14 19:57:34,338] {docker.py:276} INFO - 21/05/14 22:57:34 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 3499 ms on 5c14c3e96bf4 (executor driver) (88/200)
[2021-05-14 19:57:34,349] {docker.py:276} INFO - 21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Getting 5 (40.0 KiB) non-empty blocks including 5 (40.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:34,351] {docker.py:276} INFO - 21/05/14 22:57:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616223272881280868407_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616223272881280868407_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616223272881280868407_0004}; taskId=attempt_20210514225616223272881280868407_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fae9b04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:34,352] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Starting: Task committer attempt_20210514225616223272881280868407_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616223272881280868407_0004_m_000091_379
[2021-05-14 19:57:34,355] {docker.py:276} INFO - 21/05/14 22:57:34 INFO StagingCommitter: Task committer attempt_20210514225616223272881280868407_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616223272881280868407_0004_m_000091_379 : duration 0:00.004s
[2021-05-14 19:57:36,316] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256164208057389782572803_0004_m_000088_376: needsTaskCommit() Task attempt_202105142256164208057389782572803_0004_m_000088_376
[2021-05-14 19:57:36,317] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Task committer attempt_202105142256164208057389782572803_0004_m_000088_376: needsTaskCommit() Task attempt_202105142256164208057389782572803_0004_m_000088_376: duration 0:00.001s
21/05/14 22:57:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164208057389782572803_0004_m_000088_376
[2021-05-14 19:57:36,319] {docker.py:276} INFO - 21/05/14 22:57:36 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-14 19:57:36,320] {docker.py:276} INFO - 21/05/14 22:57:36 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (5c14c3e96bf4, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:36,322] {docker.py:276} INFO - 21/05/14 22:57:36 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-14 19:57:36,322] {docker.py:276} INFO - 21/05/14 22:57:36 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2734 ms on 5c14c3e96bf4 (executor driver) (89/200)
[2021-05-14 19:57:36,331] {docker.py:276} INFO - 21/05/14 22:57:36 INFO ShuffleBlockFetcherIterator: Getting 5 (43.6 KiB) non-empty blocks including 5 (43.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:36,333] {docker.py:276} INFO - 21/05/14 22:57:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:36,334] {docker.py:276} INFO - 21/05/14 22:57:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164184661732415966604_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164184661732415966604_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164184661732415966604_0004}; taskId=attempt_202105142256164184661732415966604_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26dff99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:36,334] {docker.py:276} INFO - 21/05/14 22:57:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:36,334] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256164184661732415966604_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164184661732415966604_0004_m_000092_380
[2021-05-14 19:57:36,337] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Task committer attempt_202105142256164184661732415966604_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164184661732415966604_0004_m_000092_380 : duration 0:00.003s
[2021-05-14 19:57:36,811] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256161437468557429358104_0004_m_000089_377: needsTaskCommit() Task attempt_202105142256161437468557429358104_0004_m_000089_377
21/05/14 22:57:36 INFO StagingCommitter: Task committer attempt_202105142256161437468557429358104_0004_m_000089_377: needsTaskCommit() Task attempt_202105142256161437468557429358104_0004_m_000089_377: duration 0:00.000s
[2021-05-14 19:57:36,812] {docker.py:276} INFO - 21/05/14 22:57:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161437468557429358104_0004_m_000089_377
[2021-05-14 19:57:36,814] {docker.py:276} INFO - 21/05/14 22:57:36 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4587 bytes result sent to driver
[2021-05-14 19:57:36,815] {docker.py:276} INFO - 21/05/14 22:57:36 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (5c14c3e96bf4, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:36,816] {docker.py:276} INFO - 21/05/14 22:57:36 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
21/05/14 22:57:36 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 2786 ms on 5c14c3e96bf4 (executor driver) (90/200)
[2021-05-14 19:57:36,826] {docker.py:276} INFO - 21/05/14 22:57:36 INFO ShuffleBlockFetcherIterator: Getting 5 (40.2 KiB) non-empty blocks including 5 (40.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:36,828] {docker.py:276} INFO - 21/05/14 22:57:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163648913893306726107_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163648913893306726107_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163648913893306726107_0004}; taskId=attempt_202105142256163648913893306726107_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@488624bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:36 INFO StagingCommitter: Starting: Task committer attempt_202105142256163648913893306726107_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163648913893306726107_0004_m_000093_381
[2021-05-14 19:57:36,830] {docker.py:276} INFO - 21/05/14 22:57:36 INFO StagingCommitter: Task committer attempt_202105142256163648913893306726107_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163648913893306726107_0004_m_000093_381 : duration 0:00.003s
[2021-05-14 19:57:37,005] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256167103963383061322486_0004_m_000090_378: needsTaskCommit() Task attempt_202105142256167103963383061322486_0004_m_000090_378
[2021-05-14 19:57:37,006] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Task committer attempt_202105142256167103963383061322486_0004_m_000090_378: needsTaskCommit() Task attempt_202105142256167103963383061322486_0004_m_000090_378: duration 0:00.000s
[2021-05-14 19:57:37,006] {docker.py:276} INFO - 21/05/14 22:57:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167103963383061322486_0004_m_000090_378
[2021-05-14 19:57:37,007] {docker.py:276} INFO - 21/05/14 22:57:37 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4587 bytes result sent to driver
[2021-05-14 19:57:37,009] {docker.py:276} INFO - 21/05/14 22:57:37 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (5c14c3e96bf4, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:37,010] {docker.py:276} INFO - 21/05/14 22:57:37 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-14 19:57:37,011] {docker.py:276} INFO - 21/05/14 22:57:37 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2781 ms on 5c14c3e96bf4 (executor driver) (91/200)
[2021-05-14 19:57:37,027] {docker.py:276} INFO - 21/05/14 22:57:37 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:37,029] {docker.py:276} INFO - 21/05/14 22:57:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:37,030] {docker.py:276} INFO - 21/05/14 22:57:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:37,030] {docker.py:276} INFO - 21/05/14 22:57:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162698258644671747063_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162698258644671747063_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162698258644671747063_0004}; taskId=attempt_202105142256162698258644671747063_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36ac51a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:37,030] {docker.py:276} INFO - 21/05/14 22:57:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256162698258644671747063_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162698258644671747063_0004_m_000094_382
[2021-05-14 19:57:37,034] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Task committer attempt_202105142256162698258644671747063_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162698258644671747063_0004_m_000094_382 : duration 0:00.003s
[2021-05-14 19:57:37,070] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Starting: Task committer attempt_20210514225616223272881280868407_0004_m_000091_379: needsTaskCommit() Task attempt_20210514225616223272881280868407_0004_m_000091_379
[2021-05-14 19:57:37,070] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Task committer attempt_20210514225616223272881280868407_0004_m_000091_379: needsTaskCommit() Task attempt_20210514225616223272881280868407_0004_m_000091_379: duration 0:00.000s
21/05/14 22:57:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616223272881280868407_0004_m_000091_379
[2021-05-14 19:57:37,071] {docker.py:276} INFO - 21/05/14 22:57:37 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4587 bytes result sent to driver
[2021-05-14 19:57:37,073] {docker.py:276} INFO - 21/05/14 22:57:37 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (5c14c3e96bf4, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:37,073] {docker.py:276} INFO - 21/05/14 22:57:37 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2741 ms on 5c14c3e96bf4 (executor driver) (92/200)
[2021-05-14 19:57:37,074] {docker.py:276} INFO - 21/05/14 22:57:37 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-14 19:57:37,081] {docker.py:276} INFO - 21/05/14 22:57:37 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:37,083] {docker.py:276} INFO - 21/05/14 22:57:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163662810678857379396_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163662810678857379396_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163662810678857379396_0004}; taskId=attempt_202105142256163662810678857379396_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6636e890}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:37,083] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256163662810678857379396_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163662810678857379396_0004_m_000095_383
[2021-05-14 19:57:37,086] {docker.py:276} INFO - 21/05/14 22:57:37 INFO StagingCommitter: Task committer attempt_202105142256163662810678857379396_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163662810678857379396_0004_m_000095_383 : duration 0:00.003s
[2021-05-14 19:57:39,172] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256164184661732415966604_0004_m_000092_380: needsTaskCommit() Task attempt_202105142256164184661732415966604_0004_m_000092_380
21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256164184661732415966604_0004_m_000092_380: needsTaskCommit() Task attempt_202105142256164184661732415966604_0004_m_000092_380: duration 0:00.001s
[2021-05-14 19:57:39,173] {docker.py:276} INFO - 21/05/14 22:57:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164184661732415966604_0004_m_000092_380
[2021-05-14 19:57:39,173] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4587 bytes result sent to driver
[2021-05-14 19:57:39,175] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (5c14c3e96bf4, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:39,175] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2859 ms on 5c14c3e96bf4 (executor driver) (93/200)
[2021-05-14 19:57:39,176] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-14 19:57:39,184] {docker.py:276} INFO - 21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Getting 5 (40.0 KiB) non-empty blocks including 5 (40.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:39,186] {docker.py:276} INFO - 21/05/14 22:57:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161592564725850581159_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161592564725850581159_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161592564725850581159_0004}; taskId=attempt_202105142256161592564725850581159_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dd9a7bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:39,187] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256161592564725850581159_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161592564725850581159_0004_m_000096_384
[2021-05-14 19:57:39,189] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256161592564725850581159_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161592564725850581159_0004_m_000096_384 : duration 0:00.003s
[2021-05-14 19:57:39,672] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256163648913893306726107_0004_m_000093_381: needsTaskCommit() Task attempt_202105142256163648913893306726107_0004_m_000093_381
[2021-05-14 19:57:39,673] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256163648913893306726107_0004_m_000093_381: needsTaskCommit() Task attempt_202105142256163648913893306726107_0004_m_000093_381: duration 0:00.002s
21/05/14 22:57:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163648913893306726107_0004_m_000093_381
[2021-05-14 19:57:39,675] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4587 bytes result sent to driver
[2021-05-14 19:57:39,676] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (5c14c3e96bf4, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:39,678] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
21/05/14 22:57:39 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2866 ms on 5c14c3e96bf4 (executor driver) (94/200)
[2021-05-14 19:57:39,688] {docker.py:276} INFO - 21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:39,690] {docker.py:276} INFO - 21/05/14 22:57:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:39,691] {docker.py:276} INFO - 21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162475580838876614406_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162475580838876614406_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162475580838876614406_0004}; taskId=attempt_202105142256162475580838876614406_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@417f771a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256162475580838876614406_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162475580838876614406_0004_m_000097_385
[2021-05-14 19:57:39,693] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256162475580838876614406_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162475580838876614406_0004_m_000097_385 : duration 0:00.003s
[2021-05-14 19:57:39,811] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256162698258644671747063_0004_m_000094_382: needsTaskCommit() Task attempt_202105142256162698258644671747063_0004_m_000094_382
[2021-05-14 19:57:39,812] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256162698258644671747063_0004_m_000094_382: needsTaskCommit() Task attempt_202105142256162698258644671747063_0004_m_000094_382: duration 0:00.001s
21/05/14 22:57:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162698258644671747063_0004_m_000094_382
[2021-05-14 19:57:39,813] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4587 bytes result sent to driver
[2021-05-14 19:57:39,814] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (5c14c3e96bf4, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:39,816] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2811 ms on 5c14c3e96bf4 (executor driver) (95/200)
[2021-05-14 19:57:39,817] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-14 19:57:39,826] {docker.py:276} INFO - 21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:39,827] {docker.py:276} INFO - 21/05/14 22:57:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161484865922799150778_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161484865922799150778_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161484865922799150778_0004}; taskId=attempt_202105142256161484865922799150778_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@662dc0a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:39,828] {docker.py:276} INFO - 21/05/14 22:57:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256161484865922799150778_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161484865922799150778_0004_m_000098_386
[2021-05-14 19:57:39,830] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256161484865922799150778_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161484865922799150778_0004_m_000098_386 : duration 0:00.002s
[2021-05-14 19:57:39,905] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256163662810678857379396_0004_m_000095_383: needsTaskCommit() Task attempt_202105142256163662810678857379396_0004_m_000095_383
[2021-05-14 19:57:39,906] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256163662810678857379396_0004_m_000095_383: needsTaskCommit() Task attempt_202105142256163662810678857379396_0004_m_000095_383: duration 0:00.001s
[2021-05-14 19:57:39,907] {docker.py:276} INFO - 21/05/14 22:57:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163662810678857379396_0004_m_000095_383
[2021-05-14 19:57:39,909] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4587 bytes result sent to driver
[2021-05-14 19:57:39,910] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (5c14c3e96bf4, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:39,911] {docker.py:276} INFO - 21/05/14 22:57:39 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-14 19:57:39,912] {docker.py:276} INFO - 21/05/14 22:57:39 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2843 ms on 5c14c3e96bf4 (executor driver) (96/200)
[2021-05-14 19:57:39,919] {docker.py:276} INFO - 21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:39,921] {docker.py:276} INFO - 21/05/14 22:57:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:39,921] {docker.py:276} INFO - 21/05/14 22:57:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161844745220081319230_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161844745220081319230_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161844745220081319230_0004}; taskId=attempt_202105142256161844745220081319230_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7878042f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256161844745220081319230_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161844745220081319230_0004_m_000099_387
[2021-05-14 19:57:39,923] {docker.py:276} INFO - 21/05/14 22:57:39 INFO StagingCommitter: Task committer attempt_202105142256161844745220081319230_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161844745220081319230_0004_m_000099_387 : duration 0:00.003s
[2021-05-14 19:57:41,817] {docker.py:276} INFO - 21/05/14 22:57:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161592564725850581159_0004_m_000096_384: needsTaskCommit() Task attempt_202105142256161592564725850581159_0004_m_000096_384
[2021-05-14 19:57:41,818] {docker.py:276} INFO - 21/05/14 22:57:41 INFO StagingCommitter: Task committer attempt_202105142256161592564725850581159_0004_m_000096_384: needsTaskCommit() Task attempt_202105142256161592564725850581159_0004_m_000096_384: duration 0:00.001s
21/05/14 22:57:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161592564725850581159_0004_m_000096_384
[2021-05-14 19:57:41,822] {docker.py:276} INFO - 21/05/14 22:57:41 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4587 bytes result sent to driver
[2021-05-14 19:57:41,823] {docker.py:276} INFO - 21/05/14 22:57:41 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (5c14c3e96bf4, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:41,823] {docker.py:276} INFO - 21/05/14 22:57:41 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-14 19:57:41,824] {docker.py:276} INFO - 21/05/14 22:57:41 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2653 ms on 5c14c3e96bf4 (executor driver) (97/200)
[2021-05-14 19:57:41,832] {docker.py:276} INFO - 21/05/14 22:57:41 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:41,834] {docker.py:276} INFO - 21/05/14 22:57:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:41,835] {docker.py:276} INFO - 21/05/14 22:57:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166005276580915895675_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166005276580915895675_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166005276580915895675_0004}; taskId=attempt_202105142256166005276580915895675_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4fbc27cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256166005276580915895675_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166005276580915895675_0004_m_000100_388
[2021-05-14 19:57:41,837] {docker.py:276} INFO - 21/05/14 22:57:41 INFO StagingCommitter: Task committer attempt_202105142256166005276580915895675_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166005276580915895675_0004_m_000100_388 : duration 0:00.002s
[2021-05-14 19:57:41,998] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256161484865922799150778_0004_m_000098_386: needsTaskCommit() Task attempt_202105142256161484865922799150778_0004_m_000098_386
21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_202105142256161484865922799150778_0004_m_000098_386: needsTaskCommit() Task attempt_202105142256161484865922799150778_0004_m_000098_386: duration 0:00.000s
21/05/14 22:57:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161484865922799150778_0004_m_000098_386
[2021-05-14 19:57:42,000] {docker.py:276} INFO - 21/05/14 22:57:42 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4587 bytes result sent to driver
[2021-05-14 19:57:42,001] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (5c14c3e96bf4, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:42,005] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2191 ms on 5c14c3e96bf4 (executor driver) (98/200)
21/05/14 22:57:42 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-14 19:57:42,011] {docker.py:276} INFO - 21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:42,014] {docker.py:276} INFO - 21/05/14 22:57:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164253616001994202654_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164253616001994202654_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164253616001994202654_0004}; taskId=attempt_202105142256164253616001994202654_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@671719d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:42,014] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256164253616001994202654_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164253616001994202654_0004_m_000101_389
[2021-05-14 19:57:42,017] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_202105142256164253616001994202654_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164253616001994202654_0004_m_000101_389 : duration 0:00.003s
[2021-05-14 19:57:42,296] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256162475580838876614406_0004_m_000097_385: needsTaskCommit() Task attempt_202105142256162475580838876614406_0004_m_000097_385
[2021-05-14 19:57:42,296] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_202105142256162475580838876614406_0004_m_000097_385: needsTaskCommit() Task attempt_202105142256162475580838876614406_0004_m_000097_385: duration 0:00.002s
21/05/14 22:57:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162475580838876614406_0004_m_000097_385
[2021-05-14 19:57:42,299] {docker.py:276} INFO - 21/05/14 22:57:42 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4587 bytes result sent to driver
[2021-05-14 19:57:42,301] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (5c14c3e96bf4, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:42,302] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2628 ms on 5c14c3e96bf4 (executor driver) (99/200)
[2021-05-14 19:57:42,302] {docker.py:276} INFO - 21/05/14 22:57:42 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-14 19:57:42,318] {docker.py:276} INFO - 21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:42,319] {docker.py:276} INFO - 21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-14 19:57:42,321] {docker.py:276} INFO - 21/05/14 22:57:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167173119858348205544_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167173119858348205544_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167173119858348205544_0004}; taskId=attempt_202105142256167173119858348205544_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f183b4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:42,322] {docker.py:276} INFO - 21/05/14 22:57:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256167173119858348205544_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167173119858348205544_0004_m_000102_390
[2021-05-14 19:57:42,325] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_202105142256167173119858348205544_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167173119858348205544_0004_m_000102_390 : duration 0:00.004s
[2021-05-14 19:57:42,865] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256161844745220081319230_0004_m_000099_387: needsTaskCommit() Task attempt_202105142256161844745220081319230_0004_m_000099_387
[2021-05-14 19:57:42,866] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_202105142256161844745220081319230_0004_m_000099_387: needsTaskCommit() Task attempt_202105142256161844745220081319230_0004_m_000099_387: duration 0:00.000s
21/05/14 22:57:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161844745220081319230_0004_m_000099_387
[2021-05-14 19:57:42,867] {docker.py:276} INFO - 21/05/14 22:57:42 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-14 19:57:42,868] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (5c14c3e96bf4, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:42,869] {docker.py:276} INFO - 21/05/14 22:57:42 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2962 ms on 5c14c3e96bf4 (executor driver) (100/200)
[2021-05-14 19:57:42,869] {docker.py:276} INFO - 21/05/14 22:57:42 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-14 19:57:42,878] {docker.py:276} INFO - 21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:42,878] {docker.py:276} INFO - 21/05/14 22:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:57:42,881] {docker.py:276} INFO - 21/05/14 22:57:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:42,881] {docker.py:276} INFO - 21/05/14 22:57:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:42,882] {docker.py:276} INFO - 21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616206845180995064518_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616206845180995064518_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616206845180995064518_0004}; taskId=attempt_20210514225616206845180995064518_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65c1dd3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:42 INFO StagingCommitter: Starting: Task committer attempt_20210514225616206845180995064518_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616206845180995064518_0004_m_000103_391
[2021-05-14 19:57:42,885] {docker.py:276} INFO - 21/05/14 22:57:42 INFO StagingCommitter: Task committer attempt_20210514225616206845180995064518_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616206845180995064518_0004_m_000103_391 : duration 0:00.003s
[2021-05-14 19:57:44,725] {docker.py:276} INFO - 21/05/14 22:57:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256164253616001994202654_0004_m_000101_389: needsTaskCommit() Task attempt_202105142256164253616001994202654_0004_m_000101_389
[2021-05-14 19:57:44,732] {docker.py:276} INFO - 21/05/14 22:57:44 INFO StagingCommitter: Task committer attempt_202105142256164253616001994202654_0004_m_000101_389: needsTaskCommit() Task attempt_202105142256164253616001994202654_0004_m_000101_389: duration 0:00.001s
21/05/14 22:57:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164253616001994202654_0004_m_000101_389
21/05/14 22:57:44 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4587 bytes result sent to driver
[2021-05-14 19:57:44,734] {docker.py:276} INFO - 21/05/14 22:57:44 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (5c14c3e96bf4, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:44,736] {docker.py:276} INFO - 21/05/14 22:57:44 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
21/05/14 22:57:44 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2738 ms on 5c14c3e96bf4 (executor driver) (101/200)
[2021-05-14 19:57:44,744] {docker.py:276} INFO - 21/05/14 22:57:44 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:44,746] {docker.py:276} INFO - 21/05/14 22:57:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165145053138957256825_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165145053138957256825_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165145053138957256825_0004}; taskId=attempt_202105142256165145053138957256825_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@139e4c0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:44,746] {docker.py:276} INFO - 21/05/14 22:57:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256165145053138957256825_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165145053138957256825_0004_m_000104_392
[2021-05-14 19:57:44,749] {docker.py:276} INFO - 21/05/14 22:57:44 INFO StagingCommitter: Task committer attempt_202105142256165145053138957256825_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165145053138957256825_0004_m_000104_392 : duration 0:00.003s
[2021-05-14 19:57:45,145] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256167173119858348205544_0004_m_000102_390: needsTaskCommit() Task attempt_202105142256167173119858348205544_0004_m_000102_390
21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_202105142256167173119858348205544_0004_m_000102_390: needsTaskCommit() Task attempt_202105142256167173119858348205544_0004_m_000102_390: duration 0:00.001s
21/05/14 22:57:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167173119858348205544_0004_m_000102_390
[2021-05-14 19:57:45,148] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4587 bytes result sent to driver
21/05/14 22:57:45 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (5c14c3e96bf4, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:45,149] {docker.py:276} INFO - 21/05/14 22:57:45 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2851 ms on 5c14c3e96bf4 (executor driver) (102/200)
[2021-05-14 19:57:45,149] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-14 19:57:45,159] {docker.py:276} INFO - 21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:45,161] {docker.py:276} INFO - 21/05/14 22:57:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:45,161] {docker.py:276} INFO - 21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168056233997788655827_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168056233997788655827_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168056233997788655827_0004}; taskId=attempt_202105142256168056233997788655827_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@598d5ad6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:45,162] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256168056233997788655827_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168056233997788655827_0004_m_000105_393
[2021-05-14 19:57:45,165] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_202105142256168056233997788655827_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168056233997788655827_0004_m_000105_393 : duration 0:00.003s
[2021-05-14 19:57:45,181] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256166005276580915895675_0004_m_000100_388: needsTaskCommit() Task attempt_202105142256166005276580915895675_0004_m_000100_388
21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_202105142256166005276580915895675_0004_m_000100_388: needsTaskCommit() Task attempt_202105142256166005276580915895675_0004_m_000100_388: duration 0:00.000s
[2021-05-14 19:57:45,181] {docker.py:276} INFO - 21/05/14 22:57:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166005276580915895675_0004_m_000100_388
[2021-05-14 19:57:45,182] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4587 bytes result sent to driver
[2021-05-14 19:57:45,182] {docker.py:276} INFO - 21/05/14 22:57:45 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (5c14c3e96bf4, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:45,184] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
21/05/14 22:57:45 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 3366 ms on 5c14c3e96bf4 (executor driver) (103/200)
[2021-05-14 19:57:45,190] {docker.py:276} INFO - 21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:45,192] {docker.py:276} INFO - 21/05/14 22:57:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162159379360918959322_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162159379360918959322_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162159379360918959322_0004}; taskId=attempt_202105142256162159379360918959322_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dbe1348}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256162159379360918959322_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162159379360918959322_0004_m_000106_394
[2021-05-14 19:57:45,194] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_202105142256162159379360918959322_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162159379360918959322_0004_m_000106_394 : duration 0:00.002s
[2021-05-14 19:57:45,604] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_20210514225616206845180995064518_0004_m_000103_391: needsTaskCommit() Task attempt_20210514225616206845180995064518_0004_m_000103_391
[2021-05-14 19:57:45,605] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_20210514225616206845180995064518_0004_m_000103_391: needsTaskCommit() Task attempt_20210514225616206845180995064518_0004_m_000103_391: duration 0:00.001s
21/05/14 22:57:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616206845180995064518_0004_m_000103_391
[2021-05-14 19:57:45,606] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4587 bytes result sent to driver
[2021-05-14 19:57:45,607] {docker.py:276} INFO - 21/05/14 22:57:45 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (5c14c3e96bf4, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:45,607] {docker.py:276} INFO - 21/05/14 22:57:45 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-14 19:57:45,608] {docker.py:276} INFO - 21/05/14 22:57:45 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2744 ms on 5c14c3e96bf4 (executor driver) (104/200)
[2021-05-14 19:57:45,618] {docker.py:276} INFO - 21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:45,620] {docker.py:276} INFO - 21/05/14 22:57:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164977008046312926594_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164977008046312926594_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164977008046312926594_0004}; taskId=attempt_202105142256164977008046312926594_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3350ead9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:45,620] {docker.py:276} INFO - 21/05/14 22:57:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256164977008046312926594_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164977008046312926594_0004_m_000107_395
[2021-05-14 19:57:45,622] {docker.py:276} INFO - 21/05/14 22:57:45 INFO StagingCommitter: Task committer attempt_202105142256164977008046312926594_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164977008046312926594_0004_m_000107_395 : duration 0:00.002s
[2021-05-14 19:57:47,515] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256165145053138957256825_0004_m_000104_392: needsTaskCommit() Task attempt_202105142256165145053138957256825_0004_m_000104_392
[2021-05-14 19:57:47,517] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Task committer attempt_202105142256165145053138957256825_0004_m_000104_392: needsTaskCommit() Task attempt_202105142256165145053138957256825_0004_m_000104_392: duration 0:00.001s
21/05/14 22:57:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165145053138957256825_0004_m_000104_392
[2021-05-14 19:57:47,518] {docker.py:276} INFO - 21/05/14 22:57:47 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4587 bytes result sent to driver
[2021-05-14 19:57:47,520] {docker.py:276} INFO - 21/05/14 22:57:47 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (5c14c3e96bf4, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:57:47 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 2790 ms on 5c14c3e96bf4 (executor driver) (105/200)
21/05/14 22:57:47 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-14 19:57:47,531] {docker.py:276} INFO - 21/05/14 22:57:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:47,532] {docker.py:276} INFO - 21/05/14 22:57:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616210649248577323937_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616210649248577323937_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616210649248577323937_0004}; taskId=attempt_20210514225616210649248577323937_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fec0835}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:47,533] {docker.py:276} INFO - 21/05/14 22:57:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:47 INFO StagingCommitter: Starting: Task committer attempt_20210514225616210649248577323937_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616210649248577323937_0004_m_000108_396
[2021-05-14 19:57:47,536] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Task committer attempt_20210514225616210649248577323937_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616210649248577323937_0004_m_000108_396 : duration 0:00.003s
[2021-05-14 19:57:47,770] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256168056233997788655827_0004_m_000105_393: needsTaskCommit() Task attempt_202105142256168056233997788655827_0004_m_000105_393
[2021-05-14 19:57:47,771] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Task committer attempt_202105142256168056233997788655827_0004_m_000105_393: needsTaskCommit() Task attempt_202105142256168056233997788655827_0004_m_000105_393: duration 0:00.001s
21/05/14 22:57:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168056233997788655827_0004_m_000105_393
[2021-05-14 19:57:47,773] {docker.py:276} INFO - 21/05/14 22:57:47 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4587 bytes result sent to driver
[2021-05-14 19:57:47,774] {docker.py:276} INFO - 21/05/14 22:57:47 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (5c14c3e96bf4, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:47,775] {docker.py:276} INFO - 21/05/14 22:57:47 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 2630 ms on 5c14c3e96bf4 (executor driver) (106/200)
[2021-05-14 19:57:47,776] {docker.py:276} INFO - 21/05/14 22:57:47 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-14 19:57:47,786] {docker.py:276} INFO - 21/05/14 22:57:47 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:47,788] {docker.py:276} INFO - 21/05/14 22:57:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163931968807979526903_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163931968807979526903_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163931968807979526903_0004}; taskId=attempt_202105142256163931968807979526903_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5371691e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256163931968807979526903_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163931968807979526903_0004_m_000109_397
[2021-05-14 19:57:47,791] {docker.py:276} INFO - 21/05/14 22:57:47 INFO StagingCommitter: Task committer attempt_202105142256163931968807979526903_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163931968807979526903_0004_m_000109_397 : duration 0:00.002s
[2021-05-14 19:57:47,973] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256164977008046312926594_0004_m_000107_395: needsTaskCommit() Task attempt_202105142256164977008046312926594_0004_m_000107_395
[2021-05-14 19:57:47,974] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Task committer attempt_202105142256164977008046312926594_0004_m_000107_395: needsTaskCommit() Task attempt_202105142256164977008046312926594_0004_m_000107_395: duration 0:00.001s
21/05/14 22:57:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164977008046312926594_0004_m_000107_395
[2021-05-14 19:57:47,975] {docker.py:276} INFO - 21/05/14 22:57:48 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
[2021-05-14 19:57:47,976] {docker.py:276} INFO - 21/05/14 22:57:48 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (5c14c3e96bf4, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:47,978] {docker.py:276} INFO - 21/05/14 22:57:48 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 2373 ms on 5c14c3e96bf4 (executor driver) (107/200)
[2021-05-14 19:57:47,978] {docker.py:276} INFO - 21/05/14 22:57:48 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-14 19:57:47,996] {docker.py:276} INFO - 21/05/14 22:57:48 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:47,998] {docker.py:276} INFO - 21/05/14 22:57:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167517435275132818222_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167517435275132818222_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167517435275132818222_0004}; taskId=attempt_202105142256167517435275132818222_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@218bd0ed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:47,999] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256167517435275132818222_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167517435275132818222_0004_m_000110_398
[2021-05-14 19:57:48,002] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Task committer attempt_202105142256167517435275132818222_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167517435275132818222_0004_m_000110_398 : duration 0:00.002s
[2021-05-14 19:57:48,075] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256162159379360918959322_0004_m_000106_394: needsTaskCommit() Task attempt_202105142256162159379360918959322_0004_m_000106_394
[2021-05-14 19:57:48,076] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Task committer attempt_202105142256162159379360918959322_0004_m_000106_394: needsTaskCommit() Task attempt_202105142256162159379360918959322_0004_m_000106_394: duration 0:00.001s
21/05/14 22:57:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162159379360918959322_0004_m_000106_394
[2021-05-14 19:57:48,077] {docker.py:276} INFO - 21/05/14 22:57:48 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4587 bytes result sent to driver
[2021-05-14 19:57:48,078] {docker.py:276} INFO - 21/05/14 22:57:48 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (5c14c3e96bf4, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:48,079] {docker.py:276} INFO - 21/05/14 22:57:48 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 2899 ms on 5c14c3e96bf4 (executor driver) (108/200)
[2021-05-14 19:57:48,079] {docker.py:276} INFO - 21/05/14 22:57:48 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-14 19:57:48,089] {docker.py:276} INFO - 21/05/14 22:57:48 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:48,091] {docker.py:276} INFO - 21/05/14 22:57:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256169040249659151903676_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169040249659151903676_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256169040249659151903676_0004}; taskId=attempt_202105142256169040249659151903676_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7976b832}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256169040249659151903676_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169040249659151903676_0004_m_000111_399
[2021-05-14 19:57:48,093] {docker.py:276} INFO - 21/05/14 22:57:48 INFO StagingCommitter: Task committer attempt_202105142256169040249659151903676_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256169040249659151903676_0004_m_000111_399 : duration 0:00.003s
[2021-05-14 19:57:50,215] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_20210514225616210649248577323937_0004_m_000108_396: needsTaskCommit() Task attempt_20210514225616210649248577323937_0004_m_000108_396
21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_20210514225616210649248577323937_0004_m_000108_396: needsTaskCommit() Task attempt_20210514225616210649248577323937_0004_m_000108_396: duration 0:00.001s
21/05/14 22:57:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616210649248577323937_0004_m_000108_396
[2021-05-14 19:57:50,216] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4587 bytes result sent to driver
[2021-05-14 19:57:50,217] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (5c14c3e96bf4, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:50,218] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-14 19:57:50,219] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 2669 ms on 5c14c3e96bf4 (executor driver) (109/200)
[2021-05-14 19:57:50,229] {docker.py:276} INFO - 21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:50,230] {docker.py:276} INFO - 21/05/14 22:57:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616919767707411365977_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616919767707411365977_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616919767707411365977_0004}; taskId=attempt_20210514225616919767707411365977_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2695e6df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:50,231] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_20210514225616919767707411365977_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616919767707411365977_0004_m_000112_400
[2021-05-14 19:57:50,233] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_20210514225616919767707411365977_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616919767707411365977_0004_m_000112_400 : duration 0:00.003s
[2021-05-14 19:57:50,564] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256167517435275132818222_0004_m_000110_398: needsTaskCommit() Task attempt_202105142256167517435275132818222_0004_m_000110_398
[2021-05-14 19:57:50,565] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_202105142256167517435275132818222_0004_m_000110_398: needsTaskCommit() Task attempt_202105142256167517435275132818222_0004_m_000110_398: duration 0:00.001s
21/05/14 22:57:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167517435275132818222_0004_m_000110_398
[2021-05-14 19:57:50,566] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4587 bytes result sent to driver
[2021-05-14 19:57:50,567] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (5c14c3e96bf4, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:50,568] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2561 ms on 5c14c3e96bf4 (executor driver) (110/200)
[2021-05-14 19:57:50,569] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-14 19:57:50,581] {docker.py:276} INFO - 21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Getting 5 (40.1 KiB) non-empty blocks including 5 (40.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:50,583] {docker.py:276} INFO - 21/05/14 22:57:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167211071679783237780_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167211071679783237780_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167211071679783237780_0004}; taskId=attempt_202105142256167211071679783237780_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4de26927}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256167211071679783237780_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167211071679783237780_0004_m_000113_401
[2021-05-14 19:57:50,586] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_202105142256167211071679783237780_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167211071679783237780_0004_m_000113_401 : duration 0:00.003s
[2021-05-14 19:57:50,959] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256169040249659151903676_0004_m_000111_399: needsTaskCommit() Task attempt_202105142256169040249659151903676_0004_m_000111_399
[2021-05-14 19:57:50,960] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_202105142256169040249659151903676_0004_m_000111_399: needsTaskCommit() Task attempt_202105142256169040249659151903676_0004_m_000111_399: duration 0:00.001s
21/05/14 22:57:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256169040249659151903676_0004_m_000111_399
[2021-05-14 19:57:50,961] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4587 bytes result sent to driver
[2021-05-14 19:57:50,964] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (5c14c3e96bf4, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:50,965] {docker.py:276} INFO - 21/05/14 22:57:50 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-14 19:57:50,966] {docker.py:276} INFO - 21/05/14 22:57:50 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2857 ms on 5c14c3e96bf4 (executor driver) (111/200)
[2021-05-14 19:57:50,974] {docker.py:276} INFO - 21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:50,976] {docker.py:276} INFO - 21/05/14 22:57:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161695931250134746993_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161695931250134746993_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161695931250134746993_0004}; taskId=attempt_202105142256161695931250134746993_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c049e4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:50,976] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256161695931250134746993_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161695931250134746993_0004_m_000114_402
[2021-05-14 19:57:50,978] {docker.py:276} INFO - 21/05/14 22:57:50 INFO StagingCommitter: Task committer attempt_202105142256161695931250134746993_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161695931250134746993_0004_m_000114_402 : duration 0:00.002s
[2021-05-14 19:57:51,141] {docker.py:276} INFO - 21/05/14 22:57:51 INFO StagingCommitter: Starting: Task committer attempt_202105142256163931968807979526903_0004_m_000109_397: needsTaskCommit() Task attempt_202105142256163931968807979526903_0004_m_000109_397
21/05/14 22:57:51 INFO StagingCommitter: Task committer attempt_202105142256163931968807979526903_0004_m_000109_397: needsTaskCommit() Task attempt_202105142256163931968807979526903_0004_m_000109_397: duration 0:00.001s
21/05/14 22:57:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163931968807979526903_0004_m_000109_397
[2021-05-14 19:57:51,144] {docker.py:276} INFO - 21/05/14 22:57:51 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4587 bytes result sent to driver
[2021-05-14 19:57:51,145] {docker.py:276} INFO - 21/05/14 22:57:51 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (5c14c3e96bf4, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:51,147] {docker.py:276} INFO - 21/05/14 22:57:51 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-14 19:57:51,147] {docker.py:276} INFO - 21/05/14 22:57:51 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 3343 ms on 5c14c3e96bf4 (executor driver) (112/200)
[2021-05-14 19:57:51,156] {docker.py:276} INFO - 21/05/14 22:57:51 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:51,158] {docker.py:276} INFO - 21/05/14 22:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168020281168965569028_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168020281168965569028_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168020281168965569028_0004}; taskId=attempt_202105142256168020281168965569028_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b73cbf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:51,158] {docker.py:276} INFO - 21/05/14 22:57:51 INFO StagingCommitter: Starting: Task committer attempt_202105142256168020281168965569028_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168020281168965569028_0004_m_000115_403
[2021-05-14 19:57:51,160] {docker.py:276} INFO - 21/05/14 22:57:51 INFO StagingCommitter: Task committer attempt_202105142256168020281168965569028_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168020281168965569028_0004_m_000115_403 : duration 0:00.002s
[2021-05-14 19:57:52,611] {docker.py:276} INFO - 21/05/14 22:57:52 INFO StagingCommitter: Starting: Task committer attempt_20210514225616919767707411365977_0004_m_000112_400: needsTaskCommit() Task attempt_20210514225616919767707411365977_0004_m_000112_400
[2021-05-14 19:57:52,613] {docker.py:276} INFO - 21/05/14 22:57:52 INFO StagingCommitter: Task committer attempt_20210514225616919767707411365977_0004_m_000112_400: needsTaskCommit() Task attempt_20210514225616919767707411365977_0004_m_000112_400: duration 0:00.002s
21/05/14 22:57:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616919767707411365977_0004_m_000112_400
[2021-05-14 19:57:52,615] {docker.py:276} INFO - 21/05/14 22:57:52 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4587 bytes result sent to driver
[2021-05-14 19:57:52,616] {docker.py:276} INFO - 21/05/14 22:57:52 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (5c14c3e96bf4, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:52,617] {docker.py:276} INFO - 21/05/14 22:57:52 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-14 19:57:52,617] {docker.py:276} INFO - 21/05/14 22:57:52 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 2403 ms on 5c14c3e96bf4 (executor driver) (113/200)
[2021-05-14 19:57:52,626] {docker.py:276} INFO - 21/05/14 22:57:52 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:52,628] {docker.py:276} INFO - 21/05/14 22:57:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163922890009879504516_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163922890009879504516_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163922890009879504516_0004}; taskId=attempt_202105142256163922890009879504516_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7774f98d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:52 INFO StagingCommitter: Starting: Task committer attempt_202105142256163922890009879504516_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163922890009879504516_0004_m_000116_404
[2021-05-14 19:57:52,631] {docker.py:276} INFO - 21/05/14 22:57:52 INFO StagingCommitter: Task committer attempt_202105142256163922890009879504516_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163922890009879504516_0004_m_000116_404 : duration 0:00.002s
[2021-05-14 19:57:53,586] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256167211071679783237780_0004_m_000113_401: needsTaskCommit() Task attempt_202105142256167211071679783237780_0004_m_000113_401
[2021-05-14 19:57:53,587] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256167211071679783237780_0004_m_000113_401: needsTaskCommit() Task attempt_202105142256167211071679783237780_0004_m_000113_401: duration 0:00.001s
21/05/14 22:57:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167211071679783237780_0004_m_000113_401
[2021-05-14 19:57:53,591] {docker.py:276} INFO - 21/05/14 22:57:53 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4587 bytes result sent to driver
[2021-05-14 19:57:53,592] {docker.py:276} INFO - 21/05/14 22:57:53 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (5c14c3e96bf4, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:53,593] {docker.py:276} INFO - 21/05/14 22:57:53 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
21/05/14 22:57:53 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 3030 ms on 5c14c3e96bf4 (executor driver) (114/200)
[2021-05-14 19:57:53,602] {docker.py:276} INFO - 21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:53,604] {docker.py:276} INFO - 21/05/14 22:57:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:53,605] {docker.py:276} INFO - 21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168755737087600974932_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755737087600974932_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168755737087600974932_0004}; taskId=attempt_202105142256168755737087600974932_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dbc3954}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:53,605] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256168755737087600974932_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755737087600974932_0004_m_000117_405
[2021-05-14 19:57:53,607] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256168755737087600974932_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755737087600974932_0004_m_000117_405 : duration 0:00.003s
[2021-05-14 19:57:53,710] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256161695931250134746993_0004_m_000114_402: needsTaskCommit() Task attempt_202105142256161695931250134746993_0004_m_000114_402
[2021-05-14 19:57:53,711] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256161695931250134746993_0004_m_000114_402: needsTaskCommit() Task attempt_202105142256161695931250134746993_0004_m_000114_402: duration 0:00.002s
21/05/14 22:57:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161695931250134746993_0004_m_000114_402
[2021-05-14 19:57:53,713] {docker.py:276} INFO - 21/05/14 22:57:53 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4587 bytes result sent to driver
[2021-05-14 19:57:53,714] {docker.py:276} INFO - 21/05/14 22:57:53 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (5c14c3e96bf4, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:53,715] {docker.py:276} INFO - 21/05/14 22:57:53 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2756 ms on 5c14c3e96bf4 (executor driver) (115/200)
21/05/14 22:57:53 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-14 19:57:53,726] {docker.py:276} INFO - 21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:53,726] {docker.py:276} INFO - 21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:53,728] {docker.py:276} INFO - 21/05/14 22:57:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:53,728] {docker.py:276} INFO - 21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161011158569872078899_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161011158569872078899_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161011158569872078899_0004}; taskId=attempt_202105142256161011158569872078899_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63be9775}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:53,729] {docker.py:276} INFO - 21/05/14 22:57:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256161011158569872078899_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161011158569872078899_0004_m_000118_406
[2021-05-14 19:57:53,732] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256161011158569872078899_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161011158569872078899_0004_m_000118_406 : duration 0:00.003s
[2021-05-14 19:57:53,826] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256168020281168965569028_0004_m_000115_403: needsTaskCommit() Task attempt_202105142256168020281168965569028_0004_m_000115_403
[2021-05-14 19:57:53,827] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256168020281168965569028_0004_m_000115_403: needsTaskCommit() Task attempt_202105142256168020281168965569028_0004_m_000115_403: duration 0:00.002s
21/05/14 22:57:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168020281168965569028_0004_m_000115_403
[2021-05-14 19:57:53,829] {docker.py:276} INFO - 21/05/14 22:57:53 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4587 bytes result sent to driver
[2021-05-14 19:57:53,830] {docker.py:276} INFO - 21/05/14 22:57:53 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (5c14c3e96bf4, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:53,831] {docker.py:276} INFO - 21/05/14 22:57:53 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
21/05/14 22:57:53 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2690 ms on 5c14c3e96bf4 (executor driver) (116/200)
[2021-05-14 19:57:53,841] {docker.py:276} INFO - 21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:53,841] {docker.py:276} INFO - 21/05/14 22:57:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:53,843] {docker.py:276} INFO - 21/05/14 22:57:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:53,844] {docker.py:276} INFO - 21/05/14 22:57:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167129327757261598298_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167129327757261598298_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167129327757261598298_0004}; taskId=attempt_202105142256167129327757261598298_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@489587d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:53,844] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Starting: Task committer attempt_202105142256167129327757261598298_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167129327757261598298_0004_m_000119_407
[2021-05-14 19:57:53,847] {docker.py:276} INFO - 21/05/14 22:57:53 INFO StagingCommitter: Task committer attempt_202105142256167129327757261598298_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167129327757261598298_0004_m_000119_407 : duration 0:00.003s
[2021-05-14 19:57:55,246] {docker.py:276} INFO - 21/05/14 22:57:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256163922890009879504516_0004_m_000116_404: needsTaskCommit() Task attempt_202105142256163922890009879504516_0004_m_000116_404
[2021-05-14 19:57:55,247] {docker.py:276} INFO - 21/05/14 22:57:55 INFO StagingCommitter: Task committer attempt_202105142256163922890009879504516_0004_m_000116_404: needsTaskCommit() Task attempt_202105142256163922890009879504516_0004_m_000116_404: duration 0:00.001s
21/05/14 22:57:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163922890009879504516_0004_m_000116_404
[2021-05-14 19:57:55,249] {docker.py:276} INFO - 21/05/14 22:57:55 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4587 bytes result sent to driver
[2021-05-14 19:57:55,250] {docker.py:276} INFO - 21/05/14 22:57:55 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (5c14c3e96bf4, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:55,252] {docker.py:276} INFO - 21/05/14 22:57:55 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-14 19:57:55,252] {docker.py:276} INFO - 21/05/14 22:57:55 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 2639 ms on 5c14c3e96bf4 (executor driver) (117/200)
[2021-05-14 19:57:55,262] {docker.py:276} INFO - 21/05/14 22:57:55 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:55,265] {docker.py:276} INFO - 21/05/14 22:57:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:55,265] {docker.py:276} INFO - 21/05/14 22:57:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166854346247247277165_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166854346247247277165_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166854346247247277165_0004}; taskId=attempt_202105142256166854346247247277165_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a67d1bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:55,265] {docker.py:276} INFO - 21/05/14 22:57:55 INFO StagingCommitter: Starting: Task committer attempt_202105142256166854346247247277165_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166854346247247277165_0004_m_000120_408
[2021-05-14 19:57:55,268] {docker.py:276} INFO - 21/05/14 22:57:55 INFO StagingCommitter: Task committer attempt_202105142256166854346247247277165_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166854346247247277165_0004_m_000120_408 : duration 0:00.003s
[2021-05-14 19:57:56,424] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256168755737087600974932_0004_m_000117_405: needsTaskCommit() Task attempt_202105142256168755737087600974932_0004_m_000117_405
[2021-05-14 19:57:56,425] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256168755737087600974932_0004_m_000117_405: needsTaskCommit() Task attempt_202105142256168755737087600974932_0004_m_000117_405: duration 0:00.000s
21/05/14 22:57:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168755737087600974932_0004_m_000117_405
[2021-05-14 19:57:56,426] {docker.py:276} INFO - 21/05/14 22:57:56 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4587 bytes result sent to driver
[2021-05-14 19:57:56,428] {docker.py:276} INFO - 21/05/14 22:57:56 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (5c14c3e96bf4, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:56,429] {docker.py:276} INFO - 21/05/14 22:57:56 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
21/05/14 22:57:56 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 2841 ms on 5c14c3e96bf4 (executor driver) (118/200)
[2021-05-14 19:57:56,439] {docker.py:276} INFO - 21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:56,441] {docker.py:276} INFO - 21/05/14 22:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164900109616070812014_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164900109616070812014_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164900109616070812014_0004}; taskId=attempt_202105142256164900109616070812014_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e310391}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:56,442] {docker.py:276} INFO - 21/05/14 22:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256164900109616070812014_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164900109616070812014_0004_m_000121_409
[2021-05-14 19:57:56,445] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256164900109616070812014_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164900109616070812014_0004_m_000121_409 : duration 0:00.002s
[2021-05-14 19:57:56,508] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256161011158569872078899_0004_m_000118_406: needsTaskCommit() Task attempt_202105142256161011158569872078899_0004_m_000118_406
[2021-05-14 19:57:56,509] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256161011158569872078899_0004_m_000118_406: needsTaskCommit() Task attempt_202105142256161011158569872078899_0004_m_000118_406: duration 0:00.001s
21/05/14 22:57:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161011158569872078899_0004_m_000118_406
[2021-05-14 19:57:56,510] {docker.py:276} INFO - 21/05/14 22:57:56 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4587 bytes result sent to driver
[2021-05-14 19:57:56,512] {docker.py:276} INFO - 21/05/14 22:57:56 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (5c14c3e96bf4, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:56,513] {docker.py:276} INFO - 21/05/14 22:57:56 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-14 19:57:56,514] {docker.py:276} INFO - 21/05/14 22:57:56 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 2803 ms on 5c14c3e96bf4 (executor driver) (119/200)
[2021-05-14 19:57:56,523] {docker.py:276} INFO - 21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:56,525] {docker.py:276} INFO - 21/05/14 22:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167910541852837350719_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167910541852837350719_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167910541852837350719_0004}; taskId=attempt_202105142256167910541852837350719_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c2af401}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256167910541852837350719_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167910541852837350719_0004_m_000122_410
[2021-05-14 19:57:56,528] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256167910541852837350719_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167910541852837350719_0004_m_000122_410 : duration 0:00.003s
[2021-05-14 19:57:56,744] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256167129327757261598298_0004_m_000119_407: needsTaskCommit() Task attempt_202105142256167129327757261598298_0004_m_000119_407
[2021-05-14 19:57:56,745] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256167129327757261598298_0004_m_000119_407: needsTaskCommit() Task attempt_202105142256167129327757261598298_0004_m_000119_407: duration 0:00.001s
21/05/14 22:57:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167129327757261598298_0004_m_000119_407
[2021-05-14 19:57:56,746] {docker.py:276} INFO - 21/05/14 22:57:56 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4587 bytes result sent to driver
[2021-05-14 19:57:56,748] {docker.py:276} INFO - 21/05/14 22:57:56 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (5c14c3e96bf4, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:56,749] {docker.py:276} INFO - 21/05/14 22:57:56 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 2923 ms on 5c14c3e96bf4 (executor driver) (120/200)
21/05/14 22:57:56 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
[2021-05-14 19:57:56,759] {docker.py:276} INFO - 21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:56,760] {docker.py:276} INFO - 21/05/14 22:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:57:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166618831302019247621_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166618831302019247621_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166618831302019247621_0004}; taskId=attempt_202105142256166618831302019247621_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@673f76eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:56 INFO StagingCommitter: Starting: Task committer attempt_202105142256166618831302019247621_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166618831302019247621_0004_m_000123_411
[2021-05-14 19:57:56,763] {docker.py:276} INFO - 21/05/14 22:57:56 INFO StagingCommitter: Task committer attempt_202105142256166618831302019247621_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166618831302019247621_0004_m_000123_411 : duration 0:00.002s
[2021-05-14 19:57:57,457] {docker.py:276} INFO - 21/05/14 22:57:57 INFO StagingCommitter: Starting: Task committer attempt_202105142256166854346247247277165_0004_m_000120_408: needsTaskCommit() Task attempt_202105142256166854346247247277165_0004_m_000120_408
21/05/14 22:57:57 INFO StagingCommitter: Task committer attempt_202105142256166854346247247277165_0004_m_000120_408: needsTaskCommit() Task attempt_202105142256166854346247247277165_0004_m_000120_408: duration 0:00.001s
[2021-05-14 19:57:57,458] {docker.py:276} INFO - 21/05/14 22:57:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166854346247247277165_0004_m_000120_408
[2021-05-14 19:57:57,459] {docker.py:276} INFO - 21/05/14 22:57:57 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4587 bytes result sent to driver
[2021-05-14 19:57:57,459] {docker.py:276} INFO - 21/05/14 22:57:57 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (5c14c3e96bf4, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:57,460] {docker.py:276} INFO - 21/05/14 22:57:57 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 2212 ms on 5c14c3e96bf4 (executor driver) (121/200)
21/05/14 22:57:57 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-14 19:57:57,469] {docker.py:276} INFO - 21/05/14 22:57:57 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:57,471] {docker.py:276} INFO - 21/05/14 22:57:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:57,472] {docker.py:276} INFO - 21/05/14 22:57:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168420765499210324124_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168420765499210324124_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168420765499210324124_0004}; taskId=attempt_202105142256168420765499210324124_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2da27793}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:57:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:57:57 INFO StagingCommitter: Starting: Task committer attempt_202105142256168420765499210324124_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168420765499210324124_0004_m_000124_412
[2021-05-14 19:57:57,474] {docker.py:276} INFO - 21/05/14 22:57:57 INFO StagingCommitter: Task committer attempt_202105142256168420765499210324124_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168420765499210324124_0004_m_000124_412 : duration 0:00.003s
[2021-05-14 19:57:59,375] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_202105142256167910541852837350719_0004_m_000122_410: needsTaskCommit() Task attempt_202105142256167910541852837350719_0004_m_000122_410
[2021-05-14 19:57:59,377] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_202105142256167910541852837350719_0004_m_000122_410: needsTaskCommit() Task attempt_202105142256167910541852837350719_0004_m_000122_410: duration 0:00.001s
21/05/14 22:57:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167910541852837350719_0004_m_000122_410
21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_202105142256164900109616070812014_0004_m_000121_409: needsTaskCommit() Task attempt_202105142256164900109616070812014_0004_m_000121_409
[2021-05-14 19:57:59,378] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_202105142256164900109616070812014_0004_m_000121_409: needsTaskCommit() Task attempt_202105142256164900109616070812014_0004_m_000121_409: duration 0:00.002s
[2021-05-14 19:57:59,379] {docker.py:276} INFO - 21/05/14 22:57:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164900109616070812014_0004_m_000121_409
21/05/14 22:57:59 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4587 bytes result sent to driver
[2021-05-14 19:57:59,380] {docker.py:276} INFO - 21/05/14 22:57:59 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4587 bytes result sent to driver
21/05/14 22:57:59 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (5c14c3e96bf4, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:57:59 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-14 19:57:59,381] {docker.py:276} INFO - 21/05/14 22:57:59 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (5c14c3e96bf4, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:59,382] {docker.py:276} INFO - 21/05/14 22:57:59 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2874 ms on 5c14c3e96bf4 (executor driver) (122/200)
[2021-05-14 19:57:59,383] {docker.py:276} INFO - 21/05/14 22:57:59 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 2959 ms on 5c14c3e96bf4 (executor driver) (123/200)
[2021-05-14 19:57:59,385] {docker.py:276} INFO - 21/05/14 22:57:59 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-14 19:57:59,400] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:59,401] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:59,402] {docker.py:276} INFO - 21/05/14 22:57:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:57:59,403] {docker.py:276} INFO - 21/05/14 22:57:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:59,403] {docker.py:276} INFO - 21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,403] {docker.py:276} INFO - 21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616865743263040587688_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616865743263040587688_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616865743263040587688_0004}; taskId=attempt_20210514225616865743263040587688_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47a84bb6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,404] {docker.py:276} INFO - 21/05/14 22:57:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:59,404] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_20210514225616865743263040587688_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616865743263040587688_0004_m_000125_413
[2021-05-14 19:57:59,404] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:59,405] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:59,407] {docker.py:276} INFO - 21/05/14 22:57:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,407] {docker.py:276} INFO - 21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164658731274974343776_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164658731274974343776_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164658731274974343776_0004}; taskId=attempt_202105142256164658731274974343776_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e7b466a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,407] {docker.py:276} INFO - 21/05/14 22:57:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:59,408] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_202105142256164658731274974343776_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164658731274974343776_0004_m_000126_414
[2021-05-14 19:57:59,409] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_20210514225616865743263040587688_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616865743263040587688_0004_m_000125_413 : duration 0:00.005s
[2021-05-14 19:57:59,412] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_202105142256164658731274974343776_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164658731274974343776_0004_m_000126_414 : duration 0:00.004s
[2021-05-14 19:57:59,929] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_202105142256168420765499210324124_0004_m_000124_412: needsTaskCommit() Task attempt_202105142256168420765499210324124_0004_m_000124_412
[2021-05-14 19:57:59,930] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_202105142256168420765499210324124_0004_m_000124_412: needsTaskCommit() Task attempt_202105142256168420765499210324124_0004_m_000124_412: duration 0:00.001s
[2021-05-14 19:57:59,930] {docker.py:276} INFO - 21/05/14 22:57:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168420765499210324124_0004_m_000124_412
[2021-05-14 19:57:59,932] {docker.py:276} INFO - 21/05/14 22:57:59 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4587 bytes result sent to driver
[2021-05-14 19:57:59,932] {docker.py:276} INFO - 21/05/14 22:57:59 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (5c14c3e96bf4, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:57:59,933] {docker.py:276} INFO - 21/05/14 22:57:59 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 2478 ms on 5c14c3e96bf4 (executor driver) (124/200)
[2021-05-14 19:57:59,934] {docker.py:276} INFO - 21/05/14 22:57:59 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-14 19:57:59,943] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:57:59,943] {docker.py:276} INFO - 21/05/14 22:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:57:59,945] {docker.py:276} INFO - 21/05/14 22:57:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:57:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:57:59,946] {docker.py:276} INFO - 21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,946] {docker.py:276} INFO - 21/05/14 22:57:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163608723435256480614_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163608723435256480614_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163608723435256480614_0004}; taskId=attempt_202105142256163608723435256480614_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c962e90}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:57:59,946] {docker.py:276} INFO - 21/05/14 22:57:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:57:59,946] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Starting: Task committer attempt_202105142256163608723435256480614_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163608723435256480614_0004_m_000127_415
[2021-05-14 19:57:59,948] {docker.py:276} INFO - 21/05/14 22:57:59 INFO StagingCommitter: Task committer attempt_202105142256163608723435256480614_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163608723435256480614_0004_m_000127_415 : duration 0:00.003s
[2021-05-14 19:58:00,101] {docker.py:276} INFO - 21/05/14 22:58:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256166618831302019247621_0004_m_000123_411: needsTaskCommit() Task attempt_202105142256166618831302019247621_0004_m_000123_411
[2021-05-14 19:58:00,102] {docker.py:276} INFO - 21/05/14 22:58:00 INFO StagingCommitter: Task committer attempt_202105142256166618831302019247621_0004_m_000123_411: needsTaskCommit() Task attempt_202105142256166618831302019247621_0004_m_000123_411: duration 0:00.001s
21/05/14 22:58:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166618831302019247621_0004_m_000123_411
[2021-05-14 19:58:00,103] {docker.py:276} INFO - 21/05/14 22:58:00 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4587 bytes result sent to driver
[2021-05-14 19:58:00,105] {docker.py:276} INFO - 21/05/14 22:58:00 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (5c14c3e96bf4, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:00,106] {docker.py:276} INFO - 21/05/14 22:58:00 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 3361 ms on 5c14c3e96bf4 (executor driver) (125/200)
[2021-05-14 19:58:00,106] {docker.py:276} INFO - 21/05/14 22:58:00 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
[2021-05-14 19:58:00,116] {docker.py:276} INFO - 21/05/14 22:58:00 INFO ShuffleBlockFetcherIterator: Getting 5 (41.0 KiB) non-empty blocks including 5 (41.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:00,118] {docker.py:276} INFO - 21/05/14 22:58:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165525694388354256238_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165525694388354256238_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165525694388354256238_0004}; taskId=attempt_202105142256165525694388354256238_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18056e27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:00,118] {docker.py:276} INFO - 21/05/14 22:58:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:00 INFO StagingCommitter: Starting: Task committer attempt_202105142256165525694388354256238_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165525694388354256238_0004_m_000128_416
[2021-05-14 19:58:00,121] {docker.py:276} INFO - 21/05/14 22:58:00 INFO StagingCommitter: Task committer attempt_202105142256165525694388354256238_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165525694388354256238_0004_m_000128_416 : duration 0:00.003s
[2021-05-14 19:58:02,201] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256164658731274974343776_0004_m_000126_414: needsTaskCommit() Task attempt_202105142256164658731274974343776_0004_m_000126_414
[2021-05-14 19:58:02,202] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256164658731274974343776_0004_m_000126_414: needsTaskCommit() Task attempt_202105142256164658731274974343776_0004_m_000126_414: duration 0:00.001s
[2021-05-14 19:58:02,203] {docker.py:276} INFO - 21/05/14 22:58:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164658731274974343776_0004_m_000126_414
[2021-05-14 19:58:02,205] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4587 bytes result sent to driver
[2021-05-14 19:58:02,206] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (5c14c3e96bf4, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:02,207] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 2830 ms on 5c14c3e96bf4 (executor driver) (126/200)
[2021-05-14 19:58:02,208] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-14 19:58:02,217] {docker.py:276} INFO - 21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Getting 5 (39.8 KiB) non-empty blocks including 5 (39.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:02,219] {docker.py:276} INFO - 21/05/14 22:58:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165354215408150760751_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165354215408150760751_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165354215408150760751_0004}; taskId=attempt_202105142256165354215408150760751_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c225ff9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256165354215408150760751_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165354215408150760751_0004_m_000129_417
[2021-05-14 19:58:02,222] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256165354215408150760751_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165354215408150760751_0004_m_000129_417 : duration 0:00.003s
[2021-05-14 19:58:02,259] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_20210514225616865743263040587688_0004_m_000125_413: needsTaskCommit() Task attempt_20210514225616865743263040587688_0004_m_000125_413
[2021-05-14 19:58:02,260] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_20210514225616865743263040587688_0004_m_000125_413: needsTaskCommit() Task attempt_20210514225616865743263040587688_0004_m_000125_413: duration 0:00.001s
21/05/14 22:58:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616865743263040587688_0004_m_000125_413
[2021-05-14 19:58:02,263] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4587 bytes result sent to driver
[2021-05-14 19:58:02,264] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (5c14c3e96bf4, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:02,265] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-14 19:58:02,266] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 2891 ms on 5c14c3e96bf4 (executor driver) (127/200)
[2021-05-14 19:58:02,276] {docker.py:276} INFO - 21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:02,277] {docker.py:276} INFO - 21/05/14 22:58:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:02,278] {docker.py:276} INFO - 21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:02,278] {docker.py:276} INFO - 21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163424711245164260375_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163424711245164260375_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163424711245164260375_0004}; taskId=attempt_202105142256163424711245164260375_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26723f8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:02,278] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256163424711245164260375_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163424711245164260375_0004_m_000130_418
[2021-05-14 19:58:02,281] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256163424711245164260375_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163424711245164260375_0004_m_000130_418 : duration 0:00.003s
[2021-05-14 19:58:02,446] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256165525694388354256238_0004_m_000128_416: needsTaskCommit() Task attempt_202105142256165525694388354256238_0004_m_000128_416
[2021-05-14 19:58:02,447] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256165525694388354256238_0004_m_000128_416: needsTaskCommit() Task attempt_202105142256165525694388354256238_0004_m_000128_416: duration 0:00.001s
[2021-05-14 19:58:02,448] {docker.py:276} INFO - 21/05/14 22:58:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165525694388354256238_0004_m_000128_416
[2021-05-14 19:58:02,449] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-14 19:58:02,450] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (5c14c3e96bf4, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:02,452] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 2350 ms on 5c14c3e96bf4 (executor driver) (128/200)
[2021-05-14 19:58:02,452] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-14 19:58:02,461] {docker.py:276} INFO - 21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:02,463] {docker.py:276} INFO - 21/05/14 22:58:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167852430957943552251_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167852430957943552251_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167852430957943552251_0004}; taskId=attempt_202105142256167852430957943552251_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f880235}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256167852430957943552251_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167852430957943552251_0004_m_000131_419
[2021-05-14 19:58:02,467] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256167852430957943552251_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167852430957943552251_0004_m_000131_419 : duration 0:00.004s
[2021-05-14 19:58:02,683] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256163608723435256480614_0004_m_000127_415: needsTaskCommit() Task attempt_202105142256163608723435256480614_0004_m_000127_415
[2021-05-14 19:58:02,684] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256163608723435256480614_0004_m_000127_415: needsTaskCommit() Task attempt_202105142256163608723435256480614_0004_m_000127_415: duration 0:00.001s
21/05/14 22:58:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163608723435256480614_0004_m_000127_415
[2021-05-14 19:58:02,686] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4587 bytes result sent to driver
[2021-05-14 19:58:02,687] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (5c14c3e96bf4, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:02,688] {docker.py:276} INFO - 21/05/14 22:58:02 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-14 19:58:02,689] {docker.py:276} INFO - 21/05/14 22:58:02 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2759 ms on 5c14c3e96bf4 (executor driver) (129/200)
[2021-05-14 19:58:02,697] {docker.py:276} INFO - 21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Getting 5 (43.6 KiB) non-empty blocks including 5 (43.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:02,699] {docker.py:276} INFO - 21/05/14 22:58:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164703978284434720401_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164703978284434720401_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164703978284434720401_0004}; taskId=attempt_202105142256164703978284434720401_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43773cd4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:02,700] {docker.py:276} INFO - 21/05/14 22:58:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:02,700] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Starting: Task committer attempt_202105142256164703978284434720401_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164703978284434720401_0004_m_000132_420
[2021-05-14 19:58:02,703] {docker.py:276} INFO - 21/05/14 22:58:02 INFO StagingCommitter: Task committer attempt_202105142256164703978284434720401_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164703978284434720401_0004_m_000132_420 : duration 0:00.003s
[2021-05-14 19:58:04,900] {docker.py:276} INFO - 21/05/14 22:58:04 INFO StagingCommitter: Starting: Task committer attempt_202105142256165354215408150760751_0004_m_000129_417: needsTaskCommit() Task attempt_202105142256165354215408150760751_0004_m_000129_417
[2021-05-14 19:58:04,901] {docker.py:276} INFO - 21/05/14 22:58:04 INFO StagingCommitter: Task committer attempt_202105142256165354215408150760751_0004_m_000129_417: needsTaskCommit() Task attempt_202105142256165354215408150760751_0004_m_000129_417: duration 0:00.001s
21/05/14 22:58:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165354215408150760751_0004_m_000129_417
[2021-05-14 19:58:04,902] {docker.py:276} INFO - 21/05/14 22:58:04 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4587 bytes result sent to driver
[2021-05-14 19:58:04,903] {docker.py:276} INFO - 21/05/14 22:58:04 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (5c14c3e96bf4, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:04,904] {docker.py:276} INFO - 21/05/14 22:58:04 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2701 ms on 5c14c3e96bf4 (executor driver) (130/200)
[2021-05-14 19:58:04,905] {docker.py:276} INFO - 21/05/14 22:58:04 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-14 19:58:04,914] {docker.py:276} INFO - 21/05/14 22:58:04 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:04,915] {docker.py:276} INFO - 21/05/14 22:58:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:04,916] {docker.py:276} INFO - 21/05/14 22:58:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168755998060872435809_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755998060872435809_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168755998060872435809_0004}; taskId=attempt_202105142256168755998060872435809_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@636c95c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:04,916] {docker.py:276} INFO - 21/05/14 22:58:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:04 INFO StagingCommitter: Starting: Task committer attempt_202105142256168755998060872435809_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755998060872435809_0004_m_000133_421
[2021-05-14 19:58:04,918] {docker.py:276} INFO - 21/05/14 22:58:04 INFO StagingCommitter: Task committer attempt_202105142256168755998060872435809_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168755998060872435809_0004_m_000133_421 : duration 0:00.002s
[2021-05-14 19:58:05,122] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256167852430957943552251_0004_m_000131_419: needsTaskCommit() Task attempt_202105142256167852430957943552251_0004_m_000131_419
[2021-05-14 19:58:05,123] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256167852430957943552251_0004_m_000131_419: needsTaskCommit() Task attempt_202105142256167852430957943552251_0004_m_000131_419: duration 0:00.001s
21/05/14 22:58:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167852430957943552251_0004_m_000131_419
[2021-05-14 19:58:05,125] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4587 bytes result sent to driver
[2021-05-14 19:58:05,126] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (5c14c3e96bf4, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:05,127] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-14 19:58:05,128] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 2680 ms on 5c14c3e96bf4 (executor driver) (131/200)
[2021-05-14 19:58:05,137] {docker.py:276} INFO - 21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:05,138] {docker.py:276} INFO - 21/05/14 22:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163394522880903173330_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163394522880903173330_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163394522880903173330_0004}; taskId=attempt_202105142256163394522880903173330_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@220bc7c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256163394522880903173330_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163394522880903173330_0004_m_000134_422
[2021-05-14 19:58:05,142] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256163394522880903173330_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163394522880903173330_0004_m_000134_422 : duration 0:00.003s
[2021-05-14 19:58:05,463] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256164703978284434720401_0004_m_000132_420: needsTaskCommit() Task attempt_202105142256164703978284434720401_0004_m_000132_420
[2021-05-14 19:58:05,463] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256164703978284434720401_0004_m_000132_420: needsTaskCommit() Task attempt_202105142256164703978284434720401_0004_m_000132_420: duration 0:00.000s
21/05/14 22:58:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164703978284434720401_0004_m_000132_420
[2021-05-14 19:58:05,464] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4587 bytes result sent to driver
[2021-05-14 19:58:05,465] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (5c14c3e96bf4, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:05,466] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-14 19:58:05,466] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 2784 ms on 5c14c3e96bf4 (executor driver) (132/200)
[2021-05-14 19:58:05,474] {docker.py:276} INFO - 21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Getting 5 (42.3 KiB) non-empty blocks including 5 (42.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:05,475] {docker.py:276} INFO - 21/05/14 22:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167122190371101069323_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167122190371101069323_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167122190371101069323_0004}; taskId=attempt_202105142256167122190371101069323_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@668dc78d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256167122190371101069323_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167122190371101069323_0004_m_000135_423
[2021-05-14 19:58:05,478] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256167122190371101069323_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167122190371101069323_0004_m_000135_423 : duration 0:00.002s
[2021-05-14 19:58:05,546] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256163424711245164260375_0004_m_000130_418: needsTaskCommit() Task attempt_202105142256163424711245164260375_0004_m_000130_418
[2021-05-14 19:58:05,547] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256163424711245164260375_0004_m_000130_418: needsTaskCommit() Task attempt_202105142256163424711245164260375_0004_m_000130_418: duration 0:00.000s
21/05/14 22:58:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163424711245164260375_0004_m_000130_418
[2021-05-14 19:58:05,549] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4587 bytes result sent to driver
[2021-05-14 19:58:05,550] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (5c14c3e96bf4, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:05,551] {docker.py:276} INFO - 21/05/14 22:58:05 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-14 19:58:05,551] {docker.py:276} INFO - 21/05/14 22:58:05 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 3291 ms on 5c14c3e96bf4 (executor driver) (133/200)
[2021-05-14 19:58:05,559] {docker.py:276} INFO - 21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:05,560] {docker.py:276} INFO - 21/05/14 22:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:05,561] {docker.py:276} INFO - 21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164640587134519018975_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164640587134519018975_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164640587134519018975_0004}; taskId=attempt_202105142256164640587134519018975_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5851d911}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:05 INFO StagingCommitter: Starting: Task committer attempt_202105142256164640587134519018975_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164640587134519018975_0004_m_000136_424
[2021-05-14 19:58:05,564] {docker.py:276} INFO - 21/05/14 22:58:05 INFO StagingCommitter: Task committer attempt_202105142256164640587134519018975_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164640587134519018975_0004_m_000136_424 : duration 0:00.003s
[2021-05-14 19:58:07,616] {docker.py:276} INFO - 21/05/14 22:58:07 INFO StagingCommitter: Starting: Task committer attempt_202105142256168755998060872435809_0004_m_000133_421: needsTaskCommit() Task attempt_202105142256168755998060872435809_0004_m_000133_421
21/05/14 22:58:07 INFO StagingCommitter: Task committer attempt_202105142256168755998060872435809_0004_m_000133_421: needsTaskCommit() Task attempt_202105142256168755998060872435809_0004_m_000133_421: duration 0:00.001s
21/05/14 22:58:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168755998060872435809_0004_m_000133_421
21/05/14 22:58:07 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4587 bytes result sent to driver
[2021-05-14 19:58:07,618] {docker.py:276} INFO - 21/05/14 22:58:07 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (5c14c3e96bf4, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:07,618] {docker.py:276} INFO - 21/05/14 22:58:07 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
[2021-05-14 19:58:07,619] {docker.py:276} INFO - 21/05/14 22:58:07 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 2717 ms on 5c14c3e96bf4 (executor driver) (134/200)
[2021-05-14 19:58:07,628] {docker.py:276} INFO - 21/05/14 22:58:07 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:07,628] {docker.py:276} INFO - 21/05/14 22:58:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:07,629] {docker.py:276} INFO - 21/05/14 22:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167471390946035561071_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167471390946035561071_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167471390946035561071_0004}; taskId=attempt_202105142256167471390946035561071_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a2eb448}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:07,630] {docker.py:276} INFO - 21/05/14 22:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:07 INFO StagingCommitter: Starting: Task committer attempt_202105142256167471390946035561071_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167471390946035561071_0004_m_000137_425
[2021-05-14 19:58:07,632] {docker.py:276} INFO - 21/05/14 22:58:07 INFO StagingCommitter: Task committer attempt_202105142256167471390946035561071_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167471390946035561071_0004_m_000137_425 : duration 0:00.003s
[2021-05-14 19:58:07,989] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256163394522880903173330_0004_m_000134_422: needsTaskCommit() Task attempt_202105142256163394522880903173330_0004_m_000134_422
[2021-05-14 19:58:07,989] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256163394522880903173330_0004_m_000134_422: needsTaskCommit() Task attempt_202105142256163394522880903173330_0004_m_000134_422: duration 0:00.001s
21/05/14 22:58:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163394522880903173330_0004_m_000134_422
[2021-05-14 19:58:07,990] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4587 bytes result sent to driver
[2021-05-14 19:58:07,991] {docker.py:276} INFO - 21/05/14 22:58:08 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (5c14c3e96bf4, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:07,991] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
21/05/14 22:58:08 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 2869 ms on 5c14c3e96bf4 (executor driver) (135/200)
[2021-05-14 19:58:08,007] {docker.py:276} INFO - 21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:08,008] {docker.py:276} INFO - 21/05/14 22:58:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166740937631285372151_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166740937631285372151_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166740937631285372151_0004}; taskId=attempt_202105142256166740937631285372151_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ad35d5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256166740937631285372151_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166740937631285372151_0004_m_000138_426
[2021-05-14 19:58:08,012] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256166740937631285372151_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166740937631285372151_0004_m_000138_426 : duration 0:00.003s
[2021-05-14 19:58:08,117] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256167122190371101069323_0004_m_000135_423: needsTaskCommit() Task attempt_202105142256167122190371101069323_0004_m_000135_423
[2021-05-14 19:58:08,117] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256167122190371101069323_0004_m_000135_423: needsTaskCommit() Task attempt_202105142256167122190371101069323_0004_m_000135_423: duration 0:00.001s
21/05/14 22:58:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167122190371101069323_0004_m_000135_423
[2021-05-14 19:58:08,119] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4587 bytes result sent to driver
[2021-05-14 19:58:08,120] {docker.py:276} INFO - 21/05/14 22:58:08 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (5c14c3e96bf4, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:08,120] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-14 19:58:08,121] {docker.py:276} INFO - 21/05/14 22:58:08 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 2659 ms on 5c14c3e96bf4 (executor driver) (136/200)
[2021-05-14 19:58:08,131] {docker.py:276} INFO - 21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:08,131] {docker.py:276} INFO - 21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:08,132] {docker.py:276} INFO - 21/05/14 22:58:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:08,133] {docker.py:276} INFO - 21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:08,133] {docker.py:276} INFO - 21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165427414043055940807_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165427414043055940807_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165427414043055940807_0004}; taskId=attempt_202105142256165427414043055940807_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23c0ae7f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:08,134] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256165427414043055940807_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165427414043055940807_0004_m_000139_427
[2021-05-14 19:58:08,136] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256165427414043055940807_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165427414043055940807_0004_m_000139_427 : duration 0:00.002s
[2021-05-14 19:58:08,417] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256164640587134519018975_0004_m_000136_424: needsTaskCommit() Task attempt_202105142256164640587134519018975_0004_m_000136_424
[2021-05-14 19:58:08,418] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256164640587134519018975_0004_m_000136_424: needsTaskCommit() Task attempt_202105142256164640587134519018975_0004_m_000136_424: duration 0:00.001s
[2021-05-14 19:58:08,419] {docker.py:276} INFO - 21/05/14 22:58:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164640587134519018975_0004_m_000136_424
[2021-05-14 19:58:08,421] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4587 bytes result sent to driver
21/05/14 22:58:08 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (5c14c3e96bf4, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:08,422] {docker.py:276} INFO - 21/05/14 22:58:08 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 2875 ms on 5c14c3e96bf4 (executor driver) (137/200)
[2021-05-14 19:58:08,423] {docker.py:276} INFO - 21/05/14 22:58:08 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-14 19:58:08,438] {docker.py:276} INFO - 21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Getting 5 (40.2 KiB) non-empty blocks including 5 (40.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:08,440] {docker.py:276} INFO - 21/05/14 22:58:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163250059587167915892_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163250059587167915892_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163250059587167915892_0004}; taskId=attempt_202105142256163250059587167915892_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e4ff25a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:08 INFO StagingCommitter: Starting: Task committer attempt_202105142256163250059587167915892_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163250059587167915892_0004_m_000140_428
[2021-05-14 19:58:08,442] {docker.py:276} INFO - 21/05/14 22:58:08 INFO StagingCommitter: Task committer attempt_202105142256163250059587167915892_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163250059587167915892_0004_m_000140_428 : duration 0:00.003s
[2021-05-14 19:58:10,510] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256167471390946035561071_0004_m_000137_425: needsTaskCommit() Task attempt_202105142256167471390946035561071_0004_m_000137_425
[2021-05-14 19:58:10,511] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256167471390946035561071_0004_m_000137_425: needsTaskCommit() Task attempt_202105142256167471390946035561071_0004_m_000137_425: duration 0:00.000s
21/05/14 22:58:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167471390946035561071_0004_m_000137_425
[2021-05-14 19:58:10,512] {docker.py:276} INFO - 21/05/14 22:58:10 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4587 bytes result sent to driver
[2021-05-14 19:58:10,513] {docker.py:276} INFO - 21/05/14 22:58:10 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (5c14c3e96bf4, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:10,514] {docker.py:276} INFO - 21/05/14 22:58:10 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 2901 ms on 5c14c3e96bf4 (executor driver) (138/200)
21/05/14 22:58:10 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-14 19:58:10,527] {docker.py:276} INFO - 21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:10,529] {docker.py:276} INFO - 21/05/14 22:58:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165715256863735903152_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165715256863735903152_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165715256863735903152_0004}; taskId=attempt_202105142256165715256863735903152_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c495eac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:10,529] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256165715256863735903152_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165715256863735903152_0004_m_000141_429
[2021-05-14 19:58:10,532] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256165715256863735903152_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165715256863735903152_0004_m_000141_429 : duration 0:00.003s
[2021-05-14 19:58:10,799] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256166740937631285372151_0004_m_000138_426: needsTaskCommit() Task attempt_202105142256166740937631285372151_0004_m_000138_426
[2021-05-14 19:58:10,800] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256166740937631285372151_0004_m_000138_426: needsTaskCommit() Task attempt_202105142256166740937631285372151_0004_m_000138_426: duration 0:00.002s
[2021-05-14 19:58:10,801] {docker.py:276} INFO - 21/05/14 22:58:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166740937631285372151_0004_m_000138_426
[2021-05-14 19:58:10,803] {docker.py:276} INFO - 21/05/14 22:58:10 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4587 bytes result sent to driver
[2021-05-14 19:58:10,805] {docker.py:276} INFO - 21/05/14 22:58:10 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (5c14c3e96bf4, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:10,806] {docker.py:276} INFO - 21/05/14 22:58:10 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-14 19:58:10,806] {docker.py:276} INFO - 21/05/14 22:58:10 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 2819 ms on 5c14c3e96bf4 (executor driver) (139/200)
[2021-05-14 19:58:10,816] {docker.py:276} INFO - 21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:10,817] {docker.py:276} INFO - 21/05/14 22:58:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164076421316251919274_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164076421316251919274_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164076421316251919274_0004}; taskId=attempt_202105142256164076421316251919274_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e3e6f1d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:10,818] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256164076421316251919274_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164076421316251919274_0004_m_000142_430
[2021-05-14 19:58:10,820] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256164076421316251919274_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164076421316251919274_0004_m_000142_430 : duration 0:00.003s
[2021-05-14 19:58:10,860] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256165427414043055940807_0004_m_000139_427: needsTaskCommit() Task attempt_202105142256165427414043055940807_0004_m_000139_427
[2021-05-14 19:58:10,861] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256165427414043055940807_0004_m_000139_427: needsTaskCommit() Task attempt_202105142256165427414043055940807_0004_m_000139_427: duration 0:00.001s
21/05/14 22:58:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165427414043055940807_0004_m_000139_427
[2021-05-14 19:58:10,869] {docker.py:276} INFO - 21/05/14 22:58:10 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4587 bytes result sent to driver
[2021-05-14 19:58:10,870] {docker.py:276} INFO - 21/05/14 22:58:10 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (5c14c3e96bf4, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:10,871] {docker.py:276} INFO - 21/05/14 22:58:10 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
21/05/14 22:58:10 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 2755 ms on 5c14c3e96bf4 (executor driver) (140/200)
[2021-05-14 19:58:10,880] {docker.py:276} INFO - 21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 5 (40.6 KiB) non-empty blocks including 5 (40.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:10,881] {docker.py:276} INFO - 21/05/14 22:58:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167818312410772686038_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167818312410772686038_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167818312410772686038_0004}; taskId=attempt_202105142256167818312410772686038_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@339b3882}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:10 INFO StagingCommitter: Starting: Task committer attempt_202105142256167818312410772686038_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167818312410772686038_0004_m_000143_431
[2021-05-14 19:58:10,883] {docker.py:276} INFO - 21/05/14 22:58:10 INFO StagingCommitter: Task committer attempt_202105142256167818312410772686038_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167818312410772686038_0004_m_000143_431 : duration 0:00.002s
[2021-05-14 19:58:11,032] {docker.py:276} INFO - 21/05/14 22:58:11 INFO StagingCommitter: Starting: Task committer attempt_202105142256163250059587167915892_0004_m_000140_428: needsTaskCommit() Task attempt_202105142256163250059587167915892_0004_m_000140_428
[2021-05-14 19:58:11,034] {docker.py:276} INFO - 21/05/14 22:58:11 INFO StagingCommitter: Task committer attempt_202105142256163250059587167915892_0004_m_000140_428: needsTaskCommit() Task attempt_202105142256163250059587167915892_0004_m_000140_428: duration 0:00.003s
[2021-05-14 19:58:11,035] {docker.py:276} INFO - 21/05/14 22:58:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163250059587167915892_0004_m_000140_428
[2021-05-14 19:58:11,038] {docker.py:276} INFO - 21/05/14 22:58:11 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4587 bytes result sent to driver
[2021-05-14 19:58:11,040] {docker.py:276} INFO - 21/05/14 22:58:11 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (5c14c3e96bf4, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:11,041] {docker.py:276} INFO - 21/05/14 22:58:11 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-14 19:58:11,041] {docker.py:276} INFO - 21/05/14 22:58:11 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 2622 ms on 5c14c3e96bf4 (executor driver) (141/200)
[2021-05-14 19:58:11,050] {docker.py:276} INFO - 21/05/14 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:11,051] {docker.py:276} INFO - 21/05/14 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:58:11,054] {docker.py:276} INFO - 21/05/14 22:58:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:11,055] {docker.py:276} INFO - 21/05/14 22:58:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:11,055] {docker.py:276} INFO - 21/05/14 22:58:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:11,055] {docker.py:276} INFO - 21/05/14 22:58:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168097776119172251489_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168097776119172251489_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168097776119172251489_0004}; taskId=attempt_202105142256168097776119172251489_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e59a71c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:11,056] {docker.py:276} INFO - 21/05/14 22:58:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:11 INFO StagingCommitter: Starting: Task committer attempt_202105142256168097776119172251489_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168097776119172251489_0004_m_000144_432
[2021-05-14 19:58:11,059] {docker.py:276} INFO - 21/05/14 22:58:11 INFO StagingCommitter: Task committer attempt_202105142256168097776119172251489_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168097776119172251489_0004_m_000144_432 : duration 0:00.002s
[2021-05-14 19:58:13,254] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256165715256863735903152_0004_m_000141_429: needsTaskCommit() Task attempt_202105142256165715256863735903152_0004_m_000141_429
[2021-05-14 19:58:13,255] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256165715256863735903152_0004_m_000141_429: needsTaskCommit() Task attempt_202105142256165715256863735903152_0004_m_000141_429: duration 0:00.001s
21/05/14 22:58:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165715256863735903152_0004_m_000141_429
[2021-05-14 19:58:13,256] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4587 bytes result sent to driver
[2021-05-14 19:58:13,257] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (5c14c3e96bf4, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:13,257] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2749 ms on 5c14c3e96bf4 (executor driver) (142/200)
21/05/14 22:58:13 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-14 19:58:13,265] {docker.py:276} INFO - 21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:13,267] {docker.py:276} INFO - 21/05/14 22:58:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:13,267] {docker.py:276} INFO - 21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161080072375672179656_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161080072375672179656_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161080072375672179656_0004}; taskId=attempt_202105142256161080072375672179656_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cfd7355}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:13,267] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256161080072375672179656_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161080072375672179656_0004_m_000145_433
[2021-05-14 19:58:13,270] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256161080072375672179656_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161080072375672179656_0004_m_000145_433 : duration 0:00.003s
[2021-05-14 19:58:13,483] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256167818312410772686038_0004_m_000143_431: needsTaskCommit() Task attempt_202105142256167818312410772686038_0004_m_000143_431
[2021-05-14 19:58:13,484] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256167818312410772686038_0004_m_000143_431: needsTaskCommit() Task attempt_202105142256167818312410772686038_0004_m_000143_431: duration 0:00.001s
21/05/14 22:58:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167818312410772686038_0004_m_000143_431
[2021-05-14 19:58:13,485] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4587 bytes result sent to driver
[2021-05-14 19:58:13,487] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (5c14c3e96bf4, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:13,488] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2621 ms on 5c14c3e96bf4 (executor driver) (143/200)
[2021-05-14 19:58:13,489] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-14 19:58:13,503] {docker.py:276} INFO - 21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:13,505] {docker.py:276} INFO - 21/05/14 22:58:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166652758417954486903_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166652758417954486903_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166652758417954486903_0004}; taskId=attempt_202105142256166652758417954486903_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7466a026}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256166652758417954486903_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166652758417954486903_0004_m_000146_434
[2021-05-14 19:58:13,507] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256166652758417954486903_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166652758417954486903_0004_m_000146_434 : duration 0:00.003s
[2021-05-14 19:58:13,559] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256164076421316251919274_0004_m_000142_430: needsTaskCommit() Task attempt_202105142256164076421316251919274_0004_m_000142_430
[2021-05-14 19:58:13,560] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256164076421316251919274_0004_m_000142_430: needsTaskCommit() Task attempt_202105142256164076421316251919274_0004_m_000142_430: duration 0:00.001s
[2021-05-14 19:58:13,561] {docker.py:276} INFO - 21/05/14 22:58:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164076421316251919274_0004_m_000142_430
[2021-05-14 19:58:13,562] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4587 bytes result sent to driver
[2021-05-14 19:58:13,562] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (5c14c3e96bf4, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:13,563] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
21/05/14 22:58:13 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2761 ms on 5c14c3e96bf4 (executor driver) (144/200)
[2021-05-14 19:58:13,580] {docker.py:276} INFO - 21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:13,581] {docker.py:276} INFO - 21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:13,582] {docker.py:276} INFO - 21/05/14 22:58:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:13,582] {docker.py:276} INFO - 21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:13,583] {docker.py:276} INFO - 21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163001328745489299126_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163001328745489299126_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163001328745489299126_0004}; taskId=attempt_202105142256163001328745489299126_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16240cf5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:13,583] {docker.py:276} INFO - 21/05/14 22:58:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:13,583] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256163001328745489299126_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163001328745489299126_0004_m_000147_435
[2021-05-14 19:58:13,586] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256163001328745489299126_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163001328745489299126_0004_m_000147_435 : duration 0:00.003s
[2021-05-14 19:58:13,849] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256168097776119172251489_0004_m_000144_432: needsTaskCommit() Task attempt_202105142256168097776119172251489_0004_m_000144_432
[2021-05-14 19:58:13,850] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256168097776119172251489_0004_m_000144_432: needsTaskCommit() Task attempt_202105142256168097776119172251489_0004_m_000144_432: duration 0:00.000s
21/05/14 22:58:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168097776119172251489_0004_m_000144_432
[2021-05-14 19:58:13,852] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4587 bytes result sent to driver
[2021-05-14 19:58:13,853] {docker.py:276} INFO - 21/05/14 22:58:13 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (5c14c3e96bf4, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:13,854] {docker.py:276} INFO - 21/05/14 22:58:13 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
21/05/14 22:58:13 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 2818 ms on 5c14c3e96bf4 (executor driver) (145/200)
[2021-05-14 19:58:13,864] {docker.py:276} INFO - 21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:13,871] {docker.py:276} INFO - 21/05/14 22:58:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164383955038610951436_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164383955038610951436_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164383955038610951436_0004}; taskId=attempt_202105142256164383955038610951436_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d74282e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:13 INFO StagingCommitter: Starting: Task committer attempt_202105142256164383955038610951436_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164383955038610951436_0004_m_000148_436
[2021-05-14 19:58:13,873] {docker.py:276} INFO - 21/05/14 22:58:13 INFO StagingCommitter: Task committer attempt_202105142256164383955038610951436_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164383955038610951436_0004_m_000148_436 : duration 0:00.002s
[2021-05-14 19:58:15,899] {docker.py:276} INFO - 21/05/14 22:58:15 INFO StagingCommitter: Starting: Task committer attempt_202105142256161080072375672179656_0004_m_000145_433: needsTaskCommit() Task attempt_202105142256161080072375672179656_0004_m_000145_433
[2021-05-14 19:58:15,899] {docker.py:276} INFO - 21/05/14 22:58:15 INFO StagingCommitter: Task committer attempt_202105142256161080072375672179656_0004_m_000145_433: needsTaskCommit() Task attempt_202105142256161080072375672179656_0004_m_000145_433: duration 0:00.001s
21/05/14 22:58:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161080072375672179656_0004_m_000145_433
[2021-05-14 19:58:15,900] {docker.py:276} INFO - 21/05/14 22:58:15 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4587 bytes result sent to driver
[2021-05-14 19:58:15,901] {docker.py:276} INFO - 21/05/14 22:58:15 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (5c14c3e96bf4, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:15,902] {docker.py:276} INFO - 21/05/14 22:58:15 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 2648 ms on 5c14c3e96bf4 (executor driver) (146/200)
[2021-05-14 19:58:15,903] {docker.py:276} INFO - 21/05/14 22:58:15 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-14 19:58:15,916] {docker.py:276} INFO - 21/05/14 22:58:15 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:15,917] {docker.py:276} INFO - 21/05/14 22:58:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165371923734328510594_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165371923734328510594_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165371923734328510594_0004}; taskId=attempt_202105142256165371923734328510594_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b7722c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:15,918] {docker.py:276} INFO - 21/05/14 22:58:15 INFO StagingCommitter: Starting: Task committer attempt_202105142256165371923734328510594_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165371923734328510594_0004_m_000149_437
[2021-05-14 19:58:15,920] {docker.py:276} INFO - 21/05/14 22:58:15 INFO StagingCommitter: Task committer attempt_202105142256165371923734328510594_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165371923734328510594_0004_m_000149_437 : duration 0:00.003s
[2021-05-14 19:58:16,222] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_202105142256166652758417954486903_0004_m_000146_434: needsTaskCommit() Task attempt_202105142256166652758417954486903_0004_m_000146_434
[2021-05-14 19:58:16,223] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_202105142256166652758417954486903_0004_m_000146_434: needsTaskCommit() Task attempt_202105142256166652758417954486903_0004_m_000146_434: duration 0:00.002s
21/05/14 22:58:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166652758417954486903_0004_m_000146_434
[2021-05-14 19:58:16,225] {docker.py:276} INFO - 21/05/14 22:58:16 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4587 bytes result sent to driver
[2021-05-14 19:58:16,226] {docker.py:276} INFO - 21/05/14 22:58:16 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (5c14c3e96bf4, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:16,227] {docker.py:276} INFO - 21/05/14 22:58:16 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 2743 ms on 5c14c3e96bf4 (executor driver) (147/200)
21/05/14 22:58:16 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-14 19:58:16,242] {docker.py:276} INFO - 21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:16,244] {docker.py:276} INFO - 21/05/14 22:58:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616511044858651271120_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616511044858651271120_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616511044858651271120_0004}; taskId=attempt_20210514225616511044858651271120_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b68d1ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:16,244] {docker.py:276} INFO - 21/05/14 22:58:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_20210514225616511044858651271120_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616511044858651271120_0004_m_000150_438
[2021-05-14 19:58:16,247] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_20210514225616511044858651271120_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616511044858651271120_0004_m_000150_438 : duration 0:00.003s
[2021-05-14 19:58:16,339] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_202105142256163001328745489299126_0004_m_000147_435: needsTaskCommit() Task attempt_202105142256163001328745489299126_0004_m_000147_435
[2021-05-14 19:58:16,340] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_202105142256163001328745489299126_0004_m_000147_435: needsTaskCommit() Task attempt_202105142256163001328745489299126_0004_m_000147_435: duration 0:00.000s
21/05/14 22:58:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163001328745489299126_0004_m_000147_435
[2021-05-14 19:58:16,342] {docker.py:276} INFO - 21/05/14 22:58:16 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4587 bytes result sent to driver
[2021-05-14 19:58:16,343] {docker.py:276} INFO - 21/05/14 22:58:16 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (5c14c3e96bf4, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:16,344] {docker.py:276} INFO - 21/05/14 22:58:16 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
21/05/14 22:58:16 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2786 ms on 5c14c3e96bf4 (executor driver) (148/200)
[2021-05-14 19:58:16,360] {docker.py:276} INFO - 21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:16,361] {docker.py:276} INFO - 21/05/14 22:58:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163296340813358389790_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163296340813358389790_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163296340813358389790_0004}; taskId=attempt_202105142256163296340813358389790_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5954df41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:16,362] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_202105142256163296340813358389790_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163296340813358389790_0004_m_000151_439
[2021-05-14 19:58:16,365] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_202105142256163296340813358389790_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163296340813358389790_0004_m_000151_439 : duration 0:00.003s
[2021-05-14 19:58:16,729] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_202105142256164383955038610951436_0004_m_000148_436: needsTaskCommit() Task attempt_202105142256164383955038610951436_0004_m_000148_436
[2021-05-14 19:58:16,730] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_202105142256164383955038610951436_0004_m_000148_436: needsTaskCommit() Task attempt_202105142256164383955038610951436_0004_m_000148_436: duration 0:00.001s
21/05/14 22:58:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164383955038610951436_0004_m_000148_436
[2021-05-14 19:58:16,732] {docker.py:276} INFO - 21/05/14 22:58:16 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4587 bytes result sent to driver
[2021-05-14 19:58:16,734] {docker.py:276} INFO - 21/05/14 22:58:16 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (5c14c3e96bf4, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:16,735] {docker.py:276} INFO - 21/05/14 22:58:16 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
21/05/14 22:58:16 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 2887 ms on 5c14c3e96bf4 (executor driver) (149/200)
[2021-05-14 19:58:16,750] {docker.py:276} INFO - 21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:16,752] {docker.py:276} INFO - 21/05/14 22:58:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616803733159281433891_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616803733159281433891_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616803733159281433891_0004}; taskId=attempt_20210514225616803733159281433891_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9d59a96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:16,753] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Starting: Task committer attempt_20210514225616803733159281433891_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616803733159281433891_0004_m_000152_440
[2021-05-14 19:58:16,755] {docker.py:276} INFO - 21/05/14 22:58:16 INFO StagingCommitter: Task committer attempt_20210514225616803733159281433891_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616803733159281433891_0004_m_000152_440 : duration 0:00.004s
[2021-05-14 19:58:18,710] {docker.py:276} INFO - 21/05/14 22:58:18 INFO StagingCommitter: Starting: Task committer attempt_202105142256165371923734328510594_0004_m_000149_437: needsTaskCommit() Task attempt_202105142256165371923734328510594_0004_m_000149_437
21/05/14 22:58:18 INFO StagingCommitter: Task committer attempt_202105142256165371923734328510594_0004_m_000149_437: needsTaskCommit() Task attempt_202105142256165371923734328510594_0004_m_000149_437: duration 0:00.001s
21/05/14 22:58:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165371923734328510594_0004_m_000149_437
[2021-05-14 19:58:18,712] {docker.py:276} INFO - 21/05/14 22:58:18 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4587 bytes result sent to driver
[2021-05-14 19:58:18,714] {docker.py:276} INFO - 21/05/14 22:58:18 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (5c14c3e96bf4, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:58:18 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 2781 ms on 5c14c3e96bf4 (executor driver) (150/200)
[2021-05-14 19:58:18,715] {docker.py:276} INFO - 21/05/14 22:58:18 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-14 19:58:18,724] {docker.py:276} INFO - 21/05/14 22:58:18 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:18,724] {docker.py:276} INFO - 21/05/14 22:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:18,726] {docker.py:276} INFO - 21/05/14 22:58:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:18,726] {docker.py:276} INFO - 21/05/14 22:58:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:18,727] {docker.py:276} INFO - 21/05/14 22:58:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:18,727] {docker.py:276} INFO - 21/05/14 22:58:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161908046343467133534_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161908046343467133534_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161908046343467133534_0004}; taskId=attempt_202105142256161908046343467133534_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f362d9b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:18,727] {docker.py:276} INFO - 21/05/14 22:58:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:18,728] {docker.py:276} INFO - 21/05/14 22:58:18 INFO StagingCommitter: Starting: Task committer attempt_202105142256161908046343467133534_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161908046343467133534_0004_m_000153_441
[2021-05-14 19:58:18,731] {docker.py:276} INFO - 21/05/14 22:58:18 INFO StagingCommitter: Task committer attempt_202105142256161908046343467133534_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161908046343467133534_0004_m_000153_441 : duration 0:00.003s
[2021-05-14 19:58:19,069] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_20210514225616511044858651271120_0004_m_000150_438: needsTaskCommit() Task attempt_20210514225616511044858651271120_0004_m_000150_438
[2021-05-14 19:58:19,069] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_20210514225616511044858651271120_0004_m_000150_438: needsTaskCommit() Task attempt_20210514225616511044858651271120_0004_m_000150_438: duration 0:00.000s
21/05/14 22:58:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616511044858651271120_0004_m_000150_438
[2021-05-14 19:58:19,070] {docker.py:276} INFO - 21/05/14 22:58:19 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4587 bytes result sent to driver
[2021-05-14 19:58:19,071] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (5c14c3e96bf4, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:19,072] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2815 ms on 5c14c3e96bf4 (executor driver) (151/200)
21/05/14 22:58:19 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-14 19:58:19,086] {docker.py:276} INFO - 21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:19,088] {docker.py:276} INFO - 21/05/14 22:58:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:19,088] {docker.py:276} INFO - 21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166445010519226856012_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166445010519226856012_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166445010519226856012_0004}; taskId=attempt_202105142256166445010519226856012_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@487097bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:19,089] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_202105142256166445010519226856012_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166445010519226856012_0004_m_000154_442
[2021-05-14 19:58:19,093] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_202105142256166445010519226856012_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166445010519226856012_0004_m_000154_442 : duration 0:00.005s
[2021-05-14 19:58:19,140] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_202105142256163296340813358389790_0004_m_000151_439: needsTaskCommit() Task attempt_202105142256163296340813358389790_0004_m_000151_439
[2021-05-14 19:58:19,140] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_202105142256163296340813358389790_0004_m_000151_439: needsTaskCommit() Task attempt_202105142256163296340813358389790_0004_m_000151_439: duration 0:00.001s
21/05/14 22:58:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163296340813358389790_0004_m_000151_439
[2021-05-14 19:58:19,143] {docker.py:276} INFO - 21/05/14 22:58:19 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4587 bytes result sent to driver
[2021-05-14 19:58:19,144] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (5c14c3e96bf4, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:19,145] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 2771 ms on 5c14c3e96bf4 (executor driver) (152/200)
[2021-05-14 19:58:19,146] {docker.py:276} INFO - 21/05/14 22:58:19 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-14 19:58:19,162] {docker.py:276} INFO - 21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:19,164] {docker.py:276} INFO - 21/05/14 22:58:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:19,165] {docker.py:276} INFO - 21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166209854272339278033_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166209854272339278033_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166209854272339278033_0004}; taskId=attempt_202105142256166209854272339278033_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@336d17c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_202105142256166209854272339278033_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166209854272339278033_0004_m_000155_443
[2021-05-14 19:58:19,168] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_202105142256166209854272339278033_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166209854272339278033_0004_m_000155_443 : duration 0:00.003s
[2021-05-14 19:58:19,564] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_20210514225616803733159281433891_0004_m_000152_440: needsTaskCommit() Task attempt_20210514225616803733159281433891_0004_m_000152_440
[2021-05-14 19:58:19,565] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_20210514225616803733159281433891_0004_m_000152_440: needsTaskCommit() Task attempt_20210514225616803733159281433891_0004_m_000152_440: duration 0:00.000s
21/05/14 22:58:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616803733159281433891_0004_m_000152_440
[2021-05-14 19:58:19,566] {docker.py:276} INFO - 21/05/14 22:58:19 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4587 bytes result sent to driver
[2021-05-14 19:58:19,567] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (5c14c3e96bf4, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:19,568] {docker.py:276} INFO - 21/05/14 22:58:19 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2804 ms on 5c14c3e96bf4 (executor driver) (153/200)
[2021-05-14 19:58:19,569] {docker.py:276} INFO - 21/05/14 22:58:19 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-14 19:58:19,584] {docker.py:276} INFO - 21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:19,585] {docker.py:276} INFO - 21/05/14 22:58:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051422561697778787482621557_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_2021051422561697778787482621557_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051422561697778787482621557_0004}; taskId=attempt_2021051422561697778787482621557_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@223283c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:19,586] {docker.py:276} INFO - 21/05/14 22:58:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:19 INFO StagingCommitter: Starting: Task committer attempt_2021051422561697778787482621557_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_2021051422561697778787482621557_0004_m_000156_444
[2021-05-14 19:58:19,588] {docker.py:276} INFO - 21/05/14 22:58:19 INFO StagingCommitter: Task committer attempt_2021051422561697778787482621557_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_2021051422561697778787482621557_0004_m_000156_444 : duration 0:00.003s
[2021-05-14 19:58:21,593] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256161908046343467133534_0004_m_000153_441: needsTaskCommit() Task attempt_202105142256161908046343467133534_0004_m_000153_441
[2021-05-14 19:58:21,599] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256161908046343467133534_0004_m_000153_441: needsTaskCommit() Task attempt_202105142256161908046343467133534_0004_m_000153_441: duration 0:00.002s
[2021-05-14 19:58:21,600] {docker.py:276} INFO - 21/05/14 22:58:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161908046343467133534_0004_m_000153_441
[2021-05-14 19:58:21,601] {docker.py:276} INFO - 21/05/14 22:58:21 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4587 bytes result sent to driver
[2021-05-14 19:58:21,601] {docker.py:276} INFO - 21/05/14 22:58:21 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (5c14c3e96bf4, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:21,602] {docker.py:276} INFO - 21/05/14 22:58:21 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2891 ms on 5c14c3e96bf4 (executor driver) (154/200)
21/05/14 22:58:21 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-14 19:58:21,614] {docker.py:276} INFO - 21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:21,616] {docker.py:276} INFO - 21/05/14 22:58:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:21,616] {docker.py:276} INFO - 21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166166275711461859449_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166166275711461859449_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166166275711461859449_0004}; taskId=attempt_202105142256166166275711461859449_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f66d984}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:21,616] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256166166275711461859449_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166166275711461859449_0004_m_000157_445
[2021-05-14 19:58:21,619] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256166166275711461859449_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166166275711461859449_0004_m_000157_445 : duration 0:00.002s
[2021-05-14 19:58:21,770] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256166445010519226856012_0004_m_000154_442: needsTaskCommit() Task attempt_202105142256166445010519226856012_0004_m_000154_442
[2021-05-14 19:58:21,771] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256166445010519226856012_0004_m_000154_442: needsTaskCommit() Task attempt_202105142256166445010519226856012_0004_m_000154_442: duration 0:00.001s
21/05/14 22:58:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166445010519226856012_0004_m_000154_442
[2021-05-14 19:58:21,774] {docker.py:276} INFO - 21/05/14 22:58:21 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4587 bytes result sent to driver
[2021-05-14 19:58:21,775] {docker.py:276} INFO - 21/05/14 22:58:21 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (5c14c3e96bf4, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:21,776] {docker.py:276} INFO - 21/05/14 22:58:21 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 2708 ms on 5c14c3e96bf4 (executor driver) (155/200)
21/05/14 22:58:21 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-14 19:58:21,791] {docker.py:276} INFO - 21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:21,792] {docker.py:276} INFO - 21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:58:21,793] {docker.py:276} INFO - 21/05/14 22:58:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161431400819044597718_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161431400819044597718_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161431400819044597718_0004}; taskId=attempt_202105142256161431400819044597718_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e8646d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:21,794] {docker.py:276} INFO - 21/05/14 22:58:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256161431400819044597718_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161431400819044597718_0004_m_000158_446
[2021-05-14 19:58:21,796] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256161431400819044597718_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161431400819044597718_0004_m_000158_446 : duration 0:00.002s
[2021-05-14 19:58:21,967] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256166209854272339278033_0004_m_000155_443: needsTaskCommit() Task attempt_202105142256166209854272339278033_0004_m_000155_443
[2021-05-14 19:58:21,967] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256166209854272339278033_0004_m_000155_443: needsTaskCommit() Task attempt_202105142256166209854272339278033_0004_m_000155_443: duration 0:00.000s
21/05/14 22:58:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166209854272339278033_0004_m_000155_443
[2021-05-14 19:58:21,969] {docker.py:276} INFO - 21/05/14 22:58:21 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4587 bytes result sent to driver
[2021-05-14 19:58:21,970] {docker.py:276} INFO - 21/05/14 22:58:21 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (5c14c3e96bf4, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:21,971] {docker.py:276} INFO - 21/05/14 22:58:21 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
21/05/14 22:58:21 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2831 ms on 5c14c3e96bf4 (executor driver) (156/200)
[2021-05-14 19:58:21,978] {docker.py:276} INFO - 21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:21,980] {docker.py:276} INFO - 21/05/14 22:58:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:21,981] {docker.py:276} INFO - 21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164948464569308609431_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164948464569308609431_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164948464569308609431_0004}; taskId=attempt_202105142256164948464569308609431_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3539bd13}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:21,981] {docker.py:276} INFO - 21/05/14 22:58:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:21,981] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Starting: Task committer attempt_202105142256164948464569308609431_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164948464569308609431_0004_m_000159_447
[2021-05-14 19:58:21,984] {docker.py:276} INFO - 21/05/14 22:58:21 INFO StagingCommitter: Task committer attempt_202105142256164948464569308609431_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164948464569308609431_0004_m_000159_447 : duration 0:00.003s
[2021-05-14 19:58:22,303] {docker.py:276} INFO - 21/05/14 22:58:22 INFO StagingCommitter: Starting: Task committer attempt_2021051422561697778787482621557_0004_m_000156_444: needsTaskCommit() Task attempt_2021051422561697778787482621557_0004_m_000156_444
[2021-05-14 19:58:22,304] {docker.py:276} INFO - 21/05/14 22:58:22 INFO StagingCommitter: Task committer attempt_2021051422561697778787482621557_0004_m_000156_444: needsTaskCommit() Task attempt_2021051422561697778787482621557_0004_m_000156_444: duration 0:00.001s
21/05/14 22:58:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051422561697778787482621557_0004_m_000156_444
[2021-05-14 19:58:22,307] {docker.py:276} INFO - 21/05/14 22:58:22 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4587 bytes result sent to driver
[2021-05-14 19:58:22,308] {docker.py:276} INFO - 21/05/14 22:58:22 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (5c14c3e96bf4, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:22,309] {docker.py:276} INFO - 21/05/14 22:58:22 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
21/05/14 22:58:22 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 2745 ms on 5c14c3e96bf4 (executor driver) (157/200)
[2021-05-14 19:58:22,328] {docker.py:276} INFO - 21/05/14 22:58:22 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:22,328] {docker.py:276} INFO - 21/05/14 22:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:22,330] {docker.py:276} INFO - 21/05/14 22:58:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:22,330] {docker.py:276} INFO - 21/05/14 22:58:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:22,330] {docker.py:276} INFO - 21/05/14 22:58:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:22,331] {docker.py:276} INFO - 21/05/14 22:58:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616231555279401433436_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616231555279401433436_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616231555279401433436_0004}; taskId=attempt_20210514225616231555279401433436_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63343cf9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:22,331] {docker.py:276} INFO - 21/05/14 22:58:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:22,331] {docker.py:276} INFO - 21/05/14 22:58:22 INFO StagingCommitter: Starting: Task committer attempt_20210514225616231555279401433436_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616231555279401433436_0004_m_000160_448
[2021-05-14 19:58:22,334] {docker.py:276} INFO - 21/05/14 22:58:22 INFO StagingCommitter: Task committer attempt_20210514225616231555279401433436_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616231555279401433436_0004_m_000160_448 : duration 0:00.003s
[2021-05-14 19:58:24,353] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256166166275711461859449_0004_m_000157_445: needsTaskCommit() Task attempt_202105142256166166275711461859449_0004_m_000157_445
[2021-05-14 19:58:24,355] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256166166275711461859449_0004_m_000157_445: needsTaskCommit() Task attempt_202105142256166166275711461859449_0004_m_000157_445: duration 0:00.000s
21/05/14 22:58:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166166275711461859449_0004_m_000157_445
[2021-05-14 19:58:24,355] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4587 bytes result sent to driver
[2021-05-14 19:58:24,357] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (5c14c3e96bf4, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:24,357] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-14 19:58:24,358] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2763 ms on 5c14c3e96bf4 (executor driver) (158/200)
[2021-05-14 19:58:24,373] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Getting 5 (41.0 KiB) non-empty blocks including 5 (41.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:24,374] {docker.py:276} INFO - 21/05/14 22:58:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:24,375] {docker.py:276} INFO - 21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165055203150793831762_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165055203150793831762_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165055203150793831762_0004}; taskId=attempt_202105142256165055203150793831762_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50827ad4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256165055203150793831762_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165055203150793831762_0004_m_000161_449
[2021-05-14 19:58:24,378] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256165055203150793831762_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165055203150793831762_0004_m_000161_449 : duration 0:00.003s
[2021-05-14 19:58:24,483] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256161431400819044597718_0004_m_000158_446: needsTaskCommit() Task attempt_202105142256161431400819044597718_0004_m_000158_446
[2021-05-14 19:58:24,484] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256161431400819044597718_0004_m_000158_446: needsTaskCommit() Task attempt_202105142256161431400819044597718_0004_m_000158_446: duration 0:00.000s
21/05/14 22:58:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161431400819044597718_0004_m_000158_446
[2021-05-14 19:58:24,486] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4587 bytes result sent to driver
[2021-05-14 19:58:24,488] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (5c14c3e96bf4, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:24,489] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 2717 ms on 5c14c3e96bf4 (executor driver) (159/200)
[2021-05-14 19:58:24,489] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-14 19:58:24,505] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:24,505] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:24,507] {docker.py:276} INFO - 21/05/14 22:58:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:24,507] {docker.py:276} INFO - 21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161165449572582046433_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161165449572582046433_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161165449572582046433_0004}; taskId=attempt_202105142256161165449572582046433_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bd1a5c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:24,508] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256161165449572582046433_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161165449572582046433_0004_m_000162_450
[2021-05-14 19:58:24,510] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256161165449572582046433_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161165449572582046433_0004_m_000162_450 : duration 0:00.002s
[2021-05-14 19:58:24,674] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_20210514225616231555279401433436_0004_m_000160_448: needsTaskCommit() Task attempt_20210514225616231555279401433436_0004_m_000160_448
[2021-05-14 19:58:24,675] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_20210514225616231555279401433436_0004_m_000160_448: needsTaskCommit() Task attempt_20210514225616231555279401433436_0004_m_000160_448: duration 0:00.001s
21/05/14 22:58:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616231555279401433436_0004_m_000160_448
[2021-05-14 19:58:24,677] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4587 bytes result sent to driver
[2021-05-14 19:58:24,678] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (5c14c3e96bf4, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:24,679] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
21/05/14 22:58:24 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2374 ms on 5c14c3e96bf4 (executor driver) (160/200)
[2021-05-14 19:58:24,695] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:24,697] {docker.py:276} INFO - 21/05/14 22:58:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164600542837260269093_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164600542837260269093_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164600542837260269093_0004}; taskId=attempt_202105142256164600542837260269093_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48c991e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:24,698] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256164600542837260269093_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164600542837260269093_0004_m_000163_451
[2021-05-14 19:58:24,700] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256164600542837260269093_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164600542837260269093_0004_m_000163_451 : duration 0:00.003s
[2021-05-14 19:58:24,741] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256164948464569308609431_0004_m_000159_447: needsTaskCommit() Task attempt_202105142256164948464569308609431_0004_m_000159_447
21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256164948464569308609431_0004_m_000159_447: needsTaskCommit() Task attempt_202105142256164948464569308609431_0004_m_000159_447: duration 0:00.000s
21/05/14 22:58:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164948464569308609431_0004_m_000159_447
[2021-05-14 19:58:24,743] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4587 bytes result sent to driver
[2021-05-14 19:58:24,744] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (5c14c3e96bf4, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:24,745] {docker.py:276} INFO - 21/05/14 22:58:24 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2778 ms on 5c14c3e96bf4 (executor driver) (161/200)
[2021-05-14 19:58:24,745] {docker.py:276} INFO - 21/05/14 22:58:24 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-14 19:58:24,762] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:24,763] {docker.py:276} INFO - 21/05/14 22:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:24,764] {docker.py:276} INFO - 21/05/14 22:58:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:24,765] {docker.py:276} INFO - 21/05/14 22:58:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:24,765] {docker.py:276} INFO - 21/05/14 22:58:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161351614756225409014_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161351614756225409014_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161351614756225409014_0004}; taskId=attempt_202105142256161351614756225409014_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@264006bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:24,766] {docker.py:276} INFO - 21/05/14 22:58:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:24,766] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Starting: Task committer attempt_202105142256161351614756225409014_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161351614756225409014_0004_m_000164_452
[2021-05-14 19:58:24,770] {docker.py:276} INFO - 21/05/14 22:58:24 INFO StagingCommitter: Task committer attempt_202105142256161351614756225409014_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161351614756225409014_0004_m_000164_452 : duration 0:00.003s
[2021-05-14 19:58:26,479] {docker.py:276} INFO - 21/05/14 22:58:26 INFO StagingCommitter: Starting: Task committer attempt_202105142256165055203150793831762_0004_m_000161_449: needsTaskCommit() Task attempt_202105142256165055203150793831762_0004_m_000161_449
[2021-05-14 19:58:26,480] {docker.py:276} INFO - 21/05/14 22:58:26 INFO StagingCommitter: Task committer attempt_202105142256165055203150793831762_0004_m_000161_449: needsTaskCommit() Task attempt_202105142256165055203150793831762_0004_m_000161_449: duration 0:00.000s
21/05/14 22:58:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165055203150793831762_0004_m_000161_449
[2021-05-14 19:58:26,482] {docker.py:276} INFO - 21/05/14 22:58:26 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4587 bytes result sent to driver
21/05/14 22:58:26 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (5c14c3e96bf4, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:26,483] {docker.py:276} INFO - 21/05/14 22:58:26 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
21/05/14 22:58:26 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 2130 ms on 5c14c3e96bf4 (executor driver) (162/200)
[2021-05-14 19:58:26,493] {docker.py:276} INFO - 21/05/14 22:58:26 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:26,495] {docker.py:276} INFO - 21/05/14 22:58:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:26,496] {docker.py:276} INFO - 21/05/14 22:58:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165891901099421670191_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165891901099421670191_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165891901099421670191_0004}; taskId=attempt_202105142256165891901099421670191_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75578eab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:26,496] {docker.py:276} INFO - 21/05/14 22:58:26 INFO StagingCommitter: Starting: Task committer attempt_202105142256165891901099421670191_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165891901099421670191_0004_m_000165_453
[2021-05-14 19:58:26,498] {docker.py:276} INFO - 21/05/14 22:58:26 INFO StagingCommitter: Task committer attempt_202105142256165891901099421670191_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165891901099421670191_0004_m_000165_453 : duration 0:00.003s
[2021-05-14 19:58:27,217] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256164600542837260269093_0004_m_000163_451: needsTaskCommit() Task attempt_202105142256164600542837260269093_0004_m_000163_451
[2021-05-14 19:58:27,218] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256164600542837260269093_0004_m_000163_451: needsTaskCommit() Task attempt_202105142256164600542837260269093_0004_m_000163_451: duration 0:00.001s
21/05/14 22:58:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164600542837260269093_0004_m_000163_451
[2021-05-14 19:58:27,220] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4587 bytes result sent to driver
[2021-05-14 19:58:27,221] {docker.py:276} INFO - 21/05/14 22:58:27 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (5c14c3e96bf4, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:27,222] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-14 19:58:27,223] {docker.py:276} INFO - 21/05/14 22:58:27 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 2548 ms on 5c14c3e96bf4 (executor driver) (163/200)
[2021-05-14 19:58:27,231] {docker.py:276} INFO - 21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:27,233] {docker.py:276} INFO - 21/05/14 22:58:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162614891805640522015_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162614891805640522015_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162614891805640522015_0004}; taskId=attempt_202105142256162614891805640522015_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e450132}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256162614891805640522015_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162614891805640522015_0004_m_000166_454
[2021-05-14 19:58:27,236] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256162614891805640522015_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162614891805640522015_0004_m_000166_454 : duration 0:00.003s
[2021-05-14 19:58:27,422] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256161351614756225409014_0004_m_000164_452: needsTaskCommit() Task attempt_202105142256161351614756225409014_0004_m_000164_452
[2021-05-14 19:58:27,423] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256161351614756225409014_0004_m_000164_452: needsTaskCommit() Task attempt_202105142256161351614756225409014_0004_m_000164_452: duration 0:00.002s
21/05/14 22:58:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161351614756225409014_0004_m_000164_452
[2021-05-14 19:58:27,424] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4587 bytes result sent to driver
[2021-05-14 19:58:27,425] {docker.py:276} INFO - 21/05/14 22:58:27 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (5c14c3e96bf4, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:27,426] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-14 19:58:27,427] {docker.py:276} INFO - 21/05/14 22:58:27 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2687 ms on 5c14c3e96bf4 (executor driver) (164/200)
[2021-05-14 19:58:27,436] {docker.py:276} INFO - 21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:27,438] {docker.py:276} INFO - 21/05/14 22:58:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162946411614372314649_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162946411614372314649_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162946411614372314649_0004}; taskId=attempt_202105142256162946411614372314649_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f934c25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256162946411614372314649_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162946411614372314649_0004_m_000167_455
[2021-05-14 19:58:27,441] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256162946411614372314649_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162946411614372314649_0004_m_000167_455 : duration 0:00.003s
[2021-05-14 19:58:27,854] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256161165449572582046433_0004_m_000162_450: needsTaskCommit() Task attempt_202105142256161165449572582046433_0004_m_000162_450
[2021-05-14 19:58:27,855] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256161165449572582046433_0004_m_000162_450: needsTaskCommit() Task attempt_202105142256161165449572582046433_0004_m_000162_450: duration 0:00.001s
21/05/14 22:58:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161165449572582046433_0004_m_000162_450
[2021-05-14 19:58:27,856] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4587 bytes result sent to driver
[2021-05-14 19:58:27,858] {docker.py:276} INFO - 21/05/14 22:58:27 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (5c14c3e96bf4, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:27,859] {docker.py:276} INFO - 21/05/14 22:58:27 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
21/05/14 22:58:27 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 3375 ms on 5c14c3e96bf4 (executor driver) (165/200)
[2021-05-14 19:58:27,868] {docker.py:276} INFO - 21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:27,870] {docker.py:276} INFO - 21/05/14 22:58:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161726461865267799063_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161726461865267799063_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161726461865267799063_0004}; taskId=attempt_202105142256161726461865267799063_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@271e3dbe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:27 INFO StagingCommitter: Starting: Task committer attempt_202105142256161726461865267799063_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161726461865267799063_0004_m_000168_456
[2021-05-14 19:58:27,873] {docker.py:276} INFO - 21/05/14 22:58:27 INFO StagingCommitter: Task committer attempt_202105142256161726461865267799063_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161726461865267799063_0004_m_000168_456 : duration 0:00.003s
[2021-05-14 19:58:28,707] {docker.py:276} INFO - 21/05/14 22:58:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256165891901099421670191_0004_m_000165_453: needsTaskCommit() Task attempt_202105142256165891901099421670191_0004_m_000165_453
[2021-05-14 19:58:28,709] {docker.py:276} INFO - 21/05/14 22:58:28 INFO StagingCommitter: Task committer attempt_202105142256165891901099421670191_0004_m_000165_453: needsTaskCommit() Task attempt_202105142256165891901099421670191_0004_m_000165_453: duration 0:00.003s
21/05/14 22:58:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165891901099421670191_0004_m_000165_453
[2021-05-14 19:58:28,710] {docker.py:276} INFO - 21/05/14 22:58:28 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4587 bytes result sent to driver
[2021-05-14 19:58:28,712] {docker.py:276} INFO - 21/05/14 22:58:28 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (5c14c3e96bf4, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:28,712] {docker.py:276} INFO - 21/05/14 22:58:28 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2232 ms on 5c14c3e96bf4 (executor driver) (166/200)
21/05/14 22:58:28 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-14 19:58:28,721] {docker.py:276} INFO - 21/05/14 22:58:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:28,723] {docker.py:276} INFO - 21/05/14 22:58:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161582762316211866684_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161582762316211866684_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161582762316211866684_0004}; taskId=attempt_202105142256161582762316211866684_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46b934e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:28,723] {docker.py:276} INFO - 21/05/14 22:58:28 INFO StagingCommitter: Starting: Task committer attempt_202105142256161582762316211866684_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161582762316211866684_0004_m_000169_457
[2021-05-14 19:58:28,727] {docker.py:276} INFO - 21/05/14 22:58:28 INFO StagingCommitter: Task committer attempt_202105142256161582762316211866684_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161582762316211866684_0004_m_000169_457 : duration 0:00.004s
[2021-05-14 19:58:29,930] {docker.py:276} INFO - 21/05/14 22:58:29 INFO StagingCommitter: Starting: Task committer attempt_202105142256162614891805640522015_0004_m_000166_454: needsTaskCommit() Task attempt_202105142256162614891805640522015_0004_m_000166_454
[2021-05-14 19:58:29,931] {docker.py:276} INFO - 21/05/14 22:58:29 INFO StagingCommitter: Task committer attempt_202105142256162614891805640522015_0004_m_000166_454: needsTaskCommit() Task attempt_202105142256162614891805640522015_0004_m_000166_454: duration 0:00.001s
[2021-05-14 19:58:29,932] {docker.py:276} INFO - 21/05/14 22:58:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162614891805640522015_0004_m_000166_454
[2021-05-14 19:58:29,934] {docker.py:276} INFO - 21/05/14 22:58:29 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4587 bytes result sent to driver
[2021-05-14 19:58:29,936] {docker.py:276} INFO - 21/05/14 22:58:29 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (5c14c3e96bf4, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:29,936] {docker.py:276} INFO - 21/05/14 22:58:29 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2720 ms on 5c14c3e96bf4 (executor driver) (167/200)
[2021-05-14 19:58:29,938] {docker.py:276} INFO - 21/05/14 22:58:29 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-14 19:58:29,948] {docker.py:276} INFO - 21/05/14 22:58:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.7 KiB) non-empty blocks including 5 (43.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:29,949] {docker.py:276} INFO - 21/05/14 22:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:29,951] {docker.py:276} INFO - 21/05/14 22:58:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:29,951] {docker.py:276} INFO - 21/05/14 22:58:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256163641760369366619344_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163641760369366619344_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256163641760369366619344_0004}; taskId=attempt_202105142256163641760369366619344_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@764e3a59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:29,951] {docker.py:276} INFO - 21/05/14 22:58:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:29,952] {docker.py:276} INFO - 21/05/14 22:58:29 INFO StagingCommitter: Starting: Task committer attempt_202105142256163641760369366619344_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163641760369366619344_0004_m_000170_458
[2021-05-14 19:58:29,954] {docker.py:276} INFO - 21/05/14 22:58:29 INFO StagingCommitter: Task committer attempt_202105142256163641760369366619344_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256163641760369366619344_0004_m_000170_458 : duration 0:00.003s
[2021-05-14 19:58:30,364] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256162946411614372314649_0004_m_000167_455: needsTaskCommit() Task attempt_202105142256162946411614372314649_0004_m_000167_455
21/05/14 22:58:30 INFO StagingCommitter: Task committer attempt_202105142256162946411614372314649_0004_m_000167_455: needsTaskCommit() Task attempt_202105142256162946411614372314649_0004_m_000167_455: duration 0:00.001s
21/05/14 22:58:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162946411614372314649_0004_m_000167_455
[2021-05-14 19:58:30,365] {docker.py:276} INFO - 21/05/14 22:58:30 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4587 bytes result sent to driver
[2021-05-14 19:58:30,366] {docker.py:276} INFO - 21/05/14 22:58:30 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (5c14c3e96bf4, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:30,367] {docker.py:276} INFO - 21/05/14 22:58:30 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
21/05/14 22:58:30 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2946 ms on 5c14c3e96bf4 (executor driver) (168/200)
[2021-05-14 19:58:30,377] {docker.py:276} INFO - 21/05/14 22:58:30 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:30,379] {docker.py:276} INFO - 21/05/14 22:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168225435355529753898_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168225435355529753898_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168225435355529753898_0004}; taskId=attempt_202105142256168225435355529753898_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6229b5d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:30,379] {docker.py:276} INFO - 21/05/14 22:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256168225435355529753898_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168225435355529753898_0004_m_000171_459
[2021-05-14 19:58:30,382] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Task committer attempt_202105142256168225435355529753898_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168225435355529753898_0004_m_000171_459 : duration 0:00.003s
[2021-05-14 19:58:30,535] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256161726461865267799063_0004_m_000168_456: needsTaskCommit() Task attempt_202105142256161726461865267799063_0004_m_000168_456
[2021-05-14 19:58:30,536] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Task committer attempt_202105142256161726461865267799063_0004_m_000168_456: needsTaskCommit() Task attempt_202105142256161726461865267799063_0004_m_000168_456: duration 0:00.002s
21/05/14 22:58:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161726461865267799063_0004_m_000168_456
[2021-05-14 19:58:30,541] {docker.py:276} INFO - 21/05/14 22:58:30 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4630 bytes result sent to driver
[2021-05-14 19:58:30,543] {docker.py:276} INFO - 21/05/14 22:58:30 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (5c14c3e96bf4, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:30,544] {docker.py:276} INFO - 21/05/14 22:58:30 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-14 19:58:30,545] {docker.py:276} INFO - 21/05/14 22:58:30 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2690 ms on 5c14c3e96bf4 (executor driver) (169/200)
[2021-05-14 19:58:30,553] {docker.py:276} INFO - 21/05/14 22:58:30 INFO ShuffleBlockFetcherIterator: Getting 5 (39.7 KiB) non-empty blocks including 5 (39.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:30,555] {docker.py:276} INFO - 21/05/14 22:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161415625833404342112_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161415625833404342112_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161415625833404342112_0004}; taskId=attempt_202105142256161415625833404342112_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b96b38e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:30,556] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Starting: Task committer attempt_202105142256161415625833404342112_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161415625833404342112_0004_m_000172_460
[2021-05-14 19:58:30,559] {docker.py:276} INFO - 21/05/14 22:58:30 INFO StagingCommitter: Task committer attempt_202105142256161415625833404342112_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161415625833404342112_0004_m_000172_460 : duration 0:00.004s
[2021-05-14 19:58:31,469] {docker.py:276} INFO - 21/05/14 22:58:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256161582762316211866684_0004_m_000169_457: needsTaskCommit() Task attempt_202105142256161582762316211866684_0004_m_000169_457
21/05/14 22:58:31 INFO StagingCommitter: Task committer attempt_202105142256161582762316211866684_0004_m_000169_457: needsTaskCommit() Task attempt_202105142256161582762316211866684_0004_m_000169_457: duration 0:00.001s
21/05/14 22:58:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161582762316211866684_0004_m_000169_457
[2021-05-14 19:58:31,471] {docker.py:276} INFO - 21/05/14 22:58:31 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4587 bytes result sent to driver
[2021-05-14 19:58:31,472] {docker.py:276} INFO - 21/05/14 22:58:31 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (5c14c3e96bf4, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:31,473] {docker.py:276} INFO - 21/05/14 22:58:31 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2766 ms on 5c14c3e96bf4 (executor driver) (170/200)
21/05/14 22:58:31 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-14 19:58:31,483] {docker.py:276} INFO - 21/05/14 22:58:31 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:31,485] {docker.py:276} INFO - 21/05/14 22:58:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162511956890495244199_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162511956890495244199_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162511956890495244199_0004}; taskId=attempt_202105142256162511956890495244199_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@323d26e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:31,485] {docker.py:276} INFO - 21/05/14 22:58:31 INFO StagingCommitter: Starting: Task committer attempt_202105142256162511956890495244199_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162511956890495244199_0004_m_000173_461
[2021-05-14 19:58:31,488] {docker.py:276} INFO - 21/05/14 22:58:31 INFO StagingCommitter: Task committer attempt_202105142256162511956890495244199_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162511956890495244199_0004_m_000173_461 : duration 0:00.003s
[2021-05-14 19:58:32,860] {docker.py:276} INFO - 21/05/14 22:58:32 INFO StagingCommitter: Starting: Task committer attempt_202105142256163641760369366619344_0004_m_000170_458: needsTaskCommit() Task attempt_202105142256163641760369366619344_0004_m_000170_458
21/05/14 22:58:32 INFO StagingCommitter: Task committer attempt_202105142256163641760369366619344_0004_m_000170_458: needsTaskCommit() Task attempt_202105142256163641760369366619344_0004_m_000170_458: duration 0:00.000s
[2021-05-14 19:58:32,861] {docker.py:276} INFO - 21/05/14 22:58:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256163641760369366619344_0004_m_000170_458
[2021-05-14 19:58:32,863] {docker.py:276} INFO - 21/05/14 22:58:32 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4587 bytes result sent to driver
[2021-05-14 19:58:32,864] {docker.py:276} INFO - 21/05/14 22:58:32 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (5c14c3e96bf4, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:32,865] {docker.py:276} INFO - 21/05/14 22:58:32 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 2933 ms on 5c14c3e96bf4 (executor driver) (171/200)
21/05/14 22:58:32 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-14 19:58:32,875] {docker.py:276} INFO - 21/05/14 22:58:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:32,877] {docker.py:276} INFO - 21/05/14 22:58:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:32,878] {docker.py:276} INFO - 21/05/14 22:58:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:32,879] {docker.py:276} INFO - 21/05/14 22:58:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167044358279401025622_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167044358279401025622_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167044358279401025622_0004}; taskId=attempt_202105142256167044358279401025622_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14973462}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:32 INFO StagingCommitter: Starting: Task committer attempt_202105142256167044358279401025622_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167044358279401025622_0004_m_000174_462
[2021-05-14 19:58:32,882] {docker.py:276} INFO - 21/05/14 22:58:32 INFO StagingCommitter: Task committer attempt_202105142256167044358279401025622_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167044358279401025622_0004_m_000174_462 : duration 0:00.003s
[2021-05-14 19:58:33,025] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256168225435355529753898_0004_m_000171_459: needsTaskCommit() Task attempt_202105142256168225435355529753898_0004_m_000171_459
[2021-05-14 19:58:33,026] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Task committer attempt_202105142256168225435355529753898_0004_m_000171_459: needsTaskCommit() Task attempt_202105142256168225435355529753898_0004_m_000171_459: duration 0:00.001s
21/05/14 22:58:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168225435355529753898_0004_m_000171_459
[2021-05-14 19:58:33,029] {docker.py:276} INFO - 21/05/14 22:58:33 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4587 bytes result sent to driver
[2021-05-14 19:58:33,030] {docker.py:276} INFO - 21/05/14 22:58:33 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (5c14c3e96bf4, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:33,031] {docker.py:276} INFO - 21/05/14 22:58:33 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-14 19:58:33,032] {docker.py:276} INFO - 21/05/14 22:58:33 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2669 ms on 5c14c3e96bf4 (executor driver) (172/200)
[2021-05-14 19:58:33,041] {docker.py:276} INFO - 21/05/14 22:58:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:33,043] {docker.py:276} INFO - 21/05/14 22:58:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162805621538948732747_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162805621538948732747_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162805621538948732747_0004}; taskId=attempt_202105142256162805621538948732747_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@377866d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:33,043] {docker.py:276} INFO - 21/05/14 22:58:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:33,043] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256162805621538948732747_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162805621538948732747_0004_m_000175_463
[2021-05-14 19:58:33,046] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Task committer attempt_202105142256162805621538948732747_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162805621538948732747_0004_m_000175_463 : duration 0:00.003s
[2021-05-14 19:58:33,279] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256161415625833404342112_0004_m_000172_460: needsTaskCommit() Task attempt_202105142256161415625833404342112_0004_m_000172_460
[2021-05-14 19:58:33,280] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Task committer attempt_202105142256161415625833404342112_0004_m_000172_460: needsTaskCommit() Task attempt_202105142256161415625833404342112_0004_m_000172_460: duration 0:00.001s
21/05/14 22:58:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161415625833404342112_0004_m_000172_460
[2021-05-14 19:58:33,281] {docker.py:276} INFO - 21/05/14 22:58:33 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4587 bytes result sent to driver
[2021-05-14 19:58:33,282] {docker.py:276} INFO - 21/05/14 22:58:33 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (5c14c3e96bf4, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:33,283] {docker.py:276} INFO - 21/05/14 22:58:33 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 2744 ms on 5c14c3e96bf4 (executor driver) (173/200)
[2021-05-14 19:58:33,283] {docker.py:276} INFO - 21/05/14 22:58:33 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-14 19:58:33,293] {docker.py:276} INFO - 21/05/14 22:58:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:33,294] {docker.py:276} INFO - 21/05/14 22:58:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:33,295] {docker.py:276} INFO - 21/05/14 22:58:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:33,296] {docker.py:276} INFO - 21/05/14 22:58:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:33,296] {docker.py:276} INFO - 21/05/14 22:58:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:33,296] {docker.py:276} INFO - 21/05/14 22:58:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165593689645538719691_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165593689645538719691_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165593689645538719691_0004}; taskId=attempt_202105142256165593689645538719691_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@634d40c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:33,297] {docker.py:276} INFO - 21/05/14 22:58:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:33,297] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Starting: Task committer attempt_202105142256165593689645538719691_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165593689645538719691_0004_m_000176_464
[2021-05-14 19:58:33,300] {docker.py:276} INFO - 21/05/14 22:58:33 INFO StagingCommitter: Task committer attempt_202105142256165593689645538719691_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165593689645538719691_0004_m_000176_464 : duration 0:00.002s
[2021-05-14 19:58:34,187] {docker.py:276} INFO - 21/05/14 22:58:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256162511956890495244199_0004_m_000173_461: needsTaskCommit() Task attempt_202105142256162511956890495244199_0004_m_000173_461
[2021-05-14 19:58:34,188] {docker.py:276} INFO - 21/05/14 22:58:34 INFO StagingCommitter: Task committer attempt_202105142256162511956890495244199_0004_m_000173_461: needsTaskCommit() Task attempt_202105142256162511956890495244199_0004_m_000173_461: duration 0:00.000s
21/05/14 22:58:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162511956890495244199_0004_m_000173_461
[2021-05-14 19:58:34,189] {docker.py:276} INFO - 21/05/14 22:58:34 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4587 bytes result sent to driver
[2021-05-14 19:58:34,190] {docker.py:276} INFO - 21/05/14 22:58:34 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (5c14c3e96bf4, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:34,190] {docker.py:276} INFO - 21/05/14 22:58:34 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2723 ms on 5c14c3e96bf4 (executor driver) (174/200)
[2021-05-14 19:58:34,191] {docker.py:276} INFO - 21/05/14 22:58:34 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
[2021-05-14 19:58:34,200] {docker.py:276} INFO - 21/05/14 22:58:34 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:34,202] {docker.py:276} INFO - 21/05/14 22:58:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:34,202] {docker.py:276} INFO - 21/05/14 22:58:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:34,202] {docker.py:276} INFO - 21/05/14 22:58:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165975991771568525990_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165975991771568525990_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165975991771568525990_0004}; taskId=attempt_202105142256165975991771568525990_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@679555e4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:34,203] {docker.py:276} INFO - 21/05/14 22:58:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:34,203] {docker.py:276} INFO - 21/05/14 22:58:34 INFO StagingCommitter: Starting: Task committer attempt_202105142256165975991771568525990_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165975991771568525990_0004_m_000177_465
[2021-05-14 19:58:34,206] {docker.py:276} INFO - 21/05/14 22:58:34 INFO StagingCommitter: Task committer attempt_202105142256165975991771568525990_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165975991771568525990_0004_m_000177_465 : duration 0:00.003s
[2021-05-14 19:58:35,692] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_202105142256162805621538948732747_0004_m_000175_463: needsTaskCommit() Task attempt_202105142256162805621538948732747_0004_m_000175_463
[2021-05-14 19:58:35,693] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_202105142256162805621538948732747_0004_m_000175_463: needsTaskCommit() Task attempt_202105142256162805621538948732747_0004_m_000175_463: duration 0:00.001s
[2021-05-14 19:58:35,694] {docker.py:276} INFO - 21/05/14 22:58:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162805621538948732747_0004_m_000175_463
[2021-05-14 19:58:35,695] {docker.py:276} INFO - 21/05/14 22:58:35 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4587 bytes result sent to driver
[2021-05-14 19:58:35,699] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (5c14c3e96bf4, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:35,700] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_202105142256167044358279401025622_0004_m_000174_462: needsTaskCommit() Task attempt_202105142256167044358279401025622_0004_m_000174_462
[2021-05-14 19:58:35,700] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_202105142256167044358279401025622_0004_m_000174_462: needsTaskCommit() Task attempt_202105142256167044358279401025622_0004_m_000174_462: duration 0:00.001s
21/05/14 22:58:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167044358279401025622_0004_m_000174_462
[2021-05-14 19:58:35,701] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2674 ms on 5c14c3e96bf4 (executor driver) (175/200)
[2021-05-14 19:58:35,701] {docker.py:276} INFO - 21/05/14 22:58:35 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4587 bytes result sent to driver
21/05/14 22:58:35 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-14 19:58:35,702] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (5c14c3e96bf4, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:35,703] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2844 ms on 5c14c3e96bf4 (executor driver) (176/200)
[2021-05-14 19:58:35,704] {docker.py:276} INFO - 21/05/14 22:58:35 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
[2021-05-14 19:58:35,712] {docker.py:276} INFO - 21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Getting 5 (40.8 KiB) non-empty blocks including 5 (40.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:35,713] {docker.py:276} INFO - 21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:35,714] {docker.py:276} INFO - 21/05/14 22:58:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:35,714] {docker.py:276} INFO - 21/05/14 22:58:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:35,715] {docker.py:276} INFO - 21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:35,715] {docker.py:276} INFO - 21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162356731071185050177_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162356731071185050177_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162356731071185050177_0004}; taskId=attempt_202105142256162356731071185050177_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68ffcbb1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:35,715] {docker.py:276} INFO - 21/05/14 22:58:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_202105142256162356731071185050177_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162356731071185050177_0004_m_000179_467
[2021-05-14 19:58:35,716] {docker.py:276} INFO - 21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168369885148211271921_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168369885148211271921_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168369885148211271921_0004}; taskId=attempt_202105142256168369885148211271921_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12f81b7a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:35,716] {docker.py:276} INFO - 21/05/14 22:58:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_202105142256168369885148211271921_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168369885148211271921_0004_m_000178_466
[2021-05-14 19:58:35,718] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_202105142256162356731071185050177_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162356731071185050177_0004_m_000179_467 : duration 0:00.003s
[2021-05-14 19:58:35,719] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_202105142256168369885148211271921_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168369885148211271921_0004_m_000178_466 : duration 0:00.004s
[2021-05-14 19:58:35,887] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_202105142256165593689645538719691_0004_m_000176_464: needsTaskCommit() Task attempt_202105142256165593689645538719691_0004_m_000176_464
[2021-05-14 19:58:35,888] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_202105142256165593689645538719691_0004_m_000176_464: needsTaskCommit() Task attempt_202105142256165593689645538719691_0004_m_000176_464: duration 0:00.001s
[2021-05-14 19:58:35,890] {docker.py:276} INFO - 21/05/14 22:58:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165593689645538719691_0004_m_000176_464
[2021-05-14 19:58:35,891] {docker.py:276} INFO - 21/05/14 22:58:35 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4587 bytes result sent to driver
[2021-05-14 19:58:35,892] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (5c14c3e96bf4, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:35,893] {docker.py:276} INFO - 21/05/14 22:58:35 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
[2021-05-14 19:58:35,894] {docker.py:276} INFO - 21/05/14 22:58:35 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2616 ms on 5c14c3e96bf4 (executor driver) (177/200)
[2021-05-14 19:58:35,903] {docker.py:276} INFO - 21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:35,904] {docker.py:276} INFO - 21/05/14 22:58:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:35,905] {docker.py:276} INFO - 21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616924308020158085667_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616924308020158085667_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616924308020158085667_0004}; taskId=attempt_20210514225616924308020158085667_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71a5ea6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:35 INFO StagingCommitter: Starting: Task committer attempt_20210514225616924308020158085667_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616924308020158085667_0004_m_000180_468
[2021-05-14 19:58:35,907] {docker.py:276} INFO - 21/05/14 22:58:35 INFO StagingCommitter: Task committer attempt_20210514225616924308020158085667_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616924308020158085667_0004_m_000180_468 : duration 0:00.003s
[2021-05-14 19:58:37,030] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256165975991771568525990_0004_m_000177_465: needsTaskCommit() Task attempt_202105142256165975991771568525990_0004_m_000177_465
[2021-05-14 19:58:37,031] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Task committer attempt_202105142256165975991771568525990_0004_m_000177_465: needsTaskCommit() Task attempt_202105142256165975991771568525990_0004_m_000177_465: duration 0:00.001s
21/05/14 22:58:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165975991771568525990_0004_m_000177_465
[2021-05-14 19:58:37,033] {docker.py:276} INFO - 21/05/14 22:58:37 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4587 bytes result sent to driver
[2021-05-14 19:58:37,034] {docker.py:276} INFO - 21/05/14 22:58:37 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (5c14c3e96bf4, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:37,035] {docker.py:276} INFO - 21/05/14 22:58:37 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
21/05/14 22:58:37 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2847 ms on 5c14c3e96bf4 (executor driver) (178/200)
[2021-05-14 19:58:37,052] {docker.py:276} INFO - 21/05/14 22:58:37 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:37,054] {docker.py:276} INFO - 21/05/14 22:58:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164568720227807814946_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164568720227807814946_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164568720227807814946_0004}; taskId=attempt_202105142256164568720227807814946_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38cde24c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:37,054] {docker.py:276} INFO - 21/05/14 22:58:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256164568720227807814946_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164568720227807814946_0004_m_000181_469
[2021-05-14 19:58:37,057] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Task committer attempt_202105142256164568720227807814946_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164568720227807814946_0004_m_000181_469 : duration 0:00.002s
[2021-05-14 19:58:37,890] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256162356731071185050177_0004_m_000179_467: needsTaskCommit() Task attempt_202105142256162356731071185050177_0004_m_000179_467
21/05/14 22:58:37 INFO StagingCommitter: Task committer attempt_202105142256162356731071185050177_0004_m_000179_467: needsTaskCommit() Task attempt_202105142256162356731071185050177_0004_m_000179_467: duration 0:00.001s
21/05/14 22:58:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162356731071185050177_0004_m_000179_467
[2021-05-14 19:58:37,891] {docker.py:276} INFO - 21/05/14 22:58:37 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4587 bytes result sent to driver
[2021-05-14 19:58:37,893] {docker.py:276} INFO - 21/05/14 22:58:37 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (5c14c3e96bf4, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:37,894] {docker.py:276} INFO - 21/05/14 22:58:37 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
21/05/14 22:58:37 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2194 ms on 5c14c3e96bf4 (executor driver) (179/200)
[2021-05-14 19:58:37,904] {docker.py:276} INFO - 21/05/14 22:58:37 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:37,905] {docker.py:276} INFO - 21/05/14 22:58:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:37,906] {docker.py:276} INFO - 21/05/14 22:58:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165778322650975589673_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165778322650975589673_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165778322650975589673_0004}; taskId=attempt_202105142256165778322650975589673_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6aa2cda7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:37,906] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Starting: Task committer attempt_202105142256165778322650975589673_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165778322650975589673_0004_m_000182_470
[2021-05-14 19:58:37,910] {docker.py:276} INFO - 21/05/14 22:58:37 INFO StagingCommitter: Task committer attempt_202105142256165778322650975589673_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165778322650975589673_0004_m_000182_470 : duration 0:00.004s
[2021-05-14 19:58:38,707] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Starting: Task committer attempt_202105142256168369885148211271921_0004_m_000178_466: needsTaskCommit() Task attempt_202105142256168369885148211271921_0004_m_000178_466
[2021-05-14 19:58:38,708] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Task committer attempt_202105142256168369885148211271921_0004_m_000178_466: needsTaskCommit() Task attempt_202105142256168369885148211271921_0004_m_000178_466: duration 0:00.000s
[2021-05-14 19:58:38,708] {docker.py:276} INFO - 21/05/14 22:58:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168369885148211271921_0004_m_000178_466
[2021-05-14 19:58:38,709] {docker.py:276} INFO - 21/05/14 22:58:38 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4587 bytes result sent to driver
[2021-05-14 19:58:38,710] {docker.py:276} INFO - 21/05/14 22:58:38 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (5c14c3e96bf4, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:38,712] {docker.py:276} INFO - 21/05/14 22:58:38 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 3018 ms on 5c14c3e96bf4 (executor driver) (180/200)
21/05/14 22:58:38 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-14 19:58:38,721] {docker.py:276} INFO - 21/05/14 22:58:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:38,723] {docker.py:276} INFO - 21/05/14 22:58:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:38,723] {docker.py:276} INFO - 21/05/14 22:58:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166088765743645910495_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166088765743645910495_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166088765743645910495_0004}; taskId=attempt_202105142256166088765743645910495_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6423f8d5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:38 INFO StagingCommitter: Starting: Task committer attempt_202105142256166088765743645910495_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166088765743645910495_0004_m_000183_471
[2021-05-14 19:58:38,726] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Task committer attempt_202105142256166088765743645910495_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166088765743645910495_0004_m_000183_471 : duration 0:00.002s
[2021-05-14 19:58:38,832] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Starting: Task committer attempt_20210514225616924308020158085667_0004_m_000180_468: needsTaskCommit() Task attempt_20210514225616924308020158085667_0004_m_000180_468
[2021-05-14 19:58:38,832] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Task committer attempt_20210514225616924308020158085667_0004_m_000180_468: needsTaskCommit() Task attempt_20210514225616924308020158085667_0004_m_000180_468: duration 0:00.001s
21/05/14 22:58:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616924308020158085667_0004_m_000180_468
[2021-05-14 19:58:38,834] {docker.py:276} INFO - 21/05/14 22:58:38 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4587 bytes result sent to driver
[2021-05-14 19:58:38,835] {docker.py:276} INFO - 21/05/14 22:58:38 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (5c14c3e96bf4, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:38,836] {docker.py:276} INFO - 21/05/14 22:58:38 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2948 ms on 5c14c3e96bf4 (executor driver) (181/200)
[2021-05-14 19:58:38,837] {docker.py:276} INFO - 21/05/14 22:58:38 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-14 19:58:38,846] {docker.py:276} INFO - 21/05/14 22:58:38 INFO ShuffleBlockFetcherIterator: Getting 5 (44.1 KiB) non-empty blocks including 5 (44.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:38,848] {docker.py:276} INFO - 21/05/14 22:58:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161693721357051912563_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161693721357051912563_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161693721357051912563_0004}; taskId=attempt_202105142256161693721357051912563_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c371ba8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:38,848] {docker.py:276} INFO - 21/05/14 22:58:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:38 INFO StagingCommitter: Starting: Task committer attempt_202105142256161693721357051912563_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161693721357051912563_0004_m_000184_472
[2021-05-14 19:58:38,851] {docker.py:276} INFO - 21/05/14 22:58:38 INFO StagingCommitter: Task committer attempt_202105142256161693721357051912563_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161693721357051912563_0004_m_000184_472 : duration 0:00.003s
[2021-05-14 19:58:39,788] {docker.py:276} INFO - 21/05/14 22:58:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256164568720227807814946_0004_m_000181_469: needsTaskCommit() Task attempt_202105142256164568720227807814946_0004_m_000181_469
[2021-05-14 19:58:39,789] {docker.py:276} INFO - 21/05/14 22:58:39 INFO StagingCommitter: Task committer attempt_202105142256164568720227807814946_0004_m_000181_469: needsTaskCommit() Task attempt_202105142256164568720227807814946_0004_m_000181_469: duration 0:00.001s
21/05/14 22:58:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164568720227807814946_0004_m_000181_469
[2021-05-14 19:58:39,790] {docker.py:276} INFO - 21/05/14 22:58:39 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4587 bytes result sent to driver
[2021-05-14 19:58:39,791] {docker.py:276} INFO - 21/05/14 22:58:39 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (5c14c3e96bf4, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:39,792] {docker.py:276} INFO - 21/05/14 22:58:39 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-14 19:58:39,793] {docker.py:276} INFO - 21/05/14 22:58:39 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2763 ms on 5c14c3e96bf4 (executor driver) (182/200)
[2021-05-14 19:58:39,803] {docker.py:276} INFO - 21/05/14 22:58:39 INFO ShuffleBlockFetcherIterator: Getting 5 (41.4 KiB) non-empty blocks including 5 (41.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:39,805] {docker.py:276} INFO - 21/05/14 22:58:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:58:39,805] {docker.py:276} INFO - 21/05/14 22:58:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:39,806] {docker.py:276} INFO - 21/05/14 22:58:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:39,806] {docker.py:276} INFO - 21/05/14 22:58:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256162635241519343967528_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162635241519343967528_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256162635241519343967528_0004}; taskId=attempt_202105142256162635241519343967528_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12c0944e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:39 INFO StagingCommitter: Starting: Task committer attempt_202105142256162635241519343967528_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162635241519343967528_0004_m_000185_473
[2021-05-14 19:58:39,809] {docker.py:276} INFO - 21/05/14 22:58:39 INFO StagingCommitter: Task committer attempt_202105142256162635241519343967528_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256162635241519343967528_0004_m_000185_473 : duration 0:00.003s
[2021-05-14 19:58:40,130] {docker.py:276} INFO - 21/05/14 22:58:40 INFO StagingCommitter: Starting: Task committer attempt_202105142256165778322650975589673_0004_m_000182_470: needsTaskCommit() Task attempt_202105142256165778322650975589673_0004_m_000182_470
[2021-05-14 19:58:40,131] {docker.py:276} INFO - 21/05/14 22:58:40 INFO StagingCommitter: Task committer attempt_202105142256165778322650975589673_0004_m_000182_470: needsTaskCommit() Task attempt_202105142256165778322650975589673_0004_m_000182_470: duration 0:00.001s
21/05/14 22:58:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165778322650975589673_0004_m_000182_470
[2021-05-14 19:58:40,133] {docker.py:276} INFO - 21/05/14 22:58:40 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4587 bytes result sent to driver
[2021-05-14 19:58:40,134] {docker.py:276} INFO - 21/05/14 22:58:40 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (5c14c3e96bf4, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:40,135] {docker.py:276} INFO - 21/05/14 22:58:40 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
[2021-05-14 19:58:40,136] {docker.py:276} INFO - 21/05/14 22:58:40 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2246 ms on 5c14c3e96bf4 (executor driver) (183/200)
[2021-05-14 19:58:40,145] {docker.py:276} INFO - 21/05/14 22:58:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:40,147] {docker.py:276} INFO - 21/05/14 22:58:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:40,147] {docker.py:276} INFO - 21/05/14 22:58:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616972054735060334499_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616972054735060334499_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616972054735060334499_0004}; taskId=attempt_20210514225616972054735060334499_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f0e01df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:40,148] {docker.py:276} INFO - 21/05/14 22:58:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:40 INFO StagingCommitter: Starting: Task committer attempt_20210514225616972054735060334499_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616972054735060334499_0004_m_000186_474
[2021-05-14 19:58:40,151] {docker.py:276} INFO - 21/05/14 22:58:40 INFO StagingCommitter: Task committer attempt_20210514225616972054735060334499_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616972054735060334499_0004_m_000186_474 : duration 0:00.004s
[2021-05-14 19:58:41,227] {docker.py:276} INFO - 21/05/14 22:58:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161693721357051912563_0004_m_000184_472: needsTaskCommit() Task attempt_202105142256161693721357051912563_0004_m_000184_472
[2021-05-14 19:58:41,227] {docker.py:276} INFO - 21/05/14 22:58:41 INFO StagingCommitter: Task committer attempt_202105142256161693721357051912563_0004_m_000184_472: needsTaskCommit() Task attempt_202105142256161693721357051912563_0004_m_000184_472: duration 0:00.000s
21/05/14 22:58:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161693721357051912563_0004_m_000184_472
[2021-05-14 19:58:41,229] {docker.py:276} INFO - 21/05/14 22:58:41 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4587 bytes result sent to driver
[2021-05-14 19:58:41,229] {docker.py:276} INFO - 21/05/14 22:58:41 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (5c14c3e96bf4, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:41,230] {docker.py:276} INFO - 21/05/14 22:58:41 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
[2021-05-14 19:58:41,231] {docker.py:276} INFO - 21/05/14 22:58:41 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 2399 ms on 5c14c3e96bf4 (executor driver) (184/200)
[2021-05-14 19:58:41,238] {docker.py:276} INFO - 21/05/14 22:58:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:41,240] {docker.py:276} INFO - 21/05/14 22:58:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:41,241] {docker.py:276} INFO - 21/05/14 22:58:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161810741083739900123_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161810741083739900123_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161810741083739900123_0004}; taskId=attempt_202105142256161810741083739900123_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f1f2975}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161810741083739900123_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161810741083739900123_0004_m_000187_475
[2021-05-14 19:58:41,243] {docker.py:276} INFO - 21/05/14 22:58:41 INFO StagingCommitter: Task committer attempt_202105142256161810741083739900123_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161810741083739900123_0004_m_000187_475 : duration 0:00.002s
[2021-05-14 19:58:41,409] {docker.py:276} INFO - 21/05/14 22:58:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256166088765743645910495_0004_m_000183_471: needsTaskCommit() Task attempt_202105142256166088765743645910495_0004_m_000183_471
21/05/14 22:58:41 INFO StagingCommitter: Task committer attempt_202105142256166088765743645910495_0004_m_000183_471: needsTaskCommit() Task attempt_202105142256166088765743645910495_0004_m_000183_471: duration 0:00.000s
21/05/14 22:58:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166088765743645910495_0004_m_000183_471
[2021-05-14 19:58:41,411] {docker.py:276} INFO - 21/05/14 22:58:41 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4587 bytes result sent to driver
[2021-05-14 19:58:41,412] {docker.py:276} INFO - 21/05/14 22:58:41 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (5c14c3e96bf4, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:41,414] {docker.py:276} INFO - 21/05/14 22:58:41 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-14 19:58:41,415] {docker.py:276} INFO - 21/05/14 22:58:41 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 2708 ms on 5c14c3e96bf4 (executor driver) (185/200)
[2021-05-14 19:58:41,423] {docker.py:276} INFO - 21/05/14 22:58:41 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:41,424] {docker.py:276} INFO - 21/05/14 22:58:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:41,425] {docker.py:276} INFO - 21/05/14 22:58:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:41,426] {docker.py:276} INFO - 21/05/14 22:58:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161141352466022127275_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161141352466022127275_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161141352466022127275_0004}; taskId=attempt_202105142256161141352466022127275_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16c0bdc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:41 INFO StagingCommitter: Starting: Task committer attempt_202105142256161141352466022127275_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161141352466022127275_0004_m_000188_476
[2021-05-14 19:58:41,437] {docker.py:276} INFO - 21/05/14 22:58:41 INFO StagingCommitter: Task committer attempt_202105142256161141352466022127275_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161141352466022127275_0004_m_000188_476 : duration 0:00.011s
[2021-05-14 19:58:42,654] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256162635241519343967528_0004_m_000185_473: needsTaskCommit() Task attempt_202105142256162635241519343967528_0004_m_000185_473
[2021-05-14 19:58:42,655] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Task committer attempt_202105142256162635241519343967528_0004_m_000185_473: needsTaskCommit() Task attempt_202105142256162635241519343967528_0004_m_000185_473: duration 0:00.001s
21/05/14 22:58:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256162635241519343967528_0004_m_000185_473
[2021-05-14 19:58:42,658] {docker.py:276} INFO - 21/05/14 22:58:42 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4587 bytes result sent to driver
[2021-05-14 19:58:42,659] {docker.py:276} INFO - 21/05/14 22:58:42 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (5c14c3e96bf4, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:42,660] {docker.py:276} INFO - 21/05/14 22:58:42 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 2873 ms on 5c14c3e96bf4 (executor driver) (186/200)
21/05/14 22:58:42 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-14 19:58:42,670] {docker.py:276} INFO - 21/05/14 22:58:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:42,672] {docker.py:276} INFO - 21/05/14 22:58:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256166924672665555843963_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166924672665555843963_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256166924672665555843963_0004}; taskId=attempt_202105142256166924672665555843963_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2de38de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:42,672] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256166924672665555843963_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166924672665555843963_0004_m_000189_477
[2021-05-14 19:58:42,675] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Task committer attempt_202105142256166924672665555843963_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256166924672665555843963_0004_m_000189_477 : duration 0:00.003s
[2021-05-14 19:58:42,921] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Starting: Task committer attempt_20210514225616972054735060334499_0004_m_000186_474: needsTaskCommit() Task attempt_20210514225616972054735060334499_0004_m_000186_474
[2021-05-14 19:58:42,922] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Task committer attempt_20210514225616972054735060334499_0004_m_000186_474: needsTaskCommit() Task attempt_20210514225616972054735060334499_0004_m_000186_474: duration 0:00.001s
21/05/14 22:58:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616972054735060334499_0004_m_000186_474
[2021-05-14 19:58:42,924] {docker.py:276} INFO - 21/05/14 22:58:42 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4587 bytes result sent to driver
[2021-05-14 19:58:42,926] {docker.py:276} INFO - 21/05/14 22:58:42 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (5c14c3e96bf4, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:42,927] {docker.py:276} INFO - 21/05/14 22:58:42 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-14 19:58:42,928] {docker.py:276} INFO - 21/05/14 22:58:42 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 2796 ms on 5c14c3e96bf4 (executor driver) (187/200)
[2021-05-14 19:58:42,936] {docker.py:276} INFO - 21/05/14 22:58:42 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:42,938] {docker.py:276} INFO - 21/05/14 22:58:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:42,938] {docker.py:276} INFO - 21/05/14 22:58:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161711230798086210201_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161711230798086210201_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161711230798086210201_0004}; taskId=attempt_202105142256161711230798086210201_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@cea5fdb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:42 INFO StagingCommitter: Starting: Task committer attempt_202105142256161711230798086210201_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161711230798086210201_0004_m_000190_478
[2021-05-14 19:58:42,942] {docker.py:276} INFO - 21/05/14 22:58:42 INFO StagingCommitter: Task committer attempt_202105142256161711230798086210201_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161711230798086210201_0004_m_000190_478 : duration 0:00.003s
[2021-05-14 19:58:44,002] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256161810741083739900123_0004_m_000187_475: needsTaskCommit() Task attempt_202105142256161810741083739900123_0004_m_000187_475
[2021-05-14 19:58:44,004] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Task committer attempt_202105142256161810741083739900123_0004_m_000187_475: needsTaskCommit() Task attempt_202105142256161810741083739900123_0004_m_000187_475: duration 0:00.001s
21/05/14 22:58:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161810741083739900123_0004_m_000187_475
[2021-05-14 19:58:44,005] {docker.py:276} INFO - 21/05/14 22:58:44 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4587 bytes result sent to driver
[2021-05-14 19:58:44,006] {docker.py:276} INFO - 21/05/14 22:58:44 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (5c14c3e96bf4, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:44,007] {docker.py:276} INFO - 21/05/14 22:58:44 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-14 19:58:44,008] {docker.py:276} INFO - 21/05/14 22:58:44 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 2782 ms on 5c14c3e96bf4 (executor driver) (188/200)
[2021-05-14 19:58:44,017] {docker.py:276} INFO - 21/05/14 22:58:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:58:44,018] {docker.py:276} INFO - 21/05/14 22:58:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:44,020] {docker.py:276} INFO - 21/05/14 22:58:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:44,020] {docker.py:276} INFO - 21/05/14 22:58:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161864520219449668979_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161864520219449668979_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161864520219449668979_0004}; taskId=attempt_202105142256161864520219449668979_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@696c4809}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:44,020] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256161864520219449668979_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161864520219449668979_0004_m_000191_479
[2021-05-14 19:58:44,023] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Task committer attempt_202105142256161864520219449668979_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161864520219449668979_0004_m_000191_479 : duration 0:00.004s
[2021-05-14 19:58:44,076] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256161141352466022127275_0004_m_000188_476: needsTaskCommit() Task attempt_202105142256161141352466022127275_0004_m_000188_476
[2021-05-14 19:58:44,077] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Task committer attempt_202105142256161141352466022127275_0004_m_000188_476: needsTaskCommit() Task attempt_202105142256161141352466022127275_0004_m_000188_476: duration 0:00.001s
21/05/14 22:58:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161141352466022127275_0004_m_000188_476
[2021-05-14 19:58:44,078] {docker.py:276} INFO - 21/05/14 22:58:44 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4587 bytes result sent to driver
[2021-05-14 19:58:44,078] {docker.py:276} INFO - 21/05/14 22:58:44 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (5c14c3e96bf4, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:44,079] {docker.py:276} INFO - 21/05/14 22:58:44 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-14 19:58:44,080] {docker.py:276} INFO - 21/05/14 22:58:44 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 2672 ms on 5c14c3e96bf4 (executor driver) (189/200)
[2021-05-14 19:58:44,097] {docker.py:276} INFO - 21/05/14 22:58:44 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:44,099] {docker.py:276} INFO - 21/05/14 22:58:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168497890386149791222_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168497890386149791222_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168497890386149791222_0004}; taskId=attempt_202105142256168497890386149791222_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1336e4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:44,099] {docker.py:276} INFO - 21/05/14 22:58:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:44,100] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Starting: Task committer attempt_202105142256168497890386149791222_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168497890386149791222_0004_m_000192_480
[2021-05-14 19:58:44,102] {docker.py:276} INFO - 21/05/14 22:58:44 INFO StagingCommitter: Task committer attempt_202105142256168497890386149791222_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168497890386149791222_0004_m_000192_480 : duration 0:00.003s
[2021-05-14 19:58:45,533] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256166924672665555843963_0004_m_000189_477: needsTaskCommit() Task attempt_202105142256166924672665555843963_0004_m_000189_477
[2021-05-14 19:58:45,535] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Task committer attempt_202105142256166924672665555843963_0004_m_000189_477: needsTaskCommit() Task attempt_202105142256166924672665555843963_0004_m_000189_477: duration 0:00.002s
21/05/14 22:58:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256166924672665555843963_0004_m_000189_477
[2021-05-14 19:58:45,536] {docker.py:276} INFO - 21/05/14 22:58:45 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4587 bytes result sent to driver
[2021-05-14 19:58:45,537] {docker.py:276} INFO - 21/05/14 22:58:45 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (5c14c3e96bf4, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:45,539] {docker.py:276} INFO - 21/05/14 22:58:45 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-14 19:58:45,540] {docker.py:276} INFO - 21/05/14 22:58:45 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 2884 ms on 5c14c3e96bf4 (executor driver) (190/200)
[2021-05-14 19:58:45,549] {docker.py:276} INFO - 21/05/14 22:58:45 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:45,550] {docker.py:276} INFO - 21/05/14 22:58:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:45,551] {docker.py:276} INFO - 21/05/14 22:58:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:45,551] {docker.py:276} INFO - 21/05/14 22:58:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256165466528463958641853_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165466528463958641853_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256165466528463958641853_0004}; taskId=attempt_202105142256165466528463958641853_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54fef917}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256165466528463958641853_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165466528463958641853_0004_m_000193_481
[2021-05-14 19:58:45,554] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Task committer attempt_202105142256165466528463958641853_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256165466528463958641853_0004_m_000193_481 : duration 0:00.003s
[2021-05-14 19:58:45,623] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256161711230798086210201_0004_m_000190_478: needsTaskCommit() Task attempt_202105142256161711230798086210201_0004_m_000190_478
[2021-05-14 19:58:45,625] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Task committer attempt_202105142256161711230798086210201_0004_m_000190_478: needsTaskCommit() Task attempt_202105142256161711230798086210201_0004_m_000190_478: duration 0:00.001s
[2021-05-14 19:58:45,626] {docker.py:276} INFO - 21/05/14 22:58:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161711230798086210201_0004_m_000190_478
[2021-05-14 19:58:45,626] {docker.py:276} INFO - 21/05/14 22:58:45 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4587 bytes result sent to driver
[2021-05-14 19:58:45,627] {docker.py:276} INFO - 21/05/14 22:58:45 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (5c14c3e96bf4, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:45,628] {docker.py:276} INFO - 21/05/14 22:58:45 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-14 19:58:45,629] {docker.py:276} INFO - 21/05/14 22:58:45 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2706 ms on 5c14c3e96bf4 (executor driver) (191/200)
[2021-05-14 19:58:45,638] {docker.py:276} INFO - 21/05/14 22:58:45 INFO ShuffleBlockFetcherIterator: Getting 5 (45.1 KiB) non-empty blocks including 5 (45.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:45,640] {docker.py:276} INFO - 21/05/14 22:58:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256168976138483872406637_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168976138483872406637_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256168976138483872406637_0004}; taskId=attempt_202105142256168976138483872406637_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a8784bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:45,640] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Starting: Task committer attempt_202105142256168976138483872406637_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168976138483872406637_0004_m_000194_482
[2021-05-14 19:58:45,643] {docker.py:276} INFO - 21/05/14 22:58:45 INFO StagingCommitter: Task committer attempt_202105142256168976138483872406637_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256168976138483872406637_0004_m_000194_482 : duration 0:00.004s
[2021-05-14 19:58:46,246] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Starting: Task committer attempt_202105142256168497890386149791222_0004_m_000192_480: needsTaskCommit() Task attempt_202105142256168497890386149791222_0004_m_000192_480
[2021-05-14 19:58:46,246] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Task committer attempt_202105142256168497890386149791222_0004_m_000192_480: needsTaskCommit() Task attempt_202105142256168497890386149791222_0004_m_000192_480: duration 0:00.000s
21/05/14 22:58:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168497890386149791222_0004_m_000192_480
[2021-05-14 19:58:46,247] {docker.py:276} INFO - 21/05/14 22:58:46 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4587 bytes result sent to driver
[2021-05-14 19:58:46,248] {docker.py:276} INFO - 21/05/14 22:58:46 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (5c14c3e96bf4, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:46,248] {docker.py:276} INFO - 21/05/14 22:58:46 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-14 19:58:46,249] {docker.py:276} INFO - 21/05/14 22:58:46 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 2174 ms on 5c14c3e96bf4 (executor driver) (192/200)
[2021-05-14 19:58:46,263] {docker.py:276} INFO - 21/05/14 22:58:46 INFO ShuffleBlockFetcherIterator: Getting 5 (39.9 KiB) non-empty blocks including 5 (39.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:46,264] {docker.py:276} INFO - 21/05/14 22:58:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164196932976197606911_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164196932976197606911_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164196932976197606911_0004}; taskId=attempt_202105142256164196932976197606911_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1420c6d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:46,265] {docker.py:276} INFO - 21/05/14 22:58:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:46 INFO StagingCommitter: Starting: Task committer attempt_202105142256164196932976197606911_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164196932976197606911_0004_m_000195_483
[2021-05-14 19:58:46,268] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Task committer attempt_202105142256164196932976197606911_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164196932976197606911_0004_m_000195_483 : duration 0:00.003s
[2021-05-14 19:58:46,749] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Starting: Task committer attempt_202105142256161864520219449668979_0004_m_000191_479: needsTaskCommit() Task attempt_202105142256161864520219449668979_0004_m_000191_479
[2021-05-14 19:58:46,750] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Task committer attempt_202105142256161864520219449668979_0004_m_000191_479: needsTaskCommit() Task attempt_202105142256161864520219449668979_0004_m_000191_479: duration 0:00.001s
21/05/14 22:58:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161864520219449668979_0004_m_000191_479
[2021-05-14 19:58:46,751] {docker.py:276} INFO - 21/05/14 22:58:46 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4587 bytes result sent to driver
[2021-05-14 19:58:46,752] {docker.py:276} INFO - 21/05/14 22:58:46 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (5c14c3e96bf4, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:46,753] {docker.py:276} INFO - 21/05/14 22:58:46 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-14 19:58:46,754] {docker.py:276} INFO - 21/05/14 22:58:46 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2751 ms on 5c14c3e96bf4 (executor driver) (193/200)
[2021-05-14 19:58:46,763] {docker.py:276} INFO - 21/05/14 22:58:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:46,764] {docker.py:276} INFO - 21/05/14 22:58:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:46,765] {docker.py:276} INFO - 21/05/14 22:58:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514225616335541707467481330_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616335541707467481330_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514225616335541707467481330_0004}; taskId=attempt_20210514225616335541707467481330_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14ad7dbb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:46 INFO StagingCommitter: Starting: Task committer attempt_20210514225616335541707467481330_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616335541707467481330_0004_m_000196_484
[2021-05-14 19:58:46,767] {docker.py:276} INFO - 21/05/14 22:58:46 INFO StagingCommitter: Task committer attempt_20210514225616335541707467481330_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_20210514225616335541707467481330_0004_m_000196_484 : duration 0:00.003s
[2021-05-14 19:58:47,851] {docker.py:276} INFO - 21/05/14 22:58:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256164196932976197606911_0004_m_000195_483: needsTaskCommit() Task attempt_202105142256164196932976197606911_0004_m_000195_483
[2021-05-14 19:58:47,852] {docker.py:276} INFO - 21/05/14 22:58:47 INFO StagingCommitter: Task committer attempt_202105142256164196932976197606911_0004_m_000195_483: needsTaskCommit() Task attempt_202105142256164196932976197606911_0004_m_000195_483: duration 0:00.000s
21/05/14 22:58:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164196932976197606911_0004_m_000195_483
[2021-05-14 19:58:47,854] {docker.py:276} INFO - 21/05/14 22:58:47 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4587 bytes result sent to driver
[2021-05-14 19:58:47,856] {docker.py:276} INFO - 21/05/14 22:58:47 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (5c14c3e96bf4, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:47,857] {docker.py:276} INFO - 21/05/14 22:58:47 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-14 19:58:47,858] {docker.py:276} INFO - 21/05/14 22:58:47 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 1610 ms on 5c14c3e96bf4 (executor driver) (194/200)
[2021-05-14 19:58:47,866] {docker.py:276} INFO - 21/05/14 22:58:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:47,868] {docker.py:276} INFO - 21/05/14 22:58:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:58:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256161475147128283266570_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161475147128283266570_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256161475147128283266570_0004}; taskId=attempt_202105142256161475147128283266570_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@184f7264}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:58:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:58:47 INFO StagingCommitter: Starting: Task committer attempt_202105142256161475147128283266570_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161475147128283266570_0004_m_000197_485
[2021-05-14 19:58:47,871] {docker.py:276} INFO - 21/05/14 22:58:47 INFO StagingCommitter: Task committer attempt_202105142256161475147128283266570_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256161475147128283266570_0004_m_000197_485 : duration 0:00.003s
[2021-05-14 19:58:48,357] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256168976138483872406637_0004_m_000194_482: needsTaskCommit() Task attempt_202105142256168976138483872406637_0004_m_000194_482
[2021-05-14 19:58:48,358] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Task committer attempt_202105142256168976138483872406637_0004_m_000194_482: needsTaskCommit() Task attempt_202105142256168976138483872406637_0004_m_000194_482: duration 0:00.001s
21/05/14 22:58:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256168976138483872406637_0004_m_000194_482
[2021-05-14 19:58:48,359] {docker.py:276} INFO - 21/05/14 22:58:48 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4587 bytes result sent to driver
[2021-05-14 19:58:48,360] {docker.py:276} INFO - 21/05/14 22:58:48 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (5c14c3e96bf4, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:48,361] {docker.py:276} INFO - 21/05/14 22:58:48 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-14 19:58:48,361] {docker.py:276} INFO - 21/05/14 22:58:48 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 2737 ms on 5c14c3e96bf4 (executor driver) (195/200)
[2021-05-14 19:58:48,371] {docker.py:276} INFO - 21/05/14 22:58:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:48,373] {docker.py:276} INFO - 21/05/14 22:58:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:58:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:48,373] {docker.py:276} INFO - 21/05/14 22:58:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256164706147556340202640_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164706147556340202640_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256164706147556340202640_0004}; taskId=attempt_202105142256164706147556340202640_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@103a054b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:48,374] {docker.py:276} INFO - 21/05/14 22:58:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:48,374] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256164706147556340202640_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164706147556340202640_0004_m_000198_486
[2021-05-14 19:58:48,376] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Task committer attempt_202105142256164706147556340202640_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256164706147556340202640_0004_m_000198_486 : duration 0:00.003s
[2021-05-14 19:58:48,920] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256165466528463958641853_0004_m_000193_481: needsTaskCommit() Task attempt_202105142256165466528463958641853_0004_m_000193_481
[2021-05-14 19:58:48,922] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Task committer attempt_202105142256165466528463958641853_0004_m_000193_481: needsTaskCommit() Task attempt_202105142256165466528463958641853_0004_m_000193_481: duration 0:00.001s
21/05/14 22:58:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256165466528463958641853_0004_m_000193_481
[2021-05-14 19:58:48,923] {docker.py:276} INFO - 21/05/14 22:58:48 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4587 bytes result sent to driver
[2021-05-14 19:58:48,924] {docker.py:276} INFO - 21/05/14 22:58:48 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (5c14c3e96bf4, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:58:48,925] {docker.py:276} INFO - 21/05/14 22:58:48 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-14 19:58:48,926] {docker.py:276} INFO - 21/05/14 22:58:48 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 3357 ms on 5c14c3e96bf4 (executor driver) (196/200)
[2021-05-14 19:58:48,935] {docker.py:276} INFO - 21/05/14 22:58:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:58:48,937] {docker.py:276} INFO - 21/05/14 22:58:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:58:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:58:48,937] {docker.py:276} INFO - 21/05/14 22:58:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:48,938] {docker.py:276} INFO - 21/05/14 22:58:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142256167450992978817038183_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167450992978817038183_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142256167450992978817038183_0004}; taskId=attempt_202105142256167450992978817038183_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67867544}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5d845ff-e305-4ec2-91c9-6b71a7b37418/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:48,938] {docker.py:276} INFO - 21/05/14 22:58:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:58:48,938] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Starting: Task committer attempt_202105142256167450992978817038183_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167450992978817038183_0004_m_000199_487
[2021-05-14 19:58:48,941] {docker.py:276} INFO - 21/05/14 22:58:48 INFO StagingCommitter: Task committer attempt_202105142256167450992978817038183_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5d845ff-e305-4ec2-91c9-6b71a7b37418/_temporary/0/_temporary/attempt_202105142256167450992978817038183_0004_m_000199_487 : duration 0:00.003s
[2021-05-14 19:58:49,709] {docker.py:276} INFO - 21/05/14 22:58:49 INFO StagingCommitter: Starting: Task committer attempt_20210514225616335541707467481330_0004_m_000196_484: needsTaskCommit() Task attempt_20210514225616335541707467481330_0004_m_000196_484
[2021-05-14 19:58:49,710] {docker.py:276} INFO - 21/05/14 22:58:49 INFO StagingCommitter: Task committer attempt_20210514225616335541707467481330_0004_m_000196_484: needsTaskCommit() Task attempt_20210514225616335541707467481330_0004_m_000196_484: duration 0:00.000s
21/05/14 22:58:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514225616335541707467481330_0004_m_000196_484
[2021-05-14 19:58:49,712] {docker.py:276} INFO - 21/05/14 22:58:49 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4630 bytes result sent to driver
[2021-05-14 19:58:49,714] {docker.py:276} INFO - 21/05/14 22:58:49 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 2931 ms on 5c14c3e96bf4 (executor driver) (197/200)
[2021-05-14 19:58:49,858] {docker.py:276} INFO - 21/05/14 22:58:49 INFO StagingCommitter: Starting: Task committer attempt_202105142256161475147128283266570_0004_m_000197_485: needsTaskCommit() Task attempt_202105142256161475147128283266570_0004_m_000197_485
[2021-05-14 19:58:49,859] {docker.py:276} INFO - 21/05/14 22:58:49 INFO StagingCommitter: Task committer attempt_202105142256161475147128283266570_0004_m_000197_485: needsTaskCommit() Task attempt_202105142256161475147128283266570_0004_m_000197_485: duration 0:00.001s
21/05/14 22:58:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256161475147128283266570_0004_m_000197_485
[2021-05-14 19:58:49,859] {docker.py:276} INFO - 21/05/14 22:58:49 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4587 bytes result sent to driver
[2021-05-14 19:58:49,861] {docker.py:276} INFO - 21/05/14 22:58:49 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 1974 ms on 5c14c3e96bf4 (executor driver) (198/200)
[2021-05-14 19:58:50,519] {docker.py:276} INFO - 21/05/14 22:58:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256164706147556340202640_0004_m_000198_486: needsTaskCommit() Task attempt_202105142256164706147556340202640_0004_m_000198_486
[2021-05-14 19:58:50,521] {docker.py:276} INFO - 21/05/14 22:58:50 INFO StagingCommitter: Task committer attempt_202105142256164706147556340202640_0004_m_000198_486: needsTaskCommit() Task attempt_202105142256164706147556340202640_0004_m_000198_486: duration 0:00.002s
[2021-05-14 19:58:50,521] {docker.py:276} INFO - 21/05/14 22:58:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256164706147556340202640_0004_m_000198_486
[2021-05-14 19:58:50,524] {docker.py:276} INFO - 21/05/14 22:58:50 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4587 bytes result sent to driver
[2021-05-14 19:58:50,525] {docker.py:276} INFO - 21/05/14 22:58:50 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2134 ms on 5c14c3e96bf4 (executor driver) (199/200)
[2021-05-14 19:58:50,812] {docker.py:276} INFO - 21/05/14 22:58:50 INFO StagingCommitter: Starting: Task committer attempt_202105142256167450992978817038183_0004_m_000199_487: needsTaskCommit() Task attempt_202105142256167450992978817038183_0004_m_000199_487
[2021-05-14 19:58:50,813] {docker.py:276} INFO - 21/05/14 22:58:50 INFO StagingCommitter: Task committer attempt_202105142256167450992978817038183_0004_m_000199_487: needsTaskCommit() Task attempt_202105142256167450992978817038183_0004_m_000199_487: duration 0:00.002s
21/05/14 22:58:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142256167450992978817038183_0004_m_000199_487
[2021-05-14 19:58:50,816] {docker.py:276} INFO - 21/05/14 22:58:50 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4544 bytes result sent to driver
[2021-05-14 19:58:50,817] {docker.py:276} INFO - 21/05/14 22:58:50 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 1895 ms on 5c14c3e96bf4 (executor driver) (200/200)
21/05/14 22:58:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-14 19:58:50,818] {docker.py:276} INFO - 21/05/14 22:58:50 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 136.885 s
[2021-05-14 19:58:50,819] {docker.py:276} INFO - 21/05/14 22:58:50 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 22:58:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-14 19:58:50,819] {docker.py:276} INFO - 21/05/14 22:58:50 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 154.481138 s
[2021-05-14 19:58:50,822] {docker.py:276} INFO - 21/05/14 22:58:50 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105142256167691469056325385884_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 19:58:50,838] {docker.py:276} INFO - 21/05/14 22:58:50 WARN AbstractS3ACommitter: Task committer attempt_202105142256167691469056325385884_0000_m_000000_0: No pending uploads to commit
[2021-05-14 19:58:51,359] {docker.py:276} INFO - 21/05/14 22:58:51 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 22:58:51 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 19:58:51,542] {docker.py:276} INFO - 21/05/14 22:58:51 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.183s
21/05/14 22:58:51 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.184s
[2021-05-14 19:58:51,544] {docker.py:276} INFO - 21/05/14 22:58:51 INFO AbstractS3ACommitter: Task committer attempt_202105142256167691469056325385884_0000_m_000000_0: commitJob((no job ID)): duration 0:00.723s
[2021-05-14 19:58:52,073] {docker.py:276} INFO - 21/05/14 22:58:52 INFO FileFormatWriter: Write Job a5d845ff-e305-4ec2-91c9-6b71a7b37418 committed.
[2021-05-14 19:58:52,082] {docker.py:276} INFO - 21/05/14 22:58:52 INFO FileFormatWriter: Finished processing stats for write job a5d845ff-e305-4ec2-91c9-6b71a7b37418.
[2021-05-14 19:58:52,186] {docker.py:276} INFO - 21/05/14 22:58:52 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 19:58:52,201] {docker.py:276} INFO - 21/05/14 22:58:52 INFO SparkUI: Stopped Spark web UI at http://5c14c3e96bf4:4040
[2021-05-14 19:58:52,223] {docker.py:276} INFO - 21/05/14 22:58:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 19:58:52,241] {docker.py:276} INFO - 21/05/14 22:58:52 INFO MemoryStore: MemoryStore cleared
[2021-05-14 19:58:52,241] {docker.py:276} INFO - 21/05/14 22:58:52 INFO BlockManager: BlockManager stopped
[2021-05-14 19:58:52,246] {docker.py:276} INFO - 21/05/14 22:58:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 19:58:52,250] {docker.py:276} INFO - 21/05/14 22:58:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 19:58:52,258] {docker.py:276} INFO - 21/05/14 22:58:52 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 19:58:52,258] {docker.py:276} INFO - 21/05/14 22:58:52 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 19:58:52,261] {docker.py:276} INFO - 21/05/14 22:58:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1611170-c2a8-4b0a-87d6-6a8366348b46
[2021-05-14 19:58:52,267] {docker.py:276} INFO - 21/05/14 22:58:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1611170-c2a8-4b0a-87d6-6a8366348b46/pyspark-4b5d91f6-edcb-4f0d-9af5-925305f1608b
[2021-05-14 19:58:52,270] {docker.py:276} INFO - 21/05/14 22:58:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-10f3ad63-eff1-4607-affd-8b181dab6ad4
[2021-05-14 19:58:52,276] {docker.py:276} INFO - 21/05/14 22:58:52 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 19:58:52,276] {docker.py:276} INFO - 21/05/14 22:58:52 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-14 19:58:52,277] {docker.py:276} INFO - 21/05/14 22:58:52 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 19:58:52,535] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210514T225447, start_date=20210514T225538, end_date=20210514T225852
[2021-05-14 19:58:52,582] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-14 19:58:52,613] {local_task_job.py:146} INFO - Task exited with return code 0
