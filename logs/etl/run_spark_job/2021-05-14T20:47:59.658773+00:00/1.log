[2021-05-14 17:48:52,253] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T20:47:59.658773+00:00 [queued]>
[2021-05-14 17:48:52,259] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T20:47:59.658773+00:00 [queued]>
[2021-05-14 17:48:52,259] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 17:48:52,259] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 17:48:52,259] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 17:48:52,264] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T20:47:59.658773+00:00
[2021-05-14 17:48:52,267] {standard_task_runner.py:52} INFO - Started process 24397 to run task
[2021-05-14 17:48:52,272] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T20:47:59.658773+00:00', '--job-id', '512', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpiz_wtumf', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpagpufyii']
[2021-05-14 17:48:52,274] {standard_task_runner.py:77} INFO - Job 512: Subtask run_spark_job
[2021-05-14 17:48:52,302] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T20:47:59.658773+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2021-05-14 17:48:52,322] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T20:47:59.658773+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T20:47:59.658773+00:00
[2021-05-14 17:48:52,327] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 17:48:55,603] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 17:48:55,607] {docker.py:312} INFO - Digest: sha256:3ff139a9dbefaff7945c18f6cbdbc77460bc981cf21cbe1d495f99b227e826c8
[2021-05-14 17:48:55,607] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 17:48:55,611] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 17:48:57,813] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 17:48:58,395] {docker.py:276} INFO - 21/05/14 20:48:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 17:49:00,933] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 17:49:00,947] {docker.py:276} INFO - 21/05/14 20:49:00 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 17:49:01,012] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceUtils: ==============================================================
[2021-05-14 17:49:01,013] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 17:49:01,013] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceUtils: ==============================================================
[2021-05-14 17:49:01,014] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SparkContext: Submitted application: spark.py
[2021-05-14 17:49:01,056] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 17:49:01,072] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 17:49:01,073] {docker.py:276} INFO - 21/05/14 20:49:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 17:49:01,146] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 17:49:01,146] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 17:49:01,147] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 17:49:01,147] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 17:49:01,148] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 17:49:01,508] {docker.py:276} INFO - 21/05/14 20:49:01 INFO Utils: Successfully started service 'sparkDriver' on port 35179.
[2021-05-14 17:49:01,546] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 17:49:01,592] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 17:49:01,626] {docker.py:276} INFO - 21/05/14 20:49:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 17:49:01,628] {docker.py:276} INFO - 21/05/14 20:49:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 17:49:01,635] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 17:49:01,666] {docker.py:276} INFO - 21/05/14 20:49:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2b5df7bd-3de1-46f9-b502-54be7a53d3b7
[2021-05-14 17:49:01,697] {docker.py:276} INFO - 21/05/14 20:49:01 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 17:49:01,719] {docker.py:276} INFO - 21/05/14 20:49:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 17:49:02,025] {docker.py:276} INFO - 21/05/14 20:49:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 17:49:02,108] {docker.py:276} INFO - 21/05/14 20:49:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://7cd3ddbc35d2:4040
[2021-05-14 17:49:02,369] {docker.py:276} INFO - 21/05/14 20:49:02 INFO Executor: Starting executor ID driver on host 7cd3ddbc35d2
[2021-05-14 17:49:02,411] {docker.py:276} INFO - 21/05/14 20:49:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39701.
21/05/14 20:49:02 INFO NettyBlockTransferService: Server created on 7cd3ddbc35d2:39701
[2021-05-14 17:49:02,415] {docker.py:276} INFO - 21/05/14 20:49:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 17:49:02,427] {docker.py:276} INFO - 21/05/14 20:49:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7cd3ddbc35d2, 39701, None)
[2021-05-14 17:49:02,440] {docker.py:276} INFO - 21/05/14 20:49:02 INFO BlockManagerMasterEndpoint: Registering block manager 7cd3ddbc35d2:39701 with 934.4 MiB RAM, BlockManagerId(driver, 7cd3ddbc35d2, 39701, None)
[2021-05-14 17:49:02,444] {docker.py:276} INFO - 21/05/14 20:49:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7cd3ddbc35d2, 39701, None)
[2021-05-14 17:49:02,446] {docker.py:276} INFO - 21/05/14 20:49:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7cd3ddbc35d2, 39701, None)
[2021-05-14 17:49:03,000] {docker.py:276} INFO - 21/05/14 20:49:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 17:49:03,001] {docker.py:276} INFO - 21/05/14 20:49:03 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 17:49:04,125] {docker.py:276} INFO - 21/05/14 20:49:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 17:49:04,191] {docker.py:276} INFO - 21/05/14 20:49:04 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/14 20:49:04 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 17:49:11,229] {docker.py:276} INFO - 21/05/14 20:49:11 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620940695_to_1620942495.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620942495_to_1620944295.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620944295_to_1620946095.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620946095_to_1620947895.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620947895_to_1620949695.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620949695_to_1620951495.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620951495_to_1620953295.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620953295_to_1620955095.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620955095_to_1620956895.csv.
[2021-05-14 17:49:11,813] {docker.py:276} INFO - 21/05/14 20:49:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:11,838] {docker.py:276} INFO - 21/05/14 20:49:11 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-14 17:49:11,840] {docker.py:276} INFO - 21/05/14 20:49:11 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 17:49:11,841] {docker.py:276} INFO - 21/05/14 20:49:11 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 17:49:11,844] {docker.py:276} INFO - 21/05/14 20:49:11 INFO DAGScheduler: Missing parents: List()
[2021-05-14 17:49:11,853] {docker.py:276} INFO - 21/05/14 20:49:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 17:49:11,967] {docker.py:276} INFO - 21/05/14 20:49:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-14 17:49:12,040] {docker.py:276} INFO - 21/05/14 20:49:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-14 17:49:12,044] {docker.py:276} INFO - 21/05/14 20:49:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7cd3ddbc35d2:39701 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 17:49:12,049] {docker.py:276} INFO - 21/05/14 20:49:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-14 17:49:12,087] {docker.py:276} INFO - 21/05/14 20:49:12 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 17:49:12,088] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-14 17:49:12,189] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (7cd3ddbc35d2, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:12,193] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (7cd3ddbc35d2, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:12,195] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (7cd3ddbc35d2, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:12,197] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (7cd3ddbc35d2, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:12,223] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 17:49:12,224] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-14 17:49:12,224] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-14 17:49:12,225] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-14 17:49:12,810] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-14 17:49:12,818] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (7cd3ddbc35d2, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:12,821] {docker.py:276} INFO - 21/05/14 20:49:12 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-14 17:49:12,831] {docker.py:276} INFO - 21/05/14 20:49:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 636 ms on 7cd3ddbc35d2 (executor driver) (1/141)
[2021-05-14 17:49:13,014] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1886 bytes result sent to driver
[2021-05-14 17:49:13,018] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (7cd3ddbc35d2, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,019] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-14 17:49:13,020] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 204 ms on 7cd3ddbc35d2 (executor driver) (2/141)
[2021-05-14 17:49:13,414] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-14 17:49:13,417] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (7cd3ddbc35d2, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,419] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-14 17:49:13,420] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 404 ms on 7cd3ddbc35d2 (executor driver) (3/141)
[2021-05-14 17:49:13,437] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-14 17:49:13,440] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-14 17:49:13,443] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (7cd3ddbc35d2, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,444] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-14 17:49:13,444] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (7cd3ddbc35d2, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,445] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1251 ms on 7cd3ddbc35d2 (executor driver) (4/141)
[2021-05-14 17:49:13,448] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-14 17:49:13,449] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-14 17:49:13,451] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (7cd3ddbc35d2, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,453] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1302 ms on 7cd3ddbc35d2 (executor driver) (5/141)
[2021-05-14 17:49:13,454] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1259 ms on 7cd3ddbc35d2 (executor driver) (6/141)
[2021-05-14 17:49:13,466] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-14 17:49:13,786] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-14 17:49:13,789] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
21/05/14 20:49:13 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (7cd3ddbc35d2, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,790] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-14 17:49:13,791] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-14 17:49:13,792] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (7cd3ddbc35d2, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,795] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 378 ms on 7cd3ddbc35d2 (executor driver) (7/141)
[2021-05-14 17:49:13,797] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 343 ms on 7cd3ddbc35d2 (executor driver) (8/141)
[2021-05-14 17:49:13,798] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-14 17:49:13,798] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 353 ms on 7cd3ddbc35d2 (executor driver) (9/141)
[2021-05-14 17:49:13,799] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (7cd3ddbc35d2, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,801] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-14 17:49:13,809] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-14 17:49:13,811] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (7cd3ddbc35d2, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,813] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 372 ms on 7cd3ddbc35d2 (executor driver) (10/141)
[2021-05-14 17:49:13,814] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-14 17:49:13,982] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-14 17:49:13,983] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-14 17:49:13,984] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (7cd3ddbc35d2, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,986] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-14 17:49:13,987] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-14 17:49:13,988] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 195 ms on 7cd3ddbc35d2 (executor driver) (11/141)
[2021-05-14 17:49:13,990] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (7cd3ddbc35d2, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,995] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-14 17:49:13,995] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (7cd3ddbc35d2, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:13,996] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-14 17:49:13,996] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 197 ms on 7cd3ddbc35d2 (executor driver) (12/141)
[2021-05-14 17:49:13,996] {docker.py:276} INFO - 21/05/14 20:49:13 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 210 ms on 7cd3ddbc35d2 (executor driver) (13/141)
[2021-05-14 17:49:14,004] {docker.py:276} INFO - 21/05/14 20:49:13 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-14 17:49:14,007] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (7cd3ddbc35d2, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,008] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-14 17:49:14,008] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 197 ms on 7cd3ddbc35d2 (executor driver) (14/141)
[2021-05-14 17:49:14,316] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-14 17:49:14,317] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1886 bytes result sent to driver
[2021-05-14 17:49:14,317] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (7cd3ddbc35d2, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,318] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1886 bytes result sent to driver
[2021-05-14 17:49:14,320] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-14 17:49:14,322] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 332 ms on 7cd3ddbc35d2 (executor driver) (15/141)
[2021-05-14 17:49:14,324] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (7cd3ddbc35d2, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,326] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (7cd3ddbc35d2, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,327] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-14 17:49:14,328] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 335 ms on 7cd3ddbc35d2 (executor driver) (16/141)
[2021-05-14 17:49:14,329] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-14 17:49:14,329] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 346 ms on 7cd3ddbc35d2 (executor driver) (17/141)
[2021-05-14 17:49:14,333] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1886 bytes result sent to driver
[2021-05-14 17:49:14,335] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (7cd3ddbc35d2, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,336] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 331 ms on 7cd3ddbc35d2 (executor driver) (18/141)
[2021-05-14 17:49:14,338] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-14 17:49:14,500] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
21/05/14 20:49:14 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-14 17:49:14,504] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (7cd3ddbc35d2, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,505] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 188 ms on 7cd3ddbc35d2 (executor driver) (19/141)
[2021-05-14 17:49:14,506] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-14 17:49:14,510] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (7cd3ddbc35d2, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,511] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-14 17:49:14,513] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 188 ms on 7cd3ddbc35d2 (executor driver) (20/141)
[2021-05-14 17:49:14,514] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-14 17:49:14,515] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (7cd3ddbc35d2, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,516] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 194 ms on 7cd3ddbc35d2 (executor driver) (21/141)
[2021-05-14 17:49:14,518] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-14 17:49:14,520] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-14 17:49:14,521] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (7cd3ddbc35d2, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,523] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 188 ms on 7cd3ddbc35d2 (executor driver) (22/141)
[2021-05-14 17:49:14,524] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-14 17:49:14,829] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-14 17:49:14,837] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (7cd3ddbc35d2, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,838] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-14 17:49:14,838] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-14 17:49:14,839] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-14 17:49:14,840] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (7cd3ddbc35d2, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,840] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-14 17:49:14,841] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (7cd3ddbc35d2, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,841] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 325 ms on 7cd3ddbc35d2 (executor driver) (23/141)
[2021-05-14 17:49:14,842] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 330 ms on 7cd3ddbc35d2 (executor driver) (24/141)
[2021-05-14 17:49:14,842] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 341 ms on 7cd3ddbc35d2 (executor driver) (25/141)
[2021-05-14 17:49:14,846] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-14 17:49:14,847] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-14 17:49:14,847] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (7cd3ddbc35d2, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:14,849] {docker.py:276} INFO - 21/05/14 20:49:14 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-14 17:49:14,849] {docker.py:276} INFO - 21/05/14 20:49:14 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 328 ms on 7cd3ddbc35d2 (executor driver) (26/141)
[2021-05-14 17:49:15,015] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
21/05/14 20:49:15 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-14 17:49:15,015] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (7cd3ddbc35d2, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,016] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 185 ms on 7cd3ddbc35d2 (executor driver) (27/141)
[2021-05-14 17:49:15,017] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-14 17:49:15,019] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 181 ms on 7cd3ddbc35d2 (executor driver) (28/141)
21/05/14 20:49:15 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-14 17:49:15,020] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (7cd3ddbc35d2, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,050] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
21/05/14 20:49:15 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-14 17:49:15,054] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (7cd3ddbc35d2, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,055] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 208 ms on 7cd3ddbc35d2 (executor driver) (29/141)
[2021-05-14 17:49:15,059] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-14 17:49:15,060] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 226 ms on 7cd3ddbc35d2 (executor driver) (30/141)
[2021-05-14 17:49:15,061] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (7cd3ddbc35d2, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,064] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-14 17:49:15,355] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-14 17:49:15,356] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-14 17:49:15,357] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-14 17:49:15,358] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (7cd3ddbc35d2, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,359] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 340 ms on 7cd3ddbc35d2 (executor driver) (31/141)
[2021-05-14 17:49:15,360] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-14 17:49:15,361] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1886 bytes result sent to driver
[2021-05-14 17:49:15,361] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 308 ms on 7cd3ddbc35d2 (executor driver) (32/141)
[2021-05-14 17:49:15,362] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (7cd3ddbc35d2, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,364] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-14 17:49:15,367] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (7cd3ddbc35d2, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,368] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-14 17:49:15,369] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 358 ms on 7cd3ddbc35d2 (executor driver) (33/141)
[2021-05-14 17:49:15,370] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 308 ms on 7cd3ddbc35d2 (executor driver) (34/141)
[2021-05-14 17:49:15,371] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (7cd3ddbc35d2, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,373] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-14 17:49:15,541] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-14 17:49:15,544] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (7cd3ddbc35d2, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,545] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-14 17:49:15,546] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 184 ms on 7cd3ddbc35d2 (executor driver) (35/141)
[2021-05-14 17:49:15,547] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-14 17:49:15,548] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 177 ms on 7cd3ddbc35d2 (executor driver) (36/141)
[2021-05-14 17:49:15,551] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-14 17:49:15,552] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (7cd3ddbc35d2, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,555] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-14 17:49:15,556] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-14 17:49:15,557] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 200 ms on 7cd3ddbc35d2 (executor driver) (37/141)
[2021-05-14 17:49:15,559] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (7cd3ddbc35d2, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,561] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (7cd3ddbc35d2, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,562] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 199 ms on 7cd3ddbc35d2 (executor driver) (38/141)
[2021-05-14 17:49:15,564] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-14 17:49:15,567] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-14 17:49:15,880] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-14 17:49:15,881] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-14 17:49:15,883] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-14 17:49:15,883] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-14 17:49:15,884] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (7cd3ddbc35d2, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,886] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-14 17:49:15,888] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (7cd3ddbc35d2, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,889] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 337 ms on 7cd3ddbc35d2 (executor driver) (39/141)
[2021-05-14 17:49:15,890] {docker.py:276} INFO - 21/05/14 20:49:15 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-14 17:49:15,891] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 329 ms on 7cd3ddbc35d2 (executor driver) (40/141)
[2021-05-14 17:49:15,894] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 349 ms on 7cd3ddbc35d2 (executor driver) (41/141)
[2021-05-14 17:49:15,895] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (7cd3ddbc35d2, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,902] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (7cd3ddbc35d2, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:15,902] {docker.py:276} INFO - 21/05/14 20:49:15 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 341 ms on 7cd3ddbc35d2 (executor driver) (42/141)
21/05/14 20:49:15 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
21/05/14 20:49:15 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-14 17:49:16,110] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1886 bytes result sent to driver
[2021-05-14 17:49:16,112] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1886 bytes result sent to driver
21/05/14 20:49:16 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1886 bytes result sent to driver
[2021-05-14 17:49:16,112] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (7cd3ddbc35d2, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,115] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1886 bytes result sent to driver
[2021-05-14 17:49:16,117] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (7cd3ddbc35d2, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:16 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-14 17:49:16,120] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 223 ms on 7cd3ddbc35d2 (executor driver) (43/141)
[2021-05-14 17:49:16,122] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 234 ms on 7cd3ddbc35d2 (executor driver) (44/141)
[2021-05-14 17:49:16,124] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-14 17:49:16,126] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (7cd3ddbc35d2, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,129] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-14 17:49:16,130] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 237 ms on 7cd3ddbc35d2 (executor driver) (45/141)
[2021-05-14 17:49:16,132] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (7cd3ddbc35d2, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,133] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 250 ms on 7cd3ddbc35d2 (executor driver) (46/141)
[2021-05-14 17:49:16,135] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-14 17:49:16,307] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-14 17:49:16,311] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (7cd3ddbc35d2, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,311] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-14 17:49:16,312] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-14 17:49:16,312] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 188 ms on 7cd3ddbc35d2 (executor driver) (47/141)
[2021-05-14 17:49:16,318] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (7cd3ddbc35d2, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,318] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-14 17:49:16,319] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 203 ms on 7cd3ddbc35d2 (executor driver) (48/141)
[2021-05-14 17:49:16,327] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-14 17:49:16,328] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-14 17:49:16,332] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (7cd3ddbc35d2, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,333] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 215 ms on 7cd3ddbc35d2 (executor driver) (49/141)
[2021-05-14 17:49:16,333] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-14 17:49:16,334] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (7cd3ddbc35d2, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,335] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 204 ms on 7cd3ddbc35d2 (executor driver) (50/141)
[2021-05-14 17:49:16,335] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-14 17:49:16,498] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-14 17:49:16,499] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
21/05/14 20:49:16 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-14 17:49:16,500] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (7cd3ddbc35d2, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,501] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-14 17:49:16,501] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 189 ms on 7cd3ddbc35d2 (executor driver) (51/141)
[2021-05-14 17:49:16,506] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-14 17:49:16,508] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (7cd3ddbc35d2, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,509] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (7cd3ddbc35d2, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,514] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-14 17:49:16,515] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 206 ms on 7cd3ddbc35d2 (executor driver) (52/141)
[2021-05-14 17:49:16,515] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 183 ms on 7cd3ddbc35d2 (executor driver) (53/141)
[2021-05-14 17:49:16,517] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (7cd3ddbc35d2, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,517] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-14 17:49:16,518] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 189 ms on 7cd3ddbc35d2 (executor driver) (54/141)
[2021-05-14 17:49:16,519] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-14 17:49:16,695] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1843 bytes result sent to driver
21/05/14 20:49:16 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (7cd3ddbc35d2, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:16 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 192 ms on 7cd3ddbc35d2 (executor driver) (55/141)
21/05/14 20:49:16 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
21/05/14 20:49:16 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-14 17:49:16,698] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
21/05/14 20:49:16 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (7cd3ddbc35d2, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,710] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
21/05/14 20:49:16 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (7cd3ddbc35d2, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:16 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 200 ms on 7cd3ddbc35d2 (executor driver) (56/141)
21/05/14 20:49:16 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 193 ms on 7cd3ddbc35d2 (executor driver) (57/141)
[2021-05-14 17:49:16,726] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-14 17:49:16,728] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1929 bytes result sent to driver
[2021-05-14 17:49:16,733] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (7cd3ddbc35d2, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:16,735] {docker.py:276} INFO - 21/05/14 20:49:16 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 229 ms on 7cd3ddbc35d2 (executor driver) (58/141)
[2021-05-14 17:49:16,736] {docker.py:276} INFO - 21/05/14 20:49:16 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-14 17:49:17,086] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-14 17:49:17,087] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1886 bytes result sent to driver
[2021-05-14 17:49:17,088] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1886 bytes result sent to driver
[2021-05-14 17:49:17,089] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (7cd3ddbc35d2, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,090] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-14 17:49:17,091] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 393 ms on 7cd3ddbc35d2 (executor driver) (59/141)
[2021-05-14 17:49:17,093] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (7cd3ddbc35d2, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,096] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-14 17:49:17,097] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-14 17:49:17,098] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (7cd3ddbc35d2, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,098] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-14 17:49:17,099] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 408 ms on 7cd3ddbc35d2 (executor driver) (60/141)
[2021-05-14 17:49:17,099] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 367 ms on 7cd3ddbc35d2 (executor driver) (61/141)
[2021-05-14 17:49:17,102] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 407 ms on 7cd3ddbc35d2 (executor driver) (62/141)
[2021-05-14 17:49:17,104] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (7cd3ddbc35d2, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,105] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-14 17:49:17,320] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-14 17:49:17,322] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-14 17:49:17,322] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (7cd3ddbc35d2, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,326] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
21/05/14 20:49:17 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (7cd3ddbc35d2, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,327] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-14 17:49:17,328] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 230 ms on 7cd3ddbc35d2 (executor driver) (63/141)
[2021-05-14 17:49:17,329] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 223 ms on 7cd3ddbc35d2 (executor driver) (64/141)
[2021-05-14 17:49:17,333] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-14 17:49:17,334] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (7cd3ddbc35d2, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,334] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 244 ms on 7cd3ddbc35d2 (executor driver) (65/141)
[2021-05-14 17:49:17,335] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-14 17:49:17,356] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-14 17:49:17,359] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (7cd3ddbc35d2, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,360] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-14 17:49:17,360] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 257 ms on 7cd3ddbc35d2 (executor driver) (66/141)
[2021-05-14 17:49:17,516] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-14 17:49:17,517] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-14 17:49:17,518] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (7cd3ddbc35d2, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,519] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-14 17:49:17,519] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (7cd3ddbc35d2, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,520] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 196 ms on 7cd3ddbc35d2 (executor driver) (67/141)
[2021-05-14 17:49:17,521] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-14 17:49:17,521] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 200 ms on 7cd3ddbc35d2 (executor driver) (68/141)
[2021-05-14 17:49:17,539] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-14 17:49:17,540] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (7cd3ddbc35d2, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,543] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
21/05/14 20:49:17 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 211 ms on 7cd3ddbc35d2 (executor driver) (69/141)
[2021-05-14 17:49:17,546] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-14 17:49:17,598] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (7cd3ddbc35d2, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:17 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 206 ms on 7cd3ddbc35d2 (executor driver) (70/141)
21/05/14 20:49:17 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-14 17:49:17,688] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-14 17:49:17,689] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (7cd3ddbc35d2, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,690] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-14 17:49:17,693] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-14 17:49:17,693] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (7cd3ddbc35d2, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,694] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 176 ms on 7cd3ddbc35d2 (executor driver) (71/141)
[2021-05-14 17:49:17,695] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-14 17:49:17,695] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 175 ms on 7cd3ddbc35d2 (executor driver) (72/141)
[2021-05-14 17:49:17,749] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1886 bytes result sent to driver
[2021-05-14 17:49:17,750] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1886 bytes result sent to driver
[2021-05-14 17:49:17,751] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (7cd3ddbc35d2, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,752] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 206 ms on 7cd3ddbc35d2 (executor driver) (73/141)
21/05/14 20:49:17 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-14 17:49:17,753] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 213 ms on 7cd3ddbc35d2 (executor driver) (74/141)
[2021-05-14 17:49:17,755] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (7cd3ddbc35d2, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,757] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-14 17:49:17,988] {docker.py:276} INFO - 21/05/14 20:49:17 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1886 bytes result sent to driver
[2021-05-14 17:49:17,990] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (7cd3ddbc35d2, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,991] {docker.py:276} INFO - 21/05/14 20:49:17 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 302 ms on 7cd3ddbc35d2 (executor driver) (75/141)
21/05/14 20:49:17 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1886 bytes result sent to driver
[2021-05-14 17:49:17,993] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (7cd3ddbc35d2, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:17,995] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 303 ms on 7cd3ddbc35d2 (executor driver) (76/141)
[2021-05-14 17:49:17,995] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-14 17:49:17,997] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-14 17:49:18,140] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-14 17:49:18,140] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-14 17:49:18,142] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (7cd3ddbc35d2, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,144] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-14 17:49:18,145] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (7cd3ddbc35d2, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,147] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-14 17:49:18,148] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 396 ms on 7cd3ddbc35d2 (executor driver) (77/141)
[2021-05-14 17:49:18,148] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 394 ms on 7cd3ddbc35d2 (executor driver) (78/141)
[2021-05-14 17:49:18,166] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-14 17:49:18,167] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-14 17:49:18,167] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (7cd3ddbc35d2, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,168] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-14 17:49:18,169] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (7cd3ddbc35d2, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,170] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
21/05/14 20:49:18 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 177 ms on 7cd3ddbc35d2 (executor driver) (79/141)
[2021-05-14 17:49:18,170] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 182 ms on 7cd3ddbc35d2 (executor driver) (80/141)
[2021-05-14 17:49:18,368] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-14 17:49:18,376] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-14 17:49:18,376] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (7cd3ddbc35d2, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,377] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-14 17:49:18,377] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (7cd3ddbc35d2, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,377] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 231 ms on 7cd3ddbc35d2 (executor driver) (81/141)
[2021-05-14 17:49:18,377] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
21/05/14 20:49:18 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 229 ms on 7cd3ddbc35d2 (executor driver) (82/141)
[2021-05-14 17:49:18,378] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-14 17:49:18,378] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-14 17:49:18,378] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (7cd3ddbc35d2, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,380] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 210 ms on 7cd3ddbc35d2 (executor driver) (83/141)
[2021-05-14 17:49:18,381] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-14 17:49:18,381] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 210 ms on 7cd3ddbc35d2 (executor driver) (84/141)
[2021-05-14 17:49:18,382] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (7cd3ddbc35d2, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,382] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-14 17:49:18,541] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-14 17:49:18,543] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (7cd3ddbc35d2, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,545] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 176 ms on 7cd3ddbc35d2 (executor driver) (85/141)
[2021-05-14 17:49:18,546] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-14 17:49:18,546] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-14 17:49:18,546] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (7cd3ddbc35d2, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,547] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 176 ms on 7cd3ddbc35d2 (executor driver) (86/141)
21/05/14 20:49:18 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-14 17:49:18,548] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-14 17:49:18,549] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (7cd3ddbc35d2, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,551] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 173 ms on 7cd3ddbc35d2 (executor driver) (87/141)
[2021-05-14 17:49:18,552] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-14 17:49:18,562] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1886 bytes result sent to driver
[2021-05-14 17:49:18,563] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (7cd3ddbc35d2, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,564] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-14 17:49:18,564] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 191 ms on 7cd3ddbc35d2 (executor driver) (88/141)
[2021-05-14 17:49:18,727] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1886 bytes result sent to driver
[2021-05-14 17:49:18,730] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1843 bytes result sent to driver
[2021-05-14 17:49:18,731] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1886 bytes result sent to driver
[2021-05-14 17:49:18,731] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (7cd3ddbc35d2, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,733] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 187 ms on 7cd3ddbc35d2 (executor driver) (89/141)
[2021-05-14 17:49:18,733] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-14 17:49:18,735] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (7cd3ddbc35d2, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,736] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 173 ms on 7cd3ddbc35d2 (executor driver) (90/141)
[2021-05-14 17:49:18,737] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (7cd3ddbc35d2, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,738] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1886 bytes result sent to driver
[2021-05-14 17:49:18,738] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 196 ms on 7cd3ddbc35d2 (executor driver) (91/141)
[2021-05-14 17:49:18,739] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-14 17:49:18,740] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (7cd3ddbc35d2, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:18,741] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-14 17:49:18,741] {docker.py:276} INFO - 21/05/14 20:49:18 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 192 ms on 7cd3ddbc35d2 (executor driver) (92/141)
[2021-05-14 17:49:18,742] {docker.py:276} INFO - 21/05/14 20:49:18 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-14 17:49:19,024] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1843 bytes result sent to driver
[2021-05-14 17:49:19,025] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1843 bytes result sent to driver
[2021-05-14 17:49:19,026] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-14 17:49:19,028] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1843 bytes result sent to driver
[2021-05-14 17:49:19,028] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (7cd3ddbc35d2, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,030] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-14 17:49:19,031] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (7cd3ddbc35d2, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,032] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 296 ms on 7cd3ddbc35d2 (executor driver) (93/141)
[2021-05-14 17:49:19,033] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-14 17:49:19,034] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 303 ms on 7cd3ddbc35d2 (executor driver) (94/141)
[2021-05-14 17:49:19,036] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (7cd3ddbc35d2, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,037] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 297 ms on 7cd3ddbc35d2 (executor driver) (95/141)
21/05/14 20:49:19 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-14 17:49:19,039] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (7cd3ddbc35d2, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,040] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 307 ms on 7cd3ddbc35d2 (executor driver) (96/141)
[2021-05-14 17:49:19,041] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-14 17:49:19,202] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-14 17:49:19,204] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (7cd3ddbc35d2, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,205] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-14 17:49:19,206] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 175 ms on 7cd3ddbc35d2 (executor driver) (97/141)
[2021-05-14 17:49:19,209] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1843 bytes result sent to driver
[2021-05-14 17:49:19,210] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
[2021-05-14 17:49:19,211] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (7cd3ddbc35d2, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,213] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-14 17:49:19,214] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-14 17:49:19,214] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (7cd3ddbc35d2, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,215] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 177 ms on 7cd3ddbc35d2 (executor driver) (98/141)
[2021-05-14 17:49:19,216] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-14 17:49:19,217] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (7cd3ddbc35d2, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,218] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 192 ms on 7cd3ddbc35d2 (executor driver) (99/141)
[2021-05-14 17:49:19,219] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 185 ms on 7cd3ddbc35d2 (executor driver) (100/141)
[2021-05-14 17:49:19,220] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-14 17:49:19,414] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
21/05/14 20:49:19 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1843 bytes result sent to driver
[2021-05-14 17:49:19,415] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-14 17:49:19,415] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (7cd3ddbc35d2, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,416] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 214 ms on 7cd3ddbc35d2 (executor driver) (101/141)
[2021-05-14 17:49:19,417] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-14 17:49:19,418] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 200 ms on 7cd3ddbc35d2 (executor driver) (102/141)
[2021-05-14 17:49:19,419] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1886 bytes result sent to driver
[2021-05-14 17:49:19,419] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (7cd3ddbc35d2, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,421] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-14 17:49:19,422] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 209 ms on 7cd3ddbc35d2 (executor driver) (103/141)
[2021-05-14 17:49:19,424] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (7cd3ddbc35d2, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,432] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-14 17:49:19,433] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 221 ms on 7cd3ddbc35d2 (executor driver) (104/141)
[2021-05-14 17:49:19,441] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (7cd3ddbc35d2, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,442] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-14 17:49:19,597] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-14 17:49:19,601] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (7cd3ddbc35d2, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,602] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 178 ms on 7cd3ddbc35d2 (executor driver) (105/141)
[2021-05-14 17:49:19,603] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1886 bytes result sent to driver
[2021-05-14 17:49:19,604] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-14 17:49:19,607] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (7cd3ddbc35d2, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,609] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 196 ms on 7cd3ddbc35d2 (executor driver) (106/141)
[2021-05-14 17:49:19,610] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-14 17:49:19,612] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1886 bytes result sent to driver
[2021-05-14 17:49:19,614] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (7cd3ddbc35d2, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,615] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 196 ms on 7cd3ddbc35d2 (executor driver) (107/141)
[2021-05-14 17:49:19,616] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-14 17:49:19,619] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1843 bytes result sent to driver
[2021-05-14 17:49:19,620] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (7cd3ddbc35d2, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,621] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 181 ms on 7cd3ddbc35d2 (executor driver) (108/141)
[2021-05-14 17:49:19,622] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-14 17:49:19,795] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1843 bytes result sent to driver
[2021-05-14 17:49:19,795] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1843 bytes result sent to driver
[2021-05-14 17:49:19,796] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (7cd3ddbc35d2, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,799] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-14 17:49:19,801] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (7cd3ddbc35d2, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,802] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 194 ms on 7cd3ddbc35d2 (executor driver) (109/141)
[2021-05-14 17:49:19,802] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-14 17:49:19,803] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 182 ms on 7cd3ddbc35d2 (executor driver) (110/141)
[2021-05-14 17:49:19,810] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-14 17:49:19,810] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1843 bytes result sent to driver
[2021-05-14 17:49:19,813] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (7cd3ddbc35d2, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,814] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-14 17:49:19,815] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (7cd3ddbc35d2, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:19,815] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 216 ms on 7cd3ddbc35d2 (executor driver) (111/141)
[2021-05-14 17:49:19,816] {docker.py:276} INFO - 21/05/14 20:49:19 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 202 ms on 7cd3ddbc35d2 (executor driver) (112/141)
[2021-05-14 17:49:19,818] {docker.py:276} INFO - 21/05/14 20:49:19 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-14 17:49:20,073] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1843 bytes result sent to driver
21/05/14 20:49:20 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-14 17:49:20,074] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (7cd3ddbc35d2, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,075] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 277 ms on 7cd3ddbc35d2 (executor driver) (113/141)
[2021-05-14 17:49:20,076] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-14 17:49:20,077] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-14 17:49:20,078] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-14 17:49:20,079] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (7cd3ddbc35d2, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,080] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 284 ms on 7cd3ddbc35d2 (executor driver) (114/141)
[2021-05-14 17:49:20,081] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-14 17:49:20,081] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 268 ms on 7cd3ddbc35d2 (executor driver) (115/141)
[2021-05-14 17:49:20,082] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (7cd3ddbc35d2, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,084] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (7cd3ddbc35d2, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,085] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
21/05/14 20:49:20 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 274 ms on 7cd3ddbc35d2 (executor driver) (116/141)
[2021-05-14 17:49:20,088] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-14 17:49:20,245] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1886 bytes result sent to driver
[2021-05-14 17:49:20,247] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1886 bytes result sent to driver
[2021-05-14 17:49:20,248] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (7cd3ddbc35d2, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,250] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (7cd3ddbc35d2, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,251] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 170 ms on 7cd3ddbc35d2 (executor driver) (117/141)
[2021-05-14 17:49:20,251] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-14 17:49:20,252] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 178 ms on 7cd3ddbc35d2 (executor driver) (118/141)
[2021-05-14 17:49:20,253] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-14 17:49:20,256] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1886 bytes result sent to driver
[2021-05-14 17:49:20,257] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (7cd3ddbc35d2, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,259] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 176 ms on 7cd3ddbc35d2 (executor driver) (119/141)
[2021-05-14 17:49:20,259] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-14 17:49:20,267] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1886 bytes result sent to driver
[2021-05-14 17:49:20,269] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (7cd3ddbc35d2, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,269] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 192 ms on 7cd3ddbc35d2 (executor driver) (120/141)
[2021-05-14 17:49:20,271] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-14 17:49:20,466] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1843 bytes result sent to driver
21/05/14 20:49:20 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1843 bytes result sent to driver
[2021-05-14 17:49:20,467] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1843 bytes result sent to driver
21/05/14 20:49:20 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (7cd3ddbc35d2, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,468] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-14 17:49:20,470] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (7cd3ddbc35d2, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,471] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-14 17:49:20,472] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 215 ms on 7cd3ddbc35d2 (executor driver) (121/141)
[2021-05-14 17:49:20,474] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 222 ms on 7cd3ddbc35d2 (executor driver) (122/141)
[2021-05-14 17:49:20,475] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
21/05/14 20:49:20 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (7cd3ddbc35d2, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,476] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 228 ms on 7cd3ddbc35d2 (executor driver) (123/141)
[2021-05-14 17:49:20,477] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 209 ms on 7cd3ddbc35d2 (executor driver) (124/141)
[2021-05-14 17:49:20,479] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (7cd3ddbc35d2, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,480] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-14 17:49:20,481] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-14 17:49:20,644] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1843 bytes result sent to driver
[2021-05-14 17:49:20,645] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-14 17:49:20,646] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (7cd3ddbc35d2, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,648] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-14 17:49:20,648] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1843 bytes result sent to driver
[2021-05-14 17:49:20,649] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (7cd3ddbc35d2, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,650] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 172 ms on 7cd3ddbc35d2 (executor driver) (125/141)
[2021-05-14 17:49:20,651] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 182 ms on 7cd3ddbc35d2 (executor driver) (126/141)
[2021-05-14 17:49:20,652] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
21/05/14 20:49:20 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 186 ms on 7cd3ddbc35d2 (executor driver) (127/141)
[2021-05-14 17:49:20,654] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-14 17:49:20,655] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (7cd3ddbc35d2, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,656] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (7cd3ddbc35d2, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,658] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
21/05/14 20:49:20 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 183 ms on 7cd3ddbc35d2 (executor driver) (128/141)
[2021-05-14 17:49:20,663] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-14 17:49:20,836] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1843 bytes result sent to driver
[2021-05-14 17:49:20,840] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (7cd3ddbc35d2, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:20 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1843 bytes result sent to driver
[2021-05-14 17:49:20,840] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1843 bytes result sent to driver
[2021-05-14 17:49:20,841] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-14 17:49:20,841] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (7cd3ddbc35d2, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,842] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 186 ms on 7cd3ddbc35d2 (executor driver) (129/141)
[2021-05-14 17:49:20,843] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 196 ms on 7cd3ddbc35d2 (executor driver) (130/141)
[2021-05-14 17:49:20,845] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1886 bytes result sent to driver
[2021-05-14 17:49:20,846] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 200 ms on 7cd3ddbc35d2 (executor driver) (131/141)
[2021-05-14 17:49:20,847] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-14 17:49:20,847] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (7cd3ddbc35d2, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,849] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (7cd3ddbc35d2, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:20,849] {docker.py:276} INFO - 21/05/14 20:49:20 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 195 ms on 7cd3ddbc35d2 (executor driver) (132/141)
[2021-05-14 17:49:20,851] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-14 17:49:20,851] {docker.py:276} INFO - 21/05/14 20:49:20 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-14 17:49:21,122] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1886 bytes result sent to driver
[2021-05-14 17:49:21,123] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1886 bytes result sent to driver
21/05/14 20:49:21 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1886 bytes result sent to driver
[2021-05-14 17:49:21,124] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (7cd3ddbc35d2, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,125] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-14 17:49:21,125] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1886 bytes result sent to driver
[2021-05-14 17:49:21,127] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (7cd3ddbc35d2, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,127] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-14 17:49:21,128] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (7cd3ddbc35d2, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,129] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 292 ms on 7cd3ddbc35d2 (executor driver) (133/141)
[2021-05-14 17:49:21,129] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 281 ms on 7cd3ddbc35d2 (executor driver) (134/141)
[2021-05-14 17:49:21,130] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 290 ms on 7cd3ddbc35d2 (executor driver) (135/141)
[2021-05-14 17:49:21,131] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (7cd3ddbc35d2, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,132] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 287 ms on 7cd3ddbc35d2 (executor driver) (136/141)
[2021-05-14 17:49:21,133] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-14 17:49:21,138] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-14 17:49:21,293] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-14 17:49:21,295] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (7cd3ddbc35d2, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,297] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1843 bytes result sent to driver
[2021-05-14 17:49:21,297] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 174 ms on 7cd3ddbc35d2 (executor driver) (137/141)
[2021-05-14 17:49:21,298] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-14 17:49:21,298] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1843 bytes result sent to driver
[2021-05-14 17:49:21,306] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 180 ms on 7cd3ddbc35d2 (executor driver) (138/141)
[2021-05-14 17:49:21,307] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 175 ms on 7cd3ddbc35d2 (executor driver) (139/141)
[2021-05-14 17:49:21,307] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1843 bytes result sent to driver
[2021-05-14 17:49:21,308] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 181 ms on 7cd3ddbc35d2 (executor driver) (140/141)
[2021-05-14 17:49:21,514] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1843 bytes result sent to driver
[2021-05-14 17:49:21,516] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 221 ms on 7cd3ddbc35d2 (executor driver) (141/141)
[2021-05-14 17:49:21,520] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 17:49:21,521] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 9.644 s
[2021-05-14 17:49:21,529] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 17:49:21,530] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 17:49:21,535] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 9.731941 s
[2021-05-14 17:49:21,583] {docker.py:276} INFO - 21/05/14 20:49:21 INFO InMemoryFileIndex: It took 10378 ms to list leaf files for 141 paths.
[2021-05-14 17:49:21,709] {docker.py:276} INFO - 21/05/14 20:49:21 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620940695_to_1620942495.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620942495_to_1620944295.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620944295_to_1620946095.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620946095_to_1620947895.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620947895_to_1620949695.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620949695_to_1620951495.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620951495_to_1620953295.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620953295_to_1620955095.csv, s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620955095_to_1620956895.csv.
[2021-05-14 17:49:21,750] {docker.py:276} INFO - 21/05/14 20:49:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:21,753] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/14 20:49:21 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 20:49:21 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 17:49:21,753] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Missing parents: List()
[2021-05-14 17:49:21,755] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 17:49:21,770] {docker.py:276} INFO - 21/05/14 20:49:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-14 17:49:21,782] {docker.py:276} INFO - 21/05/14 20:49:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-14 17:49:21,784] {docker.py:276} INFO - 21/05/14 20:49:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7cd3ddbc35d2:39701 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-14 17:49:21,786] {docker.py:276} INFO - 21/05/14 20:49:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 17:49:21,789] {docker.py:276} INFO - 21/05/14 20:49:21 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 20:49:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-14 17:49:21,792] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (7cd3ddbc35d2, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,793] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (7cd3ddbc35d2, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,794] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (7cd3ddbc35d2, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,795] {docker.py:276} INFO - 21/05/14 20:49:21 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (7cd3ddbc35d2, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:21,796] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-14 17:49:21,797] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-14 17:49:21,798] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-14 17:49:21,798] {docker.py:276} INFO - 21/05/14 20:49:21 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-14 17:49:21,876] {docker.py:276} INFO - 21/05/14 20:49:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7cd3ddbc35d2:39701 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 17:49:22,172] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-14 17:49:22,175] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-14 17:49:22,175] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (7cd3ddbc35d2, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,176] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
21/05/14 20:49:22 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (7cd3ddbc35d2, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,177] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-14 17:49:22,178] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-14 17:49:22,178] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 387 ms on 7cd3ddbc35d2 (executor driver) (1/141)
[2021-05-14 17:49:22,179] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 387 ms on 7cd3ddbc35d2 (executor driver) (2/141)
[2021-05-14 17:49:22,180] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-14 17:49:22,183] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (7cd3ddbc35d2, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,184] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 391 ms on 7cd3ddbc35d2 (executor driver) (3/141)
[2021-05-14 17:49:22,185] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-14 17:49:22,186] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (7cd3ddbc35d2, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,188] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 394 ms on 7cd3ddbc35d2 (executor driver) (4/141)
[2021-05-14 17:49:22,189] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-14 17:49:22,350] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
21/05/14 20:49:22 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-14 17:49:22,352] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-14 17:49:22,353] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (7cd3ddbc35d2, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,355] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-14 17:49:22,357] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-14 17:49:22,357] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (7cd3ddbc35d2, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,359] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-14 17:49:22,360] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (7cd3ddbc35d2, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,361] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 187 ms on 7cd3ddbc35d2 (executor driver) (5/141)
[2021-05-14 17:49:22,361] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 186 ms on 7cd3ddbc35d2 (executor driver) (6/141)
[2021-05-14 17:49:22,362] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 176 ms on 7cd3ddbc35d2 (executor driver) (7/141)
[2021-05-14 17:49:22,362] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 180 ms on 7cd3ddbc35d2 (executor driver) (8/141)
[2021-05-14 17:49:22,363] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (7cd3ddbc35d2, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,366] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-14 17:49:22,367] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-14 17:49:22,562] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
[2021-05-14 17:49:22,563] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-14 17:49:22,564] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (7cd3ddbc35d2, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,565] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1886 bytes result sent to driver
[2021-05-14 17:49:22,566] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (7cd3ddbc35d2, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,567] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-14 17:49:22,568] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 218 ms on 7cd3ddbc35d2 (executor driver) (9/141)
[2021-05-14 17:49:22,569] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 214 ms on 7cd3ddbc35d2 (executor driver) (10/141)
[2021-05-14 17:49:22,571] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
21/05/14 20:49:22 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (7cd3ddbc35d2, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,573] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 214 ms on 7cd3ddbc35d2 (executor driver) (11/141)
[2021-05-14 17:49:22,573] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 210 ms on 7cd3ddbc35d2 (executor driver) (12/141)
[2021-05-14 17:49:22,574] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
21/05/14 20:49:22 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-14 17:49:22,575] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (7cd3ddbc35d2, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,576] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-14 17:49:22,745] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1886 bytes result sent to driver
[2021-05-14 17:49:22,745] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1886 bytes result sent to driver
[2021-05-14 17:49:22,746] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (7cd3ddbc35d2, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,747] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 184 ms on 7cd3ddbc35d2 (executor driver) (13/141)
[2021-05-14 17:49:22,748] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-14 17:49:22,749] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (7cd3ddbc35d2, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,750] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 179 ms on 7cd3ddbc35d2 (executor driver) (14/141)
[2021-05-14 17:49:22,751] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-14 17:49:22,757] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1886 bytes result sent to driver
[2021-05-14 17:49:22,758] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (7cd3ddbc35d2, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,759] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 193 ms on 7cd3ddbc35d2 (executor driver) (15/141)
[2021-05-14 17:49:22,760] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1886 bytes result sent to driver
[2021-05-14 17:49:22,761] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-14 17:49:22,761] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (7cd3ddbc35d2, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,763] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-14 17:49:22,763] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 188 ms on 7cd3ddbc35d2 (executor driver) (16/141)
[2021-05-14 17:49:22,927] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-14 17:49:22,928] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1843 bytes result sent to driver
[2021-05-14 17:49:22,931] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
21/05/14 20:49:22 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (7cd3ddbc35d2, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,932] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1843 bytes result sent to driver
[2021-05-14 17:49:22,932] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 186 ms on 7cd3ddbc35d2 (executor driver) (17/141)
[2021-05-14 17:49:22,933] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-14 17:49:22,934] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (7cd3ddbc35d2, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,936] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (7cd3ddbc35d2, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,937] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 175 ms on 7cd3ddbc35d2 (executor driver) (18/141)
[2021-05-14 17:49:22,937] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
21/05/14 20:49:22 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 179 ms on 7cd3ddbc35d2 (executor driver) (19/141)
[2021-05-14 17:49:22,938] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 190 ms on 7cd3ddbc35d2 (executor driver) (20/141)
[2021-05-14 17:49:22,939] {docker.py:276} INFO - 21/05/14 20:49:22 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (7cd3ddbc35d2, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:22,940] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-14 17:49:22,940] {docker.py:276} INFO - 21/05/14 20:49:22 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-14 17:49:23,220] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-14 17:49:23,223] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (7cd3ddbc35d2, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:23 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
21/05/14 20:49:23 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
21/05/14 20:49:23 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 288 ms on 7cd3ddbc35d2 (executor driver) (21/141)
21/05/14 20:49:23 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 293 ms on 7cd3ddbc35d2 (executor driver) (22/141)
[2021-05-14 17:49:23,224] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-14 17:49:23,225] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-14 17:49:23,226] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (7cd3ddbc35d2, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,227] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 292 ms on 7cd3ddbc35d2 (executor driver) (23/141)
[2021-05-14 17:49:23,228] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-14 17:49:23,230] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (7cd3ddbc35d2, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,233] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-14 17:49:23,236] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (7cd3ddbc35d2, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,239] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-14 17:49:23,240] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 299 ms on 7cd3ddbc35d2 (executor driver) (24/141)
[2021-05-14 17:49:23,392] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
[2021-05-14 17:49:23,393] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (7cd3ddbc35d2, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,394] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 172 ms on 7cd3ddbc35d2 (executor driver) (25/141)
[2021-05-14 17:49:23,394] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-14 17:49:23,407] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
21/05/14 20:49:23 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 176 ms on 7cd3ddbc35d2 (executor driver) (26/141)
[2021-05-14 17:49:23,408] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (7cd3ddbc35d2, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:23 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-14 17:49:23,431] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1886 bytes result sent to driver
[2021-05-14 17:49:23,431] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1886 bytes result sent to driver
[2021-05-14 17:49:23,432] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (7cd3ddbc35d2, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,433] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (7cd3ddbc35d2, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,434] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-14 17:49:23,434] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-14 17:49:23,435] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 200 ms on 7cd3ddbc35d2 (executor driver) (27/141)
[2021-05-14 17:49:23,437] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 207 ms on 7cd3ddbc35d2 (executor driver) (28/141)
[2021-05-14 17:49:23,608] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1886 bytes result sent to driver
[2021-05-14 17:49:23,610] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (7cd3ddbc35d2, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,614] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1886 bytes result sent to driver
[2021-05-14 17:49:23,615] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
21/05/14 20:49:23 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-14 17:49:23,616] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-14 17:49:23,616] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (7cd3ddbc35d2, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,616] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 213 ms on 7cd3ddbc35d2 (executor driver) (29/141)
[2021-05-14 17:49:23,617] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 225 ms on 7cd3ddbc35d2 (executor driver) (30/141)
[2021-05-14 17:49:23,618] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-14 17:49:23,619] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (7cd3ddbc35d2, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,621] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 187 ms on 7cd3ddbc35d2 (executor driver) (31/141)
21/05/14 20:49:23 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-14 17:49:23,622] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (7cd3ddbc35d2, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,622] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 191 ms on 7cd3ddbc35d2 (executor driver) (32/141)
[2021-05-14 17:49:23,623] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-14 17:49:23,788] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1843 bytes result sent to driver
[2021-05-14 17:49:23,789] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-14 17:49:23,790] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1843 bytes result sent to driver
[2021-05-14 17:49:23,791] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (7cd3ddbc35d2, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,793] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1843 bytes result sent to driver
21/05/14 20:49:23 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 176 ms on 7cd3ddbc35d2 (executor driver) (33/141)
[2021-05-14 17:49:23,793] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-14 17:49:23,795] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (7cd3ddbc35d2, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,795] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 187 ms on 7cd3ddbc35d2 (executor driver) (34/141)
[2021-05-14 17:49:23,797] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-14 17:49:23,798] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (7cd3ddbc35d2, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,799] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 186 ms on 7cd3ddbc35d2 (executor driver) (35/141)
21/05/14 20:49:23 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-14 17:49:23,800] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 181 ms on 7cd3ddbc35d2 (executor driver) (36/141)
[2021-05-14 17:49:23,801] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (7cd3ddbc35d2, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,804] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-14 17:49:23,975] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1843 bytes result sent to driver
[2021-05-14 17:49:23,976] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1843 bytes result sent to driver
[2021-05-14 17:49:23,977] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
[2021-05-14 17:49:23,977] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (7cd3ddbc35d2, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,979] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1843 bytes result sent to driver
[2021-05-14 17:49:23,979] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 185 ms on 7cd3ddbc35d2 (executor driver) (37/141)
[2021-05-14 17:49:23,980] {docker.py:276} INFO - 21/05/14 20:49:23 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-14 17:49:23,981] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (7cd3ddbc35d2, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,982] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 182 ms on 7cd3ddbc35d2 (executor driver) (38/141)
[2021-05-14 17:49:23,983] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (7cd3ddbc35d2, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,985] {docker.py:276} INFO - 21/05/14 20:49:23 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (7cd3ddbc35d2, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:23,986] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 189 ms on 7cd3ddbc35d2 (executor driver) (39/141)
21/05/14 20:49:24 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-14 17:49:23,986] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 196 ms on 7cd3ddbc35d2 (executor driver) (40/141)
[2021-05-14 17:49:23,988] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-14 17:49:23,988] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-14 17:49:24,265] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1843 bytes result sent to driver
21/05/14 20:49:24 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-14 17:49:24,266] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1843 bytes result sent to driver
[2021-05-14 17:49:24,267] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-14 17:49:24,267] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (7cd3ddbc35d2, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,268] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (7cd3ddbc35d2, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,269] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 285 ms on 7cd3ddbc35d2 (executor driver) (41/141)
[2021-05-14 17:49:24,270] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (7cd3ddbc35d2, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,271] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-14 17:49:24,271] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 295 ms on 7cd3ddbc35d2 (executor driver) (42/141)
[2021-05-14 17:49:24,272] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-14 17:49:24,273] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (7cd3ddbc35d2, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,274] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
21/05/14 20:49:24 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-14 17:49:24,275] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 294 ms on 7cd3ddbc35d2 (executor driver) (43/141)
[2021-05-14 17:49:24,292] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 309 ms on 7cd3ddbc35d2 (executor driver) (44/141)
[2021-05-14 17:49:24,452] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1886 bytes result sent to driver
[2021-05-14 17:49:24,453] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1886 bytes result sent to driver
[2021-05-14 17:49:24,454] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (7cd3ddbc35d2, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,455] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 185 ms on 7cd3ddbc35d2 (executor driver) (45/141)
[2021-05-14 17:49:24,455] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-14 17:49:24,456] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (7cd3ddbc35d2, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,457] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-14 17:49:24,458] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 191 ms on 7cd3ddbc35d2 (executor driver) (46/141)
[2021-05-14 17:49:24,463] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1886 bytes result sent to driver
[2021-05-14 17:49:24,464] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (7cd3ddbc35d2, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,465] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 197 ms on 7cd3ddbc35d2 (executor driver) (47/141)
[2021-05-14 17:49:24,466] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-14 17:49:24,501] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1886 bytes result sent to driver
[2021-05-14 17:49:24,503] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (7cd3ddbc35d2, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,504] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-14 17:49:24,505] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 232 ms on 7cd3ddbc35d2 (executor driver) (48/141)
[2021-05-14 17:49:24,659] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-14 17:49:24,660] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-14 17:49:24,661] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-14 17:49:24,661] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (7cd3ddbc35d2, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,662] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
21/05/14 20:49:24 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 206 ms on 7cd3ddbc35d2 (executor driver) (49/141)
[2021-05-14 17:49:24,663] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 210 ms on 7cd3ddbc35d2 (executor driver) (50/141)
[2021-05-14 17:49:24,664] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (7cd3ddbc35d2, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,666] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (7cd3ddbc35d2, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:24 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-14 17:49:24,667] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 203 ms on 7cd3ddbc35d2 (executor driver) (51/141)
[2021-05-14 17:49:24,668] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-14 17:49:24,670] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-14 17:49:24,671] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (7cd3ddbc35d2, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,671] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-14 17:49:24,672] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 170 ms on 7cd3ddbc35d2 (executor driver) (52/141)
[2021-05-14 17:49:24,832] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1843 bytes result sent to driver
[2021-05-14 17:49:24,834] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1843 bytes result sent to driver
21/05/14 20:49:24 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (7cd3ddbc35d2, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,835] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-14 17:49:24,836] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (7cd3ddbc35d2, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,837] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-14 17:49:24,838] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 178 ms on 7cd3ddbc35d2 (executor driver) (53/141)
[2021-05-14 17:49:24,839] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 174 ms on 7cd3ddbc35d2 (executor driver) (54/141)
[2021-05-14 17:49:24,840] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1843 bytes result sent to driver
[2021-05-14 17:49:24,843] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1843 bytes result sent to driver
[2021-05-14 17:49:24,843] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (7cd3ddbc35d2, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,844] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (7cd3ddbc35d2, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:24,845] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-14 17:49:24,845] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 174 ms on 7cd3ddbc35d2 (executor driver) (55/141)
[2021-05-14 17:49:24,846] {docker.py:276} INFO - 21/05/14 20:49:24 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-14 17:49:24,847] {docker.py:276} INFO - 21/05/14 20:49:24 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 181 ms on 7cd3ddbc35d2 (executor driver) (56/141)
[2021-05-14 17:49:25,009] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1886 bytes result sent to driver
[2021-05-14 17:49:25,010] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1886 bytes result sent to driver
[2021-05-14 17:49:25,012] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (7cd3ddbc35d2, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,014] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-14 17:49:25,014] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 179 ms on 7cd3ddbc35d2 (executor driver) (57/141)
[2021-05-14 17:49:25,015] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (7cd3ddbc35d2, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,016] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 182 ms on 7cd3ddbc35d2 (executor driver) (58/141)
[2021-05-14 17:49:25,017] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-14 17:49:25,018] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1886 bytes result sent to driver
[2021-05-14 17:49:25,019] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1886 bytes result sent to driver
[2021-05-14 17:49:25,020] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (7cd3ddbc35d2, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,022] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 179 ms on 7cd3ddbc35d2 (executor driver) (59/141)
[2021-05-14 17:49:25,023] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (7cd3ddbc35d2, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:25 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-14 17:49:25,024] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 183 ms on 7cd3ddbc35d2 (executor driver) (60/141)
[2021-05-14 17:49:25,025] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-14 17:49:25,317] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-14 17:49:25,319] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1843 bytes result sent to driver
[2021-05-14 17:49:25,320] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (7cd3ddbc35d2, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,320] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1843 bytes result sent to driver
[2021-05-14 17:49:25,321] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1843 bytes result sent to driver
[2021-05-14 17:49:25,322] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-14 17:49:25,323] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 311 ms on 7cd3ddbc35d2 (executor driver) (61/141)
[2021-05-14 17:49:25,323] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 304 ms on 7cd3ddbc35d2 (executor driver) (62/141)
[2021-05-14 17:49:25,323] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (7cd3ddbc35d2, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,325] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-14 17:49:25,326] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (7cd3ddbc35d2, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,327] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-14 17:49:25,327] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (7cd3ddbc35d2, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,328] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 314 ms on 7cd3ddbc35d2 (executor driver) (63/141)
[2021-05-14 17:49:25,328] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 307 ms on 7cd3ddbc35d2 (executor driver) (64/141)
[2021-05-14 17:49:25,330] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-14 17:49:25,488] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-14 17:49:25,489] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
[2021-05-14 17:49:25,491] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (7cd3ddbc35d2, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,492] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 165 ms on 7cd3ddbc35d2 (executor driver) (65/141)
[2021-05-14 17:49:25,493] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-14 17:49:25,494] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1843 bytes result sent to driver
[2021-05-14 17:49:25,495] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1843 bytes result sent to driver
[2021-05-14 17:49:25,496] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (7cd3ddbc35d2, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,498] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 173 ms on 7cd3ddbc35d2 (executor driver) (66/141)
[2021-05-14 17:49:25,499] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-14 17:49:25,500] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (7cd3ddbc35d2, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,502] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 184 ms on 7cd3ddbc35d2 (executor driver) (67/141)
[2021-05-14 17:49:25,503] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-14 17:49:25,504] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 181 ms on 7cd3ddbc35d2 (executor driver) (68/141)
[2021-05-14 17:49:25,506] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (7cd3ddbc35d2, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,506] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-14 17:49:25,703] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-14 17:49:25,704] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-14 17:49:25,705] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-14 17:49:25,705] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-14 17:49:25,707] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (7cd3ddbc35d2, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,708] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-14 17:49:25,709] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (7cd3ddbc35d2, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,710] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 210 ms on 7cd3ddbc35d2 (executor driver) (69/141)
[2021-05-14 17:49:25,710] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 220 ms on 7cd3ddbc35d2 (executor driver) (70/141)
[2021-05-14 17:49:25,711] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-14 17:49:25,712] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (7cd3ddbc35d2, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,713] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-14 17:49:25,714] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (7cd3ddbc35d2, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,715] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 210 ms on 7cd3ddbc35d2 (executor driver) (71/141)
[2021-05-14 17:49:25,716] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 222 ms on 7cd3ddbc35d2 (executor driver) (72/141)
[2021-05-14 17:49:25,717] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-14 17:49:25,885] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1886 bytes result sent to driver
[2021-05-14 17:49:25,888] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (7cd3ddbc35d2, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,889] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1886 bytes result sent to driver
[2021-05-14 17:49:25,890] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 176 ms on 7cd3ddbc35d2 (executor driver) (73/141)
[2021-05-14 17:49:25,891] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 174 ms on 7cd3ddbc35d2 (executor driver) (74/141)
[2021-05-14 17:49:25,892] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (7cd3ddbc35d2, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,893] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-14 17:49:25,894] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1886 bytes result sent to driver
[2021-05-14 17:49:25,896] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (7cd3ddbc35d2, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,897] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 190 ms on 7cd3ddbc35d2 (executor driver) (75/141)
[2021-05-14 17:49:25,898] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-14 17:49:25,901] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-14 17:49:25,901] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1886 bytes result sent to driver
[2021-05-14 17:49:25,902] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (7cd3ddbc35d2, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:25,903] {docker.py:276} INFO - 21/05/14 20:49:25 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 198 ms on 7cd3ddbc35d2 (executor driver) (76/141)
[2021-05-14 17:49:25,904] {docker.py:276} INFO - 21/05/14 20:49:25 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
[2021-05-14 17:49:26,073] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
21/05/14 20:49:26 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1843 bytes result sent to driver
[2021-05-14 17:49:26,074] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (7cd3ddbc35d2, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,075] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-14 17:49:26,076] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 180 ms on 7cd3ddbc35d2 (executor driver) (77/141)
[2021-05-14 17:49:26,079] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (7cd3ddbc35d2, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,079] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1843 bytes result sent to driver
21/05/14 20:49:26 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-14 17:49:26,080] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 188 ms on 7cd3ddbc35d2 (executor driver) (78/141)
[2021-05-14 17:49:26,081] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (7cd3ddbc35d2, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,083] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 198 ms on 7cd3ddbc35d2 (executor driver) (79/141)
[2021-05-14 17:49:26,084] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-14 17:49:26,140] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1843 bytes result sent to driver
[2021-05-14 17:49:26,143] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (7cd3ddbc35d2, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,144] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 241 ms on 7cd3ddbc35d2 (executor driver) (80/141)
[2021-05-14 17:49:26,144] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-14 17:49:26,363] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1843 bytes result sent to driver
[2021-05-14 17:49:26,364] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1843 bytes result sent to driver
21/05/14 20:49:26 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-14 17:49:26,366] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (7cd3ddbc35d2, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,368] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 292 ms on 7cd3ddbc35d2 (executor driver) (81/141)
[2021-05-14 17:49:26,369] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-14 17:49:26,370] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 288 ms on 7cd3ddbc35d2 (executor driver) (82/141)
[2021-05-14 17:49:26,370] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (7cd3ddbc35d2, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,372] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-14 17:49:26,372] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (7cd3ddbc35d2, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,374] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 301 ms on 7cd3ddbc35d2 (executor driver) (83/141)
[2021-05-14 17:49:26,377] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-14 17:49:26,523] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1843 bytes result sent to driver
[2021-05-14 17:49:26,525] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (7cd3ddbc35d2, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,526] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 384 ms on 7cd3ddbc35d2 (executor driver) (84/141)
[2021-05-14 17:49:26,527] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-14 17:49:26,531] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
[2021-05-14 17:49:26,532] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (7cd3ddbc35d2, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,533] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 163 ms on 7cd3ddbc35d2 (executor driver) (85/141)
21/05/14 20:49:26 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-14 17:49:26,540] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-14 17:49:26,541] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
[2021-05-14 17:49:26,543] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (7cd3ddbc35d2, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,544] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 172 ms on 7cd3ddbc35d2 (executor driver) (86/141)
[2021-05-14 17:49:26,545] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (7cd3ddbc35d2, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,546] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-14 17:49:26,547] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-14 17:49:26,547] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 182 ms on 7cd3ddbc35d2 (executor driver) (87/141)
[2021-05-14 17:49:26,755] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1886 bytes result sent to driver
[2021-05-14 17:49:26,756] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1886 bytes result sent to driver
[2021-05-14 17:49:26,758] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (7cd3ddbc35d2, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,759] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1886 bytes result sent to driver
[2021-05-14 17:49:26,760] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1886 bytes result sent to driver
[2021-05-14 17:49:26,761] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (7cd3ddbc35d2, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,761] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-14 17:49:26,763] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 217 ms on 7cd3ddbc35d2 (executor driver) (88/141)
[2021-05-14 17:49:26,764] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 231 ms on 7cd3ddbc35d2 (executor driver) (89/141)
[2021-05-14 17:49:26,765] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 240 ms on 7cd3ddbc35d2 (executor driver) (90/141)
[2021-05-14 17:49:26,765] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (7cd3ddbc35d2, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,766] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-14 17:49:26,767] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-14 17:49:26,768] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 224 ms on 7cd3ddbc35d2 (executor driver) (91/141)
[2021-05-14 17:49:26,768] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (7cd3ddbc35d2, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,772] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-14 17:49:26,931] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1843 bytes result sent to driver
[2021-05-14 17:49:26,932] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1843 bytes result sent to driver
21/05/14 20:49:26 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1843 bytes result sent to driver
[2021-05-14 17:49:26,933] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (7cd3ddbc35d2, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:26 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1843 bytes result sent to driver
[2021-05-14 17:49:26,933] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-14 17:49:26,935] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (7cd3ddbc35d2, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,936] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-14 17:49:26,936] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (7cd3ddbc35d2, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,938] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (7cd3ddbc35d2, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:26,939] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 172 ms on 7cd3ddbc35d2 (executor driver) (92/141)
[2021-05-14 17:49:26,939] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 182 ms on 7cd3ddbc35d2 (executor driver) (93/141)
[2021-05-14 17:49:26,940] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 181 ms on 7cd3ddbc35d2 (executor driver) (94/141)
[2021-05-14 17:49:26,940] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-14 17:49:26,941] {docker.py:276} INFO - 21/05/14 20:49:26 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 178 ms on 7cd3ddbc35d2 (executor driver) (95/141)
[2021-05-14 17:49:26,942] {docker.py:276} INFO - 21/05/14 20:49:26 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-14 17:49:27,124] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-14 17:49:27,125] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (7cd3ddbc35d2, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:27 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1843 bytes result sent to driver
[2021-05-14 17:49:27,126] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 189 ms on 7cd3ddbc35d2 (executor driver) (96/141)
[2021-05-14 17:49:27,128] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-14 17:49:27,128] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1886 bytes result sent to driver
[2021-05-14 17:49:27,129] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (7cd3ddbc35d2, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,130] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
21/05/14 20:49:27 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 197 ms on 7cd3ddbc35d2 (executor driver) (97/141)
[2021-05-14 17:49:27,132] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (7cd3ddbc35d2, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,132] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 194 ms on 7cd3ddbc35d2 (executor driver) (98/141)
[2021-05-14 17:49:27,133] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1843 bytes result sent to driver
[2021-05-14 17:49:27,134] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (7cd3ddbc35d2, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,137] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
21/05/14 20:49:27 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-14 17:49:27,138] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 201 ms on 7cd3ddbc35d2 (executor driver) (99/141)
[2021-05-14 17:49:27,415] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-14 17:49:27,417] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-14 17:49:27,417] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-14 17:49:27,418] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (7cd3ddbc35d2, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,419] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-14 17:49:27,420] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (7cd3ddbc35d2, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,421] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-14 17:49:27,422] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-14 17:49:27,423] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (7cd3ddbc35d2, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,424] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 293 ms on 7cd3ddbc35d2 (executor driver) (100/141)
[2021-05-14 17:49:27,425] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 301 ms on 7cd3ddbc35d2 (executor driver) (101/141)
[2021-05-14 17:49:27,425] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 291 ms on 7cd3ddbc35d2 (executor driver) (102/141)
[2021-05-14 17:49:27,426] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-14 17:49:27,427] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (7cd3ddbc35d2, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,428] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 301 ms on 7cd3ddbc35d2 (executor driver) (103/141)
[2021-05-14 17:49:27,429] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-14 17:49:27,593] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1886 bytes result sent to driver
21/05/14 20:49:27 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1886 bytes result sent to driver
[2021-05-14 17:49:27,594] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (7cd3ddbc35d2, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,596] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (7cd3ddbc35d2, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,596] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-14 17:49:27,597] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1886 bytes result sent to driver
[2021-05-14 17:49:27,598] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 176 ms on 7cd3ddbc35d2 (executor driver) (104/141)
[2021-05-14 17:49:27,598] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 180 ms on 7cd3ddbc35d2 (executor driver) (105/141)
[2021-05-14 17:49:27,599] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-14 17:49:27,600] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (7cd3ddbc35d2, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,600] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 175 ms on 7cd3ddbc35d2 (executor driver) (106/141)
[2021-05-14 17:49:27,601] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-14 17:49:27,608] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1886 bytes result sent to driver
[2021-05-14 17:49:27,610] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (7cd3ddbc35d2, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,611] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-14 17:49:27,611] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 192 ms on 7cd3ddbc35d2 (executor driver) (107/141)
[2021-05-14 17:49:27,807] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1843 bytes result sent to driver
21/05/14 20:49:27 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1843 bytes result sent to driver
[2021-05-14 17:49:27,808] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1843 bytes result sent to driver
21/05/14 20:49:27 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1843 bytes result sent to driver
[2021-05-14 17:49:27,809] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (7cd3ddbc35d2, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,811] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (7cd3ddbc35d2, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,812] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-14 17:49:27,813] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-14 17:49:27,813] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (7cd3ddbc35d2, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,814] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 218 ms on 7cd3ddbc35d2 (executor driver) (108/141)
[2021-05-14 17:49:27,815] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 220 ms on 7cd3ddbc35d2 (executor driver) (109/141)
[2021-05-14 17:49:27,815] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (7cd3ddbc35d2, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,816] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 217 ms on 7cd3ddbc35d2 (executor driver) (110/141)
[2021-05-14 17:49:27,817] {docker.py:276} INFO - 21/05/14 20:49:27 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 207 ms on 7cd3ddbc35d2 (executor driver) (111/141)
[2021-05-14 17:49:27,818] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-14 17:49:27,821] {docker.py:276} INFO - 21/05/14 20:49:27 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-14 17:49:27,981] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1843 bytes result sent to driver
[2021-05-14 17:49:27,983] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (7cd3ddbc35d2, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 20:49:28 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-14 17:49:27,985] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1843 bytes result sent to driver
[2021-05-14 17:49:27,986] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (7cd3ddbc35d2, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,987] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 176 ms on 7cd3ddbc35d2 (executor driver) (112/141)
21/05/14 20:49:28 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-14 17:49:27,988] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-14 17:49:27,989] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (7cd3ddbc35d2, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,990] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 179 ms on 7cd3ddbc35d2 (executor driver) (113/141)
[2021-05-14 17:49:27,991] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 176 ms on 7cd3ddbc35d2 (executor driver) (114/141)
[2021-05-14 17:49:27,992] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-14 17:49:27,994] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1843 bytes result sent to driver
[2021-05-14 17:49:27,995] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (7cd3ddbc35d2, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:27,996] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 188 ms on 7cd3ddbc35d2 (executor driver) (115/141)
[2021-05-14 17:49:27,997] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-14 17:49:28,153] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-14 17:49:28,153] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (7cd3ddbc35d2, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,154] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 167 ms on 7cd3ddbc35d2 (executor driver) (116/141)
[2021-05-14 17:49:28,155] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-14 17:49:28,157] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (7cd3ddbc35d2, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,158] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-14 17:49:28,158] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
21/05/14 20:49:28 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 175 ms on 7cd3ddbc35d2 (executor driver) (117/141)
[2021-05-14 17:49:28,168] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-14 17:49:28,170] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (7cd3ddbc35d2, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,171] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 185 ms on 7cd3ddbc35d2 (executor driver) (118/141)
[2021-05-14 17:49:28,174] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1886 bytes result sent to driver
[2021-05-14 17:49:28,176] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (7cd3ddbc35d2, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,176] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 182 ms on 7cd3ddbc35d2 (executor driver) (119/141)
[2021-05-14 17:49:28,177] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-14 17:49:28,179] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-14 17:49:28,462] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1886 bytes result sent to driver
21/05/14 20:49:28 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1886 bytes result sent to driver
[2021-05-14 17:49:28,465] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (7cd3ddbc35d2, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,466] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
[2021-05-14 17:49:28,466] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 308 ms on 7cd3ddbc35d2 (executor driver) (120/141)
[2021-05-14 17:49:28,467] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (7cd3ddbc35d2, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,467] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1843 bytes result sent to driver
[2021-05-14 17:49:28,468] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-14 17:49:28,468] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (7cd3ddbc35d2, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,470] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 294 ms on 7cd3ddbc35d2 (executor driver) (121/141)
[2021-05-14 17:49:28,471] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-14 17:49:28,472] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (7cd3ddbc35d2, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,472] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 320 ms on 7cd3ddbc35d2 (executor driver) (122/141)
21/05/14 20:49:28 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 303 ms on 7cd3ddbc35d2 (executor driver) (123/141)
[2021-05-14 17:49:28,473] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
21/05/14 20:49:28 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-14 17:49:28,637] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1843 bytes result sent to driver
[2021-05-14 17:49:28,639] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-14 17:49:28,641] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1843 bytes result sent to driver
[2021-05-14 17:49:28,642] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (7cd3ddbc35d2, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,645] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1843 bytes result sent to driver
[2021-05-14 17:49:28,645] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 178 ms on 7cd3ddbc35d2 (executor driver) (124/141)
21/05/14 20:49:28 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-14 17:49:28,645] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (7cd3ddbc35d2, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,648] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (7cd3ddbc35d2, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,648] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-14 17:49:28,649] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-14 17:49:28,650] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (7cd3ddbc35d2, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,651] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 187 ms on 7cd3ddbc35d2 (executor driver) (125/141)
21/05/14 20:49:28 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-14 17:49:28,651] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 183 ms on 7cd3ddbc35d2 (executor driver) (126/141)
[2021-05-14 17:49:28,652] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 182 ms on 7cd3ddbc35d2 (executor driver) (127/141)
[2021-05-14 17:49:28,865] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1843 bytes result sent to driver
[2021-05-14 17:49:28,885] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (7cd3ddbc35d2, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,886] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-14 17:49:28,886] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 227 ms on 7cd3ddbc35d2 (executor driver) (128/141)
[2021-05-14 17:49:28,886] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-14 17:49:28,886] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (7cd3ddbc35d2, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,887] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 221 ms on 7cd3ddbc35d2 (executor driver) (129/141)
[2021-05-14 17:49:28,887] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-14 17:49:28,887] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1843 bytes result sent to driver
[2021-05-14 17:49:28,887] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1843 bytes result sent to driver
[2021-05-14 17:49:28,888] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (7cd3ddbc35d2, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,888] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 226 ms on 7cd3ddbc35d2 (executor driver) (130/141)
[2021-05-14 17:49:28,888] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-14 17:49:28,889] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (7cd3ddbc35d2, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:28,889] {docker.py:276} INFO - 21/05/14 20:49:28 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 231 ms on 7cd3ddbc35d2 (executor driver) (131/141)
[2021-05-14 17:49:28,889] {docker.py:276} INFO - 21/05/14 20:49:28 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-14 17:49:29,038] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1886 bytes result sent to driver
[2021-05-14 17:49:29,040] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (7cd3ddbc35d2, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,042] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 179 ms on 7cd3ddbc35d2 (executor driver) (132/141)
[2021-05-14 17:49:29,042] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-14 17:49:29,045] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1886 bytes result sent to driver
[2021-05-14 17:49:29,047] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (7cd3ddbc35d2, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,048] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 180 ms on 7cd3ddbc35d2 (executor driver) (133/141)
[2021-05-14 17:49:29,049] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-14 17:49:29,049] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1886 bytes result sent to driver
[2021-05-14 17:49:29,050] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1886 bytes result sent to driver
21/05/14 20:49:29 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (7cd3ddbc35d2, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,052] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-14 17:49:29,052] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 180 ms on 7cd3ddbc35d2 (executor driver) (134/141)
[2021-05-14 17:49:29,055] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (7cd3ddbc35d2, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,056] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 180 ms on 7cd3ddbc35d2 (executor driver) (135/141)
[2021-05-14 17:49:29,056] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-14 17:49:29,220] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1843 bytes result sent to driver
[2021-05-14 17:49:29,222] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-14 17:49:29,223] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1843 bytes result sent to driver
[2021-05-14 17:49:29,224] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (7cd3ddbc35d2, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,225] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 185 ms on 7cd3ddbc35d2 (executor driver) (136/141)
[2021-05-14 17:49:29,226] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (7cd3ddbc35d2, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:29,227] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-14 17:49:29,228] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 177 ms on 7cd3ddbc35d2 (executor driver) (137/141)
[2021-05-14 17:49:29,229] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 176 ms on 7cd3ddbc35d2 (executor driver) (138/141)
21/05/14 20:49:29 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-14 17:49:29,231] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1843 bytes result sent to driver
[2021-05-14 17:49:29,232] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 187 ms on 7cd3ddbc35d2 (executor driver) (139/141)
[2021-05-14 17:49:29,508] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-14 17:49:29,510] {docker.py:276} INFO - 21/05/14 20:49:29 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
[2021-05-14 17:49:29,511] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 285 ms on 7cd3ddbc35d2 (executor driver) (140/141)
[2021-05-14 17:49:29,512] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 290 ms on 7cd3ddbc35d2 (executor driver) (141/141)
[2021-05-14 17:49:29,513] {docker.py:276} INFO - 21/05/14 20:49:29 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 7.758 s
[2021-05-14 17:49:29,514] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 17:49:29,514] {docker.py:276} INFO - 21/05/14 20:49:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 17:49:29,514] {docker.py:276} INFO - 21/05/14 20:49:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-14 17:49:29,515] {docker.py:276} INFO - 21/05/14 20:49:29 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 7.772264 s
[2021-05-14 17:49:29,532] {docker.py:276} INFO - 21/05/14 20:49:29 INFO InMemoryFileIndex: It took 7833 ms to list leaf files for 141 paths.
[2021-05-14 17:49:32,077] {docker.py:276} INFO - 21/05/14 20:49:32 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 17:49:32,083] {docker.py:276} INFO - 21/05/14 20:49:32 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 17:49:32,087] {docker.py:276} INFO - 21/05/14 20:49:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 17:49:32,239] {docker.py:276} INFO - 21/05/14 20:49:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7cd3ddbc35d2:39701 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 17:49:32,587] {docker.py:276} INFO - 21/05/14 20:49:32 INFO CodeGenerator: Code generated in 266.9705 ms
[2021-05-14 17:49:32,599] {docker.py:276} INFO - 21/05/14 20:49:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-14 17:49:32,622] {docker.py:276} INFO - 21/05/14 20:49:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 17:49:32,623] {docker.py:276} INFO - 21/05/14 20:49:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7cd3ddbc35d2:39701 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 17:49:32,625] {docker.py:276} INFO - 21/05/14 20:49:32 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:32,644] {docker.py:276} INFO - 21/05/14 20:49:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 17:49:32,762] {docker.py:276} INFO - 21/05/14 20:49:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:32,764] {docker.py:276} INFO - 21/05/14 20:49:32 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/14 20:49:32 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 20:49:32 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 17:49:32,765] {docker.py:276} INFO - 21/05/14 20:49:32 INFO DAGScheduler: Missing parents: List()
[2021-05-14 17:49:32,766] {docker.py:276} INFO - 21/05/14 20:49:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 17:49:32,801] {docker.py:276} INFO - 21/05/14 20:49:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 17:49:32,826] {docker.py:276} INFO - 21/05/14 20:49:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 17:49:32,828] {docker.py:276} INFO - 21/05/14 20:49:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7cd3ddbc35d2:39701 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 17:49:32,828] {docker.py:276} INFO - 21/05/14 20:49:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-14 17:49:32,831] {docker.py:276} INFO - 21/05/14 20:49:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-14 17:49:32,831] {docker.py:276} INFO - 21/05/14 20:49:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-14 17:49:32,834] {docker.py:276} INFO - 21/05/14 20:49:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (7cd3ddbc35d2, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:32,835] {docker.py:276} INFO - 21/05/14 20:49:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-14 17:49:32,936] {docker.py:276} INFO - 21/05/14 20:49:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, range: 0-111646, partition values: [empty row]
[2021-05-14 17:49:32,966] {docker.py:276} INFO - 21/05/14 20:49:32 INFO CodeGenerator: Code generated in 20.878 ms
[2021-05-14 17:49:33,340] {docker.py:276} INFO - 21/05/14 20:49:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1607 bytes result sent to driver
[2021-05-14 17:49:33,342] {docker.py:276} INFO - 21/05/14 20:49:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 511 ms on 7cd3ddbc35d2 (executor driver) (1/1)
21/05/14 20:49:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 17:49:33,343] {docker.py:276} INFO - 21/05/14 20:49:33 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.573 s
[2021-05-14 17:49:33,343] {docker.py:276} INFO - 21/05/14 20:49:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 20:49:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 17:49:33,344] {docker.py:276} INFO - 21/05/14 20:49:33 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.581605 s
[2021-05-14 17:49:33,374] {docker.py:276} INFO - 21/05/14 20:49:33 INFO CodeGenerator: Code generated in 14.2023 ms
[2021-05-14 17:49:33,442] {docker.py:276} INFO - 21/05/14 20:49:33 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 20:49:33 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 17:49:33,443] {docker.py:276} INFO - 21/05/14 20:49:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 17:49:33,449] {docker.py:276} INFO - 21/05/14 20:49:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-14 17:49:33,469] {docker.py:276} INFO - 21/05/14 20:49:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 7cd3ddbc35d2:39701 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 17:49:33,469] {docker.py:276} INFO - 21/05/14 20:49:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 17:49:33,470] {docker.py:276} INFO - 21/05/14 20:49:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 7cd3ddbc35d2:39701 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 17:49:33,471] {docker.py:276} INFO - 21/05/14 20:49:33 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:33,476] {docker.py:276} INFO - 21/05/14 20:49:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 17:49:34,066] {docker.py:276} INFO - 21/05/14 20:49:34 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 17:49:34,066] {docker.py:276} INFO - 21/05/14 20:49:34 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 17:49:34,067] {docker.py:276} INFO - 21/05/14 20:49:34 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 17:49:34,689] {docker.py:276} INFO - 21/05/14 20:49:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:34,692] {docker.py:276} INFO - 21/05/14 20:49:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:34,692] {docker.py:276} INFO - 21/05/14 20:49:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049341510091530566747650_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049341510091530566747650_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049341510091530566747650_0000}; taskId=attempt_202105142049341510091530566747650_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42a7999a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:34,694] {docker.py:276} INFO - 21/05/14 20:49:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:34,727] {docker.py:276} INFO - 21/05/14 20:49:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 17:49:34,832] {docker.py:276} INFO - 21/05/14 20:49:34 INFO CodeGenerator: Code generated in 69.7909 ms
[2021-05-14 17:49:34,834] {docker.py:276} INFO - 21/05/14 20:49:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 17:49:34,900] {docker.py:276} INFO - 21/05/14 20:49:34 INFO CodeGenerator: Code generated in 55.392 ms
[2021-05-14 17:49:34,904] {docker.py:276} INFO - 21/05/14 20:49:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-14 17:49:34,912] {docker.py:276} INFO - 21/05/14 20:49:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-14 17:49:34,914] {docker.py:276} INFO - 21/05/14 20:49:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 7cd3ddbc35d2:39701 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 17:49:34,916] {docker.py:276} INFO - 21/05/14 20:49:34 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:34,931] {docker.py:276} INFO - 21/05/14 20:49:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 17:49:35,005] {docker.py:276} INFO - 21/05/14 20:49:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 7cd3ddbc35d2:39701 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 17:49:35,127] {docker.py:276} INFO - 21/05/14 20:49:35 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 17:49:35,133] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 17:49:35,142] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2021-05-14 17:49:35,142] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 17:49:35,143] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-14 17:49:35,151] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-14 17:49:35,152] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 17:49:35,167] {docker.py:276} INFO - 21/05/14 20:49:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-14 17:49:35,203] {docker.py:276} INFO - 21/05/14 20:49:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-14 17:49:35,204] {docker.py:276} INFO - 21/05/14 20:49:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 7cd3ddbc35d2:39701 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-14 17:49:35,205] {docker.py:276} INFO - 21/05/14 20:49:35 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-14 17:49:35,207] {docker.py:276} INFO - 21/05/14 20:49:35 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2021-05-14 17:49:35,208] {docker.py:276} INFO - 21/05/14 20:49:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-14 17:49:35,213] {docker.py:276} INFO - 21/05/14 20:49:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (7cd3ddbc35d2, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:35,214] {docker.py:276} INFO - 21/05/14 20:49:35 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (7cd3ddbc35d2, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:35,215] {docker.py:276} INFO - 21/05/14 20:49:35 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (7cd3ddbc35d2, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:35,215] {docker.py:276} INFO - 21/05/14 20:49:35 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (7cd3ddbc35d2, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:35,216] {docker.py:276} INFO - 21/05/14 20:49:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-14 17:49:35,217] {docker.py:276} INFO - 21/05/14 20:49:35 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-14 17:49:35,217] {docker.py:276} INFO - 21/05/14 20:49:35 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-14 17:49:35,218] {docker.py:276} INFO - 21/05/14 20:49:35 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-14 17:49:35,330] {docker.py:276} INFO - 21/05/14 20:49:35 INFO CodeGenerator: Code generated in 31.6895 ms
[2021-05-14 17:49:35,363] {docker.py:276} INFO - 21/05/14 20:49:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 7cd3ddbc35d2:39701 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 17:49:35,402] {docker.py:276} INFO - 21/05/14 20:49:35 INFO CodeGenerator: Code generated in 23.8021 ms
[2021-05-14 17:49:35,432] {docker.py:276} INFO - 21/05/14 20:49:35 INFO CodeGenerator: Code generated in 20.0897 ms
[2021-05-14 17:49:35,458] {docker.py:276} INFO - 21/05/14 20:49:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620955095_to_1620956895.csv, range: 0-104003, partition values: [empty row]
[2021-05-14 17:49:35,463] {docker.py:276} INFO - 21/05/14 20:49:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621012695_to_1621014495.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 17:49:35,463] {docker.py:276} INFO - 21/05/14 20:49:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621012695_to_1621014495.csv, range: 0-104190, partition values: [empty row]
[2021-05-14 17:49:35,468] {docker.py:276} INFO - 21/05/14 20:49:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, range: 0-111646, partition values: [empty row]
[2021-05-14 17:49:36,115] {docker.py:276} INFO - 21/05/14 20:49:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620982095_to_1620983895.csv, range: 0-103984, partition values: [empty row]
[2021-05-14 17:49:36,484] {docker.py:276} INFO - 21/05/14 20:49:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620964095_to_1620965895.csv, range: 0-103983, partition values: [empty row]
[2021-05-14 17:49:36,639] {docker.py:276} INFO - 21/05/14 20:49:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620976695_to_1620978495.csv, range: 0-104189, partition values: [empty row]
[2021-05-14 17:49:36,784] {docker.py:276} INFO - 21/05/14 20:49:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620940695_to_1620942495.csv, range: 0-111631, partition values: [empty row]
[2021-05-14 17:49:36,933] {docker.py:276} INFO - 21/05/14 20:49:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621007295_to_1621009095.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 17:49:37,117] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621005495_to_1621007295.csv, range: 0-104189, partition values: [empty row]
[2021-05-14 17:49:37,156] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620944295_to_1620946095.csv, range: 0-111623, partition values: [empty row]
[2021-05-14 17:49:37,321] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621009095_to_1621010895.csv, range: 0-104247, partition values: [empty row]
[2021-05-14 17:49:37,489] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620998295_to_1621000095.csv, range: 0-104182, partition values: [empty row]
[2021-05-14 17:49:37,645] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620942495_to_1620944295.csv, range: 0-111599, partition values: [empty row]
[2021-05-14 17:49:37,705] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620994695_to_1620996495.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 17:49:37,859] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620956895_to_1620958695.csv, range: 0-104179, partition values: [empty row]
[2021-05-14 17:49:37,933] {docker.py:276} INFO - 21/05/14 20:49:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620987495_to_1620989295.csv, range: 0-103983, partition values: [empty row]
[2021-05-14 17:49:38,019] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621021695_to_1621023495.csv, range: 0-108702, partition values: [empty row]
[2021-05-14 17:49:38,072] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620946095_to_1620947895.csv, range: 0-104242, partition values: [empty row]
[2021-05-14 17:49:38,226] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620991095_to_1620992895.csv, range: 0-104173, partition values: [empty row]
[2021-05-14 17:49:38,456] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620962295_to_1620964095.csv, range: 0-104242, partition values: [empty row]
[2021-05-14 17:49:38,457] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621009095_to_1621010895.csv, range: 0-103983, partition values: [empty row]
[2021-05-14 17:49:38,483] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620946095_to_1620947895.csv, range: 0-107207, partition values: [empty row]
[2021-05-14 17:49:38,623] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620964095_to_1620965895.csv, range: 0-104170, partition values: [empty row]
[2021-05-14 17:49:38,811] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621010895_to_1621012695.csv, range: 0-104241, partition values: [empty row]
[2021-05-14 17:49:38,818] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620940695_to_1620942495.csv, range: 0-104385, partition values: [empty row]
[2021-05-14 17:49:38,829] {docker.py:276} INFO - 21/05/14 20:49:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620949695_to_1620951495.csv, range: 0-103964, partition values: [empty row]
[2021-05-14 17:49:39,029] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621014495_to_1621016295.csv, range: 0-104169, partition values: [empty row]
[2021-05-14 17:49:39,150] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621003695_to_1621005495.csv, range: 0-104239, partition values: [empty row]
[2021-05-14 17:49:39,160] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621014495_to_1621016295.csv, range: 0-104380, partition values: [empty row]
[2021-05-14 17:49:39,282] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620951495_to_1620953295.csv, range: 0-103947, partition values: [empty row]
[2021-05-14 17:49:39,507] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620971295_to_1620973095.csv, range: 0-104238, partition values: [empty row]
[2021-05-14 17:49:39,525] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620955095_to_1620956895.csv, range: 0-104355, partition values: [empty row]
[2021-05-14 17:49:39,649] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620965895_to_1620967695.csv, range: 0-103943, partition values: [empty row]
[2021-05-14 17:49:39,673] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620965895_to_1620967695.csv, range: 0-104165, partition values: [empty row]
[2021-05-14 17:49:39,847] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620994695_to_1620996495.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 17:49:39,866] {docker.py:276} INFO - 21/05/14 20:49:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621016295_to_1621018095.csv, range: 0-104354, partition values: [empty row]
[2021-05-14 17:49:40,032] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621018095_to_1621019895.csv, range: 0-104162, partition values: [empty row]
[2021-05-14 17:49:40,097] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621010895_to_1621012695.csv, range: 0-103942, partition values: [empty row]
[2021-05-14 17:49:40,181] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620978495_to_1620980295.csv, range: 0-104234, partition values: [empty row]
[2021-05-14 17:49:40,222] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620962295_to_1620964095.csv, range: 0-104331, partition values: [empty row]
[2021-05-14 17:49:40,382] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620944295_to_1620946095.csv, range: 0-104150, partition values: [empty row]
[2021-05-14 17:49:40,450] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621012695_to_1621014495.csv, range: 0-103933, partition values: [empty row]
[2021-05-14 17:49:40,510] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621001895_to_1621003695.csv, range: 0-104234, partition values: [empty row]
[2021-05-14 17:49:40,570] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620992895_to_1620994695.csv, range: 0-104323, partition values: [empty row]
[2021-05-14 17:49:40,728] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620951495_to_1620953295.csv, range: 0-104149, partition values: [empty row]
[2021-05-14 17:49:40,812] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620958695_to_1620960495.csv, range: 0-103931, partition values: [empty row]
[2021-05-14 17:49:40,838] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620974895_to_1620976695.csv, range: 0-104229, partition values: [empty row]
[2021-05-14 17:49:40,940] {docker.py:276} INFO - 21/05/14 20:49:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620958695_to_1620960495.csv, range: 0-104308, partition values: [empty row]
[2021-05-14 17:49:41,077] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621003695_to_1621005495.csv, range: 0-104146, partition values: [empty row]
[2021-05-14 17:49:41,163] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621003695_to_1621005495.csv, range: 0-103926, partition values: [empty row]
[2021-05-14 17:49:41,178] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620967695_to_1620969495.csv, range: 0-104227, partition values: [empty row]
[2021-05-14 17:49:41,293] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620965895_to_1620967695.csv, range: 0-104302, partition values: [empty row]
[2021-05-14 17:49:41,412] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620976695_to_1620978495.csv, range: 0-104145, partition values: [empty row]
[2021-05-14 17:49:41,504] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620980295_to_1620982095.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 17:49:41,524] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620947895_to_1620949695.csv, range: 0-103920, partition values: [empty row]
[2021-05-14 17:49:41,641] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620985695_to_1620987495.csv, range: 0-104299, partition values: [empty row]
[2021-05-14 17:49:41,760] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620991095_to_1620992895.csv, range: 0-104144, partition values: [empty row]
[2021-05-14 17:49:41,860] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620987495_to_1620989295.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 17:49:41,878] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620955095_to_1620956895.csv, range: 0-103875, partition values: [empty row]
[2021-05-14 17:49:41,993] {docker.py:276} INFO - 21/05/14 20:49:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620960495_to_1620962295.csv, range: 0-104296, partition values: [empty row]
[2021-05-14 17:49:42,098] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621010895_to_1621012695.csv, range: 0-104144, partition values: [empty row]
[2021-05-14 17:49:42,198] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620942495_to_1620944295.csv, range: 0-104223, partition values: [empty row]
[2021-05-14 17:49:42,236] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620956895_to_1620958695.csv, range: 0-103872, partition values: [empty row]
[2021-05-14 17:49:42,346] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621016295_to_1621018095.csv, range: 0-104288, partition values: [empty row]
[2021-05-14 17:49:42,430] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621005495_to_1621007295.csv, range: 0-104125, partition values: [empty row]
[2021-05-14 17:49:42,535] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620973095_to_1620974895.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 17:49:42,604] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620962295_to_1620964095.csv, range: 0-103872, partition values: [empty row]
[2021-05-14 17:49:42,699] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620982095_to_1620983895.csv, range: 0-104283, partition values: [empty row]
[2021-05-14 17:49:42,786] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621000095_to_1621001895.csv, range: 0-104122, partition values: [empty row]
[2021-05-14 17:49:42,874] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621018095_to_1621019895.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 17:49:42,948] {docker.py:276} INFO - 21/05/14 20:49:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620951495_to_1620953295.csv, range: 0-103868, partition values: [empty row]
[2021-05-14 17:49:43,052] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620971295_to_1620973095.csv, range: 0-104272, partition values: [empty row]
[2021-05-14 17:49:43,137] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, range: 0-104116, partition values: [empty row]
[2021-05-14 17:49:43,201] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620989295_to_1620991095.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 17:49:43,302] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620967695_to_1620969495.csv, range: 0-103853, partition values: [empty row]
[2021-05-14 17:49:43,405] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620998295_to_1621000095.csv, range: 0-104271, partition values: [empty row]
[2021-05-14 17:49:43,477] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620953295_to_1620955095.csv, range: 0-104110, partition values: [empty row]
[2021-05-14 17:49:43,536] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620958695_to_1620960495.csv, range: 0-104217, partition values: [empty row]
[2021-05-14 17:49:43,659] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620991095_to_1620992895.csv, range: 0-103853, partition values: [empty row]
[2021-05-14 17:49:43,777] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620983895_to_1620985695.csv, range: 0-104270, partition values: [empty row]
[2021-05-14 17:49:43,866] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620974895_to_1620976695.csv, range: 0-104213, partition values: [empty row]
[2021-05-14 17:49:43,889] {docker.py:276} INFO - 21/05/14 20:49:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621009095_to_1621010895.csv, range: 0-104098, partition values: [empty row]
[2021-05-14 17:49:44,010] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620989295_to_1620991095.csv, range: 0-103844, partition values: [empty row]
[2021-05-14 17:49:44,132] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621007295_to_1621009095.csv, range: 0-104268, partition values: [empty row]
[2021-05-14 17:49:44,205] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620980295_to_1620982095.csv, range: 0-104209, partition values: [empty row]
[2021-05-14 17:49:44,250] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620996495_to_1620998295.csv, range: 0-104097, partition values: [empty row]
[2021-05-14 17:49:44,358] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620953295_to_1620955095.csv, range: 0-103836, partition values: [empty row]
[2021-05-14 17:49:44,492] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620953295_to_1620955095.csv, range: 0-104268, partition values: [empty row]
[2021-05-14 17:49:44,552] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620964095_to_1620965895.csv, range: 0-104208, partition values: [empty row]
[2021-05-14 17:49:44,581] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620980295_to_1620982095.csv, range: 0-104092, partition values: [empty row]
[2021-05-14 17:49:44,727] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620960495_to_1620962295.csv, range: 0-103833, partition values: [empty row]
[2021-05-14 17:49:44,844] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620969495_to_1620971295.csv, range: 0-104266, partition values: [empty row]
[2021-05-14 17:49:44,901] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620978495_to_1620980295.csv, range: 0-104205, partition values: [empty row]
[2021-05-14 17:49:44,965] {docker.py:276} INFO - 21/05/14 20:49:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620967695_to_1620969495.csv, range: 0-104072, partition values: [empty row]
[2021-05-14 17:49:45,145] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621005495_to_1621007295.csv, range: 0-103833, partition values: [empty row]
[2021-05-14 17:49:45,191] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620969495_to_1620971295.csv, range: 0-104264, partition values: [empty row]
[2021-05-14 17:49:45,231] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620996495_to_1620998295.csv, range: 0-104202, partition values: [empty row]
[2021-05-14 17:49:45,319] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620978495_to_1620980295.csv, range: 0-104066, partition values: [empty row]
[2021-05-14 17:49:45,530] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620942495_to_1620944295.csv, range: 0-103827, partition values: [empty row]
[2021-05-14 17:49:45,536] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620992895_to_1620994695.csv, range: 0-104263, partition values: [empty row]
[2021-05-14 17:49:45,563] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620985695_to_1620987495.csv, range: 0-104198, partition values: [empty row]
[2021-05-14 17:49:45,659] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621021695_to_1621023495.csv, range: 0-104063, partition values: [empty row]
[2021-05-14 17:49:45,913] {docker.py:276} INFO - 21/05/14 20:49:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620989295_to_1620991095.csv, range: 0-104195, partition values: [empty row]
[2021-05-14 17:49:46,026] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620949695_to_1620951495.csv, range: 0-104055, partition values: [empty row]
[2021-05-14 17:49:46,204] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621000095_to_1621001895.csv, range: 0-103824, partition values: [empty row]
[2021-05-14 17:49:46,207] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620956895_to_1620958695.csv, range: 0-104262, partition values: [empty row]
[2021-05-14 17:49:46,257] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620960495_to_1620962295.csv, range: 0-104192, partition values: [empty row]
[2021-05-14 17:49:46,359] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620940695_to_1620942495.csv, range: 0-104050, partition values: [empty row]
[2021-05-14 17:49:46,551] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620973095_to_1620974895.csv, range: 0-104261, partition values: [empty row]
[2021-05-14 17:49:46,602] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620974895_to_1620976695.csv, range: 0-103819, partition values: [empty row]
[2021-05-14 17:49:46,619] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621000095_to_1621001895.csv, range: 0-104192, partition values: [empty row]
[2021-05-14 17:49:46,692] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1621019895_to_1621021695.csv, range: 0-104048, partition values: [empty row]
[2021-05-14 17:49:46,901] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620983895_to_1620985695.csv, range: 0-104255, partition values: [empty row]
[2021-05-14 17:49:46,929] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620969495_to_1620971295.csv, range: 0-103813, partition values: [empty row]
[2021-05-14 17:49:46,974] {docker.py:276} INFO - 21/05/14 20:49:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620982095_to_1620983895.csv, range: 0-104191, partition values: [empty row]
[2021-05-14 17:49:47,037] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620971295_to_1620973095.csv, range: 0-104040, partition values: [empty row]
[2021-05-14 17:49:47,256] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621019895_to_1621021695.csv, range: 0-104253, partition values: [empty row]
[2021-05-14 17:49:47,316] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620987495_to_1620989295.csv, range: 0-104191, partition values: [empty row]
[2021-05-14 17:49:47,396] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620998295_to_1621000095.csv, range: 0-103810, partition values: [empty row]
[2021-05-14 17:49:47,442] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620938895_to_1620940695.csv, range: 0-104029, partition values: [empty row]
[2021-05-14 17:49:47,611] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620947895_to_1620949695.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 17:49:47,785] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621001895_to_1621003695.csv, range: 0-103800, partition values: [empty row]
[2021-05-14 17:49:47,791] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620994695_to_1620996495.csv, range: 0-104018, partition values: [empty row]
[2021-05-14 17:49:47,963] {docker.py:276} INFO - 21/05/14 20:49:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621001895_to_1621003695.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 17:49:48,027] {docker.py:276} INFO - 21/05/14 20:49:48 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2722 bytes result sent to driver
[2021-05-14 17:49:48,027] {docker.py:276} INFO - 21/05/14 20:49:48 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (7cd3ddbc35d2, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:48,028] {docker.py:276} INFO - 21/05/14 20:49:48 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-14 17:49:48,030] {docker.py:276} INFO - 21/05/14 20:49:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 12797 ms on 7cd3ddbc35d2 (executor driver) (1/5)
[2021-05-14 17:49:48,045] {docker.py:276} INFO - 21/05/14 20:49:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620983895_to_1620985695.csv, range: 0-103768, partition values: [empty row]
[2021-05-14 17:49:48,143] {docker.py:276} INFO - 21/05/14 20:49:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1621021695_to_1621023495.csv, range: 0-103786, partition values: [empty row]
[2021-05-14 17:49:48,410] {docker.py:276} INFO - 21/05/14 20:49:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620985695_to_1620987495.csv, range: 0-103759, partition values: [empty row]
[2021-05-14 17:49:48,440] {docker.py:276} INFO - 21/05/14 20:49:48 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-14 17:49:48,441] {docker.py:276} INFO - 21/05/14 20:49:48 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 13208 ms on 7cd3ddbc35d2 (executor driver) (2/5)
[2021-05-14 17:49:48,476] {docker.py:276} INFO - 21/05/14 20:49:48 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2679 bytes result sent to driver
[2021-05-14 17:49:48,477] {docker.py:276} INFO - 21/05/14 20:49:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 13246 ms on 7cd3ddbc35d2 (executor driver) (3/5)
[2021-05-14 17:49:48,489] {docker.py:276} INFO - 21/05/14 20:49:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620976695_to_1620978495.csv, range: 0-103769, partition values: [empty row]
[2021-05-14 17:49:48,775] {docker.py:276} INFO - 21/05/14 20:49:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620973095_to_1620974895.csv, range: 0-103752, partition values: [empty row]
[2021-05-14 17:49:49,177] {docker.py:276} INFO - 21/05/14 20:49:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620992895_to_1620994695.csv, range: 0-103769, partition values: [empty row]
[2021-05-14 17:49:49,657] {docker.py:276} INFO - 21/05/14 20:49:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621007295_to_1621009095.csv, range: 0-103746, partition values: [empty row]
[2021-05-14 17:49:49,824] {docker.py:276} INFO - 21/05/14 20:49:49 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2679 bytes result sent to driver
[2021-05-14 17:49:49,826] {docker.py:276} INFO - 21/05/14 20:49:49 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 14593 ms on 7cd3ddbc35d2 (executor driver) (4/5)
[2021-05-14 17:49:50,545] {docker.py:276} INFO - 21/05/14 20:49:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621014495_to_1621016295.csv, range: 0-103729, partition values: [empty row]
[2021-05-14 17:49:51,100] {docker.py:276} INFO - 21/05/14 20:49:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_17_48_15/from_1620947895_to_1620949695.csv, range: 0-103721, partition values: [empty row]
[2021-05-14 17:49:51,545] {docker.py:276} INFO - 21/05/14 20:49:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621018095_to_1621019895.csv, range: 0-103717, partition values: [empty row]
[2021-05-14 17:49:52,096] {docker.py:276} INFO - 21/05/14 20:49:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621019895_to_1621021695.csv, range: 0-103702, partition values: [empty row]
[2021-05-14 17:49:52,595] {docker.py:276} INFO - 21/05/14 20:49:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620949695_to_1620951495.csv, range: 0-103700, partition values: [empty row]
[2021-05-14 17:49:53,116] {docker.py:276} INFO - 21/05/14 20:49:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1621016295_to_1621018095.csv, range: 0-103697, partition values: [empty row]
[2021-05-14 17:49:53,663] {docker.py:276} INFO - 21/05/14 20:49:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620944295_to_1620946095.csv, range: 0-103677, partition values: [empty row]
[2021-05-14 17:49:54,162] {docker.py:276} INFO - 21/05/14 20:49:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_17_48_15/from_1620996495_to_1620998295.csv, range: 0-103628, partition values: [empty row]
[2021-05-14 17:49:54,687] {docker.py:276} INFO - 21/05/14 20:49:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_17_48_15/from_1620946095_to_1620947895.csv, range: 0-103265, partition values: [empty row]
[2021-05-14 17:49:55,346] {docker.py:276} INFO - 21/05/14 20:49:55 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-14 17:49:55,348] {docker.py:276} INFO - 21/05/14 20:49:55 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 7329 ms on 7cd3ddbc35d2 (executor driver) (5/5)
21/05/14 20:49:55 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-14 17:49:55,349] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 20.181 s
[2021-05-14 17:49:55,350] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 17:49:55,351] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: running: Set()
[2021-05-14 17:49:55,353] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-14 17:49:55,354] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: failed: Set()
[2021-05-14 17:49:55,358] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 17:49:55,400] {docker.py:276} INFO - 21/05/14 20:49:55 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.1 KiB, free 934.0 MiB)
[2021-05-14 17:49:55,409] {docker.py:276} INFO - 21/05/14 20:49:55 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 933.9 MiB)
[2021-05-14 17:49:55,410] {docker.py:276} INFO - 21/05/14 20:49:55 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 7cd3ddbc35d2:39701 (size: 74.1 KiB, free: 934.3 MiB)
[2021-05-14 17:49:55,411] {docker.py:276} INFO - 21/05/14 20:49:55 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-14 17:49:55,412] {docker.py:276} INFO - 21/05/14 20:49:55 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 20:49:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-14 17:49:55,421] {docker.py:276} INFO - 21/05/14 20:49:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (7cd3ddbc35d2, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:55,421] {docker.py:276} INFO - 21/05/14 20:49:55 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (7cd3ddbc35d2, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:55,422] {docker.py:276} INFO - 21/05/14 20:49:55 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (7cd3ddbc35d2, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:55,423] {docker.py:276} INFO - 21/05/14 20:49:55 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (7cd3ddbc35d2, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:55,424] {docker.py:276} INFO - 21/05/14 20:49:55 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
21/05/14 20:49:55 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-14 17:49:55,424] {docker.py:276} INFO - 21/05/14 20:49:55 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-14 17:49:55,425] {docker.py:276} INFO - 21/05/14 20:49:55 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-14 17:49:55,504] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:55,506] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:55,507] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:55,508] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2021-05-14 17:49:55,509] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2021-05-14 17:49:55,524] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:55,524] {docker.py:276} INFO - 21/05/14 20:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms
[2021-05-14 17:49:55,542] {docker.py:276} INFO - 21/05/14 20:49:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:49:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:49:55,543] {docker.py:276} INFO - 21/05/14 20:49:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:55,543] {docker.py:276} INFO - 21/05/14 20:49:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:55,543] {docker.py:276} INFO - 21/05/14 20:49:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:55,546] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,546] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,547] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,547] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,547] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357810603401765885466_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357810603401765885466_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357810603401765885466_0004}; taskId=attempt_202105142049357810603401765885466_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24ad921}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,548] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353124306554521914465_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353124306554521914465_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353124306554521914465_0004}; taskId=attempt_202105142049353124306554521914465_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b261af0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,552] {docker.py:276} INFO - 21/05/14 20:49:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:55,553] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354298900804271914135_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354298900804271914135_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354298900804271914135_0004}; taskId=attempt_202105142049354298900804271914135_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11a52723}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,553] {docker.py:276} INFO - 21/05/14 20:49:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353920553679460275319_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353920553679460275319_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353920553679460275319_0004}; taskId=attempt_202105142049353920553679460275319_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@db31e2a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:55,557] {docker.py:276} INFO - 21/05/14 20:49:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:55,557] {docker.py:276} INFO - 21/05/14 20:49:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:55,558] {docker.py:276} INFO - 21/05/14 20:49:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:55,559] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049353920553679460275319_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353920553679460275319_0004_m_000001_289
[2021-05-14 17:49:55,560] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049357810603401765885466_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357810603401765885466_0004_m_000002_290
[2021-05-14 17:49:55,561] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049354298900804271914135_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354298900804271914135_0004_m_000000_288 
21/05/14 20:49:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049353124306554521914465_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353124306554521914465_0004_m_000003_291
[2021-05-14 17:49:55,586] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Task committer attempt_202105142049353124306554521914465_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353124306554521914465_0004_m_000003_291 : duration 0:00.027s
[2021-05-14 17:49:55,608] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Task committer attempt_202105142049353920553679460275319_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353920553679460275319_0004_m_000001_289 : duration 0:00.049s
[2021-05-14 17:49:55,609] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Task committer attempt_202105142049357810603401765885466_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357810603401765885466_0004_m_000002_290 : duration 0:00.049s
[2021-05-14 17:49:55,620] {docker.py:276} INFO - 21/05/14 20:49:55 INFO StagingCommitter: Task committer attempt_202105142049354298900804271914135_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354298900804271914135_0004_m_000000_288 : duration 0:00.060s
[2021-05-14 17:49:58,387] {docker.py:276} INFO - 21/05/14 20:49:58 INFO StagingCommitter: Starting: Task committer attempt_202105142049354298900804271914135_0004_m_000000_288: needsTaskCommit() Task attempt_202105142049354298900804271914135_0004_m_000000_288
[2021-05-14 17:49:58,388] {docker.py:276} INFO - 21/05/14 20:49:58 INFO StagingCommitter: Task committer attempt_202105142049354298900804271914135_0004_m_000000_288: needsTaskCommit() Task attempt_202105142049354298900804271914135_0004_m_000000_288: duration 0:00.001s
[2021-05-14 17:49:58,389] {docker.py:276} INFO - 21/05/14 20:49:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354298900804271914135_0004_m_000000_288
[2021-05-14 17:49:58,395] {docker.py:276} INFO - 21/05/14 20:49:58 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
[2021-05-14 17:49:58,405] {docker.py:276} INFO - 21/05/14 20:49:58 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (7cd3ddbc35d2, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:58,406] {docker.py:276} INFO - 21/05/14 20:49:58 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-14 17:49:58,406] {docker.py:276} INFO - 21/05/14 20:49:58 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2991 ms on 7cd3ddbc35d2 (executor driver) (1/200)
[2021-05-14 17:49:58,415] {docker.py:276} INFO - 21/05/14 20:49:58 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:49:58,418] {docker.py:276} INFO - 21/05/14 20:49:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:49:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:49:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935292133739237102011_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935292133739237102011_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935292133739237102011_0004}; taskId=attempt_20210514204935292133739237102011_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28747093}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:49:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:49:58 INFO StagingCommitter: Starting: Task committer attempt_20210514204935292133739237102011_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935292133739237102011_0004_m_000004_292
[2021-05-14 17:49:58,424] {docker.py:276} INFO - 21/05/14 20:49:58 INFO StagingCommitter: Task committer attempt_20210514204935292133739237102011_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935292133739237102011_0004_m_000004_292 : duration 0:00.006s
[2021-05-14 17:49:59,439] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049353920553679460275319_0004_m_000001_289: needsTaskCommit() Task attempt_202105142049353920553679460275319_0004_m_000001_289
[2021-05-14 17:49:59,439] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_202105142049353920553679460275319_0004_m_000001_289: needsTaskCommit() Task attempt_202105142049353920553679460275319_0004_m_000001_289: duration 0:00.001s
[2021-05-14 17:49:59,440] {docker.py:276} INFO - 21/05/14 20:49:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353920553679460275319_0004_m_000001_289
[2021-05-14 17:49:59,441] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-14 17:49:59,442] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (7cd3ddbc35d2, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:59,443] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-14 17:49:59,443] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 4026 ms on 7cd3ddbc35d2 (executor driver) (2/200)
[2021-05-14 17:49:59,452] {docker.py:276} INFO - 21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:59,453] {docker.py:276} INFO - 21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:49:59,455] {docker.py:276} INFO - 21/05/14 20:49:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935790380411191304692_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935790380411191304692_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935790380411191304692_0004}; taskId=attempt_20210514204935790380411191304692_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45e60344}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:59,455] {docker.py:276} INFO - 21/05/14 20:49:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_20210514204935790380411191304692_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935790380411191304692_0004_m_000005_293
[2021-05-14 17:49:59,460] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_20210514204935790380411191304692_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935790380411191304692_0004_m_000005_293 : duration 0:00.005s
[2021-05-14 17:49:59,587] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049353124306554521914465_0004_m_000003_291: needsTaskCommit() Task attempt_202105142049353124306554521914465_0004_m_000003_291
[2021-05-14 17:49:59,587] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_202105142049353124306554521914465_0004_m_000003_291: needsTaskCommit() Task attempt_202105142049353124306554521914465_0004_m_000003_291: duration 0:00.002s
[2021-05-14 17:49:59,588] {docker.py:276} INFO - 21/05/14 20:49:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353124306554521914465_0004_m_000003_291
[2021-05-14 17:49:59,589] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-14 17:49:59,591] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (7cd3ddbc35d2, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:59,592] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-14 17:49:59,593] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 4175 ms on 7cd3ddbc35d2 (executor driver) (3/200)
[2021-05-14 17:49:59,601] {docker.py:276} INFO - 21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:49:59,603] {docker.py:276} INFO - 21/05/14 20:49:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:49:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:59,604] {docker.py:276} INFO - 21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:59,604] {docker.py:276} INFO - 21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352497073431064748482_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352497073431064748482_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352497073431064748482_0004}; taskId=attempt_202105142049352497073431064748482_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ddf4870}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:59,604] {docker.py:276} INFO - 21/05/14 20:49:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:49:59,605] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049352497073431064748482_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352497073431064748482_0004_m_000006_294
[2021-05-14 17:49:59,610] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_202105142049352497073431064748482_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352497073431064748482_0004_m_000006_294 : duration 0:00.006s
[2021-05-14 17:49:59,627] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049357810603401765885466_0004_m_000002_290: needsTaskCommit() Task attempt_202105142049357810603401765885466_0004_m_000002_290
[2021-05-14 17:49:59,627] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_202105142049357810603401765885466_0004_m_000002_290: needsTaskCommit() Task attempt_202105142049357810603401765885466_0004_m_000002_290: duration 0:00.001s
21/05/14 20:49:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357810603401765885466_0004_m_000002_290
[2021-05-14 17:49:59,628] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-14 17:49:59,629] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (7cd3ddbc35d2, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:49:59,631] {docker.py:276} INFO - 21/05/14 20:49:59 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-14 17:49:59,631] {docker.py:276} INFO - 21/05/14 20:49:59 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 4214 ms on 7cd3ddbc35d2 (executor driver) (4/200)
[2021-05-14 17:49:59,638] {docker.py:276} INFO - 21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:49:59,639] {docker.py:276} INFO - 21/05/14 20:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:49:59,640] {docker.py:276} INFO - 21/05/14 20:49:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:49:59,641] {docker.py:276} INFO - 21/05/14 20:49:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:49:59,641] {docker.py:276} INFO - 21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:49:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352838186165022733898_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352838186165022733898_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352838186165022733898_0004}; taskId=attempt_202105142049352838186165022733898_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@282fec9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:49:59,641] {docker.py:276} INFO - 21/05/14 20:49:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:49:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049352838186165022733898_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352838186165022733898_0004_m_000007_295
[2021-05-14 17:49:59,649] {docker.py:276} INFO - 21/05/14 20:49:59 INFO StagingCommitter: Task committer attempt_202105142049352838186165022733898_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352838186165022733898_0004_m_000007_295 : duration 0:00.008s
[2021-05-14 17:50:01,464] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049352497073431064748482_0004_m_000006_294: needsTaskCommit() Task attempt_202105142049352497073431064748482_0004_m_000006_294
[2021-05-14 17:50:01,465] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049352497073431064748482_0004_m_000006_294: needsTaskCommit() Task attempt_202105142049352497073431064748482_0004_m_000006_294: duration 0:00.002s
21/05/14 20:50:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352497073431064748482_0004_m_000006_294
[2021-05-14 17:50:01,467] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4544 bytes result sent to driver
[2021-05-14 17:50:01,468] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (7cd3ddbc35d2, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:01,470] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 1881 ms on 7cd3ddbc35d2 (executor driver) (5/200)
[2021-05-14 17:50:01,471] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-14 17:50:01,481] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:01,484] {docker.py:276} INFO - 21/05/14 20:50:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:01,484] {docker.py:276} INFO - 21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353893418155532145121_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353893418155532145121_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353893418155532145121_0004}; taskId=attempt_202105142049353893418155532145121_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9e1c8ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049353893418155532145121_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353893418155532145121_0004_m_000008_296
[2021-05-14 17:50:01,490] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049353893418155532145121_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353893418155532145121_0004_m_000008_296 : duration 0:00.005s
[2021-05-14 17:50:01,680] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049352838186165022733898_0004_m_000007_295: needsTaskCommit() Task attempt_202105142049352838186165022733898_0004_m_000007_295
[2021-05-14 17:50:01,681] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049352838186165022733898_0004_m_000007_295: needsTaskCommit() Task attempt_202105142049352838186165022733898_0004_m_000007_295: duration 0:00.002s
[2021-05-14 17:50:01,681] {docker.py:276} INFO - 21/05/14 20:50:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352838186165022733898_0004_m_000007_295
[2021-05-14 17:50:01,683] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-14 17:50:01,685] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (7cd3ddbc35d2, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:01,685] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2057 ms on 7cd3ddbc35d2 (executor driver) (6/200)
[2021-05-14 17:50:01,686] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-14 17:50:01,695] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:01,696] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:01,697] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_20210514204935292133739237102011_0004_m_000004_292: needsTaskCommit() Task attempt_20210514204935292133739237102011_0004_m_000004_292
[2021-05-14 17:50:01,698] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_20210514204935790380411191304692_0004_m_000005_293: needsTaskCommit() Task attempt_20210514204935790380411191304692_0004_m_000005_293
[2021-05-14 17:50:01,698] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_20210514204935790380411191304692_0004_m_000005_293: needsTaskCommit() Task attempt_20210514204935790380411191304692_0004_m_000005_293: duration 0:00.001s
[2021-05-14 17:50:01,698] {docker.py:276} INFO - 21/05/14 20:50:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935790380411191304692_0004_m_000005_293
[2021-05-14 17:50:01,699] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_20210514204935292133739237102011_0004_m_000004_292: needsTaskCommit() Task attempt_20210514204935292133739237102011_0004_m_000004_292: duration 0:00.002s
[2021-05-14 17:50:01,699] {docker.py:276} INFO - 21/05/14 20:50:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935292133739237102011_0004_m_000004_292
[2021-05-14 17:50:01,700] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4544 bytes result sent to driver
[2021-05-14 17:50:01,700] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (7cd3ddbc35d2, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:01,701] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 2262 ms on 7cd3ddbc35d2 (executor driver) (7/200)
[2021-05-14 17:50:01,702] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-14 17:50:01,702] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-14 17:50:01,704] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (7cd3ddbc35d2, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:01,705] {docker.py:276} INFO - 21/05/14 20:50:01 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 3303 ms on 7cd3ddbc35d2 (executor driver) (8/200)
[2021-05-14 17:50:01,706] {docker.py:276} INFO - 21/05/14 20:50:01 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-14 17:50:01,716] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:01,716] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:01,716] {docker.py:276} INFO - 21/05/14 20:50:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:01,717] {docker.py:276} INFO - 21/05/14 20:50:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:01,718] {docker.py:276} INFO - 21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352778107021055192050_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352778107021055192050_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352778107021055192050_0004}; taskId=attempt_202105142049352778107021055192050_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30a4f763}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:01,719] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049352778107021055192050_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352778107021055192050_0004_m_000009_297
[2021-05-14 17:50:01,720] {docker.py:276} INFO - 21/05/14 20:50:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:01,721] {docker.py:276} INFO - 21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:01,722] {docker.py:276} INFO - 21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353286887447396739812_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353286887447396739812_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353286887447396739812_0004}; taskId=attempt_202105142049353286887447396739812_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c086213}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:01,722] {docker.py:276} INFO - 21/05/14 20:50:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:01,723] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049353286887447396739812_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353286887447396739812_0004_m_000011_299
[2021-05-14 17:50:01,724] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049352778107021055192050_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352778107021055192050_0004_m_000009_297 : duration 0:00.006s
[2021-05-14 17:50:01,728] {docker.py:276} INFO - 21/05/14 20:50:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:01,731] {docker.py:276} INFO - 21/05/14 20:50:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351105297656432582577_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351105297656432582577_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351105297656432582577_0004}; taskId=attempt_202105142049351105297656432582577_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@477fcf3f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049351105297656432582577_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351105297656432582577_0004_m_000010_298
[2021-05-14 17:50:01,733] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049353286887447396739812_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353286887447396739812_0004_m_000011_299 : duration 0:00.009s
[2021-05-14 17:50:01,752] {docker.py:276} INFO - 21/05/14 20:50:01 INFO StagingCommitter: Task committer attempt_202105142049351105297656432582577_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351105297656432582577_0004_m_000010_298 : duration 0:00.021s
[2021-05-14 17:50:03,848] {docker.py:276} INFO - 21/05/14 20:50:03 INFO StagingCommitter: Starting: Task committer attempt_202105142049353893418155532145121_0004_m_000008_296: needsTaskCommit() Task attempt_202105142049353893418155532145121_0004_m_000008_296
[2021-05-14 17:50:03,849] {docker.py:276} INFO - 21/05/14 20:50:03 INFO StagingCommitter: Task committer attempt_202105142049353893418155532145121_0004_m_000008_296: needsTaskCommit() Task attempt_202105142049353893418155532145121_0004_m_000008_296: duration 0:00.002s
21/05/14 20:50:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353893418155532145121_0004_m_000008_296
[2021-05-14 17:50:03,851] {docker.py:276} INFO - 21/05/14 20:50:03 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-14 17:50:03,853] {docker.py:276} INFO - 21/05/14 20:50:03 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (7cd3ddbc35d2, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:03,854] {docker.py:276} INFO - 21/05/14 20:50:03 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2390 ms on 7cd3ddbc35d2 (executor driver) (9/200)
[2021-05-14 17:50:03,856] {docker.py:276} INFO - 21/05/14 20:50:03 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-14 17:50:03,866] {docker.py:276} INFO - 21/05/14 20:50:03 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:03,869] {docker.py:276} INFO - 21/05/14 20:50:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357616671028150882579_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357616671028150882579_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357616671028150882579_0004}; taskId=attempt_202105142049357616671028150882579_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4debd800}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:03 INFO StagingCommitter: Starting: Task committer attempt_202105142049357616671028150882579_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357616671028150882579_0004_m_000012_300
[2021-05-14 17:50:03,874] {docker.py:276} INFO - 21/05/14 20:50:03 INFO StagingCommitter: Task committer attempt_202105142049357616671028150882579_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357616671028150882579_0004_m_000012_300 : duration 0:00.005s
[2021-05-14 17:50:04,004] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049352778107021055192050_0004_m_000009_297: needsTaskCommit() Task attempt_202105142049352778107021055192050_0004_m_000009_297
[2021-05-14 17:50:04,004] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049352778107021055192050_0004_m_000009_297: needsTaskCommit() Task attempt_202105142049352778107021055192050_0004_m_000009_297: duration 0:00.001s
21/05/14 20:50:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352778107021055192050_0004_m_000009_297
[2021-05-14 17:50:04,005] {docker.py:276} INFO - 21/05/14 20:50:04 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-14 17:50:04,007] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (7cd3ddbc35d2, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:04,008] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 2326 ms on 7cd3ddbc35d2 (executor driver) (10/200)
21/05/14 20:50:04 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-14 17:50:04,017] {docker.py:276} INFO - 21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:04,019] {docker.py:276} INFO - 21/05/14 20:50:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352264822900125589454_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352264822900125589454_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352264822900125589454_0004}; taskId=attempt_202105142049352264822900125589454_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@622a27ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:04,019] {docker.py:276} INFO - 21/05/14 20:50:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049352264822900125589454_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352264822900125589454_0004_m_000013_301
[2021-05-14 17:50:04,023] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049352264822900125589454_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352264822900125589454_0004_m_000013_301 : duration 0:00.003s
[2021-05-14 17:50:04,067] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049351105297656432582577_0004_m_000010_298: needsTaskCommit() Task attempt_202105142049351105297656432582577_0004_m_000010_298
[2021-05-14 17:50:04,067] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049351105297656432582577_0004_m_000010_298: needsTaskCommit() Task attempt_202105142049351105297656432582577_0004_m_000010_298: duration 0:00.001s
[2021-05-14 17:50:04,068] {docker.py:276} INFO - 21/05/14 20:50:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351105297656432582577_0004_m_000010_298
[2021-05-14 17:50:04,068] {docker.py:276} INFO - 21/05/14 20:50:04 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-14 17:50:04,069] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (7cd3ddbc35d2, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:04,070] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2372 ms on 7cd3ddbc35d2 (executor driver) (11/200)
[2021-05-14 17:50:04,071] {docker.py:276} INFO - 21/05/14 20:50:04 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-14 17:50:04,072] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049353286887447396739812_0004_m_000011_299: needsTaskCommit() Task attempt_202105142049353286887447396739812_0004_m_000011_299
[2021-05-14 17:50:04,073] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049353286887447396739812_0004_m_000011_299: needsTaskCommit() Task attempt_202105142049353286887447396739812_0004_m_000011_299: duration 0:00.001s
21/05/14 20:50:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353286887447396739812_0004_m_000011_299
[2021-05-14 17:50:04,073] {docker.py:276} INFO - 21/05/14 20:50:04 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4587 bytes result sent to driver
[2021-05-14 17:50:04,074] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (7cd3ddbc35d2, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:04,075] {docker.py:276} INFO - 21/05/14 20:50:04 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2374 ms on 7cd3ddbc35d2 (executor driver) (12/200)
[2021-05-14 17:50:04,075] {docker.py:276} INFO - 21/05/14 20:50:04 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-14 17:50:04,084] {docker.py:276} INFO - 21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:04,089] {docker.py:276} INFO - 21/05/14 20:50:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:04,089] {docker.py:276} INFO - 21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355802901521899791835_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355802901521899791835_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355802901521899791835_0004}; taskId=attempt_202105142049355802901521899791835_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44968f24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:04,090] {docker.py:276} INFO - 21/05/14 20:50:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049355802901521899791835_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355802901521899791835_0004_m_000014_302
[2021-05-14 17:50:04,092] {docker.py:276} INFO - 21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:04,093] {docker.py:276} INFO - 21/05/14 20:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:04,097] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049355802901521899791835_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355802901521899791835_0004_m_000014_302 : duration 0:00.007s
[2021-05-14 17:50:04,099] {docker.py:276} INFO - 21/05/14 20:50:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:04,100] {docker.py:276} INFO - 21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:04,100] {docker.py:276} INFO - 21/05/14 20:50:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352877239858239904409_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877239858239904409_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352877239858239904409_0004}; taskId=attempt_202105142049352877239858239904409_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66c31a0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:04,101] {docker.py:276} INFO - 21/05/14 20:50:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:04,101] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049352877239858239904409_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877239858239904409_0004_m_000015_303
[2021-05-14 17:50:04,106] {docker.py:276} INFO - 21/05/14 20:50:04 INFO StagingCommitter: Task committer attempt_202105142049352877239858239904409_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877239858239904409_0004_m_000015_303 : duration 0:00.006s
[2021-05-14 17:50:06,057] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049357616671028150882579_0004_m_000012_300: needsTaskCommit() Task attempt_202105142049357616671028150882579_0004_m_000012_300
21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049357616671028150882579_0004_m_000012_300: needsTaskCommit() Task attempt_202105142049357616671028150882579_0004_m_000012_300: duration 0:00.001s
[2021-05-14 17:50:06,058] {docker.py:276} INFO - 21/05/14 20:50:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357616671028150882579_0004_m_000012_300
[2021-05-14 17:50:06,059] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-14 17:50:06,061] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (7cd3ddbc35d2, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:06,063] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
21/05/14 20:50:06 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2212 ms on 7cd3ddbc35d2 (executor driver) (13/200)
[2021-05-14 17:50:06,074] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:06,075] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:06,077] {docker.py:276} INFO - 21/05/14 20:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:06,078] {docker.py:276} INFO - 21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353388427741092116882_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353388427741092116882_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353388427741092116882_0004}; taskId=attempt_202105142049353388427741092116882_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dd0b316}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049353388427741092116882_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353388427741092116882_0004_m_000016_304
[2021-05-14 17:50:06,086] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049353388427741092116882_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353388427741092116882_0004_m_000016_304 : duration 0:00.009s
[2021-05-14 17:50:06,236] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049352264822900125589454_0004_m_000013_301: needsTaskCommit() Task attempt_202105142049352264822900125589454_0004_m_000013_301
[2021-05-14 17:50:06,238] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049352264822900125589454_0004_m_000013_301: needsTaskCommit() Task attempt_202105142049352264822900125589454_0004_m_000013_301: duration 0:00.004s
21/05/14 20:50:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352264822900125589454_0004_m_000013_301
[2021-05-14 17:50:06,241] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-14 17:50:06,242] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (7cd3ddbc35d2, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:06,244] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
21/05/14 20:50:06 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2240 ms on 7cd3ddbc35d2 (executor driver) (14/200)
[2021-05-14 17:50:06,255] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:06,258] {docker.py:276} INFO - 21/05/14 20:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:06,258] {docker.py:276} INFO - 21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351681827617533599534_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351681827617533599534_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351681827617533599534_0004}; taskId=attempt_202105142049351681827617533599534_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71f4f1e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:06,258] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049351681827617533599534_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351681827617533599534_0004_m_000017_305
[2021-05-14 17:50:06,262] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049351681827617533599534_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351681827617533599534_0004_m_000017_305 : duration 0:00.004s
[2021-05-14 17:50:06,283] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049352877239858239904409_0004_m_000015_303: needsTaskCommit() Task attempt_202105142049352877239858239904409_0004_m_000015_303
[2021-05-14 17:50:06,284] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049352877239858239904409_0004_m_000015_303: needsTaskCommit() Task attempt_202105142049352877239858239904409_0004_m_000015_303: duration 0:00.002s
21/05/14 20:50:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352877239858239904409_0004_m_000015_303
[2021-05-14 17:50:06,285] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4544 bytes result sent to driver
[2021-05-14 17:50:06,286] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (7cd3ddbc35d2, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:06,287] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2216 ms on 7cd3ddbc35d2 (executor driver) (15/200)
[2021-05-14 17:50:06,288] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-14 17:50:06,299] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:06,301] {docker.py:276} INFO - 21/05/14 20:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355284910771043342960_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355284910771043342960_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355284910771043342960_0004}; taskId=attempt_202105142049355284910771043342960_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52a56672}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:06,302] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049355284910771043342960_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355284910771043342960_0004_m_000018_306
[2021-05-14 17:50:06,305] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049355284910771043342960_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355284910771043342960_0004_m_000018_306 : duration 0:00.004s
[2021-05-14 17:50:06,408] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049355802901521899791835_0004_m_000014_302: needsTaskCommit() Task attempt_202105142049355802901521899791835_0004_m_000014_302
[2021-05-14 17:50:06,408] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049355802901521899791835_0004_m_000014_302: needsTaskCommit() Task attempt_202105142049355802901521899791835_0004_m_000014_302: duration 0:00.001s
[2021-05-14 17:50:06,408] {docker.py:276} INFO - 21/05/14 20:50:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355802901521899791835_0004_m_000014_302
[2021-05-14 17:50:06,410] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-14 17:50:06,411] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (7cd3ddbc35d2, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:06,412] {docker.py:276} INFO - 21/05/14 20:50:06 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-14 17:50:06,413] {docker.py:276} INFO - 21/05/14 20:50:06 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 2346 ms on 7cd3ddbc35d2 (executor driver) (16/200)
[2021-05-14 17:50:06,424] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Getting 5 (45.2 KiB) non-empty blocks including 5 (45.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:06,426] {docker.py:276} INFO - 21/05/14 20:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 17:50:06,430] {docker.py:276} INFO - 21/05/14 20:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:06,431] {docker.py:276} INFO - 21/05/14 20:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:06,431] {docker.py:276} INFO - 21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:06,431] {docker.py:276} INFO - 21/05/14 20:50:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354983341427007898770_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354983341427007898770_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354983341427007898770_0004}; taskId=attempt_202105142049354983341427007898770_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a1cafb3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:06,432] {docker.py:276} INFO - 21/05/14 20:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:06,432] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049354983341427007898770_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354983341427007898770_0004_m_000019_307
[2021-05-14 17:50:06,438] {docker.py:276} INFO - 21/05/14 20:50:06 INFO StagingCommitter: Task committer attempt_202105142049354983341427007898770_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354983341427007898770_0004_m_000019_307 : duration 0:00.006s
[2021-05-14 17:50:08,606] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049355284910771043342960_0004_m_000018_306: needsTaskCommit() Task attempt_202105142049355284910771043342960_0004_m_000018_306
[2021-05-14 17:50:08,607] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049355284910771043342960_0004_m_000018_306: needsTaskCommit() Task attempt_202105142049355284910771043342960_0004_m_000018_306: duration 0:00.002s
21/05/14 20:50:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355284910771043342960_0004_m_000018_306
[2021-05-14 17:50:08,609] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-14 17:50:08,611] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049353388427741092116882_0004_m_000016_304: needsTaskCommit() Task attempt_202105142049353388427741092116882_0004_m_000016_304
21/05/14 20:50:08 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (7cd3ddbc35d2, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:08,612] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049353388427741092116882_0004_m_000016_304: needsTaskCommit() Task attempt_202105142049353388427741092116882_0004_m_000016_304: duration 0:00.000s
[2021-05-14 17:50:08,613] {docker.py:276} INFO - 21/05/14 20:50:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353388427741092116882_0004_m_000016_304
[2021-05-14 17:50:08,614] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049351681827617533599534_0004_m_000017_305: needsTaskCommit() Task attempt_202105142049351681827617533599534_0004_m_000017_305
21/05/14 20:50:08 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-14 17:50:08,615] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049351681827617533599534_0004_m_000017_305: needsTaskCommit() Task attempt_202105142049351681827617533599534_0004_m_000017_305: duration 0:00.001s
21/05/14 20:50:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351681827617533599534_0004_m_000017_305
[2021-05-14 17:50:08,616] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
21/05/14 20:50:08 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-14 17:50:08,616] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (7cd3ddbc35d2, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:08,617] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-14 17:50:08,618] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (7cd3ddbc35d2, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:08,620] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2382 ms on 7cd3ddbc35d2 (executor driver) (17/200)
[2021-05-14 17:50:08,621] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
21/05/14 20:50:08 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2337 ms on 7cd3ddbc35d2 (executor driver) (18/200)
[2021-05-14 17:50:08,621] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2564 ms on 7cd3ddbc35d2 (executor driver) (19/200)
[2021-05-14 17:50:08,628] {docker.py:276} INFO - 21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:08,629] {docker.py:276} INFO - 21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:08,630] {docker.py:276} INFO - 21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 20:50:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:08,631] {docker.py:276} INFO - 21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935878656307103893743_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935878656307103893743_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935878656307103893743_0004}; taskId=attempt_20210514204935878656307103893743_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a8cb94f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_20210514204935878656307103893743_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935878656307103893743_0004_m_000022_310
[2021-05-14 17:50:08,632] {docker.py:276} INFO - 21/05/14 20:50:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:08,633] {docker.py:276} INFO - 21/05/14 20:50:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:08,633] {docker.py:276} INFO - 21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:08,634] {docker.py:276} INFO - 21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356265719966513129106_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356265719966513129106_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356265719966513129106_0004}; taskId=attempt_202105142049356265719966513129106_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19fb6e74}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:08,634] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049356265719966513129106_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356265719966513129106_0004_m_000020_308
[2021-05-14 17:50:08,637] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_20210514204935878656307103893743_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935878656307103893743_0004_m_000022_310 : duration 0:00.004s
[2021-05-14 17:50:08,639] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049356265719966513129106_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356265719966513129106_0004_m_000020_308 : duration 0:00.006s
[2021-05-14 17:50:08,640] {docker.py:276} INFO - 21/05/14 20:50:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:08,641] {docker.py:276} INFO - 21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356523323027184954537_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356523323027184954537_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356523323027184954537_0004}; taskId=attempt_202105142049356523323027184954537_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47e623e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:08,642] {docker.py:276} INFO - 21/05/14 20:50:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049356523323027184954537_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356523323027184954537_0004_m_000021_309
[2021-05-14 17:50:08,646] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049356523323027184954537_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356523323027184954537_0004_m_000021_309 : duration 0:00.005s
[2021-05-14 17:50:08,724] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049354983341427007898770_0004_m_000019_307: needsTaskCommit() Task attempt_202105142049354983341427007898770_0004_m_000019_307
[2021-05-14 17:50:08,725] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049354983341427007898770_0004_m_000019_307: needsTaskCommit() Task attempt_202105142049354983341427007898770_0004_m_000019_307: duration 0:00.002s
[2021-05-14 17:50:08,725] {docker.py:276} INFO - 21/05/14 20:50:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354983341427007898770_0004_m_000019_307
[2021-05-14 17:50:08,726] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-14 17:50:08,727] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2319 ms on 7cd3ddbc35d2 (executor driver) (20/200)
[2021-05-14 17:50:08,728] {docker.py:276} INFO - 21/05/14 20:50:08 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (7cd3ddbc35d2, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:08,729] {docker.py:276} INFO - 21/05/14 20:50:08 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-14 17:50:08,737] {docker.py:276} INFO - 21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:08,739] {docker.py:276} INFO - 21/05/14 20:50:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352382657466759812188_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352382657466759812188_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352382657466759812188_0004}; taskId=attempt_202105142049352382657466759812188_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26700b9a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:08,740] {docker.py:276} INFO - 21/05/14 20:50:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049352382657466759812188_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352382657466759812188_0004_m_000023_311
[2021-05-14 17:50:08,746] {docker.py:276} INFO - 21/05/14 20:50:08 INFO StagingCommitter: Task committer attempt_202105142049352382657466759812188_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352382657466759812188_0004_m_000023_311 : duration 0:00.006s
[2021-05-14 17:50:10,793] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049356523323027184954537_0004_m_000021_309: needsTaskCommit() Task attempt_202105142049356523323027184954537_0004_m_000021_309
[2021-05-14 17:50:10,795] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049356523323027184954537_0004_m_000021_309: needsTaskCommit() Task attempt_202105142049356523323027184954537_0004_m_000021_309: duration 0:00.002s
21/05/14 20:50:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356523323027184954537_0004_m_000021_309
[2021-05-14 17:50:10,798] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-14 17:50:10,799] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (7cd3ddbc35d2, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:10,800] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 2151 ms on 7cd3ddbc35d2 (executor driver) (21/200)
[2021-05-14 17:50:10,801] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-14 17:50:10,806] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_20210514204935878656307103893743_0004_m_000022_310: needsTaskCommit() Task attempt_20210514204935878656307103893743_0004_m_000022_310
[2021-05-14 17:50:10,806] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_20210514204935878656307103893743_0004_m_000022_310: needsTaskCommit() Task attempt_20210514204935878656307103893743_0004_m_000022_310: duration 0:00.001s
[2021-05-14 17:50:10,807] {docker.py:276} INFO - 21/05/14 20:50:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935878656307103893743_0004_m_000022_310
[2021-05-14 17:50:10,807] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-14 17:50:10,808] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (7cd3ddbc35d2, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:10,809] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2160 ms on 7cd3ddbc35d2 (executor driver) (22/200)
[2021-05-14 17:50:10,809] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-14 17:50:10,814] {docker.py:276} INFO - 21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:10,816] {docker.py:276} INFO - 21/05/14 20:50:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353244458828148818684_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353244458828148818684_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353244458828148818684_0004}; taskId=attempt_202105142049353244458828148818684_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@307fcd07}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:10,817] {docker.py:276} INFO - 21/05/14 20:50:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:10,817] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049353244458828148818684_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353244458828148818684_0004_m_000024_312
[2021-05-14 17:50:10,820] {docker.py:276} INFO - 21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:10,822] {docker.py:276} INFO - 21/05/14 20:50:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:10,823] {docker.py:276} INFO - 21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355855551424233567014_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355855551424233567014_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355855551424233567014_0004}; taskId=attempt_202105142049355855551424233567014_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6de80aae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:10,824] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049355855551424233567014_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355855551424233567014_0004_m_000025_313
[2021-05-14 17:50:10,828] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049353244458828148818684_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353244458828148818684_0004_m_000024_312 : duration 0:00.010s
21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049355855551424233567014_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355855551424233567014_0004_m_000025_313 : duration 0:00.005s
[2021-05-14 17:50:10,841] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049356265719966513129106_0004_m_000020_308: needsTaskCommit() Task attempt_202105142049356265719966513129106_0004_m_000020_308
[2021-05-14 17:50:10,842] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049356265719966513129106_0004_m_000020_308: needsTaskCommit() Task attempt_202105142049356265719966513129106_0004_m_000020_308: duration 0:00.000s
21/05/14 20:50:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356265719966513129106_0004_m_000020_308
[2021-05-14 17:50:10,843] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-14 17:50:10,844] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (7cd3ddbc35d2, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:10,845] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
21/05/14 20:50:10 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2203 ms on 7cd3ddbc35d2 (executor driver) (23/200)
[2021-05-14 17:50:10,854] {docker.py:276} INFO - 21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:10,856] {docker.py:276} INFO - 21/05/14 20:50:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:10,856] {docker.py:276} INFO - 21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355728706187311732667_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728706187311732667_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355728706187311732667_0004}; taskId=attempt_202105142049355728706187311732667_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6406d4ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:10,857] {docker.py:276} INFO - 21/05/14 20:50:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:10,857] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049355728706187311732667_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728706187311732667_0004_m_000026_314
[2021-05-14 17:50:10,862] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049355728706187311732667_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728706187311732667_0004_m_000026_314 : duration 0:00.006s
[2021-05-14 17:50:10,911] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049352382657466759812188_0004_m_000023_311: needsTaskCommit() Task attempt_202105142049352382657466759812188_0004_m_000023_311
[2021-05-14 17:50:10,911] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049352382657466759812188_0004_m_000023_311: needsTaskCommit() Task attempt_202105142049352382657466759812188_0004_m_000023_311: duration 0:00.001s
21/05/14 20:50:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352382657466759812188_0004_m_000023_311
[2021-05-14 17:50:10,912] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
[2021-05-14 17:50:10,913] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (7cd3ddbc35d2, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:10,914] {docker.py:276} INFO - 21/05/14 20:50:10 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-14 17:50:10,914] {docker.py:276} INFO - 21/05/14 20:50:10 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2154 ms on 7cd3ddbc35d2 (executor driver) (24/200)
[2021-05-14 17:50:10,922] {docker.py:276} INFO - 21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:10,924] {docker.py:276} INFO - 21/05/14 20:50:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351034446857209521362_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351034446857209521362_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351034446857209521362_0004}; taskId=attempt_202105142049351034446857209521362_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f2849c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049351034446857209521362_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351034446857209521362_0004_m_000027_315
[2021-05-14 17:50:10,928] {docker.py:276} INFO - 21/05/14 20:50:10 INFO StagingCommitter: Task committer attempt_202105142049351034446857209521362_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351034446857209521362_0004_m_000027_315 : duration 0:00.003s
[2021-05-14 17:50:12,980] {docker.py:276} INFO - 21/05/14 20:50:12 INFO StagingCommitter: Starting: Task committer attempt_202105142049353244458828148818684_0004_m_000024_312: needsTaskCommit() Task attempt_202105142049353244458828148818684_0004_m_000024_312
[2021-05-14 17:50:12,980] {docker.py:276} INFO - 21/05/14 20:50:12 INFO StagingCommitter: Task committer attempt_202105142049353244458828148818684_0004_m_000024_312: needsTaskCommit() Task attempt_202105142049353244458828148818684_0004_m_000024_312: duration 0:00.001s
21/05/14 20:50:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353244458828148818684_0004_m_000024_312
[2021-05-14 17:50:12,981] {docker.py:276} INFO - 21/05/14 20:50:12 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-14 17:50:12,982] {docker.py:276} INFO - 21/05/14 20:50:12 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (7cd3ddbc35d2, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:12,982] {docker.py:276} INFO - 21/05/14 20:50:12 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2187 ms on 7cd3ddbc35d2 (executor driver) (25/200)
[2021-05-14 17:50:12,984] {docker.py:276} INFO - 21/05/14 20:50:12 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-14 17:50:12,994] {docker.py:276} INFO - 21/05/14 20:50:12 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:12,994] {docker.py:276} INFO - 21/05/14 20:50:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:13,008] {docker.py:276} INFO - 21/05/14 20:50:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:13,009] {docker.py:276} INFO - 21/05/14 20:50:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:13,009] {docker.py:276} INFO - 21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:13,010] {docker.py:276} INFO - 21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355848336018594732454_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355848336018594732454_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355848336018594732454_0004}; taskId=attempt_202105142049355848336018594732454_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4db99587}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:13,010] {docker.py:276} INFO - 21/05/14 20:50:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049355848336018594732454_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355848336018594732454_0004_m_000028_316
[2021-05-14 17:50:13,012] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049355855551424233567014_0004_m_000025_313: needsTaskCommit() Task attempt_202105142049355855551424233567014_0004_m_000025_313
[2021-05-14 17:50:13,013] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049355855551424233567014_0004_m_000025_313: needsTaskCommit() Task attempt_202105142049355855551424233567014_0004_m_000025_313: duration 0:00.001s
21/05/14 20:50:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355855551424233567014_0004_m_000025_313
[2021-05-14 17:50:13,014] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4587 bytes result sent to driver
[2021-05-14 17:50:13,015] {docker.py:276} INFO - 21/05/14 20:50:13 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (7cd3ddbc35d2, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:13,015] {docker.py:276} INFO - 21/05/14 20:50:13 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2211 ms on 7cd3ddbc35d2 (executor driver) (26/200)
[2021-05-14 17:50:13,016] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-14 17:50:13,019] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049355848336018594732454_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355848336018594732454_0004_m_000028_316 : duration 0:00.008s
[2021-05-14 17:50:13,029] {docker.py:276} INFO - 21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:13,030] {docker.py:276} INFO - 21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:13,035] {docker.py:276} INFO - 21/05/14 20:50:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:13,035] {docker.py:276} INFO - 21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358861652442860516030_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358861652442860516030_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358861652442860516030_0004}; taskId=attempt_202105142049358861652442860516030_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1357355d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:13,036] {docker.py:276} INFO - 21/05/14 20:50:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:13,036] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049358861652442860516030_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358861652442860516030_0004_m_000029_317
[2021-05-14 17:50:13,039] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049358861652442860516030_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358861652442860516030_0004_m_000029_317 : duration 0:00.004s
[2021-05-14 17:50:13,051] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049355728706187311732667_0004_m_000026_314: needsTaskCommit() Task attempt_202105142049355728706187311732667_0004_m_000026_314
[2021-05-14 17:50:13,052] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049355728706187311732667_0004_m_000026_314: needsTaskCommit() Task attempt_202105142049355728706187311732667_0004_m_000026_314: duration 0:00.001s
21/05/14 20:50:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355728706187311732667_0004_m_000026_314
[2021-05-14 17:50:13,053] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4587 bytes result sent to driver
[2021-05-14 17:50:13,054] {docker.py:276} INFO - 21/05/14 20:50:13 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (7cd3ddbc35d2, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:13,055] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
21/05/14 20:50:13 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2215 ms on 7cd3ddbc35d2 (executor driver) (27/200)
[2021-05-14 17:50:13,063] {docker.py:276} INFO - 21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:13,067] {docker.py:276} INFO - 21/05/14 20:50:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:13,068] {docker.py:276} INFO - 21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357924111866181549121_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357924111866181549121_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357924111866181549121_0004}; taskId=attempt_202105142049357924111866181549121_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e055a64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049357924111866181549121_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357924111866181549121_0004_m_000030_318
[2021-05-14 17:50:13,071] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049357924111866181549121_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357924111866181549121_0004_m_000030_318 : duration 0:00.002s
[2021-05-14 17:50:13,109] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049351034446857209521362_0004_m_000027_315: needsTaskCommit() Task attempt_202105142049351034446857209521362_0004_m_000027_315
[2021-05-14 17:50:13,109] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049351034446857209521362_0004_m_000027_315: needsTaskCommit() Task attempt_202105142049351034446857209521362_0004_m_000027_315: duration 0:00.000s
21/05/14 20:50:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351034446857209521362_0004_m_000027_315
[2021-05-14 17:50:13,110] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4587 bytes result sent to driver
[2021-05-14 17:50:13,111] {docker.py:276} INFO - 21/05/14 20:50:13 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (7cd3ddbc35d2, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:13,114] {docker.py:276} INFO - 21/05/14 20:50:13 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2202 ms on 7cd3ddbc35d2 (executor driver) (28/200)
[2021-05-14 17:50:13,114] {docker.py:276} INFO - 21/05/14 20:50:13 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-14 17:50:13,121] {docker.py:276} INFO - 21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:13,124] {docker.py:276} INFO - 21/05/14 20:50:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358468432322849102956_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358468432322849102956_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358468432322849102956_0004}; taskId=attempt_202105142049358468432322849102956_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36d81da4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:13,124] {docker.py:276} INFO - 21/05/14 20:50:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:13 INFO StagingCommitter: Starting: Task committer attempt_202105142049358468432322849102956_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358468432322849102956_0004_m_000031_319
[2021-05-14 17:50:13,127] {docker.py:276} INFO - 21/05/14 20:50:13 INFO StagingCommitter: Task committer attempt_202105142049358468432322849102956_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358468432322849102956_0004_m_000031_319 : duration 0:00.003s
[2021-05-14 17:50:15,196] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049358861652442860516030_0004_m_000029_317: needsTaskCommit() Task attempt_202105142049358861652442860516030_0004_m_000029_317
21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049358861652442860516030_0004_m_000029_317: needsTaskCommit() Task attempt_202105142049358861652442860516030_0004_m_000029_317: duration 0:00.001s
21/05/14 20:50:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358861652442860516030_0004_m_000029_317
[2021-05-14 17:50:15,198] {docker.py:276} INFO - 21/05/14 20:50:15 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4544 bytes result sent to driver
21/05/14 20:50:15 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (7cd3ddbc35d2, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:15 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-14 17:50:15,199] {docker.py:276} INFO - 21/05/14 20:50:15 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 2186 ms on 7cd3ddbc35d2 (executor driver) (29/200)
[2021-05-14 17:50:15,213] {docker.py:276} INFO - 21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:15,214] {docker.py:276} INFO - 21/05/14 20:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:15,214] {docker.py:276} INFO - 21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351100926149209741058_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351100926149209741058_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351100926149209741058_0004}; taskId=attempt_202105142049351100926149209741058_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52d1fbbd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:15,215] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049351100926149209741058_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351100926149209741058_0004_m_000032_320
[2021-05-14 17:50:15,218] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049351100926149209741058_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351100926149209741058_0004_m_000032_320 : duration 0:00.005s
[2021-05-14 17:50:15,258] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049358468432322849102956_0004_m_000031_319: needsTaskCommit() Task attempt_202105142049358468432322849102956_0004_m_000031_319
[2021-05-14 17:50:15,258] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049358468432322849102956_0004_m_000031_319: needsTaskCommit() Task attempt_202105142049358468432322849102956_0004_m_000031_319: duration 0:00.001s
21/05/14 20:50:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358468432322849102956_0004_m_000031_319
[2021-05-14 17:50:15,265] {docker.py:276} INFO - 21/05/14 20:50:15 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
21/05/14 20:50:15 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (7cd3ddbc35d2, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:15 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
21/05/14 20:50:15 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2155 ms on 7cd3ddbc35d2 (executor driver) (30/200)
[2021-05-14 17:50:15,286] {docker.py:276} INFO - 21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 17:50:15,287] {docker.py:276} INFO - 21/05/14 20:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351321333908191109717_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351321333908191109717_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351321333908191109717_0004}; taskId=attempt_202105142049351321333908191109717_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dcf31e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049351321333908191109717_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351321333908191109717_0004_m_000033_321
[2021-05-14 17:50:15,299] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049351321333908191109717_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351321333908191109717_0004_m_000033_321 : duration 0:00.007s
[2021-05-14 17:50:15,304] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049357924111866181549121_0004_m_000030_318: needsTaskCommit() Task attempt_202105142049357924111866181549121_0004_m_000030_318
21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049357924111866181549121_0004_m_000030_318: needsTaskCommit() Task attempt_202105142049357924111866181549121_0004_m_000030_318: duration 0:00.001s
21/05/14 20:50:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357924111866181549121_0004_m_000030_318
[2021-05-14 17:50:15,307] {docker.py:276} INFO - 21/05/14 20:50:15 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
21/05/14 20:50:15 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (7cd3ddbc35d2, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:15 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
21/05/14 20:50:15 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2256 ms on 7cd3ddbc35d2 (executor driver) (31/200)
[2021-05-14 17:50:15,319] {docker.py:276} INFO - 21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:15,337] {docker.py:276} INFO - 21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:15,338] {docker.py:276} INFO - 21/05/14 20:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:15,338] {docker.py:276} INFO - 21/05/14 20:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:15,338] {docker.py:276} INFO - 21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359052617173898192472_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359052617173898192472_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359052617173898192472_0004}; taskId=attempt_202105142049359052617173898192472_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@555e635e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:15,338] {docker.py:276} INFO - 21/05/14 20:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:15,339] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049359052617173898192472_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359052617173898192472_0004_m_000034_322
[2021-05-14 17:50:15,339] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049359052617173898192472_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359052617173898192472_0004_m_000034_322 : duration 0:00.003s
[2021-05-14 17:50:15,437] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049355848336018594732454_0004_m_000028_316: needsTaskCommit() Task attempt_202105142049355848336018594732454_0004_m_000028_316
[2021-05-14 17:50:15,437] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_202105142049355848336018594732454_0004_m_000028_316: needsTaskCommit() Task attempt_202105142049355848336018594732454_0004_m_000028_316: duration 0:00.001s
21/05/14 20:50:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355848336018594732454_0004_m_000028_316
[2021-05-14 17:50:15,442] {docker.py:276} INFO - 21/05/14 20:50:15 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
21/05/14 20:50:15 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (7cd3ddbc35d2, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:15 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
21/05/14 20:50:15 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2461 ms on 7cd3ddbc35d2 (executor driver) (32/200)
[2021-05-14 17:50:15,449] {docker.py:276} INFO - 21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:15,452] {docker.py:276} INFO - 21/05/14 20:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:15,452] {docker.py:276} INFO - 21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935832112076547930945_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935832112076547930945_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935832112076547930945_0004}; taskId=attempt_20210514204935832112076547930945_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d68afee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:15 INFO StagingCommitter: Starting: Task committer attempt_20210514204935832112076547930945_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935832112076547930945_0004_m_000035_323
[2021-05-14 17:50:15,457] {docker.py:276} INFO - 21/05/14 20:50:15 INFO StagingCommitter: Task committer attempt_20210514204935832112076547930945_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935832112076547930945_0004_m_000035_323 : duration 0:00.004s
[2021-05-14 17:50:17,368] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049351100926149209741058_0004_m_000032_320: needsTaskCommit() Task attempt_202105142049351100926149209741058_0004_m_000032_320
[2021-05-14 17:50:17,369] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049351100926149209741058_0004_m_000032_320: needsTaskCommit() Task attempt_202105142049351100926149209741058_0004_m_000032_320: duration 0:00.000s
21/05/14 20:50:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351100926149209741058_0004_m_000032_320
[2021-05-14 17:50:17,372] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4544 bytes result sent to driver
[2021-05-14 17:50:17,373] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (7cd3ddbc35d2, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:17,374] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2180 ms on 7cd3ddbc35d2 (executor driver) (33/200)
[2021-05-14 17:50:17,375] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-14 17:50:17,386] {docker.py:276} INFO - 21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:17,390] {docker.py:276} INFO - 21/05/14 20:50:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:17,391] {docker.py:276} INFO - 21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359117358748502178134_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359117358748502178134_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359117358748502178134_0004}; taskId=attempt_202105142049359117358748502178134_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27d62687}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:17,391] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049359117358748502178134_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359117358748502178134_0004_m_000036_324
[2021-05-14 17:50:17,397] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049359117358748502178134_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359117358748502178134_0004_m_000036_324 : duration 0:00.005s
[2021-05-14 17:50:17,445] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049351321333908191109717_0004_m_000033_321: needsTaskCommit() Task attempt_202105142049351321333908191109717_0004_m_000033_321
[2021-05-14 17:50:17,446] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049351321333908191109717_0004_m_000033_321: needsTaskCommit() Task attempt_202105142049351321333908191109717_0004_m_000033_321: duration 0:00.000s
21/05/14 20:50:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351321333908191109717_0004_m_000033_321
[2021-05-14 17:50:17,447] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4544 bytes result sent to driver
[2021-05-14 17:50:17,448] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (7cd3ddbc35d2, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:17,449] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
21/05/14 20:50:17 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2189 ms on 7cd3ddbc35d2 (executor driver) (34/200)
[2021-05-14 17:50:17,457] {docker.py:276} INFO - 21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:17,460] {docker.py:276} INFO - 21/05/14 20:50:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:17,461] {docker.py:276} INFO - 21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354974264508290507119_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354974264508290507119_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354974264508290507119_0004}; taskId=attempt_202105142049354974264508290507119_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60fff562}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049354974264508290507119_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354974264508290507119_0004_m_000037_325
[2021-05-14 17:50:17,463] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049354974264508290507119_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354974264508290507119_0004_m_000037_325 : duration 0:00.003s
[2021-05-14 17:50:17,493] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049359052617173898192472_0004_m_000034_322: needsTaskCommit() Task attempt_202105142049359052617173898192472_0004_m_000034_322
[2021-05-14 17:50:17,494] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049359052617173898192472_0004_m_000034_322: needsTaskCommit() Task attempt_202105142049359052617173898192472_0004_m_000034_322: duration 0:00.000s
[2021-05-14 17:50:17,494] {docker.py:276} INFO - 21/05/14 20:50:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359052617173898192472_0004_m_000034_322
[2021-05-14 17:50:17,494] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4544 bytes result sent to driver
[2021-05-14 17:50:17,495] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (7cd3ddbc35d2, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:17,496] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-14 17:50:17,496] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2193 ms on 7cd3ddbc35d2 (executor driver) (35/200)
[2021-05-14 17:50:17,503] {docker.py:276} INFO - 21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:17,504] {docker.py:276} INFO - 21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:17,506] {docker.py:276} INFO - 21/05/14 20:50:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:17,506] {docker.py:276} INFO - 21/05/14 20:50:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:17,507] {docker.py:276} INFO - 21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358496215417823621507_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358496215417823621507_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358496215417823621507_0004}; taskId=attempt_202105142049358496215417823621507_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b7a8dfb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:17,507] {docker.py:276} INFO - 21/05/14 20:50:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:17,507] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049358496215417823621507_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358496215417823621507_0004_m_000038_326
[2021-05-14 17:50:17,509] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049358496215417823621507_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358496215417823621507_0004_m_000038_326 : duration 0:00.003s
[2021-05-14 17:50:17,617] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_20210514204935832112076547930945_0004_m_000035_323: needsTaskCommit() Task attempt_20210514204935832112076547930945_0004_m_000035_323
[2021-05-14 17:50:17,618] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_20210514204935832112076547930945_0004_m_000035_323: needsTaskCommit() Task attempt_20210514204935832112076547930945_0004_m_000035_323: duration 0:00.001s
21/05/14 20:50:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935832112076547930945_0004_m_000035_323
[2021-05-14 17:50:17,620] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-14 17:50:17,622] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (7cd3ddbc35d2, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:17,623] {docker.py:276} INFO - 21/05/14 20:50:17 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2186 ms on 7cd3ddbc35d2 (executor driver) (36/200)
[2021-05-14 17:50:17,625] {docker.py:276} INFO - 21/05/14 20:50:17 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-14 17:50:17,643] {docker.py:276} INFO - 21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:17,646] {docker.py:276} INFO - 21/05/14 20:50:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:17,646] {docker.py:276} INFO - 21/05/14 20:50:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352877123856501461497_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877123856501461497_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352877123856501461497_0004}; taskId=attempt_202105142049352877123856501461497_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1006d372}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:17 INFO StagingCommitter: Starting: Task committer attempt_202105142049352877123856501461497_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877123856501461497_0004_m_000039_327
[2021-05-14 17:50:17,650] {docker.py:276} INFO - 21/05/14 20:50:17 INFO StagingCommitter: Task committer attempt_202105142049352877123856501461497_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352877123856501461497_0004_m_000039_327 : duration 0:00.004s
[2021-05-14 17:50:19,611] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049359117358748502178134_0004_m_000036_324: needsTaskCommit() Task attempt_202105142049359117358748502178134_0004_m_000036_324
21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049359117358748502178134_0004_m_000036_324: needsTaskCommit() Task attempt_202105142049359117358748502178134_0004_m_000036_324: duration 0:00.001s
21/05/14 20:50:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359117358748502178134_0004_m_000036_324
[2021-05-14 17:50:19,612] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4587 bytes result sent to driver
[2021-05-14 17:50:19,612] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (7cd3ddbc35d2, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:19,613] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2243 ms on 7cd3ddbc35d2 (executor driver) (37/200)
[2021-05-14 17:50:19,614] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-14 17:50:19,624] {docker.py:276} INFO - 21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:19,626] {docker.py:276} INFO - 21/05/14 20:50:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:19,626] {docker.py:276} INFO - 21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357951543921439203898_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357951543921439203898_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357951543921439203898_0004}; taskId=attempt_202105142049357951543921439203898_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11e8f26e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049357951543921439203898_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357951543921439203898_0004_m_000040_328
[2021-05-14 17:50:19,629] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049357951543921439203898_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357951543921439203898_0004_m_000040_328 : duration 0:00.003s
[2021-05-14 17:50:19,657] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049358496215417823621507_0004_m_000038_326: needsTaskCommit() Task attempt_202105142049358496215417823621507_0004_m_000038_326
21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049358496215417823621507_0004_m_000038_326: needsTaskCommit() Task attempt_202105142049358496215417823621507_0004_m_000038_326: duration 0:00.000s
[2021-05-14 17:50:19,658] {docker.py:276} INFO - 21/05/14 20:50:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358496215417823621507_0004_m_000038_326
[2021-05-14 17:50:19,658] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4587 bytes result sent to driver
[2021-05-14 17:50:19,659] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (7cd3ddbc35d2, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:19,660] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 2167 ms on 7cd3ddbc35d2 (executor driver) (38/200)
[2021-05-14 17:50:19,660] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-14 17:50:19,667] {docker.py:276} INFO - 21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:19,668] {docker.py:276} INFO - 21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:19,670] {docker.py:276} INFO - 21/05/14 20:50:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:19,670] {docker.py:276} INFO - 21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358005736677111456448_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358005736677111456448_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358005736677111456448_0004}; taskId=attempt_202105142049358005736677111456448_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d1911c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049358005736677111456448_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358005736677111456448_0004_m_000041_329
[2021-05-14 17:50:19,673] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049358005736677111456448_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358005736677111456448_0004_m_000041_329 : duration 0:00.004s
[2021-05-14 17:50:19,783] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049352877123856501461497_0004_m_000039_327: needsTaskCommit() Task attempt_202105142049352877123856501461497_0004_m_000039_327
[2021-05-14 17:50:19,784] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049352877123856501461497_0004_m_000039_327: needsTaskCommit() Task attempt_202105142049352877123856501461497_0004_m_000039_327: duration 0:00.002s
21/05/14 20:50:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352877123856501461497_0004_m_000039_327
[2021-05-14 17:50:19,785] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4587 bytes result sent to driver
[2021-05-14 17:50:19,787] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (7cd3ddbc35d2, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:19,789] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
21/05/14 20:50:19 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2170 ms on 7cd3ddbc35d2 (executor driver) (39/200)
[2021-05-14 17:50:19,800] {docker.py:276} INFO - 21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:19,802] {docker.py:276} INFO - 21/05/14 20:50:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:19,803] {docker.py:276} INFO - 21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356334209948415726177_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356334209948415726177_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356334209948415726177_0004}; taskId=attempt_202105142049356334209948415726177_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fe83e5f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:19,803] {docker.py:276} INFO - 21/05/14 20:50:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049356334209948415726177_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356334209948415726177_0004_m_000042_330
[2021-05-14 17:50:19,805] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049356334209948415726177_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356334209948415726177_0004_m_000042_330 : duration 0:00.003s
[2021-05-14 17:50:19,876] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049354974264508290507119_0004_m_000037_325: needsTaskCommit() Task attempt_202105142049354974264508290507119_0004_m_000037_325
[2021-05-14 17:50:19,878] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049354974264508290507119_0004_m_000037_325: needsTaskCommit() Task attempt_202105142049354974264508290507119_0004_m_000037_325: duration 0:00.002s
21/05/14 20:50:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354974264508290507119_0004_m_000037_325
[2021-05-14 17:50:19,880] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4587 bytes result sent to driver
[2021-05-14 17:50:19,881] {docker.py:276} INFO - 21/05/14 20:50:19 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (7cd3ddbc35d2, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:19,882] {docker.py:276} INFO - 21/05/14 20:50:19 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
21/05/14 20:50:19 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2436 ms on 7cd3ddbc35d2 (executor driver) (40/200)
[2021-05-14 17:50:19,894] {docker.py:276} INFO - 21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:19,896] {docker.py:276} INFO - 21/05/14 20:50:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:19,897] {docker.py:276} INFO - 21/05/14 20:50:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356865604761691003496_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356865604761691003496_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356865604761691003496_0004}; taskId=attempt_202105142049356865604761691003496_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d0d365f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049356865604761691003496_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356865604761691003496_0004_m_000043_331
[2021-05-14 17:50:19,899] {docker.py:276} INFO - 21/05/14 20:50:19 INFO StagingCommitter: Task committer attempt_202105142049356865604761691003496_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356865604761691003496_0004_m_000043_331 : duration 0:00.003s
[2021-05-14 17:50:21,899] {docker.py:276} INFO - 21/05/14 20:50:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049357951543921439203898_0004_m_000040_328: needsTaskCommit() Task attempt_202105142049357951543921439203898_0004_m_000040_328
21/05/14 20:50:21 INFO StagingCommitter: Task committer attempt_202105142049357951543921439203898_0004_m_000040_328: needsTaskCommit() Task attempt_202105142049357951543921439203898_0004_m_000040_328: duration 0:00.001s
21/05/14 20:50:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357951543921439203898_0004_m_000040_328
[2021-05-14 17:50:21,901] {docker.py:276} INFO - 21/05/14 20:50:21 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4544 bytes result sent to driver
[2021-05-14 17:50:21,902] {docker.py:276} INFO - 21/05/14 20:50:21 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (7cd3ddbc35d2, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:21,904] {docker.py:276} INFO - 21/05/14 20:50:21 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 2295 ms on 7cd3ddbc35d2 (executor driver) (41/200)
21/05/14 20:50:21 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-14 17:50:21,910] {docker.py:276} INFO - 21/05/14 20:50:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049358005736677111456448_0004_m_000041_329: needsTaskCommit() Task attempt_202105142049358005736677111456448_0004_m_000041_329
[2021-05-14 17:50:21,911] {docker.py:276} INFO - 21/05/14 20:50:21 INFO StagingCommitter: Task committer attempt_202105142049358005736677111456448_0004_m_000041_329: needsTaskCommit() Task attempt_202105142049358005736677111456448_0004_m_000041_329: duration 0:00.000s
[2021-05-14 17:50:21,912] {docker.py:276} INFO - 21/05/14 20:50:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358005736677111456448_0004_m_000041_329
[2021-05-14 17:50:21,913] {docker.py:276} INFO - 21/05/14 20:50:21 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4544 bytes result sent to driver
[2021-05-14 17:50:21,913] {docker.py:276} INFO - 21/05/14 20:50:21 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (7cd3ddbc35d2, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:21,914] {docker.py:276} INFO - 21/05/14 20:50:21 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
21/05/14 20:50:21 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 2258 ms on 7cd3ddbc35d2 (executor driver) (42/200)
[2021-05-14 17:50:21,918] {docker.py:276} INFO - 21/05/14 20:50:21 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:21,923] {docker.py:276} INFO - 21/05/14 20:50:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:21,924] {docker.py:276} INFO - 21/05/14 20:50:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352208138831595867604_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352208138831595867604_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352208138831595867604_0004}; taskId=attempt_202105142049352208138831595867604_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f5e523f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049352208138831595867604_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352208138831595867604_0004_m_000044_332
[2021-05-14 17:50:21,924] {docker.py:276} INFO - 21/05/14 20:50:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:21,925] {docker.py:276} INFO - 21/05/14 20:50:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354711156368073453046_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354711156368073453046_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354711156368073453046_0004}; taskId=attempt_202105142049354711156368073453046_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@731ebcc1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049354711156368073453046_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354711156368073453046_0004_m_000045_333
[2021-05-14 17:50:21,926] {docker.py:276} INFO - 21/05/14 20:50:21 INFO StagingCommitter: Task committer attempt_202105142049352208138831595867604_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352208138831595867604_0004_m_000044_332 : duration 0:00.005s
[2021-05-14 17:50:21,931] {docker.py:276} INFO - 21/05/14 20:50:21 INFO StagingCommitter: Task committer attempt_202105142049354711156368073453046_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354711156368073453046_0004_m_000045_333 : duration 0:00.006s
[2021-05-14 17:50:22,092] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Starting: Task committer attempt_202105142049356334209948415726177_0004_m_000042_330: needsTaskCommit() Task attempt_202105142049356334209948415726177_0004_m_000042_330
[2021-05-14 17:50:22,092] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Task committer attempt_202105142049356334209948415726177_0004_m_000042_330: needsTaskCommit() Task attempt_202105142049356334209948415726177_0004_m_000042_330: duration 0:00.000s
21/05/14 20:50:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356334209948415726177_0004_m_000042_330
[2021-05-14 17:50:22,093] {docker.py:276} INFO - 21/05/14 20:50:22 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4544 bytes result sent to driver
[2021-05-14 17:50:22,095] {docker.py:276} INFO - 21/05/14 20:50:22 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (7cd3ddbc35d2, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:22,096] {docker.py:276} INFO - 21/05/14 20:50:22 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2311 ms on 7cd3ddbc35d2 (executor driver) (43/200)
[2021-05-14 17:50:22,096] {docker.py:276} INFO - 21/05/14 20:50:22 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-14 17:50:22,106] {docker.py:276} INFO - 21/05/14 20:50:22 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:22,108] {docker.py:276} INFO - 21/05/14 20:50:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353471773177378046194_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353471773177378046194_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353471773177378046194_0004}; taskId=attempt_202105142049353471773177378046194_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7913857e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:22,108] {docker.py:276} INFO - 21/05/14 20:50:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:22 INFO StagingCommitter: Starting: Task committer attempt_202105142049353471773177378046194_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353471773177378046194_0004_m_000046_334
[2021-05-14 17:50:22,111] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Task committer attempt_202105142049353471773177378046194_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353471773177378046194_0004_m_000046_334 : duration 0:00.003s
[2021-05-14 17:50:22,247] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Starting: Task committer attempt_202105142049356865604761691003496_0004_m_000043_331: needsTaskCommit() Task attempt_202105142049356865604761691003496_0004_m_000043_331
[2021-05-14 17:50:22,249] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Task committer attempt_202105142049356865604761691003496_0004_m_000043_331: needsTaskCommit() Task attempt_202105142049356865604761691003496_0004_m_000043_331: duration 0:00.001s
[2021-05-14 17:50:22,250] {docker.py:276} INFO - 21/05/14 20:50:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356865604761691003496_0004_m_000043_331
[2021-05-14 17:50:22,251] {docker.py:276} INFO - 21/05/14 20:50:22 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4544 bytes result sent to driver
[2021-05-14 17:50:22,253] {docker.py:276} INFO - 21/05/14 20:50:22 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (7cd3ddbc35d2, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:22,253] {docker.py:276} INFO - 21/05/14 20:50:22 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2376 ms on 7cd3ddbc35d2 (executor driver) (44/200)
[2021-05-14 17:50:22,255] {docker.py:276} INFO - 21/05/14 20:50:22 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-14 17:50:22,263] {docker.py:276} INFO - 21/05/14 20:50:22 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:22,266] {docker.py:276} INFO - 21/05/14 20:50:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935800573498199473650_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935800573498199473650_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935800573498199473650_0004}; taskId=attempt_20210514204935800573498199473650_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b3ba310}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:22,266] {docker.py:276} INFO - 21/05/14 20:50:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:22,267] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Starting: Task committer attempt_20210514204935800573498199473650_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935800573498199473650_0004_m_000047_335
[2021-05-14 17:50:22,270] {docker.py:276} INFO - 21/05/14 20:50:22 INFO StagingCommitter: Task committer attempt_20210514204935800573498199473650_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935800573498199473650_0004_m_000047_335 : duration 0:00.004s
[2021-05-14 17:50:24,193] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049354711156368073453046_0004_m_000045_333: needsTaskCommit() Task attempt_202105142049354711156368073453046_0004_m_000045_333
[2021-05-14 17:50:24,194] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049354711156368073453046_0004_m_000045_333: needsTaskCommit() Task attempt_202105142049354711156368073453046_0004_m_000045_333: duration 0:00.001s
21/05/14 20:50:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354711156368073453046_0004_m_000045_333
[2021-05-14 17:50:24,195] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4544 bytes result sent to driver
[2021-05-14 17:50:24,197] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (7cd3ddbc35d2, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:24,198] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
21/05/14 20:50:24 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2287 ms on 7cd3ddbc35d2 (executor driver) (45/200)
[2021-05-14 17:50:24,216] {docker.py:276} INFO - 21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Getting 5 (41.9 KiB) non-empty blocks including 5 (41.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:24,218] {docker.py:276} INFO - 21/05/14 20:50:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:24,219] {docker.py:276} INFO - 21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358183898087479122787_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358183898087479122787_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358183898087479122787_0004}; taskId=attempt_202105142049358183898087479122787_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b764186}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049358183898087479122787_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358183898087479122787_0004_m_000048_336
[2021-05-14 17:50:24,221] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049358183898087479122787_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358183898087479122787_0004_m_000048_336 : duration 0:00.003s
[2021-05-14 17:50:24,250] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049353471773177378046194_0004_m_000046_334: needsTaskCommit() Task attempt_202105142049353471773177378046194_0004_m_000046_334
[2021-05-14 17:50:24,251] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049353471773177378046194_0004_m_000046_334: needsTaskCommit() Task attempt_202105142049353471773177378046194_0004_m_000046_334: duration 0:00.000s
[2021-05-14 17:50:24,251] {docker.py:276} INFO - 21/05/14 20:50:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353471773177378046194_0004_m_000046_334
[2021-05-14 17:50:24,252] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049352208138831595867604_0004_m_000044_332: needsTaskCommit() Task attempt_202105142049352208138831595867604_0004_m_000044_332
21/05/14 20:50:24 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4587 bytes result sent to driver
[2021-05-14 17:50:24,253] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049352208138831595867604_0004_m_000044_332: needsTaskCommit() Task attempt_202105142049352208138831595867604_0004_m_000044_332: duration 0:00.000s
21/05/14 20:50:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352208138831595867604_0004_m_000044_332
[2021-05-14 17:50:24,253] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (7cd3ddbc35d2, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:24 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4587 bytes result sent to driver
[2021-05-14 17:50:24,253] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-14 17:50:24,254] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (7cd3ddbc35d2, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:24,255] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2356 ms on 7cd3ddbc35d2 (executor driver) (46/200)
[2021-05-14 17:50:24,256] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-14 17:50:24,256] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2164 ms on 7cd3ddbc35d2 (executor driver) (47/200)
[2021-05-14 17:50:24,263] {docker.py:276} INFO - 21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:24,265] {docker.py:276} INFO - 21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:24,265] {docker.py:276} INFO - 21/05/14 20:50:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:24,266] {docker.py:276} INFO - 21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935577161484590819593_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935577161484590819593_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935577161484590819593_0004}; taskId=attempt_20210514204935577161484590819593_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ae7020}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:24,266] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_20210514204935577161484590819593_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935577161484590819593_0004_m_000050_338
[2021-05-14 17:50:24,268] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_20210514204935577161484590819593_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935577161484590819593_0004_m_000050_338 : duration 0:00.002s
[2021-05-14 17:50:24,271] {docker.py:276} INFO - 21/05/14 20:50:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:24,271] {docker.py:276} INFO - 21/05/14 20:50:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:24,272] {docker.py:276} INFO - 21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:24,272] {docker.py:276} INFO - 21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351517926513640130007_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351517926513640130007_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351517926513640130007_0004}; taskId=attempt_202105142049351517926513640130007_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54221673}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:24,273] {docker.py:276} INFO - 21/05/14 20:50:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:24,274] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049351517926513640130007_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351517926513640130007_0004_m_000049_337
[2021-05-14 17:50:24,276] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049351517926513640130007_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351517926513640130007_0004_m_000049_337 : duration 0:00.003s
[2021-05-14 17:50:24,577] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_20210514204935800573498199473650_0004_m_000047_335: needsTaskCommit() Task attempt_20210514204935800573498199473650_0004_m_000047_335
21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_20210514204935800573498199473650_0004_m_000047_335: needsTaskCommit() Task attempt_20210514204935800573498199473650_0004_m_000047_335: duration 0:00.000s
21/05/14 20:50:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935800573498199473650_0004_m_000047_335
[2021-05-14 17:50:24,578] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4587 bytes result sent to driver
[2021-05-14 17:50:24,581] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (7cd3ddbc35d2, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:24,582] {docker.py:276} INFO - 21/05/14 20:50:24 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-14 17:50:24,583] {docker.py:276} INFO - 21/05/14 20:50:24 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2332 ms on 7cd3ddbc35d2 (executor driver) (48/200)
[2021-05-14 17:50:24,592] {docker.py:276} INFO - 21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:24,594] {docker.py:276} INFO - 21/05/14 20:50:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356857377332061815396_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356857377332061815396_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356857377332061815396_0004}; taskId=attempt_202105142049356857377332061815396_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@702867c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:24,594] {docker.py:276} INFO - 21/05/14 20:50:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:24,595] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049356857377332061815396_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356857377332061815396_0004_m_000051_339
[2021-05-14 17:50:24,597] {docker.py:276} INFO - 21/05/14 20:50:24 INFO StagingCommitter: Task committer attempt_202105142049356857377332061815396_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356857377332061815396_0004_m_000051_339 : duration 0:00.003s
[2021-05-14 17:50:26,408] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_20210514204935577161484590819593_0004_m_000050_338: needsTaskCommit() Task attempt_20210514204935577161484590819593_0004_m_000050_338
[2021-05-14 17:50:26,409] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_20210514204935577161484590819593_0004_m_000050_338: needsTaskCommit() Task attempt_20210514204935577161484590819593_0004_m_000050_338: duration 0:00.001s
21/05/14 20:50:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935577161484590819593_0004_m_000050_338
[2021-05-14 17:50:26,412] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-14 17:50:26,414] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (7cd3ddbc35d2, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:26,415] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2163 ms on 7cd3ddbc35d2 (executor driver) (49/200)
[2021-05-14 17:50:26,415] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-14 17:50:26,416] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049351517926513640130007_0004_m_000049_337: needsTaskCommit() Task attempt_202105142049351517926513640130007_0004_m_000049_337
[2021-05-14 17:50:26,416] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049351517926513640130007_0004_m_000049_337: needsTaskCommit() Task attempt_202105142049351517926513640130007_0004_m_000049_337: duration 0:00.000s
21/05/14 20:50:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351517926513640130007_0004_m_000049_337
[2021-05-14 17:50:26,419] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-14 17:50:26,420] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (7cd3ddbc35d2, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:26,420] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2171 ms on 7cd3ddbc35d2 (executor driver) (50/200)
[2021-05-14 17:50:26,421] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-14 17:50:26,430] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:26,430] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:26,430] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:26,431] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:26,432] {docker.py:276} INFO - 21/05/14 20:50:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:26,433] {docker.py:276} INFO - 21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355599212807302769347_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355599212807302769347_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355599212807302769347_0004}; taskId=attempt_202105142049355599212807302769347_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@db9da35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049355599212807302769347_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355599212807302769347_0004_m_000052_340
[2021-05-14 17:50:26,433] {docker.py:276} INFO - 21/05/14 20:50:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:26,434] {docker.py:276} INFO - 21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:26,434] {docker.py:276} INFO - 21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358621290100393145065_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358621290100393145065_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358621290100393145065_0004}; taskId=attempt_202105142049358621290100393145065_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12161e6b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:26,434] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049358621290100393145065_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358621290100393145065_0004_m_000053_341
[2021-05-14 17:50:26,436] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049355599212807302769347_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355599212807302769347_0004_m_000052_340 : duration 0:00.004s
[2021-05-14 17:50:26,436] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049358621290100393145065_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358621290100393145065_0004_m_000053_341 : duration 0:00.003s
[2021-05-14 17:50:26,540] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049358183898087479122787_0004_m_000048_336: needsTaskCommit() Task attempt_202105142049358183898087479122787_0004_m_000048_336
[2021-05-14 17:50:26,541] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049358183898087479122787_0004_m_000048_336: needsTaskCommit() Task attempt_202105142049358183898087479122787_0004_m_000048_336: duration 0:00.001s
21/05/14 20:50:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358183898087479122787_0004_m_000048_336
[2021-05-14 17:50:26,542] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4587 bytes result sent to driver
[2021-05-14 17:50:26,543] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (7cd3ddbc35d2, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:26,544] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 2352 ms on 7cd3ddbc35d2 (executor driver) (51/200)
[2021-05-14 17:50:26,545] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-14 17:50:26,553] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:26,555] {docker.py:276} INFO - 21/05/14 20:50:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:26,555] {docker.py:276} INFO - 21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352025785016258004215_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352025785016258004215_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352025785016258004215_0004}; taskId=attempt_202105142049352025785016258004215_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c4c79fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:26,555] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049352025785016258004215_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352025785016258004215_0004_m_000054_342
[2021-05-14 17:50:26,558] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049352025785016258004215_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352025785016258004215_0004_m_000054_342 : duration 0:00.003s
[2021-05-14 17:50:26,847] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049356857377332061815396_0004_m_000051_339: needsTaskCommit() Task attempt_202105142049356857377332061815396_0004_m_000051_339
21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049356857377332061815396_0004_m_000051_339: needsTaskCommit() Task attempt_202105142049356857377332061815396_0004_m_000051_339: duration 0:00.001s
21/05/14 20:50:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356857377332061815396_0004_m_000051_339
[2021-05-14 17:50:26,849] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-14 17:50:26,851] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (7cd3ddbc35d2, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:26,852] {docker.py:276} INFO - 21/05/14 20:50:26 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-14 17:50:26,853] {docker.py:276} INFO - 21/05/14 20:50:26 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2276 ms on 7cd3ddbc35d2 (executor driver) (52/200)
[2021-05-14 17:50:26,862] {docker.py:276} INFO - 21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:26,864] {docker.py:276} INFO - 21/05/14 20:50:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351334917358798568698_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351334917358798568698_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351334917358798568698_0004}; taskId=attempt_202105142049351334917358798568698_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75b2fc8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049351334917358798568698_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351334917358798568698_0004_m_000055_343
[2021-05-14 17:50:26,870] {docker.py:276} INFO - 21/05/14 20:50:26 INFO StagingCommitter: Task committer attempt_202105142049351334917358798568698_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351334917358798568698_0004_m_000055_343 : duration 0:00.005s
[2021-05-14 17:50:28,476] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049355599212807302769347_0004_m_000052_340: needsTaskCommit() Task attempt_202105142049355599212807302769347_0004_m_000052_340
[2021-05-14 17:50:28,477] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049355599212807302769347_0004_m_000052_340: needsTaskCommit() Task attempt_202105142049355599212807302769347_0004_m_000052_340: duration 0:00.001s
21/05/14 20:50:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355599212807302769347_0004_m_000052_340
[2021-05-14 17:50:28,478] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4544 bytes result sent to driver
[2021-05-14 17:50:28,480] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (7cd3ddbc35d2, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:28,481] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2070 ms on 7cd3ddbc35d2 (executor driver) (53/200)
21/05/14 20:50:28 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-14 17:50:28,491] {docker.py:276} INFO - 21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:28,493] {docker.py:276} INFO - 21/05/14 20:50:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:28,494] {docker.py:276} INFO - 21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358801008878812188259_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358801008878812188259_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358801008878812188259_0004}; taskId=attempt_202105142049358801008878812188259_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75260c15}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049358801008878812188259_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358801008878812188259_0004_m_000056_344
[2021-05-14 17:50:28,499] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049358801008878812188259_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358801008878812188259_0004_m_000056_344 : duration 0:00.005s
[2021-05-14 17:50:28,702] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049358621290100393145065_0004_m_000053_341: needsTaskCommit() Task attempt_202105142049358621290100393145065_0004_m_000053_341
[2021-05-14 17:50:28,703] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049358621290100393145065_0004_m_000053_341: needsTaskCommit() Task attempt_202105142049358621290100393145065_0004_m_000053_341: duration 0:00.001s
21/05/14 20:50:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358621290100393145065_0004_m_000053_341
[2021-05-14 17:50:28,704] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-14 17:50:28,704] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (7cd3ddbc35d2, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:28,705] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-14 17:50:28,706] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2289 ms on 7cd3ddbc35d2 (executor driver) (54/200)
[2021-05-14 17:50:28,716] {docker.py:276} INFO - 21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:28,718] {docker.py:276} INFO - 21/05/14 20:50:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:28,718] {docker.py:276} INFO - 21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358530074193058988644_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358530074193058988644_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358530074193058988644_0004}; taskId=attempt_202105142049358530074193058988644_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22ad190b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049358530074193058988644_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358530074193058988644_0004_m_000057_345
[2021-05-14 17:50:28,721] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049358530074193058988644_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358530074193058988644_0004_m_000057_345 : duration 0:00.003s
[2021-05-14 17:50:28,802] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049352025785016258004215_0004_m_000054_342: needsTaskCommit() Task attempt_202105142049352025785016258004215_0004_m_000054_342
[2021-05-14 17:50:28,803] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049352025785016258004215_0004_m_000054_342: needsTaskCommit() Task attempt_202105142049352025785016258004215_0004_m_000054_342: duration 0:00.001s
21/05/14 20:50:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352025785016258004215_0004_m_000054_342
[2021-05-14 17:50:28,804] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-14 17:50:28,806] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (7cd3ddbc35d2, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:28,807] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-14 17:50:28,809] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2267 ms on 7cd3ddbc35d2 (executor driver) (55/200)
[2021-05-14 17:50:28,820] {docker.py:276} INFO - 21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:28,820] {docker.py:276} INFO - 21/05/14 20:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:28,822] {docker.py:276} INFO - 21/05/14 20:50:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:28,823] {docker.py:276} INFO - 21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:28,823] {docker.py:276} INFO - 21/05/14 20:50:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358840216142924552719_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358840216142924552719_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358840216142924552719_0004}; taskId=attempt_202105142049358840216142924552719_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a88a73c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:28,823] {docker.py:276} INFO - 21/05/14 20:50:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:28,823] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049358840216142924552719_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358840216142924552719_0004_m_000058_346
[2021-05-14 17:50:28,826] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049358840216142924552719_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358840216142924552719_0004_m_000058_346 : duration 0:00.003s
[2021-05-14 17:50:28,969] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049351334917358798568698_0004_m_000055_343: needsTaskCommit() Task attempt_202105142049351334917358798568698_0004_m_000055_343
[2021-05-14 17:50:28,970] {docker.py:276} INFO - 21/05/14 20:50:28 INFO StagingCommitter: Task committer attempt_202105142049351334917358798568698_0004_m_000055_343: needsTaskCommit() Task attempt_202105142049351334917358798568698_0004_m_000055_343: duration 0:00.001s
[2021-05-14 17:50:28,970] {docker.py:276} INFO - 21/05/14 20:50:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351334917358798568698_0004_m_000055_343
[2021-05-14 17:50:28,972] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-14 17:50:28,973] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (7cd3ddbc35d2, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:28,975] {docker.py:276} INFO - 21/05/14 20:50:28 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-14 17:50:28,975] {docker.py:276} INFO - 21/05/14 20:50:28 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2127 ms on 7cd3ddbc35d2 (executor driver) (56/200)
[2021-05-14 17:50:28,998] {docker.py:276} INFO - 21/05/14 20:50:29 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:28,999] {docker.py:276} INFO - 21/05/14 20:50:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:29,000] {docker.py:276} INFO - 21/05/14 20:50:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357704891907552565227_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357704891907552565227_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357704891907552565227_0004}; taskId=attempt_202105142049357704891907552565227_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@56890f73}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:29,000] {docker.py:276} INFO - 21/05/14 20:50:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049357704891907552565227_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357704891907552565227_0004_m_000059_347
[2021-05-14 17:50:29,002] {docker.py:276} INFO - 21/05/14 20:50:29 INFO StagingCommitter: Task committer attempt_202105142049357704891907552565227_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357704891907552565227_0004_m_000059_347 : duration 0:00.003s
[2021-05-14 17:50:30,683] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Starting: Task committer attempt_202105142049358801008878812188259_0004_m_000056_344: needsTaskCommit() Task attempt_202105142049358801008878812188259_0004_m_000056_344
21/05/14 20:50:30 INFO StagingCommitter: Task committer attempt_202105142049358801008878812188259_0004_m_000056_344: needsTaskCommit() Task attempt_202105142049358801008878812188259_0004_m_000056_344: duration 0:00.000s
21/05/14 20:50:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358801008878812188259_0004_m_000056_344
[2021-05-14 17:50:30,686] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-14 17:50:30,687] {docker.py:276} INFO - 21/05/14 20:50:30 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (7cd3ddbc35d2, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:30,688] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
21/05/14 20:50:30 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2211 ms on 7cd3ddbc35d2 (executor driver) (57/200)
[2021-05-14 17:50:30,699] {docker.py:276} INFO - 21/05/14 20:50:30 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:30,701] {docker.py:276} INFO - 21/05/14 20:50:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:30,701] {docker.py:276} INFO - 21/05/14 20:50:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:30,702] {docker.py:276} INFO - 21/05/14 20:50:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351462715270504889151_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351462715270504889151_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351462715270504889151_0004}; taskId=attempt_202105142049351462715270504889151_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15aa9f8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:30,702] {docker.py:276} INFO - 21/05/14 20:50:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:30,702] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Starting: Task committer attempt_202105142049351462715270504889151_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351462715270504889151_0004_m_000060_348
[2021-05-14 17:50:30,707] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Task committer attempt_202105142049351462715270504889151_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351462715270504889151_0004_m_000060_348 : duration 0:00.005s
[2021-05-14 17:50:30,962] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Starting: Task committer attempt_202105142049358530074193058988644_0004_m_000057_345: needsTaskCommit() Task attempt_202105142049358530074193058988644_0004_m_000057_345
[2021-05-14 17:50:30,963] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Task committer attempt_202105142049358530074193058988644_0004_m_000057_345: needsTaskCommit() Task attempt_202105142049358530074193058988644_0004_m_000057_345: duration 0:00.001s
21/05/14 20:50:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358530074193058988644_0004_m_000057_345
[2021-05-14 17:50:30,964] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4587 bytes result sent to driver
[2021-05-14 17:50:30,968] {docker.py:276} INFO - 21/05/14 20:50:30 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (7cd3ddbc35d2, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:30,969] {docker.py:276} INFO - 21/05/14 20:50:30 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2266 ms on 7cd3ddbc35d2 (executor driver) (58/200)
[2021-05-14 17:50:30,970] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Starting: Task committer attempt_202105142049358840216142924552719_0004_m_000058_346: needsTaskCommit() Task attempt_202105142049358840216142924552719_0004_m_000058_346
[2021-05-14 17:50:30,971] {docker.py:276} INFO - 21/05/14 20:50:30 INFO StagingCommitter: Task committer attempt_202105142049358840216142924552719_0004_m_000058_346: needsTaskCommit() Task attempt_202105142049358840216142924552719_0004_m_000058_346: duration 0:00.001s
[2021-05-14 17:50:30,971] {docker.py:276} INFO - 21/05/14 20:50:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358840216142924552719_0004_m_000058_346
[2021-05-14 17:50:30,972] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-14 17:50:30,972] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-14 17:50:30,973] {docker.py:276} INFO - 21/05/14 20:50:30 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (7cd3ddbc35d2, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:30,973] {docker.py:276} INFO - 21/05/14 20:50:30 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2170 ms on 7cd3ddbc35d2 (executor driver) (59/200)
[2021-05-14 17:50:30,974] {docker.py:276} INFO - 21/05/14 20:50:30 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-14 17:50:30,982] {docker.py:276} INFO - 21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:30,983] {docker.py:276} INFO - 21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:30,985] {docker.py:276} INFO - 21/05/14 20:50:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:30,985] {docker.py:276} INFO - 21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355903142507762395989_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355903142507762395989_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355903142507762395989_0004}; taskId=attempt_202105142049355903142507762395989_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73d559fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:30,986] {docker.py:276} INFO - 21/05/14 20:50:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049355903142507762395989_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355903142507762395989_0004_m_000062_350
[2021-05-14 17:50:30,986] {docker.py:276} INFO - 21/05/14 20:50:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:30,986] {docker.py:276} INFO - 21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935552820768403370076_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935552820768403370076_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935552820768403370076_0004}; taskId=attempt_20210514204935552820768403370076_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26aaaffc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:30,987] {docker.py:276} INFO - 21/05/14 20:50:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:30,987] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Starting: Task committer attempt_20210514204935552820768403370076_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935552820768403370076_0004_m_000061_349
[2021-05-14 17:50:30,990] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Task committer attempt_202105142049355903142507762395989_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355903142507762395989_0004_m_000062_350 : duration 0:00.004s
[2021-05-14 17:50:30,990] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Task committer attempt_20210514204935552820768403370076_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935552820768403370076_0004_m_000061_349 : duration 0:00.003s
[2021-05-14 17:50:31,164] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049357704891907552565227_0004_m_000059_347: needsTaskCommit() Task attempt_202105142049357704891907552565227_0004_m_000059_347
[2021-05-14 17:50:31,166] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Task committer attempt_202105142049357704891907552565227_0004_m_000059_347: needsTaskCommit() Task attempt_202105142049357704891907552565227_0004_m_000059_347: duration 0:00.002s
[2021-05-14 17:50:31,167] {docker.py:276} INFO - 21/05/14 20:50:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357704891907552565227_0004_m_000059_347
[2021-05-14 17:50:31,167] {docker.py:276} INFO - 21/05/14 20:50:31 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-14 17:50:31,168] {docker.py:276} INFO - 21/05/14 20:50:31 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (7cd3ddbc35d2, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:31,170] {docker.py:276} INFO - 21/05/14 20:50:31 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2199 ms on 7cd3ddbc35d2 (executor driver) (60/200)
21/05/14 20:50:31 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-14 17:50:31,180] {docker.py:276} INFO - 21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:31,180] {docker.py:276} INFO - 21/05/14 20:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:31,185] {docker.py:276} INFO - 21/05/14 20:50:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:31,185] {docker.py:276} INFO - 21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:31,186] {docker.py:276} INFO - 21/05/14 20:50:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351993969654361689508_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351993969654361689508_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351993969654361689508_0004}; taskId=attempt_202105142049351993969654361689508_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c8af93f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:31,186] {docker.py:276} INFO - 21/05/14 20:50:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049351993969654361689508_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351993969654361689508_0004_m_000063_351
[2021-05-14 17:50:31,189] {docker.py:276} INFO - 21/05/14 20:50:31 INFO StagingCommitter: Task committer attempt_202105142049351993969654361689508_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351993969654361689508_0004_m_000063_351 : duration 0:00.004s
[2021-05-14 17:50:32,900] {docker.py:276} INFO - 21/05/14 20:50:32 INFO StagingCommitter: Starting: Task committer attempt_202105142049351462715270504889151_0004_m_000060_348: needsTaskCommit() Task attempt_202105142049351462715270504889151_0004_m_000060_348
21/05/14 20:50:32 INFO StagingCommitter: Task committer attempt_202105142049351462715270504889151_0004_m_000060_348: needsTaskCommit() Task attempt_202105142049351462715270504889151_0004_m_000060_348: duration 0:00.000s
21/05/14 20:50:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351462715270504889151_0004_m_000060_348
[2021-05-14 17:50:32,901] {docker.py:276} INFO - 21/05/14 20:50:32 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4544 bytes result sent to driver
[2021-05-14 17:50:32,903] {docker.py:276} INFO - 21/05/14 20:50:32 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (7cd3ddbc35d2, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:32,903] {docker.py:276} INFO - 21/05/14 20:50:32 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 2221 ms on 7cd3ddbc35d2 (executor driver) (61/200)
[2021-05-14 17:50:32,904] {docker.py:276} INFO - 21/05/14 20:50:32 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-14 17:50:32,912] {docker.py:276} INFO - 21/05/14 20:50:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:32,912] {docker.py:276} INFO - 21/05/14 20:50:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:32,913] {docker.py:276} INFO - 21/05/14 20:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:32,914] {docker.py:276} INFO - 21/05/14 20:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:32,914] {docker.py:276} INFO - 21/05/14 20:50:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:32,914] {docker.py:276} INFO - 21/05/14 20:50:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355384490427903111162_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355384490427903111162_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355384490427903111162_0004}; taskId=attempt_202105142049355384490427903111162_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a888a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:32,915] {docker.py:276} INFO - 21/05/14 20:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:32,915] {docker.py:276} INFO - 21/05/14 20:50:32 INFO StagingCommitter: Starting: Task committer attempt_202105142049355384490427903111162_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355384490427903111162_0004_m_000064_352
[2021-05-14 17:50:32,918] {docker.py:276} INFO - 21/05/14 20:50:32 INFO StagingCommitter: Task committer attempt_202105142049355384490427903111162_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355384490427903111162_0004_m_000064_352 : duration 0:00.003s
[2021-05-14 17:50:33,168] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_20210514204935552820768403370076_0004_m_000061_349: needsTaskCommit() Task attempt_20210514204935552820768403370076_0004_m_000061_349
[2021-05-14 17:50:33,169] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_20210514204935552820768403370076_0004_m_000061_349: needsTaskCommit() Task attempt_20210514204935552820768403370076_0004_m_000061_349: duration 0:00.000s
21/05/14 20:50:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935552820768403370076_0004_m_000061_349
[2021-05-14 17:50:33,170] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-14 17:50:33,172] {docker.py:276} INFO - 21/05/14 20:50:33 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (7cd3ddbc35d2, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:33,173] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
21/05/14 20:50:33 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2210 ms on 7cd3ddbc35d2 (executor driver) (62/200)
[2021-05-14 17:50:33,184] {docker.py:276} INFO - 21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Getting 5 (45.2 KiB) non-empty blocks including 5 (45.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:33,187] {docker.py:276} INFO - 21/05/14 20:50:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:33,187] {docker.py:276} INFO - 21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:33,188] {docker.py:276} INFO - 21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355619711293134306060_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355619711293134306060_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355619711293134306060_0004}; taskId=attempt_202105142049355619711293134306060_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79b74af9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:33,188] {docker.py:276} INFO - 21/05/14 20:50:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_202105142049355619711293134306060_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355619711293134306060_0004_m_000065_353
[2021-05-14 17:50:33,191] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_202105142049355619711293134306060_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355619711293134306060_0004_m_000065_353 : duration 0:00.004s
[2021-05-14 17:50:33,197] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_202105142049355903142507762395989_0004_m_000062_350: needsTaskCommit() Task attempt_202105142049355903142507762395989_0004_m_000062_350
21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_202105142049355903142507762395989_0004_m_000062_350: needsTaskCommit() Task attempt_202105142049355903142507762395989_0004_m_000062_350: duration 0:00.001s
21/05/14 20:50:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355903142507762395989_0004_m_000062_350
[2021-05-14 17:50:33,199] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-14 17:50:33,200] {docker.py:276} INFO - 21/05/14 20:50:33 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (7cd3ddbc35d2, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:33,201] {docker.py:276} INFO - 21/05/14 20:50:33 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2232 ms on 7cd3ddbc35d2 (executor driver) (63/200)
[2021-05-14 17:50:33,202] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-14 17:50:33,209] {docker.py:276} INFO - 21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:33,211] {docker.py:276} INFO - 21/05/14 20:50:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354890366873039826009_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354890366873039826009_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354890366873039826009_0004}; taskId=attempt_202105142049354890366873039826009_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@eb2b6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:33,212] {docker.py:276} INFO - 21/05/14 20:50:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_202105142049354890366873039826009_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354890366873039826009_0004_m_000066_354
[2021-05-14 17:50:33,215] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_202105142049354890366873039826009_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354890366873039826009_0004_m_000066_354 : duration 0:00.003s
[2021-05-14 17:50:33,475] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_202105142049351993969654361689508_0004_m_000063_351: needsTaskCommit() Task attempt_202105142049351993969654361689508_0004_m_000063_351
[2021-05-14 17:50:33,476] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_202105142049351993969654361689508_0004_m_000063_351: needsTaskCommit() Task attempt_202105142049351993969654361689508_0004_m_000063_351: duration 0:00.001s
21/05/14 20:50:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351993969654361689508_0004_m_000063_351
[2021-05-14 17:50:33,477] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-14 17:50:33,479] {docker.py:276} INFO - 21/05/14 20:50:33 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (7cd3ddbc35d2, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:33,480] {docker.py:276} INFO - 21/05/14 20:50:33 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-14 17:50:33,481] {docker.py:276} INFO - 21/05/14 20:50:33 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2316 ms on 7cd3ddbc35d2 (executor driver) (64/200)
[2021-05-14 17:50:33,491] {docker.py:276} INFO - 21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:33,493] {docker.py:276} INFO - 21/05/14 20:50:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:33,494] {docker.py:276} INFO - 21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:33,494] {docker.py:276} INFO - 21/05/14 20:50:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355071419890475166556_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355071419890475166556_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355071419890475166556_0004}; taskId=attempt_202105142049355071419890475166556_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17f3bd4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:33,494] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Starting: Task committer attempt_202105142049355071419890475166556_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355071419890475166556_0004_m_000067_355
[2021-05-14 17:50:33,497] {docker.py:276} INFO - 21/05/14 20:50:33 INFO StagingCommitter: Task committer attempt_202105142049355071419890475166556_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355071419890475166556_0004_m_000067_355 : duration 0:00.003s
[2021-05-14 17:50:35,236] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049355384490427903111162_0004_m_000064_352: needsTaskCommit() Task attempt_202105142049355384490427903111162_0004_m_000064_352
[2021-05-14 17:50:35,237] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049355384490427903111162_0004_m_000064_352: needsTaskCommit() Task attempt_202105142049355384490427903111162_0004_m_000064_352: duration 0:00.000s
21/05/14 20:50:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355384490427903111162_0004_m_000064_352
[2021-05-14 17:50:35,239] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-14 17:50:35,240] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (7cd3ddbc35d2, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:35,241] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2342 ms on 7cd3ddbc35d2 (executor driver) (65/200)
21/05/14 20:50:35 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-14 17:50:35,250] {docker.py:276} INFO - 21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:35,252] {docker.py:276} INFO - 21/05/14 20:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353366160961201789326_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353366160961201789326_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353366160961201789326_0004}; taskId=attempt_202105142049353366160961201789326_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d69f59a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049353366160961201789326_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353366160961201789326_0004_m_000068_356
[2021-05-14 17:50:35,255] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049353366160961201789326_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353366160961201789326_0004_m_000068_356 : duration 0:00.003s
[2021-05-14 17:50:35,305] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049355619711293134306060_0004_m_000065_353: needsTaskCommit() Task attempt_202105142049355619711293134306060_0004_m_000065_353
[2021-05-14 17:50:35,306] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049355619711293134306060_0004_m_000065_353: needsTaskCommit() Task attempt_202105142049355619711293134306060_0004_m_000065_353: duration 0:00.001s
[2021-05-14 17:50:35,307] {docker.py:276} INFO - 21/05/14 20:50:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355619711293134306060_0004_m_000065_353
[2021-05-14 17:50:35,309] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4587 bytes result sent to driver
[2021-05-14 17:50:35,311] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (7cd3ddbc35d2, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:35,312] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-14 17:50:35,312] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2143 ms on 7cd3ddbc35d2 (executor driver) (66/200)
[2021-05-14 17:50:35,322] {docker.py:276} INFO - 21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:35,324] {docker.py:276} INFO - 21/05/14 20:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935154181868238268561_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935154181868238268561_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935154181868238268561_0004}; taskId=attempt_20210514204935154181868238268561_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9cb2b12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:35,324] {docker.py:276} INFO - 21/05/14 20:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_20210514204935154181868238268561_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935154181868238268561_0004_m_000069_357
[2021-05-14 17:50:35,327] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_20210514204935154181868238268561_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935154181868238268561_0004_m_000069_357 : duration 0:00.002s
[2021-05-14 17:50:35,408] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049354890366873039826009_0004_m_000066_354: needsTaskCommit() Task attempt_202105142049354890366873039826009_0004_m_000066_354
[2021-05-14 17:50:35,410] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049354890366873039826009_0004_m_000066_354: needsTaskCommit() Task attempt_202105142049354890366873039826009_0004_m_000066_354: duration 0:00.002s
21/05/14 20:50:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354890366873039826009_0004_m_000066_354
[2021-05-14 17:50:35,412] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4587 bytes result sent to driver
[2021-05-14 17:50:35,414] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (7cd3ddbc35d2, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:35,414] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2217 ms on 7cd3ddbc35d2 (executor driver) (67/200)
[2021-05-14 17:50:35,416] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-14 17:50:35,427] {docker.py:276} INFO - 21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:35,427] {docker.py:276} INFO - 21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:35,430] {docker.py:276} INFO - 21/05/14 20:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:35,430] {docker.py:276} INFO - 21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:35,431] {docker.py:276} INFO - 21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354596165032496756273_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354596165032496756273_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354596165032496756273_0004}; taskId=attempt_202105142049354596165032496756273_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ba651a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:35,431] {docker.py:276} INFO - 21/05/14 20:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:35,431] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049354596165032496756273_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354596165032496756273_0004_m_000070_358
[2021-05-14 17:50:35,434] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049354596165032496756273_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354596165032496756273_0004_m_000070_358 : duration 0:00.004s
[2021-05-14 17:50:35,723] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049355071419890475166556_0004_m_000067_355: needsTaskCommit() Task attempt_202105142049355071419890475166556_0004_m_000067_355
[2021-05-14 17:50:35,724] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049355071419890475166556_0004_m_000067_355: needsTaskCommit() Task attempt_202105142049355071419890475166556_0004_m_000067_355: duration 0:00.001s
21/05/14 20:50:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355071419890475166556_0004_m_000067_355
[2021-05-14 17:50:35,725] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4587 bytes result sent to driver
[2021-05-14 17:50:35,727] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (7cd3ddbc35d2, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:35,727] {docker.py:276} INFO - 21/05/14 20:50:35 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-14 17:50:35,728] {docker.py:276} INFO - 21/05/14 20:50:35 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2252 ms on 7cd3ddbc35d2 (executor driver) (68/200)
[2021-05-14 17:50:35,739] {docker.py:276} INFO - 21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:35,741] {docker.py:276} INFO - 21/05/14 20:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:35,742] {docker.py:276} INFO - 21/05/14 20:50:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356516284036784733189_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356516284036784733189_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356516284036784733189_0004}; taskId=attempt_202105142049356516284036784733189_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e610519}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049356516284036784733189_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356516284036784733189_0004_m_000071_359
[2021-05-14 17:50:35,744] {docker.py:276} INFO - 21/05/14 20:50:35 INFO StagingCommitter: Task committer attempt_202105142049356516284036784733189_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356516284036784733189_0004_m_000071_359 : duration 0:00.003s
[2021-05-14 17:50:37,423] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049353366160961201789326_0004_m_000068_356: needsTaskCommit() Task attempt_202105142049353366160961201789326_0004_m_000068_356
[2021-05-14 17:50:37,424] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_202105142049353366160961201789326_0004_m_000068_356: needsTaskCommit() Task attempt_202105142049353366160961201789326_0004_m_000068_356: duration 0:00.001s
21/05/14 20:50:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353366160961201789326_0004_m_000068_356
[2021-05-14 17:50:37,425] {docker.py:276} INFO - 21/05/14 20:50:37 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4587 bytes result sent to driver
[2021-05-14 17:50:37,426] {docker.py:276} INFO - 21/05/14 20:50:37 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (7cd3ddbc35d2, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:37,427] {docker.py:276} INFO - 21/05/14 20:50:37 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-14 17:50:37,428] {docker.py:276} INFO - 21/05/14 20:50:37 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2190 ms on 7cd3ddbc35d2 (executor driver) (69/200)
[2021-05-14 17:50:37,439] {docker.py:276} INFO - 21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:37,441] {docker.py:276} INFO - 21/05/14 20:50:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:37,441] {docker.py:276} INFO - 21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355066385376884238911_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355066385376884238911_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355066385376884238911_0004}; taskId=attempt_202105142049355066385376884238911_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25943284}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049355066385376884238911_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355066385376884238911_0004_m_000072_360
[2021-05-14 17:50:37,443] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_202105142049355066385376884238911_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355066385376884238911_0004_m_000072_360 : duration 0:00.003s
[2021-05-14 17:50:37,511] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_20210514204935154181868238268561_0004_m_000069_357: needsTaskCommit() Task attempt_20210514204935154181868238268561_0004_m_000069_357
[2021-05-14 17:50:37,512] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_20210514204935154181868238268561_0004_m_000069_357: needsTaskCommit() Task attempt_20210514204935154181868238268561_0004_m_000069_357: duration 0:00.001s
21/05/14 20:50:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935154181868238268561_0004_m_000069_357
[2021-05-14 17:50:37,514] {docker.py:276} INFO - 21/05/14 20:50:37 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4544 bytes result sent to driver
[2021-05-14 17:50:37,515] {docker.py:276} INFO - 21/05/14 20:50:37 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (7cd3ddbc35d2, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:37,516] {docker.py:276} INFO - 21/05/14 20:50:37 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2208 ms on 7cd3ddbc35d2 (executor driver) (70/200)
21/05/14 20:50:37 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-14 17:50:37,526] {docker.py:276} INFO - 21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:37,528] {docker.py:276} INFO - 21/05/14 20:50:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:37,528] {docker.py:276} INFO - 21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:37,529] {docker.py:276} INFO - 21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358879100349494446439_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358879100349494446439_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358879100349494446439_0004}; taskId=attempt_202105142049358879100349494446439_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c09e5fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:37,529] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049358879100349494446439_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358879100349494446439_0004_m_000073_361
[2021-05-14 17:50:37,532] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_202105142049358879100349494446439_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358879100349494446439_0004_m_000073_361 : duration 0:00.004s
[2021-05-14 17:50:37,835] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049354596165032496756273_0004_m_000070_358: needsTaskCommit() Task attempt_202105142049354596165032496756273_0004_m_000070_358
[2021-05-14 17:50:37,835] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_202105142049354596165032496756273_0004_m_000070_358: needsTaskCommit() Task attempt_202105142049354596165032496756273_0004_m_000070_358: duration 0:00.001s
[2021-05-14 17:50:37,836] {docker.py:276} INFO - 21/05/14 20:50:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354596165032496756273_0004_m_000070_358
[2021-05-14 17:50:37,838] {docker.py:276} INFO - 21/05/14 20:50:37 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4544 bytes result sent to driver
[2021-05-14 17:50:37,839] {docker.py:276} INFO - 21/05/14 20:50:37 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (7cd3ddbc35d2, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:37,840] {docker.py:276} INFO - 21/05/14 20:50:37 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
21/05/14 20:50:37 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2430 ms on 7cd3ddbc35d2 (executor driver) (71/200)
[2021-05-14 17:50:37,850] {docker.py:276} INFO - 21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:37,852] {docker.py:276} INFO - 21/05/14 20:50:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:37,853] {docker.py:276} INFO - 21/05/14 20:50:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351658969237574280274_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351658969237574280274_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351658969237574280274_0004}; taskId=attempt_202105142049351658969237574280274_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1edbd07f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049351658969237574280274_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351658969237574280274_0004_m_000074_362
[2021-05-14 17:50:37,856] {docker.py:276} INFO - 21/05/14 20:50:37 INFO StagingCommitter: Task committer attempt_202105142049351658969237574280274_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351658969237574280274_0004_m_000074_362 : duration 0:00.004s
[2021-05-14 17:50:37,998] {docker.py:276} INFO - 21/05/14 20:50:38 INFO StagingCommitter: Starting: Task committer attempt_202105142049356516284036784733189_0004_m_000071_359: needsTaskCommit() Task attempt_202105142049356516284036784733189_0004_m_000071_359
[2021-05-14 17:50:37,998] {docker.py:276} INFO - 21/05/14 20:50:38 INFO StagingCommitter: Task committer attempt_202105142049356516284036784733189_0004_m_000071_359: needsTaskCommit() Task attempt_202105142049356516284036784733189_0004_m_000071_359: duration 0:00.001s
[2021-05-14 17:50:37,998] {docker.py:276} INFO - 21/05/14 20:50:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356516284036784733189_0004_m_000071_359
[2021-05-14 17:50:38,000] {docker.py:276} INFO - 21/05/14 20:50:38 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4544 bytes result sent to driver
[2021-05-14 17:50:38,001] {docker.py:276} INFO - 21/05/14 20:50:38 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (7cd3ddbc35d2, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:38,002] {docker.py:276} INFO - 21/05/14 20:50:38 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-14 17:50:38,003] {docker.py:276} INFO - 21/05/14 20:50:38 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2279 ms on 7cd3ddbc35d2 (executor driver) (72/200)
[2021-05-14 17:50:38,010] {docker.py:276} INFO - 21/05/14 20:50:38 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:38,012] {docker.py:276} INFO - 21/05/14 20:50:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935181960482673585964_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935181960482673585964_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935181960482673585964_0004}; taskId=attempt_20210514204935181960482673585964_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30713691}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:38 INFO StagingCommitter: Starting: Task committer attempt_20210514204935181960482673585964_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935181960482673585964_0004_m_000075_363
[2021-05-14 17:50:38,015] {docker.py:276} INFO - 21/05/14 20:50:38 INFO StagingCommitter: Task committer attempt_20210514204935181960482673585964_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935181960482673585964_0004_m_000075_363 : duration 0:00.003s
[2021-05-14 17:50:40,179] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049355066385376884238911_0004_m_000072_360: needsTaskCommit() Task attempt_202105142049355066385376884238911_0004_m_000072_360
[2021-05-14 17:50:40,180] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049351658969237574280274_0004_m_000074_362: needsTaskCommit() Task attempt_202105142049351658969237574280274_0004_m_000074_362
[2021-05-14 17:50:40,181] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049351658969237574280274_0004_m_000074_362: needsTaskCommit() Task attempt_202105142049351658969237574280274_0004_m_000074_362: duration 0:00.001s
21/05/14 20:50:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351658969237574280274_0004_m_000074_362
[2021-05-14 17:50:40,182] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049355066385376884238911_0004_m_000072_360: needsTaskCommit() Task attempt_202105142049355066385376884238911_0004_m_000072_360: duration 0:00.002s
21/05/14 20:50:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355066385376884238911_0004_m_000072_360
[2021-05-14 17:50:40,182] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-14 17:50:40,183] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (7cd3ddbc35d2, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:40,184] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4544 bytes result sent to driver
[2021-05-14 17:50:40,184] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
[2021-05-14 17:50:40,185] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (7cd3ddbc35d2, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:40,186] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 2350 ms on 7cd3ddbc35d2 (executor driver) (73/200)
[2021-05-14 17:50:40,187] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 2764 ms on 7cd3ddbc35d2 (executor driver) (74/200)
21/05/14 20:50:40 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
[2021-05-14 17:50:40,188] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_20210514204935181960482673585964_0004_m_000075_363: needsTaskCommit() Task attempt_20210514204935181960482673585964_0004_m_000075_363
[2021-05-14 17:50:40,188] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_20210514204935181960482673585964_0004_m_000075_363: needsTaskCommit() Task attempt_20210514204935181960482673585964_0004_m_000075_363: duration 0:00.000s
21/05/14 20:50:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935181960482673585964_0004_m_000075_363
[2021-05-14 17:50:40,189] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049358879100349494446439_0004_m_000073_361: needsTaskCommit() Task attempt_202105142049358879100349494446439_0004_m_000073_361
[2021-05-14 17:50:40,189] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049358879100349494446439_0004_m_000073_361: needsTaskCommit() Task attempt_202105142049358879100349494446439_0004_m_000073_361: duration 0:00.001s
21/05/14 20:50:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358879100349494446439_0004_m_000073_361
[2021-05-14 17:50:40,190] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (7cd3ddbc35d2, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:40,191] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2192 ms on 7cd3ddbc35d2 (executor driver) (75/200)
[2021-05-14 17:50:40,192] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4544 bytes result sent to driver
[2021-05-14 17:50:40,193] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (7cd3ddbc35d2, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:40,193] {docker.py:276} INFO - 21/05/14 20:50:40 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2682 ms on 7cd3ddbc35d2 (executor driver) (76/200)
[2021-05-14 17:50:40,194] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-14 17:50:40,195] {docker.py:276} INFO - 21/05/14 20:50:40 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-14 17:50:40,204] {docker.py:276} INFO - 21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:40,204] {docker.py:276} INFO - 21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:40,205] {docker.py:276} INFO - 21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:40,208] {docker.py:276} INFO - 21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:40,209] {docker.py:276} INFO - 21/05/14 20:50:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:40,209] {docker.py:276} INFO - 21/05/14 20:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:40,210] {docker.py:276} INFO - 21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354895039776879951888_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354895039776879951888_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354895039776879951888_0004}; taskId=attempt_202105142049354895039776879951888_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61d32995}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:40,211] {docker.py:276} INFO - 21/05/14 20:50:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049354895039776879951888_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354895039776879951888_0004_m_000079_367
[2021-05-14 17:50:40,211] {docker.py:276} INFO - 21/05/14 20:50:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:40,212] {docker.py:276} INFO - 21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352403283315537695336_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352403283315537695336_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352403283315537695336_0004}; taskId=attempt_202105142049352403283315537695336_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a7b1c25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:40,212] {docker.py:276} INFO - 21/05/14 20:50:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:40,213] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049352403283315537695336_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352403283315537695336_0004_m_000077_365
[2021-05-14 17:50:40,213] {docker.py:276} INFO - 21/05/14 20:50:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:40,214] {docker.py:276} INFO - 21/05/14 20:50:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:40,214] {docker.py:276} INFO - 21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358033869666040596999_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358033869666040596999_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358033869666040596999_0004}; taskId=attempt_202105142049358033869666040596999_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a777217}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:40,214] {docker.py:276} INFO - 21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:40,214] {docker.py:276} INFO - 21/05/14 20:50:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357959523217362179503_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357959523217362179503_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357959523217362179503_0004}; taskId=attempt_202105142049357959523217362179503_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45755c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:40,215] {docker.py:276} INFO - 21/05/14 20:50:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:40,215] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049354895039776879951888_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354895039776879951888_0004_m_000079_367 : duration 0:00.005s
[2021-05-14 17:50:40,215] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049358033869666040596999_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358033869666040596999_0004_m_000076_364
[2021-05-14 17:50:40,216] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Starting: Task committer attempt_202105142049357959523217362179503_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357959523217362179503_0004_m_000078_366
[2021-05-14 17:50:40,218] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049352403283315537695336_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352403283315537695336_0004_m_000077_365 : duration 0:00.007s
[2021-05-14 17:50:40,225] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049357959523217362179503_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357959523217362179503_0004_m_000078_366 : duration 0:00.009s
[2021-05-14 17:50:40,230] {docker.py:276} INFO - 21/05/14 20:50:40 INFO StagingCommitter: Task committer attempt_202105142049358033869666040596999_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358033869666040596999_0004_m_000076_364 : duration 0:00.017s
[2021-05-14 17:50:42,353] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049352403283315537695336_0004_m_000077_365: needsTaskCommit() Task attempt_202105142049352403283315537695336_0004_m_000077_365
[2021-05-14 17:50:42,354] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049358033869666040596999_0004_m_000076_364: needsTaskCommit() Task attempt_202105142049358033869666040596999_0004_m_000076_364
21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049352403283315537695336_0004_m_000077_365: needsTaskCommit() Task attempt_202105142049352403283315537695336_0004_m_000077_365: duration 0:00.001s
21/05/14 20:50:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352403283315537695336_0004_m_000077_365
[2021-05-14 17:50:42,355] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049358033869666040596999_0004_m_000076_364: needsTaskCommit() Task attempt_202105142049358033869666040596999_0004_m_000076_364: duration 0:00.003s
21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049354895039776879951888_0004_m_000079_367: needsTaskCommit() Task attempt_202105142049354895039776879951888_0004_m_000079_367
21/05/14 20:50:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358033869666040596999_0004_m_000076_364
[2021-05-14 17:50:42,356] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049354895039776879951888_0004_m_000079_367: needsTaskCommit() Task attempt_202105142049354895039776879951888_0004_m_000079_367: duration 0:00.001s
[2021-05-14 17:50:42,358] {docker.py:276} INFO - 21/05/14 20:50:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354895039776879951888_0004_m_000079_367
[2021-05-14 17:50:42,358] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4587 bytes result sent to driver
[2021-05-14 17:50:42,359] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-14 17:50:42,360] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (7cd3ddbc35d2, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:42,361] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
21/05/14 20:50:42 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (7cd3ddbc35d2, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:42,361] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2144 ms on 7cd3ddbc35d2 (executor driver) (77/200)
21/05/14 20:50:42 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-14 17:50:42,363] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2147 ms on 7cd3ddbc35d2 (executor driver) (78/200)
[2021-05-14 17:50:42,364] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049357959523217362179503_0004_m_000078_366: needsTaskCommit() Task attempt_202105142049357959523217362179503_0004_m_000078_366
[2021-05-14 17:50:42,365] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4587 bytes result sent to driver
[2021-05-14 17:50:42,366] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049357959523217362179503_0004_m_000078_366: needsTaskCommit() Task attempt_202105142049357959523217362179503_0004_m_000078_366: duration 0:00.000s
[2021-05-14 17:50:42,366] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (7cd3ddbc35d2, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:42,367] {docker.py:276} INFO - 21/05/14 20:50:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357959523217362179503_0004_m_000078_366
[2021-05-14 17:50:42,367] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2141 ms on 7cd3ddbc35d2 (executor driver) (79/200)
[2021-05-14 17:50:42,368] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-14 17:50:42,371] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4587 bytes result sent to driver
[2021-05-14 17:50:42,372] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (7cd3ddbc35d2, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:42,372] {docker.py:276} INFO - 21/05/14 20:50:42 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2149 ms on 7cd3ddbc35d2 (executor driver) (80/200)
[2021-05-14 17:50:42,373] {docker.py:276} INFO - 21/05/14 20:50:42 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-14 17:50:42,378] {docker.py:276} INFO - 21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:42,378] {docker.py:276} INFO - 21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:42,379] {docker.py:276} INFO - 21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:42,380] {docker.py:276} INFO - 21/05/14 20:50:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:42,381] {docker.py:276} INFO - 21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358342035077142035975_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358342035077142035975_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358342035077142035975_0004}; taskId=attempt_202105142049358342035077142035975_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13c16673}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049358342035077142035975_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358342035077142035975_0004_m_000081_369
[2021-05-14 17:50:42,382] {docker.py:276} INFO - 21/05/14 20:50:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:42,382] {docker.py:276} INFO - 21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:42,383] {docker.py:276} INFO - 21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935604711510384552075_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935604711510384552075_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935604711510384552075_0004}; taskId=attempt_20210514204935604711510384552075_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e2a1c3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:42,383] {docker.py:276} INFO - 21/05/14 20:50:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:42,384] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_20210514204935604711510384552075_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935604711510384552075_0004_m_000082_370
[2021-05-14 17:50:42,384] {docker.py:276} INFO - 21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:42,385] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_20210514204935604711510384552075_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935604711510384552075_0004_m_000082_370 : duration 0:00.003s
[2021-05-14 17:50:42,386] {docker.py:276} INFO - 21/05/14 20:50:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357857097671161256322_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357857097671161256322_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357857097671161256322_0004}; taskId=attempt_202105142049357857097671161256322_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58875f87}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049357857097671161256322_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357857097671161256322_0004_m_000083_371
[2021-05-14 17:50:42,388] {docker.py:276} INFO - 21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:42,390] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049358342035077142035975_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358342035077142035975_0004_m_000081_369 : duration 0:00.009s
[2021-05-14 17:50:42,391] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049357857097671161256322_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357857097671161256322_0004_m_000083_371 : duration 0:00.004s
[2021-05-14 17:50:42,394] {docker.py:276} INFO - 21/05/14 20:50:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:42,397] {docker.py:276} INFO - 21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:42,397] {docker.py:276} INFO - 21/05/14 20:50:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351195203572582173516_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351195203572582173516_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351195203572582173516_0004}; taskId=attempt_202105142049351195203572582173516_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3768d1a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:42,398] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Starting: Task committer attempt_202105142049351195203572582173516_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351195203572582173516_0004_m_000080_368
[2021-05-14 17:50:42,401] {docker.py:276} INFO - 21/05/14 20:50:42 INFO StagingCommitter: Task committer attempt_202105142049351195203572582173516_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351195203572582173516_0004_m_000080_368 : duration 0:00.003s
[2021-05-14 17:50:44,512] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_20210514204935604711510384552075_0004_m_000082_370: needsTaskCommit() Task attempt_20210514204935604711510384552075_0004_m_000082_370
[2021-05-14 17:50:44,512] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_20210514204935604711510384552075_0004_m_000082_370: needsTaskCommit() Task attempt_20210514204935604711510384552075_0004_m_000082_370: duration 0:00.001s
21/05/14 20:50:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935604711510384552075_0004_m_000082_370
[2021-05-14 17:50:44,513] {docker.py:276} INFO - 21/05/14 20:50:44 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-14 17:50:44,515] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (7cd3ddbc35d2, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:44,516] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2152 ms on 7cd3ddbc35d2 (executor driver) (81/200)
21/05/14 20:50:44 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-14 17:50:44,525] {docker.py:276} INFO - 21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:44,527] {docker.py:276} INFO - 21/05/14 20:50:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:44,527] {docker.py:276} INFO - 21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358663974707698126832_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358663974707698126832_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358663974707698126832_0004}; taskId=attempt_202105142049358663974707698126832_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ec3089d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:44,527] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049358663974707698126832_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358663974707698126832_0004_m_000084_372
[2021-05-14 17:50:44,530] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049358663974707698126832_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358663974707698126832_0004_m_000084_372 : duration 0:00.003s
[2021-05-14 17:50:44,533] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049358342035077142035975_0004_m_000081_369: needsTaskCommit() Task attempt_202105142049358342035077142035975_0004_m_000081_369
[2021-05-14 17:50:44,534] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049358342035077142035975_0004_m_000081_369: needsTaskCommit() Task attempt_202105142049358342035077142035975_0004_m_000081_369: duration 0:00.000s
[2021-05-14 17:50:44,534] {docker.py:276} INFO - 21/05/14 20:50:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358342035077142035975_0004_m_000081_369
21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049351195203572582173516_0004_m_000080_368: needsTaskCommit() Task attempt_202105142049351195203572582173516_0004_m_000080_368
[2021-05-14 17:50:44,534] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049351195203572582173516_0004_m_000080_368: needsTaskCommit() Task attempt_202105142049351195203572582173516_0004_m_000080_368: duration 0:00.000s
21/05/14 20:50:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351195203572582173516_0004_m_000080_368
[2021-05-14 17:50:44,535] {docker.py:276} INFO - 21/05/14 20:50:44 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4544 bytes result sent to driver
21/05/14 20:50:44 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4544 bytes result sent to driver
[2021-05-14 17:50:44,536] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (7cd3ddbc35d2, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:44,537] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (7cd3ddbc35d2, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:44,538] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2184 ms on 7cd3ddbc35d2 (executor driver) (82/200)
21/05/14 20:50:44 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2181 ms on 7cd3ddbc35d2 (executor driver) (83/200)
[2021-05-14 17:50:44,539] {docker.py:276} INFO - 21/05/14 20:50:44 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
21/05/14 20:50:44 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-14 17:50:44,546] {docker.py:276} INFO - 21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:44,548] {docker.py:276} INFO - 21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:44,549] {docker.py:276} INFO - 21/05/14 20:50:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:44,549] {docker.py:276} INFO - 21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357568259139285819263_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357568259139285819263_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357568259139285819263_0004}; taskId=attempt_202105142049357568259139285819263_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@510a37df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:44,549] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049357568259139285819263_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357568259139285819263_0004_m_000085_373
[2021-05-14 17:50:44,551] {docker.py:276} INFO - 21/05/14 20:50:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:44,551] {docker.py:276} INFO - 21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:44,552] {docker.py:276} INFO - 21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051420493545894971461554877_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_2021051420493545894971461554877_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051420493545894971461554877_0004}; taskId=attempt_2021051420493545894971461554877_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ac1b664}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:44,552] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_2021051420493545894971461554877_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_2021051420493545894971461554877_0004_m_000086_374
[2021-05-14 17:50:44,553] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049357568259139285819263_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357568259139285819263_0004_m_000085_373 : duration 0:00.004s
[2021-05-14 17:50:44,557] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_2021051420493545894971461554877_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_2021051420493545894971461554877_0004_m_000086_374 : duration 0:00.004s
[2021-05-14 17:50:44,677] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049357857097671161256322_0004_m_000083_371: needsTaskCommit() Task attempt_202105142049357857097671161256322_0004_m_000083_371
21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049357857097671161256322_0004_m_000083_371: needsTaskCommit() Task attempt_202105142049357857097671161256322_0004_m_000083_371: duration 0:00.000s
21/05/14 20:50:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357857097671161256322_0004_m_000083_371
[2021-05-14 17:50:44,678] {docker.py:276} INFO - 21/05/14 20:50:44 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-14 17:50:44,680] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (7cd3ddbc35d2, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:44,681] {docker.py:276} INFO - 21/05/14 20:50:44 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-14 17:50:44,682] {docker.py:276} INFO - 21/05/14 20:50:44 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2314 ms on 7cd3ddbc35d2 (executor driver) (84/200)
[2021-05-14 17:50:44,693] {docker.py:276} INFO - 21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:44,696] {docker.py:276} INFO - 21/05/14 20:50:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352170862985147364018_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352170862985147364018_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352170862985147364018_0004}; taskId=attempt_202105142049352170862985147364018_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c470f22}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:44,696] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049352170862985147364018_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352170862985147364018_0004_m_000087_375
[2021-05-14 17:50:44,699] {docker.py:276} INFO - 21/05/14 20:50:44 INFO StagingCommitter: Task committer attempt_202105142049352170862985147364018_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352170862985147364018_0004_m_000087_375 : duration 0:00.003s
[2021-05-14 17:50:46,662] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_2021051420493545894971461554877_0004_m_000086_374: needsTaskCommit() Task attempt_2021051420493545894971461554877_0004_m_000086_374
[2021-05-14 17:50:46,663] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_2021051420493545894971461554877_0004_m_000086_374: needsTaskCommit() Task attempt_2021051420493545894971461554877_0004_m_000086_374: duration 0:00.001s
21/05/14 20:50:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051420493545894971461554877_0004_m_000086_374
[2021-05-14 17:50:46,665] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4544 bytes result sent to driver
[2021-05-14 17:50:46,666] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (7cd3ddbc35d2, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:46,667] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2132 ms on 7cd3ddbc35d2 (executor driver) (85/200)
21/05/14 20:50:46 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-14 17:50:46,677] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049358663974707698126832_0004_m_000084_372: needsTaskCommit() Task attempt_202105142049358663974707698126832_0004_m_000084_372
[2021-05-14 17:50:46,677] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049358663974707698126832_0004_m_000084_372: needsTaskCommit() Task attempt_202105142049358663974707698126832_0004_m_000084_372: duration 0:00.000s
21/05/14 20:50:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358663974707698126832_0004_m_000084_372
[2021-05-14 17:50:46,678] {docker.py:276} INFO - 21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/05/14 20:50:46 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-14 17:50:46,679] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (7cd3ddbc35d2, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:46,680] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 2169 ms on 7cd3ddbc35d2 (executor driver) (86/200)
[2021-05-14 17:50:46,681] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-14 17:50:46,682] {docker.py:276} INFO - 21/05/14 20:50:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353284514889130074985_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353284514889130074985_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353284514889130074985_0004}; taskId=attempt_202105142049353284514889130074985_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e7c9ef8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:46,683] {docker.py:276} INFO - 21/05/14 20:50:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049353284514889130074985_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353284514889130074985_0004_m_000088_376
[2021-05-14 17:50:46,686] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049353284514889130074985_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353284514889130074985_0004_m_000088_376 : duration 0:00.004s
[2021-05-14 17:50:46,690] {docker.py:276} INFO - 21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:46,692] {docker.py:276} INFO - 21/05/14 20:50:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935910338499245797864_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935910338499245797864_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935910338499245797864_0004}; taskId=attempt_20210514204935910338499245797864_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7acb0c82}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_20210514204935910338499245797864_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935910338499245797864_0004_m_000089_377
[2021-05-14 17:50:46,695] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_20210514204935910338499245797864_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935910338499245797864_0004_m_000089_377 : duration 0:00.002s
[2021-05-14 17:50:46,748] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049357568259139285819263_0004_m_000085_373: needsTaskCommit() Task attempt_202105142049357568259139285819263_0004_m_000085_373
[2021-05-14 17:50:46,749] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049357568259139285819263_0004_m_000085_373: needsTaskCommit() Task attempt_202105142049357568259139285819263_0004_m_000085_373: duration 0:00.000s
21/05/14 20:50:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357568259139285819263_0004_m_000085_373
[2021-05-14 17:50:46,751] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4544 bytes result sent to driver
[2021-05-14 17:50:46,752] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (7cd3ddbc35d2, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:46,753] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2219 ms on 7cd3ddbc35d2 (executor driver) (87/200)
[2021-05-14 17:50:46,754] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-14 17:50:46,771] {docker.py:276} INFO - 21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:46,773] {docker.py:276} INFO - 21/05/14 20:50:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:46,774] {docker.py:276} INFO - 21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353861743195955238834_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353861743195955238834_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353861743195955238834_0004}; taskId=attempt_202105142049353861743195955238834_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3acbf7ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:46,774] {docker.py:276} INFO - 21/05/14 20:50:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049353861743195955238834_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353861743195955238834_0004_m_000090_378
[2021-05-14 17:50:46,776] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049353861743195955238834_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353861743195955238834_0004_m_000090_378 : duration 0:00.003s
[2021-05-14 17:50:46,807] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049352170862985147364018_0004_m_000087_375: needsTaskCommit() Task attempt_202105142049352170862985147364018_0004_m_000087_375
[2021-05-14 17:50:46,808] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049352170862985147364018_0004_m_000087_375: needsTaskCommit() Task attempt_202105142049352170862985147364018_0004_m_000087_375: duration 0:00.001s
[2021-05-14 17:50:46,809] {docker.py:276} INFO - 21/05/14 20:50:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352170862985147364018_0004_m_000087_375
[2021-05-14 17:50:46,813] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4587 bytes result sent to driver
[2021-05-14 17:50:46,815] {docker.py:276} INFO - 21/05/14 20:50:46 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (7cd3ddbc35d2, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:46,816] {docker.py:276} INFO - 21/05/14 20:50:46 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
21/05/14 20:50:46 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2140 ms on 7cd3ddbc35d2 (executor driver) (88/200)
[2021-05-14 17:50:46,825] {docker.py:276} INFO - 21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:46,825] {docker.py:276} INFO - 21/05/14 20:50:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:46,827] {docker.py:276} INFO - 21/05/14 20:50:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:46,827] {docker.py:276} INFO - 21/05/14 20:50:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354714030898840900763_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354714030898840900763_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354714030898840900763_0004}; taskId=attempt_202105142049354714030898840900763_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ace505b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049354714030898840900763_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354714030898840900763_0004_m_000091_379
[2021-05-14 17:50:46,830] {docker.py:276} INFO - 21/05/14 20:50:46 INFO StagingCommitter: Task committer attempt_202105142049354714030898840900763_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354714030898840900763_0004_m_000091_379 : duration 0:00.003s
[2021-05-14 17:50:48,816] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_20210514204935910338499245797864_0004_m_000089_377: needsTaskCommit() Task attempt_20210514204935910338499245797864_0004_m_000089_377
[2021-05-14 17:50:48,817] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_20210514204935910338499245797864_0004_m_000089_377: needsTaskCommit() Task attempt_20210514204935910338499245797864_0004_m_000089_377: duration 0:00.000s
21/05/14 20:50:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935910338499245797864_0004_m_000089_377
[2021-05-14 17:50:48,819] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4587 bytes result sent to driver
[2021-05-14 17:50:48,821] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (7cd3ddbc35d2, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:48,821] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 2144 ms on 7cd3ddbc35d2 (executor driver) (89/200)
[2021-05-14 17:50:48,822] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-14 17:50:48,832] {docker.py:276} INFO - 21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:48,834] {docker.py:276} INFO - 21/05/14 20:50:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:48,835] {docker.py:276} INFO - 21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935384813338479465709_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935384813338479465709_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935384813338479465709_0004}; taskId=attempt_20210514204935384813338479465709_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b191141}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_20210514204935384813338479465709_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935384813338479465709_0004_m_000092_380
[2021-05-14 17:50:48,837] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_20210514204935384813338479465709_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935384813338479465709_0004_m_000092_380 : duration 0:00.003s
[2021-05-14 17:50:48,854] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049353284514889130074985_0004_m_000088_376: needsTaskCommit() Task attempt_202105142049353284514889130074985_0004_m_000088_376
[2021-05-14 17:50:48,855] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_202105142049353284514889130074985_0004_m_000088_376: needsTaskCommit() Task attempt_202105142049353284514889130074985_0004_m_000088_376: duration 0:00.001s
21/05/14 20:50:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353284514889130074985_0004_m_000088_376
[2021-05-14 17:50:48,856] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-14 17:50:48,857] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (7cd3ddbc35d2, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:48,857] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2195 ms on 7cd3ddbc35d2 (executor driver) (90/200)
[2021-05-14 17:50:48,858] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-14 17:50:48,865] {docker.py:276} INFO - 21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:48,867] {docker.py:276} INFO - 21/05/14 20:50:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356086990639614432310_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356086990639614432310_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356086990639614432310_0004}; taskId=attempt_202105142049356086990639614432310_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dc5726c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049356086990639614432310_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356086990639614432310_0004_m_000093_381
[2021-05-14 17:50:48,870] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_202105142049356086990639614432310_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356086990639614432310_0004_m_000093_381 : duration 0:00.003s
[2021-05-14 17:50:48,929] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049353861743195955238834_0004_m_000090_378: needsTaskCommit() Task attempt_202105142049353861743195955238834_0004_m_000090_378
[2021-05-14 17:50:48,930] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_202105142049353861743195955238834_0004_m_000090_378: needsTaskCommit() Task attempt_202105142049353861743195955238834_0004_m_000090_378: duration 0:00.001s
[2021-05-14 17:50:48,931] {docker.py:276} INFO - 21/05/14 20:50:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353861743195955238834_0004_m_000090_378
[2021-05-14 17:50:48,932] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4587 bytes result sent to driver
[2021-05-14 17:50:48,933] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (7cd3ddbc35d2, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:48,935] {docker.py:276} INFO - 21/05/14 20:50:48 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2186 ms on 7cd3ddbc35d2 (executor driver) (91/200)
[2021-05-14 17:50:48,936] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-14 17:50:48,944] {docker.py:276} INFO - 21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:48,946] {docker.py:276} INFO - 21/05/14 20:50:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354622161956000687598_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354622161956000687598_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354622161956000687598_0004}; taskId=attempt_202105142049354622161956000687598_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18256b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:48,947] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049354622161956000687598_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354622161956000687598_0004_m_000094_382
[2021-05-14 17:50:48,950] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_202105142049354622161956000687598_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354622161956000687598_0004_m_000094_382 : duration 0:00.004s
[2021-05-14 17:50:48,987] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049354714030898840900763_0004_m_000091_379: needsTaskCommit() Task attempt_202105142049354714030898840900763_0004_m_000091_379
[2021-05-14 17:50:48,988] {docker.py:276} INFO - 21/05/14 20:50:48 INFO StagingCommitter: Task committer attempt_202105142049354714030898840900763_0004_m_000091_379: needsTaskCommit() Task attempt_202105142049354714030898840900763_0004_m_000091_379: duration 0:00.001s
21/05/14 20:50:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354714030898840900763_0004_m_000091_379
[2021-05-14 17:50:48,989] {docker.py:276} INFO - 21/05/14 20:50:48 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4544 bytes result sent to driver
[2021-05-14 17:50:48,991] {docker.py:276} INFO - 21/05/14 20:50:49 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (7cd3ddbc35d2, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:48,991] {docker.py:276} INFO - 21/05/14 20:50:49 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2179 ms on 7cd3ddbc35d2 (executor driver) (92/200)
[2021-05-14 17:50:48,992] {docker.py:276} INFO - 21/05/14 20:50:49 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-14 17:50:49,002] {docker.py:276} INFO - 21/05/14 20:50:49 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:49,002] {docker.py:276} INFO - 21/05/14 20:50:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:49,004] {docker.py:276} INFO - 21/05/14 20:50:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:50:49,004] {docker.py:276} INFO - 21/05/14 20:50:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:49,005] {docker.py:276} INFO - 21/05/14 20:50:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:49,005] {docker.py:276} INFO - 21/05/14 20:50:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353238733012440694122_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353238733012440694122_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353238733012440694122_0004}; taskId=attempt_202105142049353238733012440694122_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18beb187}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:49,005] {docker.py:276} INFO - 21/05/14 20:50:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:49,006] {docker.py:276} INFO - 21/05/14 20:50:49 INFO StagingCommitter: Starting: Task committer attempt_202105142049353238733012440694122_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353238733012440694122_0004_m_000095_383
[2021-05-14 17:50:49,007] {docker.py:276} INFO - 21/05/14 20:50:49 INFO StagingCommitter: Task committer attempt_202105142049353238733012440694122_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353238733012440694122_0004_m_000095_383 : duration 0:00.003s
[2021-05-14 17:50:50,975] {docker.py:276} INFO - 21/05/14 20:50:50 INFO StagingCommitter: Starting: Task committer attempt_20210514204935384813338479465709_0004_m_000092_380: needsTaskCommit() Task attempt_20210514204935384813338479465709_0004_m_000092_380
[2021-05-14 17:50:50,976] {docker.py:276} INFO - 21/05/14 20:50:50 INFO StagingCommitter: Task committer attempt_20210514204935384813338479465709_0004_m_000092_380: needsTaskCommit() Task attempt_20210514204935384813338479465709_0004_m_000092_380: duration 0:00.001s
[2021-05-14 17:50:50,976] {docker.py:276} INFO - 21/05/14 20:50:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935384813338479465709_0004_m_000092_380
[2021-05-14 17:50:50,978] {docker.py:276} INFO - 21/05/14 20:50:50 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-14 17:50:50,980] {docker.py:276} INFO - 21/05/14 20:50:50 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (7cd3ddbc35d2, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:50,981] {docker.py:276} INFO - 21/05/14 20:50:50 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2163 ms on 7cd3ddbc35d2 (executor driver) (93/200)
[2021-05-14 17:50:50,982] {docker.py:276} INFO - 21/05/14 20:50:50 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-14 17:50:50,991] {docker.py:276} INFO - 21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:50,993] {docker.py:276} INFO - 21/05/14 20:50:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:50,993] {docker.py:276} INFO - 21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359130800178009712793_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359130800178009712793_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359130800178009712793_0004}; taskId=attempt_202105142049359130800178009712793_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21b86a1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049359130800178009712793_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359130800178009712793_0004_m_000096_384
[2021-05-14 17:50:50,995] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049359130800178009712793_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359130800178009712793_0004_m_000096_384 : duration 0:00.003s
[2021-05-14 17:50:51,021] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049356086990639614432310_0004_m_000093_381: needsTaskCommit() Task attempt_202105142049356086990639614432310_0004_m_000093_381
[2021-05-14 17:50:51,022] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049356086990639614432310_0004_m_000093_381: needsTaskCommit() Task attempt_202105142049356086990639614432310_0004_m_000093_381: duration 0:00.000s
21/05/14 20:50:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356086990639614432310_0004_m_000093_381
[2021-05-14 17:50:51,023] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4544 bytes result sent to driver
[2021-05-14 17:50:51,024] {docker.py:276} INFO - 21/05/14 20:50:51 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (7cd3ddbc35d2, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:51,025] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
21/05/14 20:50:51 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2170 ms on 7cd3ddbc35d2 (executor driver) (94/200)
[2021-05-14 17:50:51,032] {docker.py:276} INFO - 21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:51,033] {docker.py:276} INFO - 21/05/14 20:50:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358265831915562898796_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358265831915562898796_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358265831915562898796_0004}; taskId=attempt_202105142049358265831915562898796_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d5b62dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:51,034] {docker.py:276} INFO - 21/05/14 20:50:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049358265831915562898796_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358265831915562898796_0004_m_000097_385
[2021-05-14 17:50:51,036] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049358265831915562898796_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358265831915562898796_0004_m_000097_385 : duration 0:00.003s
[2021-05-14 17:50:51,052] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049354622161956000687598_0004_m_000094_382: needsTaskCommit() Task attempt_202105142049354622161956000687598_0004_m_000094_382
[2021-05-14 17:50:51,053] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049354622161956000687598_0004_m_000094_382: needsTaskCommit() Task attempt_202105142049354622161956000687598_0004_m_000094_382: duration 0:00.000s
21/05/14 20:50:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354622161956000687598_0004_m_000094_382
[2021-05-14 17:50:51,053] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-14 17:50:51,054] {docker.py:276} INFO - 21/05/14 20:50:51 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (7cd3ddbc35d2, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:51,054] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
21/05/14 20:50:51 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2124 ms on 7cd3ddbc35d2 (executor driver) (95/200)
[2021-05-14 17:50:51,062] {docker.py:276} INFO - 21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:51,063] {docker.py:276} INFO - 21/05/14 20:50:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356676244612588007147_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356676244612588007147_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356676244612588007147_0004}; taskId=attempt_202105142049356676244612588007147_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e77dc39}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049356676244612588007147_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356676244612588007147_0004_m_000098_386
[2021-05-14 17:50:51,065] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049356676244612588007147_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356676244612588007147_0004_m_000098_386 : duration 0:00.002s
[2021-05-14 17:50:51,154] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049353238733012440694122_0004_m_000095_383: needsTaskCommit() Task attempt_202105142049353238733012440694122_0004_m_000095_383
[2021-05-14 17:50:51,155] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049353238733012440694122_0004_m_000095_383: needsTaskCommit() Task attempt_202105142049353238733012440694122_0004_m_000095_383: duration 0:00.001s
21/05/14 20:50:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353238733012440694122_0004_m_000095_383
[2021-05-14 17:50:51,157] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-14 17:50:51,158] {docker.py:276} INFO - 21/05/14 20:50:51 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (7cd3ddbc35d2, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:51,159] {docker.py:276} INFO - 21/05/14 20:50:51 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-14 17:50:51,161] {docker.py:276} INFO - 21/05/14 20:50:51 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2173 ms on 7cd3ddbc35d2 (executor driver) (96/200)
[2021-05-14 17:50:51,171] {docker.py:276} INFO - 21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:51,171] {docker.py:276} INFO - 21/05/14 20:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:51,172] {docker.py:276} INFO - 21/05/14 20:50:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:51,173] {docker.py:276} INFO - 21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:51,173] {docker.py:276} INFO - 21/05/14 20:50:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359111004696371035919_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359111004696371035919_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359111004696371035919_0004}; taskId=attempt_202105142049359111004696371035919_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7890e21f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049359111004696371035919_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359111004696371035919_0004_m_000099_387
[2021-05-14 17:50:51,175] {docker.py:276} INFO - 21/05/14 20:50:51 INFO StagingCommitter: Task committer attempt_202105142049359111004696371035919_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359111004696371035919_0004_m_000099_387 : duration 0:00.002s
[2021-05-14 17:50:53,095] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049359130800178009712793_0004_m_000096_384: needsTaskCommit() Task attempt_202105142049359130800178009712793_0004_m_000096_384
[2021-05-14 17:50:53,095] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049359130800178009712793_0004_m_000096_384: needsTaskCommit() Task attempt_202105142049359130800178009712793_0004_m_000096_384: duration 0:00.001s
[2021-05-14 17:50:53,096] {docker.py:276} INFO - 21/05/14 20:50:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359130800178009712793_0004_m_000096_384
[2021-05-14 17:50:53,097] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4587 bytes result sent to driver
[2021-05-14 17:50:53,097] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (7cd3ddbc35d2, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:53,098] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-14 17:50:53,099] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2122 ms on 7cd3ddbc35d2 (executor driver) (97/200)
[2021-05-14 17:50:53,110] {docker.py:276} INFO - 21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:53,112] {docker.py:276} INFO - 21/05/14 20:50:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351919829861146989601_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351919829861146989601_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351919829861146989601_0004}; taskId=attempt_202105142049351919829861146989601_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f6cfda6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049351919829861146989601_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351919829861146989601_0004_m_000100_388
[2021-05-14 17:50:53,115] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049351919829861146989601_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351919829861146989601_0004_m_000100_388 : duration 0:00.003s
[2021-05-14 17:50:53,208] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049358265831915562898796_0004_m_000097_385: needsTaskCommit() Task attempt_202105142049358265831915562898796_0004_m_000097_385
[2021-05-14 17:50:53,212] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049358265831915562898796_0004_m_000097_385: needsTaskCommit() Task attempt_202105142049358265831915562898796_0004_m_000097_385: duration 0:00.004s
[2021-05-14 17:50:53,213] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049356676244612588007147_0004_m_000098_386: needsTaskCommit() Task attempt_202105142049356676244612588007147_0004_m_000098_386
[2021-05-14 17:50:53,214] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049356676244612588007147_0004_m_000098_386: needsTaskCommit() Task attempt_202105142049356676244612588007147_0004_m_000098_386: duration 0:00.001s
21/05/14 20:50:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356676244612588007147_0004_m_000098_386
[2021-05-14 17:50:53,214] {docker.py:276} INFO - 21/05/14 20:50:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358265831915562898796_0004_m_000097_385
[2021-05-14 17:50:53,215] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4587 bytes result sent to driver
[2021-05-14 17:50:53,217] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (7cd3ddbc35d2, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:53 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4587 bytes result sent to driver
[2021-05-14 17:50:53,218] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (7cd3ddbc35d2, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:50:53 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2197 ms on 7cd3ddbc35d2 (executor driver) (98/200)
[2021-05-14 17:50:53,219] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
21/05/14 20:50:53 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-14 17:50:53,220] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2169 ms on 7cd3ddbc35d2 (executor driver) (99/200)
[2021-05-14 17:50:53,230] {docker.py:276} INFO - 21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:53,232] {docker.py:276} INFO - 21/05/14 20:50:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:53,233] {docker.py:276} INFO - 21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351164231118166239310_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351164231118166239310_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351164231118166239310_0004}; taskId=attempt_202105142049351164231118166239310_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79037e21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049351164231118166239310_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351164231118166239310_0004_m_000101_389
[2021-05-14 17:50:53,234] {docker.py:276} INFO - 21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Getting 5 (46.8 KiB) non-empty blocks including 5 (46.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:50:53,236] {docker.py:276} INFO - 21/05/14 20:50:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:53,237] {docker.py:276} INFO - 21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353675721557137969529_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353675721557137969529_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353675721557137969529_0004}; taskId=attempt_202105142049353675721557137969529_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19462f08}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:53,237] {docker.py:276} INFO - 21/05/14 20:50:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:53,238] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049353675721557137969529_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353675721557137969529_0004_m_000102_390
[2021-05-14 17:50:53,238] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049351164231118166239310_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351164231118166239310_0004_m_000101_389 : duration 0:00.004s
[2021-05-14 17:50:53,242] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049353675721557137969529_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353675721557137969529_0004_m_000102_390 : duration 0:00.005s
[2021-05-14 17:50:53,352] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049359111004696371035919_0004_m_000099_387: needsTaskCommit() Task attempt_202105142049359111004696371035919_0004_m_000099_387
[2021-05-14 17:50:53,353] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049359111004696371035919_0004_m_000099_387: needsTaskCommit() Task attempt_202105142049359111004696371035919_0004_m_000099_387: duration 0:00.000s
21/05/14 20:50:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359111004696371035919_0004_m_000099_387
[2021-05-14 17:50:53,354] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-14 17:50:53,355] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (7cd3ddbc35d2, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:53,356] {docker.py:276} INFO - 21/05/14 20:50:53 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2202 ms on 7cd3ddbc35d2 (executor driver) (100/200)
[2021-05-14 17:50:53,357] {docker.py:276} INFO - 21/05/14 20:50:53 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-14 17:50:53,366] {docker.py:276} INFO - 21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Getting 5 (45.2 KiB) non-empty blocks including 5 (45.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:53,369] {docker.py:276} INFO - 21/05/14 20:50:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:53,369] {docker.py:276} INFO - 21/05/14 20:50:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351085300823768036218_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351085300823768036218_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351085300823768036218_0004}; taskId=attempt_202105142049351085300823768036218_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3456e569}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:53,370] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049351085300823768036218_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351085300823768036218_0004_m_000103_391
[2021-05-14 17:50:53,372] {docker.py:276} INFO - 21/05/14 20:50:53 INFO StagingCommitter: Task committer attempt_202105142049351085300823768036218_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351085300823768036218_0004_m_000103_391 : duration 0:00.003s
[2021-05-14 17:50:55,260] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049351919829861146989601_0004_m_000100_388: needsTaskCommit() Task attempt_202105142049351919829861146989601_0004_m_000100_388
[2021-05-14 17:50:55,262] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049351919829861146989601_0004_m_000100_388: needsTaskCommit() Task attempt_202105142049351919829861146989601_0004_m_000100_388: duration 0:00.001s
[2021-05-14 17:50:55,262] {docker.py:276} INFO - 21/05/14 20:50:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351919829861146989601_0004_m_000100_388
[2021-05-14 17:50:55,264] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4544 bytes result sent to driver
[2021-05-14 17:50:55,266] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (7cd3ddbc35d2, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:55,267] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 2172 ms on 7cd3ddbc35d2 (executor driver) (101/200)
[2021-05-14 17:50:55,268] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-14 17:50:55,277] {docker.py:276} INFO - 21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:55,279] {docker.py:276} INFO - 21/05/14 20:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355728405879376261071_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728405879376261071_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355728405879376261071_0004}; taskId=attempt_202105142049355728405879376261071_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2166c63c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:55,280] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049355728405879376261071_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728405879376261071_0004_m_000104_392
[2021-05-14 17:50:55,282] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049355728405879376261071_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355728405879376261071_0004_m_000104_392 : duration 0:00.003s
[2021-05-14 17:50:55,399] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049353675721557137969529_0004_m_000102_390: needsTaskCommit() Task attempt_202105142049353675721557137969529_0004_m_000102_390
[2021-05-14 17:50:55,400] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049353675721557137969529_0004_m_000102_390: needsTaskCommit() Task attempt_202105142049353675721557137969529_0004_m_000102_390: duration 0:00.001s
21/05/14 20:50:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353675721557137969529_0004_m_000102_390
[2021-05-14 17:50:55,402] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4544 bytes result sent to driver
[2021-05-14 17:50:55,404] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (7cd3ddbc35d2, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:55,405] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-14 17:50:55,405] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2189 ms on 7cd3ddbc35d2 (executor driver) (102/200)
[2021-05-14 17:50:55,414] {docker.py:276} INFO - 21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:55,416] {docker.py:276} INFO - 21/05/14 20:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:55,416] {docker.py:276} INFO - 21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353360660881501024168_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353360660881501024168_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353360660881501024168_0004}; taskId=attempt_202105142049353360660881501024168_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10deaf38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049353360660881501024168_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353360660881501024168_0004_m_000105_393
[2021-05-14 17:50:55,419] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049353360660881501024168_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353360660881501024168_0004_m_000105_393 : duration 0:00.003s
[2021-05-14 17:50:55,502] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049351164231118166239310_0004_m_000101_389: needsTaskCommit() Task attempt_202105142049351164231118166239310_0004_m_000101_389
[2021-05-14 17:50:55,503] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049351164231118166239310_0004_m_000101_389: needsTaskCommit() Task attempt_202105142049351164231118166239310_0004_m_000101_389: duration 0:00.000s
21/05/14 20:50:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351164231118166239310_0004_m_000101_389
[2021-05-14 17:50:55,504] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4544 bytes result sent to driver
[2021-05-14 17:50:55,505] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049351085300823768036218_0004_m_000103_391: needsTaskCommit() Task attempt_202105142049351085300823768036218_0004_m_000103_391
21/05/14 20:50:55 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (7cd3ddbc35d2, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:55,506] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049351085300823768036218_0004_m_000103_391: needsTaskCommit() Task attempt_202105142049351085300823768036218_0004_m_000103_391: duration 0:00.000s
[2021-05-14 17:50:55,507] {docker.py:276} INFO - 21/05/14 20:50:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351085300823768036218_0004_m_000103_391
[2021-05-14 17:50:55,507] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2292 ms on 7cd3ddbc35d2 (executor driver) (103/200)
[2021-05-14 17:50:55,508] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-14 17:50:55,510] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4544 bytes result sent to driver
[2021-05-14 17:50:55,511] {docker.py:276} INFO - 21/05/14 20:50:55 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (7cd3ddbc35d2, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:55,512] {docker.py:276} INFO - 21/05/14 20:50:55 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
21/05/14 20:50:55 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2160 ms on 7cd3ddbc35d2 (executor driver) (104/200)
[2021-05-14 17:50:55,518] {docker.py:276} INFO - 21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:55,521] {docker.py:276} INFO - 21/05/14 20:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:55,521] {docker.py:276} INFO - 21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935860157968936844426_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935860157968936844426_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935860157968936844426_0004}; taskId=attempt_20210514204935860157968936844426_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f79e9d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_20210514204935860157968936844426_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935860157968936844426_0004_m_000106_394
[2021-05-14 17:50:55,522] {docker.py:276} INFO - 21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:55,524] {docker.py:276} INFO - 21/05/14 20:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357893331703449898679_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357893331703449898679_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357893331703449898679_0004}; taskId=attempt_202105142049357893331703449898679_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59cc5697}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:55,524] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049357893331703449898679_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357893331703449898679_0004_m_000107_395
[2021-05-14 17:50:55,524] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_20210514204935860157968936844426_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935860157968936844426_0004_m_000106_394 : duration 0:00.004s
[2021-05-14 17:50:55,527] {docker.py:276} INFO - 21/05/14 20:50:55 INFO StagingCommitter: Task committer attempt_202105142049357893331703449898679_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357893331703449898679_0004_m_000107_395 : duration 0:00.003s
[2021-05-14 17:50:57,401] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049355728405879376261071_0004_m_000104_392: needsTaskCommit() Task attempt_202105142049355728405879376261071_0004_m_000104_392
21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049355728405879376261071_0004_m_000104_392: needsTaskCommit() Task attempt_202105142049355728405879376261071_0004_m_000104_392: duration 0:00.000s
21/05/14 20:50:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355728405879376261071_0004_m_000104_392
[2021-05-14 17:50:57,403] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4544 bytes result sent to driver
[2021-05-14 17:50:57,404] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (7cd3ddbc35d2, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:57,405] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-14 17:50:57,405] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 2141 ms on 7cd3ddbc35d2 (executor driver) (105/200)
[2021-05-14 17:50:57,415] {docker.py:276} INFO - 21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:57,417] {docker.py:276} INFO - 21/05/14 20:50:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356278499667593344191_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356278499667593344191_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356278499667593344191_0004}; taskId=attempt_202105142049356278499667593344191_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@540491c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049356278499667593344191_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356278499667593344191_0004_m_000108_396
[2021-05-14 17:50:57,420] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049356278499667593344191_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356278499667593344191_0004_m_000108_396 : duration 0:00.003s
[2021-05-14 17:50:57,557] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049353360660881501024168_0004_m_000105_393: needsTaskCommit() Task attempt_202105142049353360660881501024168_0004_m_000105_393
[2021-05-14 17:50:57,558] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049353360660881501024168_0004_m_000105_393: needsTaskCommit() Task attempt_202105142049353360660881501024168_0004_m_000105_393: duration 0:00.001s
[2021-05-14 17:50:57,559] {docker.py:276} INFO - 21/05/14 20:50:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353360660881501024168_0004_m_000105_393
[2021-05-14 17:50:57,561] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4544 bytes result sent to driver
[2021-05-14 17:50:57,563] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (7cd3ddbc35d2, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:57,563] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-14 17:50:57,564] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 2164 ms on 7cd3ddbc35d2 (executor driver) (106/200)
[2021-05-14 17:50:57,574] {docker.py:276} INFO - 21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:57,574] {docker.py:276} INFO - 21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:57,576] {docker.py:276} INFO - 21/05/14 20:50:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935947222806857005624_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935947222806857005624_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935947222806857005624_0004}; taskId=attempt_20210514204935947222806857005624_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@248eb54f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:57,577] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_20210514204935947222806857005624_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935947222806857005624_0004_m_000109_397
[2021-05-14 17:50:57,579] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_20210514204935947222806857005624_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935947222806857005624_0004_m_000109_397 : duration 0:00.003s
[2021-05-14 17:50:57,675] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_20210514204935860157968936844426_0004_m_000106_394: needsTaskCommit() Task attempt_20210514204935860157968936844426_0004_m_000106_394
[2021-05-14 17:50:57,675] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_20210514204935860157968936844426_0004_m_000106_394: needsTaskCommit() Task attempt_20210514204935860157968936844426_0004_m_000106_394: duration 0:00.002s
[2021-05-14 17:50:57,676] {docker.py:276} INFO - 21/05/14 20:50:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935860157968936844426_0004_m_000106_394
[2021-05-14 17:50:57,677] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049357893331703449898679_0004_m_000107_395: needsTaskCommit() Task attempt_202105142049357893331703449898679_0004_m_000107_395
[2021-05-14 17:50:57,678] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049357893331703449898679_0004_m_000107_395: needsTaskCommit() Task attempt_202105142049357893331703449898679_0004_m_000107_395: duration 0:00.000s
21/05/14 20:50:57 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4544 bytes result sent to driver
21/05/14 20:50:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357893331703449898679_0004_m_000107_395
[2021-05-14 17:50:57,678] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (7cd3ddbc35d2, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:57,679] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 2177 ms on 7cd3ddbc35d2 (executor driver) (107/200)
21/05/14 20:50:57 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-14 17:50:57,681] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
[2021-05-14 17:50:57,682] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (7cd3ddbc35d2, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:57,683] {docker.py:276} INFO - 21/05/14 20:50:57 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 2174 ms on 7cd3ddbc35d2 (executor driver) (108/200)
[2021-05-14 17:50:57,684] {docker.py:276} INFO - 21/05/14 20:50:57 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-14 17:50:57,693] {docker.py:276} INFO - 21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:57,695] {docker.py:276} INFO - 21/05/14 20:50:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:57,696] {docker.py:276} INFO - 21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354247246946432776594_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354247246946432776594_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354247246946432776594_0004}; taskId=attempt_202105142049354247246946432776594_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@277fefd6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:57,696] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049354247246946432776594_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354247246946432776594_0004_m_000111_399
[2021-05-14 17:50:57,697] {docker.py:276} INFO - 21/05/14 20:50:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358648663060954255606_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358648663060954255606_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358648663060954255606_0004}; taskId=attempt_202105142049358648663060954255606_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d89a1f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049358648663060954255606_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358648663060954255606_0004_m_000110_398
[2021-05-14 17:50:57,700] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049354247246946432776594_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354247246946432776594_0004_m_000111_399 : duration 0:00.004s
[2021-05-14 17:50:57,700] {docker.py:276} INFO - 21/05/14 20:50:57 INFO StagingCommitter: Task committer attempt_202105142049358648663060954255606_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358648663060954255606_0004_m_000110_398 : duration 0:00.003s
[2021-05-14 17:50:59,131] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049356278499667593344191_0004_m_000108_396: needsTaskCommit() Task attempt_202105142049356278499667593344191_0004_m_000108_396
21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049356278499667593344191_0004_m_000108_396: needsTaskCommit() Task attempt_202105142049356278499667593344191_0004_m_000108_396: duration 0:00.000s
21/05/14 20:50:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356278499667593344191_0004_m_000108_396
[2021-05-14 17:50:59,132] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4587 bytes result sent to driver
[2021-05-14 17:50:59,133] {docker.py:276} INFO - 21/05/14 20:50:59 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (7cd3ddbc35d2, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:59,134] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-14 17:50:59,135] {docker.py:276} INFO - 21/05/14 20:50:59 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 1734 ms on 7cd3ddbc35d2 (executor driver) (109/200)
[2021-05-14 17:50:59,142] {docker.py:276} INFO - 21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:59,144] {docker.py:276} INFO - 21/05/14 20:50:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353459674775774213906_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353459674775774213906_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353459674775774213906_0004}; taskId=attempt_202105142049353459674775774213906_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4cbf0baf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:59,145] {docker.py:276} INFO - 21/05/14 20:50:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:50:59,145] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049353459674775774213906_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353459674775774213906_0004_m_000112_400
[2021-05-14 17:50:59,148] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049353459674775774213906_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353459674775774213906_0004_m_000112_400 : duration 0:00.003s
[2021-05-14 17:50:59,881] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049354247246946432776594_0004_m_000111_399: needsTaskCommit() Task attempt_202105142049354247246946432776594_0004_m_000111_399
21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049358648663060954255606_0004_m_000110_398: needsTaskCommit() Task attempt_202105142049358648663060954255606_0004_m_000110_398
[2021-05-14 17:50:59,882] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049354247246946432776594_0004_m_000111_399: needsTaskCommit() Task attempt_202105142049354247246946432776594_0004_m_000111_399: duration 0:00.001s
[2021-05-14 17:50:59,883] {docker.py:276} INFO - 21/05/14 20:50:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354247246946432776594_0004_m_000111_399
[2021-05-14 17:50:59,884] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049358648663060954255606_0004_m_000110_398: needsTaskCommit() Task attempt_202105142049358648663060954255606_0004_m_000110_398: duration 0:00.002s
21/05/14 20:50:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358648663060954255606_0004_m_000110_398
[2021-05-14 17:50:59,885] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4587 bytes result sent to driver
[2021-05-14 17:50:59,886] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4587 bytes result sent to driver
21/05/14 20:50:59 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (7cd3ddbc35d2, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:59,887] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-14 17:50:59,888] {docker.py:276} INFO - 21/05/14 20:50:59 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (7cd3ddbc35d2, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:50:59,889] {docker.py:276} INFO - 21/05/14 20:50:59 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2210 ms on 7cd3ddbc35d2 (executor driver) (110/200)
21/05/14 20:50:59 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2215 ms on 7cd3ddbc35d2 (executor driver) (111/200)
[2021-05-14 17:50:59,891] {docker.py:276} INFO - 21/05/14 20:50:59 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-14 17:50:59,900] {docker.py:276} INFO - 21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:50:59,902] {docker.py:276} INFO - 21/05/14 20:50:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:59,903] {docker.py:276} INFO - 21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Getting 5 (40.9 KiB) non-empty blocks including 5 (40.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:50:59,903] {docker.py:276} INFO - 21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356629304961665553320_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356629304961665553320_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356629304961665553320_0004}; taskId=attempt_202105142049356629304961665553320_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1615292b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:50:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049356629304961665553320_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356629304961665553320_0004_m_000113_401
[2021-05-14 17:50:59,905] {docker.py:276} INFO - 21/05/14 20:50:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:50:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:50:59,905] {docker.py:276} INFO - 21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:59,906] {docker.py:276} INFO - 21/05/14 20:50:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353330105566741978588_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353330105566741978588_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353330105566741978588_0004}; taskId=attempt_202105142049353330105566741978588_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e864a61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:50:59,906] {docker.py:276} INFO - 21/05/14 20:50:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:50:59 INFO StagingCommitter: Starting: Task committer attempt_202105142049353330105566741978588_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353330105566741978588_0004_m_000114_402
[2021-05-14 17:50:59,906] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049356629304961665553320_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356629304961665553320_0004_m_000113_401 : duration 0:00.003s
[2021-05-14 17:50:59,910] {docker.py:276} INFO - 21/05/14 20:50:59 INFO StagingCommitter: Task committer attempt_202105142049353330105566741978588_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353330105566741978588_0004_m_000114_402 : duration 0:00.004s
[2021-05-14 17:51:00,271] {docker.py:276} INFO - 21/05/14 20:51:00 INFO StagingCommitter: Starting: Task committer attempt_20210514204935947222806857005624_0004_m_000109_397: needsTaskCommit() Task attempt_20210514204935947222806857005624_0004_m_000109_397
[2021-05-14 17:51:00,272] {docker.py:276} INFO - 21/05/14 20:51:00 INFO StagingCommitter: Task committer attempt_20210514204935947222806857005624_0004_m_000109_397: needsTaskCommit() Task attempt_20210514204935947222806857005624_0004_m_000109_397: duration 0:00.001s
21/05/14 20:51:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935947222806857005624_0004_m_000109_397
[2021-05-14 17:51:00,275] {docker.py:276} INFO - 21/05/14 20:51:00 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4587 bytes result sent to driver
[2021-05-14 17:51:00,276] {docker.py:276} INFO - 21/05/14 20:51:00 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (7cd3ddbc35d2, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:00,277] {docker.py:276} INFO - 21/05/14 20:51:00 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 2717 ms on 7cd3ddbc35d2 (executor driver) (112/200)
[2021-05-14 17:51:00,278] {docker.py:276} INFO - 21/05/14 20:51:00 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-14 17:51:00,287] {docker.py:276} INFO - 21/05/14 20:51:00 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:00,289] {docker.py:276} INFO - 21/05/14 20:51:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:00,289] {docker.py:276} INFO - 21/05/14 20:51:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935412641898456815262_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935412641898456815262_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935412641898456815262_0004}; taskId=attempt_20210514204935412641898456815262_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f24538e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:00 INFO StagingCommitter: Starting: Task committer attempt_20210514204935412641898456815262_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935412641898456815262_0004_m_000115_403
[2021-05-14 17:51:00,292] {docker.py:276} INFO - 21/05/14 20:51:00 INFO StagingCommitter: Task committer attempt_20210514204935412641898456815262_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935412641898456815262_0004_m_000115_403 : duration 0:00.003s
[2021-05-14 17:51:01,194] {docker.py:276} INFO - 21/05/14 20:51:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049353459674775774213906_0004_m_000112_400: needsTaskCommit() Task attempt_202105142049353459674775774213906_0004_m_000112_400
[2021-05-14 17:51:01,195] {docker.py:276} INFO - 21/05/14 20:51:01 INFO StagingCommitter: Task committer attempt_202105142049353459674775774213906_0004_m_000112_400: needsTaskCommit() Task attempt_202105142049353459674775774213906_0004_m_000112_400: duration 0:00.001s
21/05/14 20:51:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353459674775774213906_0004_m_000112_400
[2021-05-14 17:51:01,197] {docker.py:276} INFO - 21/05/14 20:51:01 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4544 bytes result sent to driver
[2021-05-14 17:51:01,198] {docker.py:276} INFO - 21/05/14 20:51:01 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (7cd3ddbc35d2, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:01,199] {docker.py:276} INFO - 21/05/14 20:51:01 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 2069 ms on 7cd3ddbc35d2 (executor driver) (113/200)
[2021-05-14 17:51:01,200] {docker.py:276} INFO - 21/05/14 20:51:01 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-14 17:51:01,209] {docker.py:276} INFO - 21/05/14 20:51:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:01,211] {docker.py:276} INFO - 21/05/14 20:51:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:01,211] {docker.py:276} INFO - 21/05/14 20:51:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352637352769198692263_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352637352769198692263_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352637352769198692263_0004}; taskId=attempt_202105142049352637352769198692263_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c015186}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:01 INFO StagingCommitter: Starting: Task committer attempt_202105142049352637352769198692263_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352637352769198692263_0004_m_000116_404
[2021-05-14 17:51:01,214] {docker.py:276} INFO - 21/05/14 20:51:01 INFO StagingCommitter: Task committer attempt_202105142049352637352769198692263_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352637352769198692263_0004_m_000116_404 : duration 0:00.003s
[2021-05-14 17:51:02,065] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_202105142049356629304961665553320_0004_m_000113_401: needsTaskCommit() Task attempt_202105142049356629304961665553320_0004_m_000113_401
21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_202105142049356629304961665553320_0004_m_000113_401: needsTaskCommit() Task attempt_202105142049356629304961665553320_0004_m_000113_401: duration 0:00.000s
21/05/14 20:51:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356629304961665553320_0004_m_000113_401
[2021-05-14 17:51:02,067] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4544 bytes result sent to driver
[2021-05-14 17:51:02,068] {docker.py:276} INFO - 21/05/14 20:51:02 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (7cd3ddbc35d2, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:02,069] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
21/05/14 20:51:02 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 2186 ms on 7cd3ddbc35d2 (executor driver) (114/200)
[2021-05-14 17:51:02,078] {docker.py:276} INFO - 21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:02,080] {docker.py:276} INFO - 21/05/14 20:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357169644739639171301_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357169644739639171301_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357169644739639171301_0004}; taskId=attempt_202105142049357169644739639171301_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@610e3bb8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_202105142049357169644739639171301_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357169644739639171301_0004_m_000117_405
[2021-05-14 17:51:02,084] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_202105142049357169644739639171301_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357169644739639171301_0004_m_000117_405 : duration 0:00.004s
[2021-05-14 17:51:02,122] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_202105142049353330105566741978588_0004_m_000114_402: needsTaskCommit() Task attempt_202105142049353330105566741978588_0004_m_000114_402
21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_202105142049353330105566741978588_0004_m_000114_402: needsTaskCommit() Task attempt_202105142049353330105566741978588_0004_m_000114_402: duration 0:00.000s
21/05/14 20:51:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353330105566741978588_0004_m_000114_402
[2021-05-14 17:51:02,124] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4544 bytes result sent to driver
[2021-05-14 17:51:02,125] {docker.py:276} INFO - 21/05/14 20:51:02 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (7cd3ddbc35d2, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:02,127] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-14 17:51:02,128] {docker.py:276} INFO - 21/05/14 20:51:02 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2243 ms on 7cd3ddbc35d2 (executor driver) (115/200)
[2021-05-14 17:51:02,137] {docker.py:276} INFO - 21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:02,143] {docker.py:276} INFO - 21/05/14 20:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354909778993652478907_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354909778993652478907_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354909778993652478907_0004}; taskId=attempt_202105142049354909778993652478907_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@365466ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_202105142049354909778993652478907_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354909778993652478907_0004_m_000118_406
[2021-05-14 17:51:02,147] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_202105142049354909778993652478907_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354909778993652478907_0004_m_000118_406 : duration 0:00.007s
[2021-05-14 17:51:02,646] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_20210514204935412641898456815262_0004_m_000115_403: needsTaskCommit() Task attempt_20210514204935412641898456815262_0004_m_000115_403
[2021-05-14 17:51:02,647] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_20210514204935412641898456815262_0004_m_000115_403: needsTaskCommit() Task attempt_20210514204935412641898456815262_0004_m_000115_403: duration 0:00.001s
21/05/14 20:51:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935412641898456815262_0004_m_000115_403
[2021-05-14 17:51:02,649] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4544 bytes result sent to driver
[2021-05-14 17:51:02,650] {docker.py:276} INFO - 21/05/14 20:51:02 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (7cd3ddbc35d2, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:02,651] {docker.py:276} INFO - 21/05/14 20:51:02 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
21/05/14 20:51:02 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2379 ms on 7cd3ddbc35d2 (executor driver) (116/200)
[2021-05-14 17:51:02,661] {docker.py:276} INFO - 21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:02,663] {docker.py:276} INFO - 21/05/14 20:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354249444507274205452_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354249444507274205452_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354249444507274205452_0004}; taskId=attempt_202105142049354249444507274205452_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@649d9ebc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:02,664] {docker.py:276} INFO - 21/05/14 20:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:02 INFO StagingCommitter: Starting: Task committer attempt_202105142049354249444507274205452_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354249444507274205452_0004_m_000119_407
[2021-05-14 17:51:02,666] {docker.py:276} INFO - 21/05/14 20:51:02 INFO StagingCommitter: Task committer attempt_202105142049354249444507274205452_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354249444507274205452_0004_m_000119_407 : duration 0:00.003s
[2021-05-14 17:51:03,520] {docker.py:276} INFO - 21/05/14 20:51:03 INFO StagingCommitter: Starting: Task committer attempt_202105142049352637352769198692263_0004_m_000116_404: needsTaskCommit() Task attempt_202105142049352637352769198692263_0004_m_000116_404
21/05/14 20:51:03 INFO StagingCommitter: Task committer attempt_202105142049352637352769198692263_0004_m_000116_404: needsTaskCommit() Task attempt_202105142049352637352769198692263_0004_m_000116_404: duration 0:00.001s
21/05/14 20:51:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352637352769198692263_0004_m_000116_404
[2021-05-14 17:51:03,522] {docker.py:276} INFO - 21/05/14 20:51:03 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4544 bytes result sent to driver
[2021-05-14 17:51:03,524] {docker.py:276} INFO - 21/05/14 20:51:03 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (7cd3ddbc35d2, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:03,525] {docker.py:276} INFO - 21/05/14 20:51:03 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-14 17:51:03,526] {docker.py:276} INFO - 21/05/14 20:51:03 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 2331 ms on 7cd3ddbc35d2 (executor driver) (117/200)
[2021-05-14 17:51:03,535] {docker.py:276} INFO - 21/05/14 20:51:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:03,537] {docker.py:276} INFO - 21/05/14 20:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935211917513861526487_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935211917513861526487_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935211917513861526487_0004}; taskId=attempt_20210514204935211917513861526487_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a84d994}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:03 INFO StagingCommitter: Starting: Task committer attempt_20210514204935211917513861526487_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935211917513861526487_0004_m_000120_408
[2021-05-14 17:51:03,540] {docker.py:276} INFO - 21/05/14 20:51:03 INFO StagingCommitter: Task committer attempt_20210514204935211917513861526487_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935211917513861526487_0004_m_000120_408 : duration 0:00.003s
[2021-05-14 17:51:04,249] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049357169644739639171301_0004_m_000117_405: needsTaskCommit() Task attempt_202105142049357169644739639171301_0004_m_000117_405
[2021-05-14 17:51:04,251] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049357169644739639171301_0004_m_000117_405: needsTaskCommit() Task attempt_202105142049357169644739639171301_0004_m_000117_405: duration 0:00.002s
21/05/14 20:51:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357169644739639171301_0004_m_000117_405
[2021-05-14 17:51:04,252] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4544 bytes result sent to driver
[2021-05-14 17:51:04,253] {docker.py:276} INFO - 21/05/14 20:51:04 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (7cd3ddbc35d2, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:04,254] {docker.py:276} INFO - 21/05/14 20:51:04 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 2190 ms on 7cd3ddbc35d2 (executor driver) (118/200)
[2021-05-14 17:51:04,255] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
[2021-05-14 17:51:04,279] {docker.py:276} INFO - 21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:04,280] {docker.py:276} INFO - 21/05/14 20:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:04,281] {docker.py:276} INFO - 21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:04,281] {docker.py:276} INFO - 21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356076681342823886350_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356076681342823886350_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356076681342823886350_0004}; taskId=attempt_202105142049356076681342823886350_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12aa2421}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:04,281] {docker.py:276} INFO - 21/05/14 20:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:04,282] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049356076681342823886350_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356076681342823886350_0004_m_000121_409
[2021-05-14 17:51:04,284] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049356076681342823886350_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356076681342823886350_0004_m_000121_409 : duration 0:00.002s
[2021-05-14 17:51:04,400] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049354909778993652478907_0004_m_000118_406: needsTaskCommit() Task attempt_202105142049354909778993652478907_0004_m_000118_406
[2021-05-14 17:51:04,401] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049354909778993652478907_0004_m_000118_406: needsTaskCommit() Task attempt_202105142049354909778993652478907_0004_m_000118_406: duration 0:00.001s
21/05/14 20:51:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354909778993652478907_0004_m_000118_406
[2021-05-14 17:51:04,403] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4587 bytes result sent to driver
[2021-05-14 17:51:04,405] {docker.py:276} INFO - 21/05/14 20:51:04 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (7cd3ddbc35d2, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:04,406] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-14 17:51:04,407] {docker.py:276} INFO - 21/05/14 20:51:04 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 2285 ms on 7cd3ddbc35d2 (executor driver) (119/200)
[2021-05-14 17:51:04,417] {docker.py:276} INFO - 21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:04,419] {docker.py:276} INFO - 21/05/14 20:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:04,419] {docker.py:276} INFO - 21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357582132273976100122_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357582132273976100122_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357582132273976100122_0004}; taskId=attempt_202105142049357582132273976100122_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@572caebb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049357582132273976100122_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357582132273976100122_0004_m_000122_410
[2021-05-14 17:51:04,421] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049357582132273976100122_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357582132273976100122_0004_m_000122_410 : duration 0:00.003s
[2021-05-14 17:51:04,941] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049354249444507274205452_0004_m_000119_407: needsTaskCommit() Task attempt_202105142049354249444507274205452_0004_m_000119_407
[2021-05-14 17:51:04,942] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049354249444507274205452_0004_m_000119_407: needsTaskCommit() Task attempt_202105142049354249444507274205452_0004_m_000119_407: duration 0:00.001s
21/05/14 20:51:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354249444507274205452_0004_m_000119_407
[2021-05-14 17:51:04,944] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4587 bytes result sent to driver
[2021-05-14 17:51:04,945] {docker.py:276} INFO - 21/05/14 20:51:04 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (7cd3ddbc35d2, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:04,947] {docker.py:276} INFO - 21/05/14 20:51:04 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
21/05/14 20:51:04 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 2300 ms on 7cd3ddbc35d2 (executor driver) (120/200)
[2021-05-14 17:51:04,958] {docker.py:276} INFO - 21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Getting 5 (44.5 KiB) non-empty blocks including 5 (44.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:04,960] {docker.py:276} INFO - 21/05/14 20:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356495222045106288992_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356495222045106288992_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356495222045106288992_0004}; taskId=attempt_202105142049356495222045106288992_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@524d40c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:04 INFO StagingCommitter: Starting: Task committer attempt_202105142049356495222045106288992_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356495222045106288992_0004_m_000123_411
[2021-05-14 17:51:04,963] {docker.py:276} INFO - 21/05/14 20:51:04 INFO StagingCommitter: Task committer attempt_202105142049356495222045106288992_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356495222045106288992_0004_m_000123_411 : duration 0:00.003s
[2021-05-14 17:51:05,341] {docker.py:276} INFO - 21/05/14 20:51:05 INFO StagingCommitter: Starting: Task committer attempt_20210514204935211917513861526487_0004_m_000120_408: needsTaskCommit() Task attempt_20210514204935211917513861526487_0004_m_000120_408
[2021-05-14 17:51:05,342] {docker.py:276} INFO - 21/05/14 20:51:05 INFO StagingCommitter: Task committer attempt_20210514204935211917513861526487_0004_m_000120_408: needsTaskCommit() Task attempt_20210514204935211917513861526487_0004_m_000120_408: duration 0:00.002s
21/05/14 20:51:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935211917513861526487_0004_m_000120_408
[2021-05-14 17:51:05,345] {docker.py:276} INFO - 21/05/14 20:51:05 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4587 bytes result sent to driver
[2021-05-14 17:51:05,346] {docker.py:276} INFO - 21/05/14 20:51:05 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (7cd3ddbc35d2, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:05,347] {docker.py:276} INFO - 21/05/14 20:51:05 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-14 17:51:05,348] {docker.py:276} INFO - 21/05/14 20:51:05 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 1827 ms on 7cd3ddbc35d2 (executor driver) (121/200)
[2021-05-14 17:51:05,358] {docker.py:276} INFO - 21/05/14 20:51:05 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:05,360] {docker.py:276} INFO - 21/05/14 20:51:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:05,361] {docker.py:276} INFO - 21/05/14 20:51:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356405201047762078481_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356405201047762078481_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356405201047762078481_0004}; taskId=attempt_202105142049356405201047762078481_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b6bd14d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:05,361] {docker.py:276} INFO - 21/05/14 20:51:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:05 INFO StagingCommitter: Starting: Task committer attempt_202105142049356405201047762078481_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356405201047762078481_0004_m_000124_412
[2021-05-14 17:51:05,364] {docker.py:276} INFO - 21/05/14 20:51:05 INFO StagingCommitter: Task committer attempt_202105142049356405201047762078481_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356405201047762078481_0004_m_000124_412 : duration 0:00.004s
[2021-05-14 17:51:06,488] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049356076681342823886350_0004_m_000121_409: needsTaskCommit() Task attempt_202105142049356076681342823886350_0004_m_000121_409
[2021-05-14 17:51:06,489] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Task committer attempt_202105142049356076681342823886350_0004_m_000121_409: needsTaskCommit() Task attempt_202105142049356076681342823886350_0004_m_000121_409: duration 0:00.001s
21/05/14 20:51:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356076681342823886350_0004_m_000121_409
[2021-05-14 17:51:06,491] {docker.py:276} INFO - 21/05/14 20:51:06 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4587 bytes result sent to driver
[2021-05-14 17:51:06,492] {docker.py:276} INFO - 21/05/14 20:51:06 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (7cd3ddbc35d2, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:06,494] {docker.py:276} INFO - 21/05/14 20:51:06 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 2243 ms on 7cd3ddbc35d2 (executor driver) (122/200)
21/05/14 20:51:06 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-14 17:51:06,504] {docker.py:276} INFO - 21/05/14 20:51:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:06,506] {docker.py:276} INFO - 21/05/14 20:51:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353205685537813848102_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205685537813848102_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353205685537813848102_0004}; taskId=attempt_202105142049353205685537813848102_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c7abf66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049353205685537813848102_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205685537813848102_0004_m_000125_413
[2021-05-14 17:51:06,509] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Task committer attempt_202105142049353205685537813848102_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205685537813848102_0004_m_000125_413 : duration 0:00.003s
[2021-05-14 17:51:06,704] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049357582132273976100122_0004_m_000122_410: needsTaskCommit() Task attempt_202105142049357582132273976100122_0004_m_000122_410
[2021-05-14 17:51:06,705] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Task committer attempt_202105142049357582132273976100122_0004_m_000122_410: needsTaskCommit() Task attempt_202105142049357582132273976100122_0004_m_000122_410: duration 0:00.002s
[2021-05-14 17:51:06,705] {docker.py:276} INFO - 21/05/14 20:51:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357582132273976100122_0004_m_000122_410
[2021-05-14 17:51:06,710] {docker.py:276} INFO - 21/05/14 20:51:06 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-14 17:51:06,711] {docker.py:276} INFO - 21/05/14 20:51:06 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (7cd3ddbc35d2, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:06,712] {docker.py:276} INFO - 21/05/14 20:51:06 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-14 17:51:06,712] {docker.py:276} INFO - 21/05/14 20:51:06 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2308 ms on 7cd3ddbc35d2 (executor driver) (123/200)
[2021-05-14 17:51:06,730] {docker.py:276} INFO - 21/05/14 20:51:06 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:06,770] {docker.py:276} INFO - 21/05/14 20:51:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352147645942531116255_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352147645942531116255_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352147645942531116255_0004}; taskId=attempt_202105142049352147645942531116255_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@698286fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:06 INFO StagingCommitter: Starting: Task committer attempt_202105142049352147645942531116255_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352147645942531116255_0004_m_000126_414
[2021-05-14 17:51:06,771] {docker.py:276} INFO - 21/05/14 20:51:06 INFO StagingCommitter: Task committer attempt_202105142049352147645942531116255_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352147645942531116255_0004_m_000126_414 : duration 0:00.005s
[2021-05-14 17:51:07,278] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Starting: Task committer attempt_202105142049356495222045106288992_0004_m_000123_411: needsTaskCommit() Task attempt_202105142049356495222045106288992_0004_m_000123_411
21/05/14 20:51:07 INFO StagingCommitter: Task committer attempt_202105142049356495222045106288992_0004_m_000123_411: needsTaskCommit() Task attempt_202105142049356495222045106288992_0004_m_000123_411: duration 0:00.000s
[2021-05-14 17:51:07,279] {docker.py:276} INFO - 21/05/14 20:51:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356495222045106288992_0004_m_000123_411
[2021-05-14 17:51:07,282] {docker.py:276} INFO - 21/05/14 20:51:07 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4544 bytes result sent to driver
[2021-05-14 17:51:07,284] {docker.py:276} INFO - 21/05/14 20:51:07 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (7cd3ddbc35d2, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:07,285] {docker.py:276} INFO - 21/05/14 20:51:07 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 2343 ms on 7cd3ddbc35d2 (executor driver) (124/200)
[2021-05-14 17:51:07,287] {docker.py:276} INFO - 21/05/14 20:51:07 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-14 17:51:07,296] {docker.py:276} INFO - 21/05/14 20:51:07 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:07,298] {docker.py:276} INFO - 21/05/14 20:51:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:07,299] {docker.py:276} INFO - 21/05/14 20:51:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935530477108845219709_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935530477108845219709_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935530477108845219709_0004}; taskId=attempt_20210514204935530477108845219709_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72cea48c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:07 INFO StagingCommitter: Starting: Task committer attempt_20210514204935530477108845219709_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935530477108845219709_0004_m_000127_415
[2021-05-14 17:51:07,301] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Task committer attempt_20210514204935530477108845219709_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935530477108845219709_0004_m_000127_415 : duration 0:00.003s
[2021-05-14 17:51:07,510] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Starting: Task committer attempt_202105142049356405201047762078481_0004_m_000124_412: needsTaskCommit() Task attempt_202105142049356405201047762078481_0004_m_000124_412
[2021-05-14 17:51:07,512] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Task committer attempt_202105142049356405201047762078481_0004_m_000124_412: needsTaskCommit() Task attempt_202105142049356405201047762078481_0004_m_000124_412: duration 0:00.001s
21/05/14 20:51:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356405201047762078481_0004_m_000124_412
[2021-05-14 17:51:07,513] {docker.py:276} INFO - 21/05/14 20:51:07 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4544 bytes result sent to driver
[2021-05-14 17:51:07,514] {docker.py:276} INFO - 21/05/14 20:51:07 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (7cd3ddbc35d2, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:07,515] {docker.py:276} INFO - 21/05/14 20:51:07 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 2170 ms on 7cd3ddbc35d2 (executor driver) (125/200)
[2021-05-14 17:51:07,516] {docker.py:276} INFO - 21/05/14 20:51:07 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
[2021-05-14 17:51:07,525] {docker.py:276} INFO - 21/05/14 20:51:07 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:07,527] {docker.py:276} INFO - 21/05/14 20:51:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355391271498523475100_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355391271498523475100_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355391271498523475100_0004}; taskId=attempt_202105142049355391271498523475100_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5764b262}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:07,528] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Starting: Task committer attempt_202105142049355391271498523475100_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355391271498523475100_0004_m_000128_416
[2021-05-14 17:51:07,531] {docker.py:276} INFO - 21/05/14 20:51:07 INFO StagingCommitter: Task committer attempt_202105142049355391271498523475100_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355391271498523475100_0004_m_000128_416 : duration 0:00.004s
[2021-05-14 17:51:08,025] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049353205685537813848102_0004_m_000125_413: needsTaskCommit() Task attempt_202105142049353205685537813848102_0004_m_000125_413
[2021-05-14 17:51:08,026] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Task committer attempt_202105142049353205685537813848102_0004_m_000125_413: needsTaskCommit() Task attempt_202105142049353205685537813848102_0004_m_000125_413: duration 0:00.000s
21/05/14 20:51:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353205685537813848102_0004_m_000125_413
[2021-05-14 17:51:08,026] {docker.py:276} INFO - 21/05/14 20:51:08 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4544 bytes result sent to driver
[2021-05-14 17:51:08,027] {docker.py:276} INFO - 21/05/14 20:51:08 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (7cd3ddbc35d2, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:08,028] {docker.py:276} INFO - 21/05/14 20:51:08 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-14 17:51:08,028] {docker.py:276} INFO - 21/05/14 20:51:08 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 1538 ms on 7cd3ddbc35d2 (executor driver) (126/200)
[2021-05-14 17:51:08,038] {docker.py:276} INFO - 21/05/14 20:51:08 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:08,039] {docker.py:276} INFO - 21/05/14 20:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:51:08,040] {docker.py:276} INFO - 21/05/14 20:51:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:08,041] {docker.py:276} INFO - 21/05/14 20:51:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935427792467269996855_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935427792467269996855_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935427792467269996855_0004}; taskId=attempt_20210514204935427792467269996855_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3306532}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:08,041] {docker.py:276} INFO - 21/05/14 20:51:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:08,043] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Starting: Task committer attempt_20210514204935427792467269996855_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935427792467269996855_0004_m_000129_417
[2021-05-14 17:51:08,047] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Task committer attempt_20210514204935427792467269996855_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935427792467269996855_0004_m_000129_417 : duration 0:00.006s
[2021-05-14 17:51:08,888] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049352147645942531116255_0004_m_000126_414: needsTaskCommit() Task attempt_202105142049352147645942531116255_0004_m_000126_414
[2021-05-14 17:51:08,891] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Task committer attempt_202105142049352147645942531116255_0004_m_000126_414: needsTaskCommit() Task attempt_202105142049352147645942531116255_0004_m_000126_414: duration 0:00.003s
21/05/14 20:51:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352147645942531116255_0004_m_000126_414
[2021-05-14 17:51:08,894] {docker.py:276} INFO - 21/05/14 20:51:08 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4544 bytes result sent to driver
[2021-05-14 17:51:08,896] {docker.py:276} INFO - 21/05/14 20:51:08 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (7cd3ddbc35d2, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:08,896] {docker.py:276} INFO - 21/05/14 20:51:08 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-14 17:51:08,897] {docker.py:276} INFO - 21/05/14 20:51:08 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 2192 ms on 7cd3ddbc35d2 (executor driver) (127/200)
[2021-05-14 17:51:08,906] {docker.py:276} INFO - 21/05/14 20:51:08 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:08,907] {docker.py:276} INFO - 21/05/14 20:51:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353928760559110527901_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353928760559110527901_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353928760559110527901_0004}; taskId=attempt_202105142049353928760559110527901_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4389c0da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:08 INFO StagingCommitter: Starting: Task committer attempt_202105142049353928760559110527901_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353928760559110527901_0004_m_000130_418
[2021-05-14 17:51:08,910] {docker.py:276} INFO - 21/05/14 20:51:08 INFO StagingCommitter: Task committer attempt_202105142049353928760559110527901_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353928760559110527901_0004_m_000130_418 : duration 0:00.003s
[2021-05-14 17:51:09,579] {docker.py:276} INFO - 21/05/14 20:51:09 INFO StagingCommitter: Starting: Task committer attempt_20210514204935530477108845219709_0004_m_000127_415: needsTaskCommit() Task attempt_20210514204935530477108845219709_0004_m_000127_415
21/05/14 20:51:09 INFO StagingCommitter: Task committer attempt_20210514204935530477108845219709_0004_m_000127_415: needsTaskCommit() Task attempt_20210514204935530477108845219709_0004_m_000127_415: duration 0:00.001s
21/05/14 20:51:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935530477108845219709_0004_m_000127_415
[2021-05-14 17:51:09,580] {docker.py:276} INFO - 21/05/14 20:51:09 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4544 bytes result sent to driver
[2021-05-14 17:51:09,582] {docker.py:276} INFO - 21/05/14 20:51:09 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (7cd3ddbc35d2, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:09,583] {docker.py:276} INFO - 21/05/14 20:51:09 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-14 17:51:09,584] {docker.py:276} INFO - 21/05/14 20:51:09 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2302 ms on 7cd3ddbc35d2 (executor driver) (128/200)
[2021-05-14 17:51:09,594] {docker.py:276} INFO - 21/05/14 20:51:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:09,596] {docker.py:276} INFO - 21/05/14 20:51:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356627476437736267841_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356627476437736267841_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356627476437736267841_0004}; taskId=attempt_202105142049356627476437736267841_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@273bdf38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:09 INFO StagingCommitter: Starting: Task committer attempt_202105142049356627476437736267841_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356627476437736267841_0004_m_000131_419
[2021-05-14 17:51:09,599] {docker.py:276} INFO - 21/05/14 20:51:09 INFO StagingCommitter: Task committer attempt_202105142049356627476437736267841_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356627476437736267841_0004_m_000131_419 : duration 0:00.003s
[2021-05-14 17:51:10,960] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Starting: Task committer attempt_20210514204935427792467269996855_0004_m_000129_417: needsTaskCommit() Task attempt_20210514204935427792467269996855_0004_m_000129_417
[2021-05-14 17:51:10,962] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Task committer attempt_20210514204935427792467269996855_0004_m_000129_417: needsTaskCommit() Task attempt_20210514204935427792467269996855_0004_m_000129_417: duration 0:00.001s
21/05/14 20:51:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935427792467269996855_0004_m_000129_417
[2021-05-14 17:51:10,963] {docker.py:276} INFO - 21/05/14 20:51:10 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4544 bytes result sent to driver
[2021-05-14 17:51:10,963] {docker.py:276} INFO - 21/05/14 20:51:10 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (7cd3ddbc35d2, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:10,963] {docker.py:276} INFO - 21/05/14 20:51:10 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2904 ms on 7cd3ddbc35d2 (executor driver) (129/200)
21/05/14 20:51:10 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-14 17:51:10,977] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049355391271498523475100_0004_m_000128_416: needsTaskCommit() Task attempt_202105142049355391271498523475100_0004_m_000128_416
[2021-05-14 17:51:10,977] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Task committer attempt_202105142049355391271498523475100_0004_m_000128_416: needsTaskCommit() Task attempt_202105142049355391271498523475100_0004_m_000128_416: duration 0:00.000s
21/05/14 20:51:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355391271498523475100_0004_m_000128_416
[2021-05-14 17:51:10,978] {docker.py:276} INFO - 21/05/14 20:51:10 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-14 17:51:10,979] {docker.py:276} INFO - 21/05/14 20:51:10 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (7cd3ddbc35d2, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:10,979] {docker.py:276} INFO - 21/05/14 20:51:10 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-14 17:51:10,980] {docker.py:276} INFO - 21/05/14 20:51:10 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 3436 ms on 7cd3ddbc35d2 (executor driver) (130/200)
[2021-05-14 17:51:10,982] {docker.py:276} INFO - 21/05/14 20:51:10 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:10,984] {docker.py:276} INFO - 21/05/14 20:51:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:10,984] {docker.py:276} INFO - 21/05/14 20:51:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354284817500683930320_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354284817500683930320_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354284817500683930320_0004}; taskId=attempt_202105142049354284817500683930320_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13aafdb8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049354284817500683930320_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354284817500683930320_0004_m_000132_420
[2021-05-14 17:51:10,986] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Task committer attempt_202105142049354284817500683930320_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354284817500683930320_0004_m_000132_420 : duration 0:00.003s
[2021-05-14 17:51:10,989] {docker.py:276} INFO - 21/05/14 20:51:10 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:10,990] {docker.py:276} INFO - 21/05/14 20:51:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:10,991] {docker.py:276} INFO - 21/05/14 20:51:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359118916951482368949_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359118916951482368949_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359118916951482368949_0004}; taskId=attempt_202105142049359118916951482368949_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d459746}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:10,992] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Starting: Task committer attempt_202105142049359118916951482368949_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359118916951482368949_0004_m_000133_421
[2021-05-14 17:51:10,994] {docker.py:276} INFO - 21/05/14 20:51:10 INFO StagingCommitter: Task committer attempt_202105142049359118916951482368949_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359118916951482368949_0004_m_000133_421 : duration 0:00.003s
[2021-05-14 17:51:11,281] {docker.py:276} INFO - 21/05/14 20:51:11 INFO StagingCommitter: Starting: Task committer attempt_202105142049353928760559110527901_0004_m_000130_418: needsTaskCommit() Task attempt_202105142049353928760559110527901_0004_m_000130_418
[2021-05-14 17:51:11,282] {docker.py:276} INFO - 21/05/14 20:51:11 INFO StagingCommitter: Task committer attempt_202105142049353928760559110527901_0004_m_000130_418: needsTaskCommit() Task attempt_202105142049353928760559110527901_0004_m_000130_418: duration 0:00.001s
[2021-05-14 17:51:11,283] {docker.py:276} INFO - 21/05/14 20:51:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353928760559110527901_0004_m_000130_418
[2021-05-14 17:51:11,284] {docker.py:276} INFO - 21/05/14 20:51:11 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4587 bytes result sent to driver
[2021-05-14 17:51:11,285] {docker.py:276} INFO - 21/05/14 20:51:11 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (7cd3ddbc35d2, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:11,286] {docker.py:276} INFO - 21/05/14 20:51:11 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 2358 ms on 7cd3ddbc35d2 (executor driver) (131/200)
[2021-05-14 17:51:11,287] {docker.py:276} INFO - 21/05/14 20:51:11 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-14 17:51:11,296] {docker.py:276} INFO - 21/05/14 20:51:11 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:11,296] {docker.py:276} INFO - 21/05/14 20:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:11,298] {docker.py:276} INFO - 21/05/14 20:51:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:11,298] {docker.py:276} INFO - 21/05/14 20:51:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:11,299] {docker.py:276} INFO - 21/05/14 20:51:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:11,299] {docker.py:276} INFO - 21/05/14 20:51:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352395584124268453062_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352395584124268453062_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352395584124268453062_0004}; taskId=attempt_202105142049352395584124268453062_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@677dc32b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:11,299] {docker.py:276} INFO - 21/05/14 20:51:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:11,299] {docker.py:276} INFO - 21/05/14 20:51:11 INFO StagingCommitter: Starting: Task committer attempt_202105142049352395584124268453062_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352395584124268453062_0004_m_000134_422
[2021-05-14 17:51:11,302] {docker.py:276} INFO - 21/05/14 20:51:11 INFO StagingCommitter: Task committer attempt_202105142049352395584124268453062_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352395584124268453062_0004_m_000134_422 : duration 0:00.003s
[2021-05-14 17:51:12,814] {docker.py:276} INFO - 21/05/14 20:51:12 INFO StagingCommitter: Starting: Task committer attempt_202105142049356627476437736267841_0004_m_000131_419: needsTaskCommit() Task attempt_202105142049356627476437736267841_0004_m_000131_419
[2021-05-14 17:51:12,815] {docker.py:276} INFO - 21/05/14 20:51:12 INFO StagingCommitter: Task committer attempt_202105142049356627476437736267841_0004_m_000131_419: needsTaskCommit() Task attempt_202105142049356627476437736267841_0004_m_000131_419: duration 0:00.001s
[2021-05-14 17:51:12,816] {docker.py:276} INFO - 21/05/14 20:51:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356627476437736267841_0004_m_000131_419
[2021-05-14 17:51:12,816] {docker.py:276} INFO - 21/05/14 20:51:12 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4587 bytes result sent to driver
[2021-05-14 17:51:12,818] {docker.py:276} INFO - 21/05/14 20:51:12 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (7cd3ddbc35d2, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:12,819] {docker.py:276} INFO - 21/05/14 20:51:12 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-14 17:51:12,819] {docker.py:276} INFO - 21/05/14 20:51:12 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 3205 ms on 7cd3ddbc35d2 (executor driver) (132/200)
[2021-05-14 17:51:12,829] {docker.py:276} INFO - 21/05/14 20:51:12 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:12,831] {docker.py:276} INFO - 21/05/14 20:51:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:12,832] {docker.py:276} INFO - 21/05/14 20:51:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357644973741677737014_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357644973741677737014_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357644973741677737014_0004}; taskId=attempt_202105142049357644973741677737014_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7436ce71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:12,832] {docker.py:276} INFO - 21/05/14 20:51:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:12,832] {docker.py:276} INFO - 21/05/14 20:51:12 INFO StagingCommitter: Starting: Task committer attempt_202105142049357644973741677737014_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357644973741677737014_0004_m_000135_423
[2021-05-14 17:51:12,835] {docker.py:276} INFO - 21/05/14 20:51:12 INFO StagingCommitter: Task committer attempt_202105142049357644973741677737014_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357644973741677737014_0004_m_000135_423 : duration 0:00.003s
[2021-05-14 17:51:14,882] {docker.py:276} INFO - 21/05/14 20:51:14 INFO StagingCommitter: Starting: Task committer attempt_202105142049354284817500683930320_0004_m_000132_420: needsTaskCommit() Task attempt_202105142049354284817500683930320_0004_m_000132_420
[2021-05-14 17:51:14,884] {docker.py:276} INFO - 21/05/14 20:51:14 INFO StagingCommitter: Task committer attempt_202105142049354284817500683930320_0004_m_000132_420: needsTaskCommit() Task attempt_202105142049354284817500683930320_0004_m_000132_420: duration 0:00.002s
21/05/14 20:51:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354284817500683930320_0004_m_000132_420
[2021-05-14 17:51:14,886] {docker.py:276} INFO - 21/05/14 20:51:14 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4587 bytes result sent to driver
[2021-05-14 17:51:14,890] {docker.py:276} INFO - 21/05/14 20:51:14 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (7cd3ddbc35d2, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:51:14 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 3931 ms on 7cd3ddbc35d2 (executor driver) (133/200)
[2021-05-14 17:51:14,891] {docker.py:276} INFO - 21/05/14 20:51:14 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-14 17:51:14,900] {docker.py:276} INFO - 21/05/14 20:51:14 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:14,901] {docker.py:276} INFO - 21/05/14 20:51:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358020134639838902863_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358020134639838902863_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358020134639838902863_0004}; taskId=attempt_202105142049358020134639838902863_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19f9ed63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:14,902] {docker.py:276} INFO - 21/05/14 20:51:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:14 INFO StagingCommitter: Starting: Task committer attempt_202105142049358020134639838902863_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358020134639838902863_0004_m_000136_424
[2021-05-14 17:51:14,905] {docker.py:276} INFO - 21/05/14 20:51:14 INFO StagingCommitter: Task committer attempt_202105142049358020134639838902863_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358020134639838902863_0004_m_000136_424 : duration 0:00.004s
[2021-05-14 17:51:14,990] {docker.py:276} INFO - 21/05/14 20:51:14 INFO StagingCommitter: Starting: Task committer attempt_202105142049352395584124268453062_0004_m_000134_422: needsTaskCommit() Task attempt_202105142049352395584124268453062_0004_m_000134_422
[2021-05-14 17:51:14,990] {docker.py:276} INFO - 21/05/14 20:51:14 INFO StagingCommitter: Task committer attempt_202105142049352395584124268453062_0004_m_000134_422: needsTaskCommit() Task attempt_202105142049352395584124268453062_0004_m_000134_422: duration 0:00.000s
21/05/14 20:51:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352395584124268453062_0004_m_000134_422
[2021-05-14 17:51:14,992] {docker.py:276} INFO - 21/05/14 20:51:14 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4544 bytes result sent to driver
[2021-05-14 17:51:14,994] {docker.py:276} INFO - 21/05/14 20:51:14 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (7cd3ddbc35d2, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:14,994] {docker.py:276} INFO - 21/05/14 20:51:14 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
21/05/14 20:51:14 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 3713 ms on 7cd3ddbc35d2 (executor driver) (134/200)
[2021-05-14 17:51:15,003] {docker.py:276} INFO - 21/05/14 20:51:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:15,005] {docker.py:276} INFO - 21/05/14 20:51:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:15,005] {docker.py:276} INFO - 21/05/14 20:51:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356271191525696398658_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356271191525696398658_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356271191525696398658_0004}; taskId=attempt_202105142049356271191525696398658_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e5a80b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049356271191525696398658_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356271191525696398658_0004_m_000137_425
[2021-05-14 17:51:15,008] {docker.py:276} INFO - 21/05/14 20:51:15 INFO StagingCommitter: Task committer attempt_202105142049356271191525696398658_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356271191525696398658_0004_m_000137_425 : duration 0:00.003s
[2021-05-14 17:51:15,053] {docker.py:276} INFO - 21/05/14 20:51:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049359118916951482368949_0004_m_000133_421: needsTaskCommit() Task attempt_202105142049359118916951482368949_0004_m_000133_421
[2021-05-14 17:51:15,054] {docker.py:276} INFO - 21/05/14 20:51:15 INFO StagingCommitter: Task committer attempt_202105142049359118916951482368949_0004_m_000133_421: needsTaskCommit() Task attempt_202105142049359118916951482368949_0004_m_000133_421: duration 0:00.001s
[2021-05-14 17:51:15,055] {docker.py:276} INFO - 21/05/14 20:51:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359118916951482368949_0004_m_000133_421
[2021-05-14 17:51:15,056] {docker.py:276} INFO - 21/05/14 20:51:15 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-14 17:51:15,057] {docker.py:276} INFO - 21/05/14 20:51:15 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (7cd3ddbc35d2, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:15,059] {docker.py:276} INFO - 21/05/14 20:51:15 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 4084 ms on 7cd3ddbc35d2 (executor driver) (135/200)
21/05/14 20:51:15 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-14 17:51:15,068] {docker.py:276} INFO - 21/05/14 20:51:15 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:15,070] {docker.py:276} INFO - 21/05/14 20:51:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358126338352423463606_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358126338352423463606_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358126338352423463606_0004}; taskId=attempt_202105142049358126338352423463606_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f88ba71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:15,071] {docker.py:276} INFO - 21/05/14 20:51:15 INFO StagingCommitter: Starting: Task committer attempt_202105142049358126338352423463606_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358126338352423463606_0004_m_000138_426
[2021-05-14 17:51:15,074] {docker.py:276} INFO - 21/05/14 20:51:15 INFO StagingCommitter: Task committer attempt_202105142049358126338352423463606_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358126338352423463606_0004_m_000138_426 : duration 0:00.003s
[2021-05-14 17:51:16,119] {docker.py:276} INFO - 21/05/14 20:51:16 INFO StagingCommitter: Starting: Task committer attempt_202105142049357644973741677737014_0004_m_000135_423: needsTaskCommit() Task attempt_202105142049357644973741677737014_0004_m_000135_423
[2021-05-14 17:51:16,119] {docker.py:276} INFO - 21/05/14 20:51:16 INFO StagingCommitter: Task committer attempt_202105142049357644973741677737014_0004_m_000135_423: needsTaskCommit() Task attempt_202105142049357644973741677737014_0004_m_000135_423: duration 0:00.000s
21/05/14 20:51:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357644973741677737014_0004_m_000135_423
[2021-05-14 17:51:16,120] {docker.py:276} INFO - 21/05/14 20:51:16 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4544 bytes result sent to driver
[2021-05-14 17:51:16,122] {docker.py:276} INFO - 21/05/14 20:51:16 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (7cd3ddbc35d2, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:16,123] {docker.py:276} INFO - 21/05/14 20:51:16 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-14 17:51:16,124] {docker.py:276} INFO - 21/05/14 20:51:16 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 3310 ms on 7cd3ddbc35d2 (executor driver) (136/200)
[2021-05-14 17:51:16,133] {docker.py:276} INFO - 21/05/14 20:51:16 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:16,134] {docker.py:276} INFO - 21/05/14 20:51:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:16,135] {docker.py:276} INFO - 21/05/14 20:51:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:16,136] {docker.py:276} INFO - 21/05/14 20:51:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358099226643962748977_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358099226643962748977_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358099226643962748977_0004}; taskId=attempt_202105142049358099226643962748977_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9af8ac0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:16,136] {docker.py:276} INFO - 21/05/14 20:51:16 INFO StagingCommitter: Starting: Task committer attempt_202105142049358099226643962748977_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358099226643962748977_0004_m_000139_427
[2021-05-14 17:51:16,139] {docker.py:276} INFO - 21/05/14 20:51:16 INFO StagingCommitter: Task committer attempt_202105142049358099226643962748977_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358099226643962748977_0004_m_000139_427 : duration 0:00.003s
[2021-05-14 17:51:18,055] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049356271191525696398658_0004_m_000137_425: needsTaskCommit() Task attempt_202105142049356271191525696398658_0004_m_000137_425
[2021-05-14 17:51:18,056] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049356271191525696398658_0004_m_000137_425: needsTaskCommit() Task attempt_202105142049356271191525696398658_0004_m_000137_425: duration 0:00.000s
21/05/14 20:51:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356271191525696398658_0004_m_000137_425
[2021-05-14 17:51:18,057] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4544 bytes result sent to driver
[2021-05-14 17:51:18,058] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (7cd3ddbc35d2, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:18,059] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 3070 ms on 7cd3ddbc35d2 (executor driver) (137/200)
[2021-05-14 17:51:18,059] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-14 17:51:18,067] {docker.py:276} INFO - 21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:18,068] {docker.py:276} INFO - 21/05/14 20:51:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355639619670540314301_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355639619670540314301_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355639619670540314301_0004}; taskId=attempt_202105142049355639619670540314301_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@570eb08a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049355639619670540314301_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355639619670540314301_0004_m_000140_428
[2021-05-14 17:51:18,071] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049355639619670540314301_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355639619670540314301_0004_m_000140_428 : duration 0:00.003s
[2021-05-14 17:51:18,220] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049358020134639838902863_0004_m_000136_424: needsTaskCommit() Task attempt_202105142049358020134639838902863_0004_m_000136_424
[2021-05-14 17:51:18,222] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049358020134639838902863_0004_m_000136_424: needsTaskCommit() Task attempt_202105142049358020134639838902863_0004_m_000136_424: duration 0:00.001s
21/05/14 20:51:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358020134639838902863_0004_m_000136_424
[2021-05-14 17:51:18,223] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4544 bytes result sent to driver
[2021-05-14 17:51:18,224] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (7cd3ddbc35d2, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:18,225] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-14 17:51:18,226] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 3344 ms on 7cd3ddbc35d2 (executor driver) (138/200)
[2021-05-14 17:51:18,236] {docker.py:276} INFO - 21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:18,238] {docker.py:276} INFO - 21/05/14 20:51:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354356434727853817943_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354356434727853817943_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354356434727853817943_0004}; taskId=attempt_202105142049354356434727853817943_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@748e46ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049354356434727853817943_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354356434727853817943_0004_m_000141_429
[2021-05-14 17:51:18,240] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049354356434727853817943_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354356434727853817943_0004_m_000141_429 : duration 0:00.002s
[2021-05-14 17:51:18,581] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049358126338352423463606_0004_m_000138_426: needsTaskCommit() Task attempt_202105142049358126338352423463606_0004_m_000138_426
[2021-05-14 17:51:18,582] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049358126338352423463606_0004_m_000138_426: needsTaskCommit() Task attempt_202105142049358126338352423463606_0004_m_000138_426: duration 0:00.001s
[2021-05-14 17:51:18,583] {docker.py:276} INFO - 21/05/14 20:51:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358126338352423463606_0004_m_000138_426
[2021-05-14 17:51:18,585] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4544 bytes result sent to driver
[2021-05-14 17:51:18,586] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (7cd3ddbc35d2, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:18,588] {docker.py:276} INFO - 21/05/14 20:51:18 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 3536 ms on 7cd3ddbc35d2 (executor driver) (139/200)
[2021-05-14 17:51:18,589] {docker.py:276} INFO - 21/05/14 20:51:18 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-14 17:51:18,598] {docker.py:276} INFO - 21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Getting 5 (45.8 KiB) non-empty blocks including 5 (45.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:18,600] {docker.py:276} INFO - 21/05/14 20:51:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358010893864646435171_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358010893864646435171_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358010893864646435171_0004}; taskId=attempt_202105142049358010893864646435171_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10c58dc6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:18 INFO StagingCommitter: Starting: Task committer attempt_202105142049358010893864646435171_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358010893864646435171_0004_m_000142_430
[2021-05-14 17:51:18,603] {docker.py:276} INFO - 21/05/14 20:51:18 INFO StagingCommitter: Task committer attempt_202105142049358010893864646435171_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358010893864646435171_0004_m_000142_430 : duration 0:00.003s
[2021-05-14 17:51:19,606] {docker.py:276} INFO - 21/05/14 20:51:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049358099226643962748977_0004_m_000139_427: needsTaskCommit() Task attempt_202105142049358099226643962748977_0004_m_000139_427
[2021-05-14 17:51:19,607] {docker.py:276} INFO - 21/05/14 20:51:19 INFO StagingCommitter: Task committer attempt_202105142049358099226643962748977_0004_m_000139_427: needsTaskCommit() Task attempt_202105142049358099226643962748977_0004_m_000139_427: duration 0:00.001s
21/05/14 20:51:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358099226643962748977_0004_m_000139_427
[2021-05-14 17:51:19,608] {docker.py:276} INFO - 21/05/14 20:51:19 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4587 bytes result sent to driver
[2021-05-14 17:51:19,609] {docker.py:276} INFO - 21/05/14 20:51:19 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (7cd3ddbc35d2, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:19,610] {docker.py:276} INFO - 21/05/14 20:51:19 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 3494 ms on 7cd3ddbc35d2 (executor driver) (140/200)
[2021-05-14 17:51:19,611] {docker.py:276} INFO - 21/05/14 20:51:19 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-14 17:51:19,620] {docker.py:276} INFO - 21/05/14 20:51:19 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:19,622] {docker.py:276} INFO - 21/05/14 20:51:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356452193724884611856_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356452193724884611856_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356452193724884611856_0004}; taskId=attempt_202105142049356452193724884611856_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8b518c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:19 INFO StagingCommitter: Starting: Task committer attempt_202105142049356452193724884611856_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356452193724884611856_0004_m_000143_431
[2021-05-14 17:51:19,626] {docker.py:276} INFO - 21/05/14 20:51:19 INFO StagingCommitter: Task committer attempt_202105142049356452193724884611856_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356452193724884611856_0004_m_000143_431 : duration 0:00.004s
[2021-05-14 17:51:20,649] {docker.py:276} INFO - 21/05/14 20:51:20 INFO StagingCommitter: Starting: Task committer attempt_202105142049354356434727853817943_0004_m_000141_429: needsTaskCommit() Task attempt_202105142049354356434727853817943_0004_m_000141_429
[2021-05-14 17:51:20,649] {docker.py:276} INFO - 21/05/14 20:51:20 INFO StagingCommitter: Task committer attempt_202105142049354356434727853817943_0004_m_000141_429: needsTaskCommit() Task attempt_202105142049354356434727853817943_0004_m_000141_429: duration 0:00.000s
21/05/14 20:51:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354356434727853817943_0004_m_000141_429
[2021-05-14 17:51:20,651] {docker.py:276} INFO - 21/05/14 20:51:20 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4587 bytes result sent to driver
[2021-05-14 17:51:20,652] {docker.py:276} INFO - 21/05/14 20:51:20 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (7cd3ddbc35d2, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:20,652] {docker.py:276} INFO - 21/05/14 20:51:20 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-14 17:51:20,653] {docker.py:276} INFO - 21/05/14 20:51:20 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2431 ms on 7cd3ddbc35d2 (executor driver) (141/200)
[2021-05-14 17:51:20,663] {docker.py:276} INFO - 21/05/14 20:51:20 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:20,665] {docker.py:276} INFO - 21/05/14 20:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355091620161943468945_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355091620161943468945_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355091620161943468945_0004}; taskId=attempt_202105142049355091620161943468945_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9bd7d87}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:20,666] {docker.py:276} INFO - 21/05/14 20:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:20 INFO StagingCommitter: Starting: Task committer attempt_202105142049355091620161943468945_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355091620161943468945_0004_m_000144_432
[2021-05-14 17:51:20,669] {docker.py:276} INFO - 21/05/14 20:51:20 INFO StagingCommitter: Task committer attempt_202105142049355091620161943468945_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355091620161943468945_0004_m_000144_432 : duration 0:00.003s
[2021-05-14 17:51:21,206] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049358010893864646435171_0004_m_000142_430: needsTaskCommit() Task attempt_202105142049358010893864646435171_0004_m_000142_430
[2021-05-14 17:51:21,207] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Task committer attempt_202105142049358010893864646435171_0004_m_000142_430: needsTaskCommit() Task attempt_202105142049358010893864646435171_0004_m_000142_430: duration 0:00.001s
21/05/14 20:51:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358010893864646435171_0004_m_000142_430
[2021-05-14 17:51:21,208] {docker.py:276} INFO - 21/05/14 20:51:21 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4587 bytes result sent to driver
[2021-05-14 17:51:21,209] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049355639619670540314301_0004_m_000140_428: needsTaskCommit() Task attempt_202105142049355639619670540314301_0004_m_000140_428
21/05/14 20:51:21 INFO StagingCommitter: Task committer attempt_202105142049355639619670540314301_0004_m_000140_428: needsTaskCommit() Task attempt_202105142049355639619670540314301_0004_m_000140_428: duration 0:00.000s
21/05/14 20:51:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355639619670540314301_0004_m_000140_428
[2021-05-14 17:51:21,210] {docker.py:276} INFO - 21/05/14 20:51:21 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (7cd3ddbc35d2, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:21,211] {docker.py:276} INFO - 21/05/14 20:51:21 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4587 bytes result sent to driver
[2021-05-14 17:51:21,212] {docker.py:276} INFO - 21/05/14 20:51:21 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
21/05/14 20:51:21 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (7cd3ddbc35d2, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:21,212] {docker.py:276} INFO - 21/05/14 20:51:21 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 3158 ms on 7cd3ddbc35d2 (executor driver) (142/200)
[2021-05-14 17:51:21,215] {docker.py:276} INFO - 21/05/14 20:51:21 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-14 17:51:21,216] {docker.py:276} INFO - 21/05/14 20:51:21 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2630 ms on 7cd3ddbc35d2 (executor driver) (143/200)
[2021-05-14 17:51:21,222] {docker.py:276} INFO - 21/05/14 20:51:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:21,222] {docker.py:276} INFO - 21/05/14 20:51:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:21,224] {docker.py:276} INFO - 21/05/14 20:51:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:21,224] {docker.py:276} INFO - 21/05/14 20:51:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356262713677421631503_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356262713677421631503_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356262713677421631503_0004}; taskId=attempt_202105142049356262713677421631503_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33a9ea6c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049356262713677421631503_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356262713677421631503_0004_m_000145_433
[2021-05-14 17:51:21,225] {docker.py:276} INFO - 21/05/14 20:51:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:21,225] {docker.py:276} INFO - 21/05/14 20:51:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357160293343624715914_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357160293343624715914_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357160293343624715914_0004}; taskId=attempt_202105142049357160293343624715914_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@694a1ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:21,226] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Starting: Task committer attempt_202105142049357160293343624715914_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357160293343624715914_0004_m_000146_434
[2021-05-14 17:51:21,228] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Task committer attempt_202105142049356262713677421631503_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356262713677421631503_0004_m_000145_433 : duration 0:00.005s
[2021-05-14 17:51:21,230] {docker.py:276} INFO - 21/05/14 20:51:21 INFO StagingCommitter: Task committer attempt_202105142049357160293343624715914_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357160293343624715914_0004_m_000146_434 : duration 0:00.005s
[2021-05-14 17:51:22,415] {docker.py:276} INFO - 21/05/14 20:51:22 INFO StagingCommitter: Starting: Task committer attempt_202105142049356452193724884611856_0004_m_000143_431: needsTaskCommit() Task attempt_202105142049356452193724884611856_0004_m_000143_431
[2021-05-14 17:51:22,416] {docker.py:276} INFO - 21/05/14 20:51:22 INFO StagingCommitter: Task committer attempt_202105142049356452193724884611856_0004_m_000143_431: needsTaskCommit() Task attempt_202105142049356452193724884611856_0004_m_000143_431: duration 0:00.001s
21/05/14 20:51:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356452193724884611856_0004_m_000143_431
[2021-05-14 17:51:22,417] {docker.py:276} INFO - 21/05/14 20:51:22 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4544 bytes result sent to driver
[2021-05-14 17:51:22,418] {docker.py:276} INFO - 21/05/14 20:51:22 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (7cd3ddbc35d2, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:22,419] {docker.py:276} INFO - 21/05/14 20:51:22 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2813 ms on 7cd3ddbc35d2 (executor driver) (144/200)
21/05/14 20:51:22 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-14 17:51:22,430] {docker.py:276} INFO - 21/05/14 20:51:22 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:22,432] {docker.py:276} INFO - 21/05/14 20:51:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354751827114711471006_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354751827114711471006_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354751827114711471006_0004}; taskId=attempt_202105142049354751827114711471006_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16256a19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:22 INFO StagingCommitter: Starting: Task committer attempt_202105142049354751827114711471006_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354751827114711471006_0004_m_000147_435
[2021-05-14 17:51:22,435] {docker.py:276} INFO - 21/05/14 20:51:22 INFO StagingCommitter: Task committer attempt_202105142049354751827114711471006_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354751827114711471006_0004_m_000147_435 : duration 0:00.003s
[2021-05-14 17:51:23,299] {docker.py:276} INFO - 21/05/14 20:51:23 INFO StagingCommitter: Starting: Task committer attempt_202105142049355091620161943468945_0004_m_000144_432: needsTaskCommit() Task attempt_202105142049355091620161943468945_0004_m_000144_432
[2021-05-14 17:51:23,299] {docker.py:276} INFO - 21/05/14 20:51:23 INFO StagingCommitter: Task committer attempt_202105142049355091620161943468945_0004_m_000144_432: needsTaskCommit() Task attempt_202105142049355091620161943468945_0004_m_000144_432: duration 0:00.001s
21/05/14 20:51:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355091620161943468945_0004_m_000144_432
[2021-05-14 17:51:23,300] {docker.py:276} INFO - 21/05/14 20:51:23 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4544 bytes result sent to driver
[2021-05-14 17:51:23,301] {docker.py:276} INFO - 21/05/14 20:51:23 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (7cd3ddbc35d2, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:23,303] {docker.py:276} INFO - 21/05/14 20:51:23 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 2654 ms on 7cd3ddbc35d2 (executor driver) (145/200)
21/05/14 20:51:23 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-14 17:51:23,312] {docker.py:276} INFO - 21/05/14 20:51:23 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:23,313] {docker.py:276} INFO - 21/05/14 20:51:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358597597375648647746_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358597597375648647746_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358597597375648647746_0004}; taskId=attempt_202105142049358597597375648647746_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b02c423}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:23,314] {docker.py:276} INFO - 21/05/14 20:51:23 INFO StagingCommitter: Starting: Task committer attempt_202105142049358597597375648647746_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358597597375648647746_0004_m_000148_436
[2021-05-14 17:51:23,317] {docker.py:276} INFO - 21/05/14 20:51:23 INFO StagingCommitter: Task committer attempt_202105142049358597597375648647746_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358597597375648647746_0004_m_000148_436 : duration 0:00.003s
[2021-05-14 17:51:24,361] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049357160293343624715914_0004_m_000146_434: needsTaskCommit() Task attempt_202105142049357160293343624715914_0004_m_000146_434
[2021-05-14 17:51:24,362] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Task committer attempt_202105142049357160293343624715914_0004_m_000146_434: needsTaskCommit() Task attempt_202105142049357160293343624715914_0004_m_000146_434: duration 0:00.001s
21/05/14 20:51:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357160293343624715914_0004_m_000146_434
[2021-05-14 17:51:24,363] {docker.py:276} INFO - 21/05/14 20:51:24 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4544 bytes result sent to driver
[2021-05-14 17:51:24,364] {docker.py:276} INFO - 21/05/14 20:51:24 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (7cd3ddbc35d2, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:24,366] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049356262713677421631503_0004_m_000145_433: needsTaskCommit() Task attempt_202105142049356262713677421631503_0004_m_000145_433
21/05/14 20:51:24 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-14 17:51:24,366] {docker.py:276} INFO - 21/05/14 20:51:24 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 3157 ms on 7cd3ddbc35d2 (executor driver) (146/200)
[2021-05-14 17:51:24,367] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Task committer attempt_202105142049356262713677421631503_0004_m_000145_433: needsTaskCommit() Task attempt_202105142049356262713677421631503_0004_m_000145_433: duration 0:00.003s
21/05/14 20:51:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356262713677421631503_0004_m_000145_433
[2021-05-14 17:51:24,368] {docker.py:276} INFO - 21/05/14 20:51:24 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4544 bytes result sent to driver
[2021-05-14 17:51:24,369] {docker.py:276} INFO - 21/05/14 20:51:24 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (7cd3ddbc35d2, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:24,370] {docker.py:276} INFO - 21/05/14 20:51:24 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-14 17:51:24,371] {docker.py:276} INFO - 21/05/14 20:51:24 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 3164 ms on 7cd3ddbc35d2 (executor driver) (147/200)
[2021-05-14 17:51:24,377] {docker.py:276} INFO - 21/05/14 20:51:24 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:24,378] {docker.py:276} INFO - 21/05/14 20:51:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351707790627967173001_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351707790627967173001_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351707790627967173001_0004}; taskId=attempt_202105142049351707790627967173001_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26165df3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:24,379] {docker.py:276} INFO - 21/05/14 20:51:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049351707790627967173001_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351707790627967173001_0004_m_000149_437
[2021-05-14 17:51:24,381] {docker.py:276} INFO - 21/05/14 20:51:24 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:24,381] {docker.py:276} INFO - 21/05/14 20:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:24,382] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Task committer attempt_202105142049351707790627967173001_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351707790627967173001_0004_m_000149_437 : duration 0:00.003s
[2021-05-14 17:51:24,383] {docker.py:276} INFO - 21/05/14 20:51:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:24,383] {docker.py:276} INFO - 21/05/14 20:51:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:24,384] {docker.py:276} INFO - 21/05/14 20:51:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:24,384] {docker.py:276} INFO - 21/05/14 20:51:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357877142887557399984_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357877142887557399984_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357877142887557399984_0004}; taskId=attempt_202105142049357877142887557399984_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44782901}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:24,384] {docker.py:276} INFO - 21/05/14 20:51:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:24,384] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Starting: Task committer attempt_202105142049357877142887557399984_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357877142887557399984_0004_m_000150_438
[2021-05-14 17:51:24,387] {docker.py:276} INFO - 21/05/14 20:51:24 INFO StagingCommitter: Task committer attempt_202105142049357877142887557399984_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357877142887557399984_0004_m_000150_438 : duration 0:00.003s
[2021-05-14 17:51:25,237] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Starting: Task committer attempt_202105142049358597597375648647746_0004_m_000148_436: needsTaskCommit() Task attempt_202105142049358597597375648647746_0004_m_000148_436
[2021-05-14 17:51:25,240] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Task committer attempt_202105142049358597597375648647746_0004_m_000148_436: needsTaskCommit() Task attempt_202105142049358597597375648647746_0004_m_000148_436: duration 0:00.003s
21/05/14 20:51:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358597597375648647746_0004_m_000148_436
[2021-05-14 17:51:25,243] {docker.py:276} INFO - 21/05/14 20:51:25 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4544 bytes result sent to driver
[2021-05-14 17:51:25,244] {docker.py:276} INFO - 21/05/14 20:51:25 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (7cd3ddbc35d2, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:25,246] {docker.py:276} INFO - 21/05/14 20:51:25 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-14 17:51:25,247] {docker.py:276} INFO - 21/05/14 20:51:25 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 1946 ms on 7cd3ddbc35d2 (executor driver) (148/200)
[2021-05-14 17:51:25,254] {docker.py:276} INFO - 21/05/14 20:51:25 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:25,255] {docker.py:276} INFO - 21/05/14 20:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:25,257] {docker.py:276} INFO - 21/05/14 20:51:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356252115764160887880_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356252115764160887880_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356252115764160887880_0004}; taskId=attempt_202105142049356252115764160887880_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20fbdd3a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:25 INFO StagingCommitter: Starting: Task committer attempt_202105142049356252115764160887880_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356252115764160887880_0004_m_000151_439
[2021-05-14 17:51:25,259] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Task committer attempt_202105142049356252115764160887880_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356252115764160887880_0004_m_000151_439 : duration 0:00.002s
[2021-05-14 17:51:25,394] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Starting: Task committer attempt_202105142049354751827114711471006_0004_m_000147_435: needsTaskCommit() Task attempt_202105142049354751827114711471006_0004_m_000147_435
[2021-05-14 17:51:25,394] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Task committer attempt_202105142049354751827114711471006_0004_m_000147_435: needsTaskCommit() Task attempt_202105142049354751827114711471006_0004_m_000147_435: duration 0:00.002s
[2021-05-14 17:51:25,395] {docker.py:276} INFO - 21/05/14 20:51:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354751827114711471006_0004_m_000147_435
[2021-05-14 17:51:25,397] {docker.py:276} INFO - 21/05/14 20:51:25 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4544 bytes result sent to driver
[2021-05-14 17:51:25,399] {docker.py:276} INFO - 21/05/14 20:51:25 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (7cd3ddbc35d2, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:25,399] {docker.py:276} INFO - 21/05/14 20:51:25 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-14 17:51:25,400] {docker.py:276} INFO - 21/05/14 20:51:25 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2985 ms on 7cd3ddbc35d2 (executor driver) (149/200)
[2021-05-14 17:51:25,407] {docker.py:276} INFO - 21/05/14 20:51:25 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:25,409] {docker.py:276} INFO - 21/05/14 20:51:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358777051201656661405_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358777051201656661405_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358777051201656661405_0004}; taskId=attempt_202105142049358777051201656661405_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2aee9f47}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:25 INFO StagingCommitter: Starting: Task committer attempt_202105142049358777051201656661405_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358777051201656661405_0004_m_000152_440
[2021-05-14 17:51:25,412] {docker.py:276} INFO - 21/05/14 20:51:25 INFO StagingCommitter: Task committer attempt_202105142049358777051201656661405_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358777051201656661405_0004_m_000152_440 : duration 0:00.003s
[2021-05-14 17:51:26,594] {docker.py:276} INFO - 21/05/14 20:51:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049357877142887557399984_0004_m_000150_438: needsTaskCommit() Task attempt_202105142049357877142887557399984_0004_m_000150_438
[2021-05-14 17:51:26,596] {docker.py:276} INFO - 21/05/14 20:51:26 INFO StagingCommitter: Task committer attempt_202105142049357877142887557399984_0004_m_000150_438: needsTaskCommit() Task attempt_202105142049357877142887557399984_0004_m_000150_438: duration 0:00.002s
21/05/14 20:51:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357877142887557399984_0004_m_000150_438
[2021-05-14 17:51:26,598] {docker.py:276} INFO - 21/05/14 20:51:26 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4544 bytes result sent to driver
[2021-05-14 17:51:26,599] {docker.py:276} INFO - 21/05/14 20:51:26 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (7cd3ddbc35d2, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:26,600] {docker.py:276} INFO - 21/05/14 20:51:26 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-14 17:51:26,602] {docker.py:276} INFO - 21/05/14 20:51:26 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2235 ms on 7cd3ddbc35d2 (executor driver) (150/200)
[2021-05-14 17:51:26,610] {docker.py:276} INFO - 21/05/14 20:51:26 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:26,613] {docker.py:276} INFO - 21/05/14 20:51:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:26,614] {docker.py:276} INFO - 21/05/14 20:51:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352867647220524151491_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352867647220524151491_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352867647220524151491_0004}; taskId=attempt_202105142049352867647220524151491_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a5ac1ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:26,614] {docker.py:276} INFO - 21/05/14 20:51:26 INFO StagingCommitter: Starting: Task committer attempt_202105142049352867647220524151491_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352867647220524151491_0004_m_000153_441
[2021-05-14 17:51:26,618] {docker.py:276} INFO - 21/05/14 20:51:26 INFO StagingCommitter: Task committer attempt_202105142049352867647220524151491_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352867647220524151491_0004_m_000153_441 : duration 0:00.004s
[2021-05-14 17:51:27,334] {docker.py:276} INFO - 21/05/14 20:51:27 INFO StagingCommitter: Starting: Task committer attempt_202105142049351707790627967173001_0004_m_000149_437: needsTaskCommit() Task attempt_202105142049351707790627967173001_0004_m_000149_437
21/05/14 20:51:27 INFO StagingCommitter: Task committer attempt_202105142049351707790627967173001_0004_m_000149_437: needsTaskCommit() Task attempt_202105142049351707790627967173001_0004_m_000149_437: duration 0:00.000s
[2021-05-14 17:51:27,335] {docker.py:276} INFO - 21/05/14 20:51:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351707790627967173001_0004_m_000149_437
[2021-05-14 17:51:27,336] {docker.py:276} INFO - 21/05/14 20:51:27 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4587 bytes result sent to driver
[2021-05-14 17:51:27,337] {docker.py:276} INFO - 21/05/14 20:51:27 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (7cd3ddbc35d2, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:27,339] {docker.py:276} INFO - 21/05/14 20:51:27 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-14 17:51:27,340] {docker.py:276} INFO - 21/05/14 20:51:27 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 2980 ms on 7cd3ddbc35d2 (executor driver) (151/200)
[2021-05-14 17:51:27,350] {docker.py:276} INFO - 21/05/14 20:51:27 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:27,353] {docker.py:276} INFO - 21/05/14 20:51:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:27,355] {docker.py:276} INFO - 21/05/14 20:51:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355153592053227564522_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355153592053227564522_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355153592053227564522_0004}; taskId=attempt_202105142049355153592053227564522_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f191942}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:27 INFO StagingCommitter: Starting: Task committer attempt_202105142049355153592053227564522_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355153592053227564522_0004_m_000154_442
[2021-05-14 17:51:27,358] {docker.py:276} INFO - 21/05/14 20:51:27 INFO StagingCommitter: Task committer attempt_202105142049355153592053227564522_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355153592053227564522_0004_m_000154_442 : duration 0:00.003s
[2021-05-14 17:51:28,380] {docker.py:276} INFO - 21/05/14 20:51:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049358777051201656661405_0004_m_000152_440: needsTaskCommit() Task attempt_202105142049358777051201656661405_0004_m_000152_440
[2021-05-14 17:51:28,380] {docker.py:276} INFO - 21/05/14 20:51:28 INFO StagingCommitter: Task committer attempt_202105142049358777051201656661405_0004_m_000152_440: needsTaskCommit() Task attempt_202105142049358777051201656661405_0004_m_000152_440: duration 0:00.001s
21/05/14 20:51:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358777051201656661405_0004_m_000152_440
[2021-05-14 17:51:28,381] {docker.py:276} INFO - 21/05/14 20:51:28 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4587 bytes result sent to driver
[2021-05-14 17:51:28,382] {docker.py:276} INFO - 21/05/14 20:51:28 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (7cd3ddbc35d2, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:28,383] {docker.py:276} INFO - 21/05/14 20:51:28 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2989 ms on 7cd3ddbc35d2 (executor driver) (152/200)
[2021-05-14 17:51:28,384] {docker.py:276} INFO - 21/05/14 20:51:28 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-14 17:51:28,393] {docker.py:276} INFO - 21/05/14 20:51:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:28,393] {docker.py:276} INFO - 21/05/14 20:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:28,395] {docker.py:276} INFO - 21/05/14 20:51:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:28,395] {docker.py:276} INFO - 21/05/14 20:51:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:28,396] {docker.py:276} INFO - 21/05/14 20:51:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351740215344286177723_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351740215344286177723_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351740215344286177723_0004}; taskId=attempt_202105142049351740215344286177723_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76904b43}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:28,396] {docker.py:276} INFO - 21/05/14 20:51:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:28,396] {docker.py:276} INFO - 21/05/14 20:51:28 INFO StagingCommitter: Starting: Task committer attempt_202105142049351740215344286177723_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351740215344286177723_0004_m_000155_443
[2021-05-14 17:51:28,399] {docker.py:276} INFO - 21/05/14 20:51:28 INFO StagingCommitter: Task committer attempt_202105142049351740215344286177723_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351740215344286177723_0004_m_000155_443 : duration 0:00.003s
[2021-05-14 17:51:29,036] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049356252115764160887880_0004_m_000151_439: needsTaskCommit() Task attempt_202105142049356252115764160887880_0004_m_000151_439
[2021-05-14 17:51:29,038] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049356252115764160887880_0004_m_000151_439: needsTaskCommit() Task attempt_202105142049356252115764160887880_0004_m_000151_439: duration 0:00.001s
[2021-05-14 17:51:29,039] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049352867647220524151491_0004_m_000153_441: needsTaskCommit() Task attempt_202105142049352867647220524151491_0004_m_000153_441
[2021-05-14 17:51:29,040] {docker.py:276} INFO - 21/05/14 20:51:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356252115764160887880_0004_m_000151_439
[2021-05-14 17:51:29,040] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049352867647220524151491_0004_m_000153_441: needsTaskCommit() Task attempt_202105142049352867647220524151491_0004_m_000153_441: duration 0:00.002s
[2021-05-14 17:51:29,040] {docker.py:276} INFO - 21/05/14 20:51:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352867647220524151491_0004_m_000153_441
[2021-05-14 17:51:29,041] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4587 bytes result sent to driver
[2021-05-14 17:51:29,042] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (7cd3ddbc35d2, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:29,043] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-14 17:51:29,044] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 3803 ms on 7cd3ddbc35d2 (executor driver) (153/200)
[2021-05-14 17:51:29,044] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4587 bytes result sent to driver
[2021-05-14 17:51:29,046] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (7cd3ddbc35d2, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:29,047] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-14 17:51:29,050] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2451 ms on 7cd3ddbc35d2 (executor driver) (154/200)
[2021-05-14 17:51:29,056] {docker.py:276} INFO - 21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:29,056] {docker.py:276} INFO - 21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:29,057] {docker.py:276} INFO - 21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:29,058] {docker.py:276} INFO - 21/05/14 20:51:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:29,058] {docker.py:276} INFO - 21/05/14 20:51:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:29,059] {docker.py:276} INFO - 21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:29,059] {docker.py:276} INFO - 21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358484795673166965125_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358484795673166965125_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358484795673166965125_0004}; taskId=attempt_202105142049358484795673166965125_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2aa8d043}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:29,059] {docker.py:276} INFO - 21/05/14 20:51:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:29,060] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049358484795673166965125_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358484795673166965125_0004_m_000157_445
[2021-05-14 17:51:29,060] {docker.py:276} INFO - 21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:29,060] {docker.py:276} INFO - 21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351685926357599819932_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351685926357599819932_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351685926357599819932_0004}; taskId=attempt_202105142049351685926357599819932_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@643709bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049351685926357599819932_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351685926357599819932_0004_m_000156_444
[2021-05-14 17:51:29,062] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049358484795673166965125_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358484795673166965125_0004_m_000157_445 : duration 0:00.004s
[2021-05-14 17:51:29,064] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049351685926357599819932_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351685926357599819932_0004_m_000156_444 : duration 0:00.005s
[2021-05-14 17:51:29,209] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049355153592053227564522_0004_m_000154_442: needsTaskCommit() Task attempt_202105142049355153592053227564522_0004_m_000154_442
[2021-05-14 17:51:29,210] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049355153592053227564522_0004_m_000154_442: needsTaskCommit() Task attempt_202105142049355153592053227564522_0004_m_000154_442: duration 0:00.000s
21/05/14 20:51:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355153592053227564522_0004_m_000154_442
[2021-05-14 17:51:29,211] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4544 bytes result sent to driver
[2021-05-14 17:51:29,212] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (7cd3ddbc35d2, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:29,213] {docker.py:276} INFO - 21/05/14 20:51:29 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-14 17:51:29,214] {docker.py:276} INFO - 21/05/14 20:51:29 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 1878 ms on 7cd3ddbc35d2 (executor driver) (155/200)
[2021-05-14 17:51:29,226] {docker.py:276} INFO - 21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:29,228] {docker.py:276} INFO - 21/05/14 20:51:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:29,228] {docker.py:276} INFO - 21/05/14 20:51:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355437204203084361159_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355437204203084361159_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355437204203084361159_0004}; taskId=attempt_202105142049355437204203084361159_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e58ee16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:29 INFO StagingCommitter: Starting: Task committer attempt_202105142049355437204203084361159_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355437204203084361159_0004_m_000158_446
[2021-05-14 17:51:29,231] {docker.py:276} INFO - 21/05/14 20:51:29 INFO StagingCommitter: Task committer attempt_202105142049355437204203084361159_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355437204203084361159_0004_m_000158_446 : duration 0:00.003s
[2021-05-14 17:51:31,135] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049351740215344286177723_0004_m_000155_443: needsTaskCommit() Task attempt_202105142049351740215344286177723_0004_m_000155_443
[2021-05-14 17:51:31,136] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Task committer attempt_202105142049351740215344286177723_0004_m_000155_443: needsTaskCommit() Task attempt_202105142049351740215344286177723_0004_m_000155_443: duration 0:00.000s
21/05/14 20:51:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351740215344286177723_0004_m_000155_443
[2021-05-14 17:51:31,138] {docker.py:276} INFO - 21/05/14 20:51:31 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4544 bytes result sent to driver
[2021-05-14 17:51:31,139] {docker.py:276} INFO - 21/05/14 20:51:31 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (7cd3ddbc35d2, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:31,139] {docker.py:276} INFO - 21/05/14 20:51:31 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2761 ms on 7cd3ddbc35d2 (executor driver) (156/200)
[2021-05-14 17:51:31,141] {docker.py:276} INFO - 21/05/14 20:51:31 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-14 17:51:31,149] {docker.py:276} INFO - 21/05/14 20:51:31 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:31,150] {docker.py:276} INFO - 21/05/14 20:51:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:31,151] {docker.py:276} INFO - 21/05/14 20:51:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:31,152] {docker.py:276} INFO - 21/05/14 20:51:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355199302857953404830_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355199302857953404830_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355199302857953404830_0004}; taskId=attempt_202105142049355199302857953404830_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b4486ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:31,152] {docker.py:276} INFO - 21/05/14 20:51:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049355199302857953404830_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355199302857953404830_0004_m_000159_447
[2021-05-14 17:51:31,154] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Task committer attempt_202105142049355199302857953404830_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355199302857953404830_0004_m_000159_447 : duration 0:00.003s
[2021-05-14 17:51:31,317] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049358484795673166965125_0004_m_000157_445: needsTaskCommit() Task attempt_202105142049358484795673166965125_0004_m_000157_445
[2021-05-14 17:51:31,336] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Task committer attempt_202105142049358484795673166965125_0004_m_000157_445: needsTaskCommit() Task attempt_202105142049358484795673166965125_0004_m_000157_445: duration 0:00.000s
[2021-05-14 17:51:31,338] {docker.py:276} INFO - 21/05/14 20:51:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358484795673166965125_0004_m_000157_445
[2021-05-14 17:51:31,338] {docker.py:276} INFO - 21/05/14 20:51:31 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4544 bytes result sent to driver
[2021-05-14 17:51:31,338] {docker.py:276} INFO - 21/05/14 20:51:31 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (7cd3ddbc35d2, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:31,338] {docker.py:276} INFO - 21/05/14 20:51:31 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
[2021-05-14 17:51:31,339] {docker.py:276} INFO - 21/05/14 20:51:31 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2280 ms on 7cd3ddbc35d2 (executor driver) (157/200)
[2021-05-14 17:51:31,339] {docker.py:276} INFO - 21/05/14 20:51:31 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:31,339] {docker.py:276} INFO - 21/05/14 20:51:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:31,339] {docker.py:276} INFO - 21/05/14 20:51:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357858753100805351997_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357858753100805351997_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357858753100805351997_0004}; taskId=attempt_202105142049357858753100805351997_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54a192c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:31,340] {docker.py:276} INFO - 21/05/14 20:51:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:31,340] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Starting: Task committer attempt_202105142049357858753100805351997_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357858753100805351997_0004_m_000160_448
[2021-05-14 17:51:31,340] {docker.py:276} INFO - 21/05/14 20:51:31 INFO StagingCommitter: Task committer attempt_202105142049357858753100805351997_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357858753100805351997_0004_m_000160_448 : duration 0:00.003s
[2021-05-14 17:51:32,353] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Starting: Task committer attempt_202105142049355437204203084361159_0004_m_000158_446: needsTaskCommit() Task attempt_202105142049355437204203084361159_0004_m_000158_446
[2021-05-14 17:51:32,354] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Task committer attempt_202105142049355437204203084361159_0004_m_000158_446: needsTaskCommit() Task attempt_202105142049355437204203084361159_0004_m_000158_446: duration 0:00.000s
21/05/14 20:51:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355437204203084361159_0004_m_000158_446
[2021-05-14 17:51:32,355] {docker.py:276} INFO - 21/05/14 20:51:32 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4544 bytes result sent to driver
[2021-05-14 17:51:32,357] {docker.py:276} INFO - 21/05/14 20:51:32 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (7cd3ddbc35d2, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:32,358] {docker.py:276} INFO - 21/05/14 20:51:32 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 3149 ms on 7cd3ddbc35d2 (executor driver) (158/200)
[2021-05-14 17:51:32,359] {docker.py:276} INFO - 21/05/14 20:51:32 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-14 17:51:32,368] {docker.py:276} INFO - 21/05/14 20:51:32 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:32,368] {docker.py:276} INFO - 21/05/14 20:51:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:32,370] {docker.py:276} INFO - 21/05/14 20:51:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:32,370] {docker.py:276} INFO - 21/05/14 20:51:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:32,371] {docker.py:276} INFO - 21/05/14 20:51:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353522428041914970582_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353522428041914970582_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353522428041914970582_0004}; taskId=attempt_202105142049353522428041914970582_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22a6cb67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:32,371] {docker.py:276} INFO - 21/05/14 20:51:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:32,372] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Starting: Task committer attempt_202105142049353522428041914970582_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353522428041914970582_0004_m_000161_449
[2021-05-14 17:51:32,375] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Task committer attempt_202105142049353522428041914970582_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353522428041914970582_0004_m_000161_449 : duration 0:00.003s
[2021-05-14 17:51:32,576] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Starting: Task committer attempt_202105142049351685926357599819932_0004_m_000156_444: needsTaskCommit() Task attempt_202105142049351685926357599819932_0004_m_000156_444
21/05/14 20:51:32 INFO StagingCommitter: Task committer attempt_202105142049351685926357599819932_0004_m_000156_444: needsTaskCommit() Task attempt_202105142049351685926357599819932_0004_m_000156_444: duration 0:00.001s
21/05/14 20:51:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351685926357599819932_0004_m_000156_444
[2021-05-14 17:51:32,577] {docker.py:276} INFO - 21/05/14 20:51:32 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4544 bytes result sent to driver
[2021-05-14 17:51:32,579] {docker.py:276} INFO - 21/05/14 20:51:32 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (7cd3ddbc35d2, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:32,580] {docker.py:276} INFO - 21/05/14 20:51:32 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 3542 ms on 7cd3ddbc35d2 (executor driver) (159/200)
[2021-05-14 17:51:32,580] {docker.py:276} INFO - 21/05/14 20:51:32 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-14 17:51:32,589] {docker.py:276} INFO - 21/05/14 20:51:32 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:32,603] {docker.py:276} INFO - 21/05/14 20:51:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:32,603] {docker.py:276} INFO - 21/05/14 20:51:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935588498804873024337_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935588498804873024337_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935588498804873024337_0004}; taskId=attempt_20210514204935588498804873024337_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e1b4387}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:32 INFO StagingCommitter: Starting: Task committer attempt_20210514204935588498804873024337_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935588498804873024337_0004_m_000162_450
[2021-05-14 17:51:32,604] {docker.py:276} INFO - 21/05/14 20:51:32 INFO StagingCommitter: Task committer attempt_20210514204935588498804873024337_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935588498804873024337_0004_m_000162_450 : duration 0:00.004s
[2021-05-14 17:51:34,076] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049355199302857953404830_0004_m_000159_447: needsTaskCommit() Task attempt_202105142049355199302857953404830_0004_m_000159_447
[2021-05-14 17:51:34,077] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049357858753100805351997_0004_m_000160_448: needsTaskCommit() Task attempt_202105142049357858753100805351997_0004_m_000160_448
[2021-05-14 17:51:34,077] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049357858753100805351997_0004_m_000160_448: needsTaskCommit() Task attempt_202105142049357858753100805351997_0004_m_000160_448: duration 0:00.001s
[2021-05-14 17:51:34,078] {docker.py:276} INFO - 21/05/14 20:51:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357858753100805351997_0004_m_000160_448
[2021-05-14 17:51:34,079] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049355199302857953404830_0004_m_000159_447: needsTaskCommit() Task attempt_202105142049355199302857953404830_0004_m_000159_447: duration 0:00.002s
21/05/14 20:51:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355199302857953404830_0004_m_000159_447
[2021-05-14 17:51:34,080] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4544 bytes result sent to driver
[2021-05-14 17:51:34,080] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4544 bytes result sent to driver
[2021-05-14 17:51:34,081] {docker.py:276} INFO - 21/05/14 20:51:34 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (7cd3ddbc35d2, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:34,082] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-14 17:51:34,082] {docker.py:276} INFO - 21/05/14 20:51:34 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (7cd3ddbc35d2, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:34,083] {docker.py:276} INFO - 21/05/14 20:51:34 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2765 ms on 7cd3ddbc35d2 (executor driver) (160/200)
21/05/14 20:51:34 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2948 ms on 7cd3ddbc35d2 (executor driver) (161/200)
[2021-05-14 17:51:34,083] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-14 17:51:34,092] {docker.py:276} INFO - 21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:34,093] {docker.py:276} INFO - 21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:51:34,094] {docker.py:276} INFO - 21/05/14 20:51:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:34,095] {docker.py:276} INFO - 21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357335605689007071397_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357335605689007071397_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357335605689007071397_0004}; taskId=attempt_202105142049357335605689007071397_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e8f5a4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:34,095] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049357335605689007071397_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357335605689007071397_0004_m_000163_451
[2021-05-14 17:51:34,096] {docker.py:276} INFO - 21/05/14 20:51:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:34,096] {docker.py:276} INFO - 21/05/14 20:51:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:34,096] {docker.py:276} INFO - 21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355688906584982138250_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355688906584982138250_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355688906584982138250_0004}; taskId=attempt_202105142049355688906584982138250_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20b15f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:34,097] {docker.py:276} INFO - 21/05/14 20:51:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:34,097] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049355688906584982138250_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355688906584982138250_0004_m_000164_452
[2021-05-14 17:51:34,100] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049357335605689007071397_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357335605689007071397_0004_m_000163_451 : duration 0:00.004s
[2021-05-14 17:51:34,103] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049355688906584982138250_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355688906584982138250_0004_m_000164_452 : duration 0:00.006s
[2021-05-14 17:51:34,826] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049353522428041914970582_0004_m_000161_449: needsTaskCommit() Task attempt_202105142049353522428041914970582_0004_m_000161_449
[2021-05-14 17:51:34,827] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049353522428041914970582_0004_m_000161_449: needsTaskCommit() Task attempt_202105142049353522428041914970582_0004_m_000161_449: duration 0:00.001s
[2021-05-14 17:51:34,828] {docker.py:276} INFO - 21/05/14 20:51:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353522428041914970582_0004_m_000161_449
[2021-05-14 17:51:34,828] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4587 bytes result sent to driver
[2021-05-14 17:51:34,830] {docker.py:276} INFO - 21/05/14 20:51:34 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (7cd3ddbc35d2, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:34,831] {docker.py:276} INFO - 21/05/14 20:51:34 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 2477 ms on 7cd3ddbc35d2 (executor driver) (162/200)
[2021-05-14 17:51:34,831] {docker.py:276} INFO - 21/05/14 20:51:34 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
[2021-05-14 17:51:34,838] {docker.py:276} INFO - 21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:34,839] {docker.py:276} INFO - 21/05/14 20:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:34,840] {docker.py:276} INFO - 21/05/14 20:51:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:34,841] {docker.py:276} INFO - 21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:34,841] {docker.py:276} INFO - 21/05/14 20:51:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352406997634411821991_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352406997634411821991_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352406997634411821991_0004}; taskId=attempt_202105142049352406997634411821991_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60d7fd60}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:34,841] {docker.py:276} INFO - 21/05/14 20:51:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:34,842] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Starting: Task committer attempt_202105142049352406997634411821991_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352406997634411821991_0004_m_000165_453
[2021-05-14 17:51:34,848] {docker.py:276} INFO - 21/05/14 20:51:34 INFO StagingCommitter: Task committer attempt_202105142049352406997634411821991_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352406997634411821991_0004_m_000165_453 : duration 0:00.006s
[2021-05-14 17:51:35,461] {docker.py:276} INFO - 21/05/14 20:51:35 INFO StagingCommitter: Starting: Task committer attempt_20210514204935588498804873024337_0004_m_000162_450: needsTaskCommit() Task attempt_20210514204935588498804873024337_0004_m_000162_450
[2021-05-14 17:51:35,462] {docker.py:276} INFO - 21/05/14 20:51:35 INFO StagingCommitter: Task committer attempt_20210514204935588498804873024337_0004_m_000162_450: needsTaskCommit() Task attempt_20210514204935588498804873024337_0004_m_000162_450: duration 0:00.001s
21/05/14 20:51:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935588498804873024337_0004_m_000162_450
[2021-05-14 17:51:35,463] {docker.py:276} INFO - 21/05/14 20:51:35 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4587 bytes result sent to driver
[2021-05-14 17:51:35,464] {docker.py:276} INFO - 21/05/14 20:51:35 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (7cd3ddbc35d2, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:35,465] {docker.py:276} INFO - 21/05/14 20:51:35 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 2890 ms on 7cd3ddbc35d2 (executor driver) (163/200)
[2021-05-14 17:51:35,465] {docker.py:276} INFO - 21/05/14 20:51:35 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-14 17:51:35,472] {docker.py:276} INFO - 21/05/14 20:51:35 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:35,475] {docker.py:276} INFO - 21/05/14 20:51:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358557943145666084052_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358557943145666084052_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358557943145666084052_0004}; taskId=attempt_202105142049358557943145666084052_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45741dd1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:35,476] {docker.py:276} INFO - 21/05/14 20:51:35 INFO StagingCommitter: Starting: Task committer attempt_202105142049358557943145666084052_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358557943145666084052_0004_m_000166_454
[2021-05-14 17:51:35,478] {docker.py:276} INFO - 21/05/14 20:51:35 INFO StagingCommitter: Task committer attempt_202105142049358557943145666084052_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358557943145666084052_0004_m_000166_454 : duration 0:00.004s
[2021-05-14 17:51:36,335] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Starting: Task committer attempt_202105142049357335605689007071397_0004_m_000163_451: needsTaskCommit() Task attempt_202105142049357335605689007071397_0004_m_000163_451
[2021-05-14 17:51:36,337] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Task committer attempt_202105142049357335605689007071397_0004_m_000163_451: needsTaskCommit() Task attempt_202105142049357335605689007071397_0004_m_000163_451: duration 0:00.001s
[2021-05-14 17:51:36,337] {docker.py:276} INFO - 21/05/14 20:51:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357335605689007071397_0004_m_000163_451
[2021-05-14 17:51:36,340] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4587 bytes result sent to driver
[2021-05-14 17:51:36,341] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (7cd3ddbc35d2, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:36,342] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-14 17:51:36,342] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 2266 ms on 7cd3ddbc35d2 (executor driver) (164/200)
[2021-05-14 17:51:36,351] {docker.py:276} INFO - 21/05/14 20:51:36 INFO ShuffleBlockFetcherIterator: Getting 5 (44.1 KiB) non-empty blocks including 5 (44.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:36,353] {docker.py:276} INFO - 21/05/14 20:51:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354270600187881363953_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354270600187881363953_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354270600187881363953_0004}; taskId=attempt_202105142049354270600187881363953_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50560037}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:36 INFO StagingCommitter: Starting: Task committer attempt_202105142049354270600187881363953_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354270600187881363953_0004_m_000167_455
[2021-05-14 17:51:36,357] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Task committer attempt_202105142049354270600187881363953_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354270600187881363953_0004_m_000167_455 : duration 0:00.004s
[2021-05-14 17:51:36,390] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Starting: Task committer attempt_202105142049355688906584982138250_0004_m_000164_452: needsTaskCommit() Task attempt_202105142049355688906584982138250_0004_m_000164_452
[2021-05-14 17:51:36,390] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Task committer attempt_202105142049355688906584982138250_0004_m_000164_452: needsTaskCommit() Task attempt_202105142049355688906584982138250_0004_m_000164_452: duration 0:00.001s
[2021-05-14 17:51:36,391] {docker.py:276} INFO - 21/05/14 20:51:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355688906584982138250_0004_m_000164_452
[2021-05-14 17:51:36,393] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4587 bytes result sent to driver
[2021-05-14 17:51:36,394] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (7cd3ddbc35d2, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:36,395] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-14 17:51:36,395] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2317 ms on 7cd3ddbc35d2 (executor driver) (165/200)
[2021-05-14 17:51:36,405] {docker.py:276} INFO - 21/05/14 20:51:36 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:36,406] {docker.py:276} INFO - 21/05/14 20:51:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:36,407] {docker.py:276} INFO - 21/05/14 20:51:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352314467771985956569_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352314467771985956569_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352314467771985956569_0004}; taskId=attempt_202105142049352314467771985956569_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@618d7374}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:36 INFO StagingCommitter: Starting: Task committer attempt_202105142049352314467771985956569_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352314467771985956569_0004_m_000168_456
[2021-05-14 17:51:36,410] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Task committer attempt_202105142049352314467771985956569_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352314467771985956569_0004_m_000168_456 : duration 0:00.003s
[2021-05-14 17:51:36,961] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Starting: Task committer attempt_202105142049352406997634411821991_0004_m_000165_453: needsTaskCommit() Task attempt_202105142049352406997634411821991_0004_m_000165_453
[2021-05-14 17:51:36,965] {docker.py:276} INFO - 21/05/14 20:51:36 INFO StagingCommitter: Task committer attempt_202105142049352406997634411821991_0004_m_000165_453: needsTaskCommit() Task attempt_202105142049352406997634411821991_0004_m_000165_453: duration 0:00.001s
21/05/14 20:51:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352406997634411821991_0004_m_000165_453
[2021-05-14 17:51:36,965] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4544 bytes result sent to driver
[2021-05-14 17:51:36,966] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (7cd3ddbc35d2, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:36,966] {docker.py:276} INFO - 21/05/14 20:51:36 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2139 ms on 7cd3ddbc35d2 (executor driver) (166/200)
[2021-05-14 17:51:36,967] {docker.py:276} INFO - 21/05/14 20:51:36 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-14 17:51:36,976] {docker.py:276} INFO - 21/05/14 20:51:37 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:36,978] {docker.py:276} INFO - 21/05/14 20:51:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358158140898331821578_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358158140898331821578_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358158140898331821578_0004}; taskId=attempt_202105142049358158140898331821578_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@146aabf9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:36,978] {docker.py:276} INFO - 21/05/14 20:51:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:37 INFO StagingCommitter: Starting: Task committer attempt_202105142049358158140898331821578_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358158140898331821578_0004_m_000169_457
[2021-05-14 17:51:36,981] {docker.py:276} INFO - 21/05/14 20:51:37 INFO StagingCommitter: Task committer attempt_202105142049358158140898331821578_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358158140898331821578_0004_m_000169_457 : duration 0:00.003s
[2021-05-14 17:51:38,026] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Starting: Task committer attempt_202105142049358557943145666084052_0004_m_000166_454: needsTaskCommit() Task attempt_202105142049358557943145666084052_0004_m_000166_454
21/05/14 20:51:38 INFO StagingCommitter: Task committer attempt_202105142049358557943145666084052_0004_m_000166_454: needsTaskCommit() Task attempt_202105142049358557943145666084052_0004_m_000166_454: duration 0:00.000s
21/05/14 20:51:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358557943145666084052_0004_m_000166_454
[2021-05-14 17:51:38,028] {docker.py:276} INFO - 21/05/14 20:51:38 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4544 bytes result sent to driver
21/05/14 20:51:38 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (7cd3ddbc35d2, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:51:38 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2567 ms on 7cd3ddbc35d2 (executor driver) (167/200)
21/05/14 20:51:38 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-14 17:51:38,037] {docker.py:276} INFO - 21/05/14 20:51:38 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:38,040] {docker.py:276} INFO - 21/05/14 20:51:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:38,040] {docker.py:276} INFO - 21/05/14 20:51:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353666327351036158990_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353666327351036158990_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353666327351036158990_0004}; taskId=attempt_202105142049353666327351036158990_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@336a4d68}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:38,041] {docker.py:276} INFO - 21/05/14 20:51:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:38,041] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Starting: Task committer attempt_202105142049353666327351036158990_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353666327351036158990_0004_m_000170_458
[2021-05-14 17:51:38,044] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Task committer attempt_202105142049353666327351036158990_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353666327351036158990_0004_m_000170_458 : duration 0:00.003s
[2021-05-14 17:51:38,914] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Starting: Task committer attempt_202105142049352314467771985956569_0004_m_000168_456: needsTaskCommit() Task attempt_202105142049352314467771985956569_0004_m_000168_456
[2021-05-14 17:51:38,915] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Task committer attempt_202105142049352314467771985956569_0004_m_000168_456: needsTaskCommit() Task attempt_202105142049352314467771985956569_0004_m_000168_456: duration 0:00.002s
[2021-05-14 17:51:38,915] {docker.py:276} INFO - 21/05/14 20:51:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352314467771985956569_0004_m_000168_456
[2021-05-14 17:51:38,917] {docker.py:276} INFO - 21/05/14 20:51:38 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4544 bytes result sent to driver
[2021-05-14 17:51:38,918] {docker.py:276} INFO - 21/05/14 20:51:38 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (7cd3ddbc35d2, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:38,919] {docker.py:276} INFO - 21/05/14 20:51:38 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
21/05/14 20:51:38 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2529 ms on 7cd3ddbc35d2 (executor driver) (168/200)
[2021-05-14 17:51:38,928] {docker.py:276} INFO - 21/05/14 20:51:38 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:38,930] {docker.py:276} INFO - 21/05/14 20:51:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049351223293112777764899_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351223293112777764899_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049351223293112777764899_0004}; taskId=attempt_202105142049351223293112777764899_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bf7dad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:38,930] {docker.py:276} INFO - 21/05/14 20:51:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:38 INFO StagingCommitter: Starting: Task committer attempt_202105142049351223293112777764899_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351223293112777764899_0004_m_000171_459
[2021-05-14 17:51:38,933] {docker.py:276} INFO - 21/05/14 20:51:38 INFO StagingCommitter: Task committer attempt_202105142049351223293112777764899_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049351223293112777764899_0004_m_000171_459 : duration 0:00.002s
[2021-05-14 17:51:39,092] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049354270600187881363953_0004_m_000167_455: needsTaskCommit() Task attempt_202105142049354270600187881363953_0004_m_000167_455
[2021-05-14 17:51:39,093] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049354270600187881363953_0004_m_000167_455: needsTaskCommit() Task attempt_202105142049354270600187881363953_0004_m_000167_455: duration 0:00.001s
21/05/14 20:51:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354270600187881363953_0004_m_000167_455
[2021-05-14 17:51:39,094] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4544 bytes result sent to driver
[2021-05-14 17:51:39,096] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (7cd3ddbc35d2, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:39,097] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-14 17:51:39,098] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2760 ms on 7cd3ddbc35d2 (executor driver) (169/200)
[2021-05-14 17:51:39,105] {docker.py:276} INFO - 21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:39,106] {docker.py:276} INFO - 21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:39,107] {docker.py:276} INFO - 21/05/14 20:51:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:39,108] {docker.py:276} INFO - 21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357905532301674925701_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357905532301674925701_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357905532301674925701_0004}; taskId=attempt_202105142049357905532301674925701_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13f18918}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:39,108] {docker.py:276} INFO - 21/05/14 20:51:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049357905532301674925701_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357905532301674925701_0004_m_000172_460
[2021-05-14 17:51:39,110] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049357905532301674925701_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357905532301674925701_0004_m_000172_460 : duration 0:00.003s
[2021-05-14 17:51:39,274] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049358158140898331821578_0004_m_000169_457: needsTaskCommit() Task attempt_202105142049358158140898331821578_0004_m_000169_457
[2021-05-14 17:51:39,275] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049358158140898331821578_0004_m_000169_457: needsTaskCommit() Task attempt_202105142049358158140898331821578_0004_m_000169_457: duration 0:00.001s
21/05/14 20:51:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358158140898331821578_0004_m_000169_457
[2021-05-14 17:51:39,277] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4544 bytes result sent to driver
[2021-05-14 17:51:39,279] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (7cd3ddbc35d2, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:39,280] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-14 17:51:39,281] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2318 ms on 7cd3ddbc35d2 (executor driver) (170/200)
[2021-05-14 17:51:39,291] {docker.py:276} INFO - 21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:39,291] {docker.py:276} INFO - 21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:39,293] {docker.py:276} INFO - 21/05/14 20:51:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:39,294] {docker.py:276} INFO - 21/05/14 20:51:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:39,294] {docker.py:276} INFO - 21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:39,295] {docker.py:276} INFO - 21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049352174816386163833428_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352174816386163833428_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049352174816386163833428_0004}; taskId=attempt_202105142049352174816386163833428_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fb5a1bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049352174816386163833428_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352174816386163833428_0004_m_000173_461
[2021-05-14 17:51:39,297] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049352174816386163833428_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049352174816386163833428_0004_m_000173_461 : duration 0:00.003s
[2021-05-14 17:51:39,790] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049353666327351036158990_0004_m_000170_458: needsTaskCommit() Task attempt_202105142049353666327351036158990_0004_m_000170_458
[2021-05-14 17:51:39,791] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049353666327351036158990_0004_m_000170_458: needsTaskCommit() Task attempt_202105142049353666327351036158990_0004_m_000170_458: duration 0:00.001s
[2021-05-14 17:51:39,792] {docker.py:276} INFO - 21/05/14 20:51:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353666327351036158990_0004_m_000170_458
[2021-05-14 17:51:39,793] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4544 bytes result sent to driver
[2021-05-14 17:51:39,795] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (7cd3ddbc35d2, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:39,796] {docker.py:276} INFO - 21/05/14 20:51:39 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 1770 ms on 7cd3ddbc35d2 (executor driver) (171/200)
[2021-05-14 17:51:39,797] {docker.py:276} INFO - 21/05/14 20:51:39 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-14 17:51:39,807] {docker.py:276} INFO - 21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:39,809] {docker.py:276} INFO - 21/05/14 20:51:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356580755628604383000_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356580755628604383000_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356580755628604383000_0004}; taskId=attempt_202105142049356580755628604383000_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7faef2a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:39 INFO StagingCommitter: Starting: Task committer attempt_202105142049356580755628604383000_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356580755628604383000_0004_m_000174_462
[2021-05-14 17:51:39,812] {docker.py:276} INFO - 21/05/14 20:51:39 INFO StagingCommitter: Task committer attempt_202105142049356580755628604383000_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356580755628604383000_0004_m_000174_462 : duration 0:00.003s
[2021-05-14 17:51:41,752] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049351223293112777764899_0004_m_000171_459: needsTaskCommit() Task attempt_202105142049351223293112777764899_0004_m_000171_459
[2021-05-14 17:51:41,756] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049351223293112777764899_0004_m_000171_459: needsTaskCommit() Task attempt_202105142049351223293112777764899_0004_m_000171_459: duration 0:00.001s
21/05/14 20:51:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049351223293112777764899_0004_m_000171_459
[2021-05-14 17:51:41,756] {docker.py:276} INFO - 21/05/14 20:51:41 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4587 bytes result sent to driver
[2021-05-14 17:51:41,757] {docker.py:276} INFO - 21/05/14 20:51:41 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (7cd3ddbc35d2, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:41,757] {docker.py:276} INFO - 21/05/14 20:51:41 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2805 ms on 7cd3ddbc35d2 (executor driver) (172/200)
[2021-05-14 17:51:41,757] {docker.py:276} INFO - 21/05/14 20:51:41 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049352174816386163833428_0004_m_000173_461: needsTaskCommit() Task attempt_202105142049352174816386163833428_0004_m_000173_461
[2021-05-14 17:51:41,758] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049352174816386163833428_0004_m_000173_461: needsTaskCommit() Task attempt_202105142049352174816386163833428_0004_m_000173_461: duration 0:00.000s
21/05/14 20:51:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049352174816386163833428_0004_m_000173_461
[2021-05-14 17:51:41,759] {docker.py:276} INFO - 21/05/14 20:51:41 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4587 bytes result sent to driver
21/05/14 20:51:41 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (7cd3ddbc35d2, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:51:41 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-14 17:51:41,760] {docker.py:276} INFO - 21/05/14 20:51:41 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2449 ms on 7cd3ddbc35d2 (executor driver) (173/200)
[2021-05-14 17:51:41,769] {docker.py:276} INFO - 21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:41,769] {docker.py:276} INFO - 21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:41,770] {docker.py:276} INFO - 21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:41,771] {docker.py:276} INFO - 21/05/14 20:51:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935673031579759067040_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935673031579759067040_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935673031579759067040_0004}; taskId=attempt_20210514204935673031579759067040_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4342f212}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:41,772] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_20210514204935673031579759067040_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935673031579759067040_0004_m_000175_463
[2021-05-14 17:51:41,772] {docker.py:276} INFO - 21/05/14 20:51:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:41,772] {docker.py:276} INFO - 21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353114076716908186672_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353114076716908186672_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353114076716908186672_0004}; taskId=attempt_202105142049353114076716908186672_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e49a527}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049353114076716908186672_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353114076716908186672_0004_m_000176_464
[2021-05-14 17:51:41,775] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_20210514204935673031579759067040_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935673031579759067040_0004_m_000175_463 : duration 0:00.004s
[2021-05-14 17:51:41,775] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049353114076716908186672_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353114076716908186672_0004_m_000176_464 : duration 0:00.004s
[2021-05-14 17:51:41,960] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049356580755628604383000_0004_m_000174_462: needsTaskCommit() Task attempt_202105142049356580755628604383000_0004_m_000174_462
[2021-05-14 17:51:41,961] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049356580755628604383000_0004_m_000174_462: needsTaskCommit() Task attempt_202105142049356580755628604383000_0004_m_000174_462: duration 0:00.000s
21/05/14 20:51:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356580755628604383000_0004_m_000174_462
[2021-05-14 17:51:41,962] {docker.py:276} INFO - 21/05/14 20:51:41 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4587 bytes result sent to driver
[2021-05-14 17:51:41,963] {docker.py:276} INFO - 21/05/14 20:51:41 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (7cd3ddbc35d2, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:41,964] {docker.py:276} INFO - 21/05/14 20:51:41 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2137 ms on 7cd3ddbc35d2 (executor driver) (174/200)
21/05/14 20:51:41 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
[2021-05-14 17:51:41,969] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049357905532301674925701_0004_m_000172_460: needsTaskCommit() Task attempt_202105142049357905532301674925701_0004_m_000172_460
21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049357905532301674925701_0004_m_000172_460: needsTaskCommit() Task attempt_202105142049357905532301674925701_0004_m_000172_460: duration 0:00.001s
21/05/14 20:51:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357905532301674925701_0004_m_000172_460
21/05/14 20:51:41 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4587 bytes result sent to driver
21/05/14 20:51:41 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (7cd3ddbc35d2, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 20:51:41 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 2841 ms on 7cd3ddbc35d2 (executor driver) (175/200)
21/05/14 20:51:41 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-14 17:51:41,973] {docker.py:276} INFO - 21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:41,975] {docker.py:276} INFO - 21/05/14 20:51:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049354566907108332374656_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354566907108332374656_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049354566907108332374656_0004}; taskId=attempt_202105142049354566907108332374656_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79dce765}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:41,975] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049354566907108332374656_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354566907108332374656_0004_m_000177_465
[2021-05-14 17:51:41,976] {docker.py:276} INFO - 21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:41,978] {docker.py:276} INFO - 21/05/14 20:51:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358260368361234808464_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358260368361234808464_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358260368361234808464_0004}; taskId=attempt_202105142049358260368361234808464_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@688d2a0a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:41 INFO StagingCommitter: Starting: Task committer attempt_202105142049358260368361234808464_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358260368361234808464_0004_m_000178_466
[2021-05-14 17:51:41,979] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049354566907108332374656_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049354566907108332374656_0004_m_000177_465 : duration 0:00.004s
[2021-05-14 17:51:41,981] {docker.py:276} INFO - 21/05/14 20:51:41 INFO StagingCommitter: Task committer attempt_202105142049358260368361234808464_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358260368361234808464_0004_m_000178_466 : duration 0:00.003s
[2021-05-14 17:51:43,917] {docker.py:276} INFO - 21/05/14 20:51:43 INFO StagingCommitter: Starting: Task committer attempt_20210514204935673031579759067040_0004_m_000175_463: needsTaskCommit() Task attempt_20210514204935673031579759067040_0004_m_000175_463
[2021-05-14 17:51:43,918] {docker.py:276} INFO - 21/05/14 20:51:43 INFO StagingCommitter: Task committer attempt_20210514204935673031579759067040_0004_m_000175_463: needsTaskCommit() Task attempt_20210514204935673031579759067040_0004_m_000175_463: duration 0:00.001s
21/05/14 20:51:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935673031579759067040_0004_m_000175_463
[2021-05-14 17:51:43,919] {docker.py:276} INFO - 21/05/14 20:51:43 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4544 bytes result sent to driver
[2021-05-14 17:51:43,920] {docker.py:276} INFO - 21/05/14 20:51:43 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (7cd3ddbc35d2, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:43,922] {docker.py:276} INFO - 21/05/14 20:51:43 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
[2021-05-14 17:51:43,923] {docker.py:276} INFO - 21/05/14 20:51:43 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2169 ms on 7cd3ddbc35d2 (executor driver) (176/200)
[2021-05-14 17:51:43,931] {docker.py:276} INFO - 21/05/14 20:51:43 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:43,933] {docker.py:276} INFO - 21/05/14 20:51:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353992601946672134721_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353992601946672134721_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353992601946672134721_0004}; taskId=attempt_202105142049353992601946672134721_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7aed8e02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:43 INFO StagingCommitter: Starting: Task committer attempt_202105142049353992601946672134721_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353992601946672134721_0004_m_000179_467
[2021-05-14 17:51:43,936] {docker.py:276} INFO - 21/05/14 20:51:43 INFO StagingCommitter: Task committer attempt_202105142049353992601946672134721_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353992601946672134721_0004_m_000179_467 : duration 0:00.003s
[2021-05-14 17:51:44,159] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049353114076716908186672_0004_m_000176_464: needsTaskCommit() Task attempt_202105142049353114076716908186672_0004_m_000176_464
[2021-05-14 17:51:44,160] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_202105142049353114076716908186672_0004_m_000176_464: needsTaskCommit() Task attempt_202105142049353114076716908186672_0004_m_000176_464: duration 0:00.001s
21/05/14 20:51:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353114076716908186672_0004_m_000176_464
[2021-05-14 17:51:44,161] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4544 bytes result sent to driver
[2021-05-14 17:51:44,162] {docker.py:276} INFO - 21/05/14 20:51:44 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (7cd3ddbc35d2, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:44,164] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
21/05/14 20:51:44 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2408 ms on 7cd3ddbc35d2 (executor driver) (177/200)
[2021-05-14 17:51:44,172] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049358260368361234808464_0004_m_000178_466: needsTaskCommit() Task attempt_202105142049358260368361234808464_0004_m_000178_466
[2021-05-14 17:51:44,172] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_202105142049358260368361234808464_0004_m_000178_466: needsTaskCommit() Task attempt_202105142049358260368361234808464_0004_m_000178_466: duration 0:00.000s
[2021-05-14 17:51:44,173] {docker.py:276} INFO - 21/05/14 20:51:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358260368361234808464_0004_m_000178_466
[2021-05-14 17:51:44,173] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4544 bytes result sent to driver
[2021-05-14 17:51:44,174] {docker.py:276} INFO - 21/05/14 20:51:44 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (7cd3ddbc35d2, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:44,175] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
21/05/14 20:51:44 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 2211 ms on 7cd3ddbc35d2 (executor driver) (178/200)
[2021-05-14 17:51:44,176] {docker.py:276} INFO - 21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:44,177] {docker.py:276} INFO - 21/05/14 20:51:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:44,178] {docker.py:276} INFO - 21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355200479802258425377_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355200479802258425377_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355200479802258425377_0004}; taskId=attempt_202105142049355200479802258425377_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46617fbe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:44,179] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049355200479802258425377_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355200479802258425377_0004_m_000180_468
[2021-05-14 17:51:44,182] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_202105142049355200479802258425377_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355200479802258425377_0004_m_000180_468 : duration 0:00.004s
[2021-05-14 17:51:44,184] {docker.py:276} INFO - 21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:44,186] {docker.py:276} INFO - 21/05/14 20:51:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935168505337196454316_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935168505337196454316_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935168505337196454316_0004}; taskId=attempt_20210514204935168505337196454316_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@439224ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:44,187] {docker.py:276} INFO - 21/05/14 20:51:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_20210514204935168505337196454316_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935168505337196454316_0004_m_000181_469
[2021-05-14 17:51:44,191] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_20210514204935168505337196454316_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935168505337196454316_0004_m_000181_469 : duration 0:00.005s
[2021-05-14 17:51:44,341] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049354566907108332374656_0004_m_000177_465: needsTaskCommit() Task attempt_202105142049354566907108332374656_0004_m_000177_465
[2021-05-14 17:51:44,342] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_202105142049354566907108332374656_0004_m_000177_465: needsTaskCommit() Task attempt_202105142049354566907108332374656_0004_m_000177_465: duration 0:00.001s
[2021-05-14 17:51:44,342] {docker.py:276} INFO - 21/05/14 20:51:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049354566907108332374656_0004_m_000177_465
[2021-05-14 17:51:44,344] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4544 bytes result sent to driver
[2021-05-14 17:51:44,345] {docker.py:276} INFO - 21/05/14 20:51:44 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (7cd3ddbc35d2, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:44,346] {docker.py:276} INFO - 21/05/14 20:51:44 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2387 ms on 7cd3ddbc35d2 (executor driver) (179/200)
[2021-05-14 17:51:44,347] {docker.py:276} INFO - 21/05/14 20:51:44 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-14 17:51:44,356] {docker.py:276} INFO - 21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:44,358] {docker.py:276} INFO - 21/05/14 20:51:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353004851803179653539_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353004851803179653539_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353004851803179653539_0004}; taskId=attempt_202105142049353004851803179653539_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7332e6d2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:44,359] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Starting: Task committer attempt_202105142049353004851803179653539_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353004851803179653539_0004_m_000182_470
[2021-05-14 17:51:44,363] {docker.py:276} INFO - 21/05/14 20:51:44 INFO StagingCommitter: Task committer attempt_202105142049353004851803179653539_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353004851803179653539_0004_m_000182_470 : duration 0:00.005s
[2021-05-14 17:51:46,043] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049353992601946672134721_0004_m_000179_467: needsTaskCommit() Task attempt_202105142049353992601946672134721_0004_m_000179_467
21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049353992601946672134721_0004_m_000179_467: needsTaskCommit() Task attempt_202105142049353992601946672134721_0004_m_000179_467: duration 0:00.000s
21/05/14 20:51:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353992601946672134721_0004_m_000179_467
[2021-05-14 17:51:46,044] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4544 bytes result sent to driver
[2021-05-14 17:51:46,045] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (7cd3ddbc35d2, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:46,046] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-14 17:51:46,047] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2130 ms on 7cd3ddbc35d2 (executor driver) (180/200)
[2021-05-14 17:51:46,054] {docker.py:276} INFO - 21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Getting 5 (45.1 KiB) non-empty blocks including 5 (45.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:46,055] {docker.py:276} INFO - 21/05/14 20:51:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359001581104031721639_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359001581104031721639_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359001581104031721639_0004}; taskId=attempt_202105142049359001581104031721639_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3439526d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049359001581104031721639_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359001581104031721639_0004_m_000183_471
[2021-05-14 17:51:46,057] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049359001581104031721639_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359001581104031721639_0004_m_000183_471 : duration 0:00.002s
[2021-05-14 17:51:46,327] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_20210514204935168505337196454316_0004_m_000181_469: needsTaskCommit() Task attempt_20210514204935168505337196454316_0004_m_000181_469
[2021-05-14 17:51:46,327] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_20210514204935168505337196454316_0004_m_000181_469: needsTaskCommit() Task attempt_20210514204935168505337196454316_0004_m_000181_469: duration 0:00.000s
21/05/14 20:51:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935168505337196454316_0004_m_000181_469
[2021-05-14 17:51:46,329] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-14 17:51:46,330] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (7cd3ddbc35d2, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:46,331] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2160 ms on 7cd3ddbc35d2 (executor driver) (181/200)
[2021-05-14 17:51:46,331] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-14 17:51:46,340] {docker.py:276} INFO - 21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 17:51:46,340] {docker.py:276} INFO - 21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:46,341] {docker.py:276} INFO - 21/05/14 20:51:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:46,342] {docker.py:276} INFO - 21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:46,342] {docker.py:276} INFO - 21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353410161806038701691_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353410161806038701691_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353410161806038701691_0004}; taskId=attempt_202105142049353410161806038701691_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bbb47a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:46,342] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049353410161806038701691_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353410161806038701691_0004_m_000184_472
[2021-05-14 17:51:46,345] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049353410161806038701691_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353410161806038701691_0004_m_000184_472 : duration 0:00.003s
[2021-05-14 17:51:46,489] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049353004851803179653539_0004_m_000182_470: needsTaskCommit() Task attempt_202105142049353004851803179653539_0004_m_000182_470
[2021-05-14 17:51:46,489] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049353004851803179653539_0004_m_000182_470: needsTaskCommit() Task attempt_202105142049353004851803179653539_0004_m_000182_470: duration 0:00.000s
21/05/14 20:51:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353004851803179653539_0004_m_000182_470
[2021-05-14 17:51:46,491] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4544 bytes result sent to driver
[2021-05-14 17:51:46,491] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (7cd3ddbc35d2, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:46,492] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2150 ms on 7cd3ddbc35d2 (executor driver) (182/200)
[2021-05-14 17:51:46,492] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-14 17:51:46,500] {docker.py:276} INFO - 21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:46,502] {docker.py:276} INFO - 21/05/14 20:51:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:46,503] {docker.py:276} INFO - 21/05/14 20:51:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:46,515] {docker.py:276} INFO - 21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355710282420260267162_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355710282420260267162_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355710282420260267162_0004}; taskId=attempt_202105142049355710282420260267162_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79e6908a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:46,515] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049355710282420260267162_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355710282420260267162_0004_m_000185_473
[2021-05-14 17:51:46,519] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049355710282420260267162_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355710282420260267162_0004_m_000185_473 : duration 0:00.004s
[2021-05-14 17:51:46,623] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049355200479802258425377_0004_m_000180_468: needsTaskCommit() Task attempt_202105142049355200479802258425377_0004_m_000180_468
[2021-05-14 17:51:46,624] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049355200479802258425377_0004_m_000180_468: needsTaskCommit() Task attempt_202105142049355200479802258425377_0004_m_000180_468: duration 0:00.000s
21/05/14 20:51:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355200479802258425377_0004_m_000180_468
[2021-05-14 17:51:46,626] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4587 bytes result sent to driver
[2021-05-14 17:51:46,627] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (7cd3ddbc35d2, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:46,628] {docker.py:276} INFO - 21/05/14 20:51:46 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2469 ms on 7cd3ddbc35d2 (executor driver) (183/200)
[2021-05-14 17:51:46,629] {docker.py:276} INFO - 21/05/14 20:51:46 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
[2021-05-14 17:51:46,638] {docker.py:276} INFO - 21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:46,640] {docker.py:276} INFO - 21/05/14 20:51:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:46,641] {docker.py:276} INFO - 21/05/14 20:51:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356694324178381645058_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356694324178381645058_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356694324178381645058_0004}; taskId=attempt_202105142049356694324178381645058_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ae42e0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:46,642] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Starting: Task committer attempt_202105142049356694324178381645058_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356694324178381645058_0004_m_000186_474
[2021-05-14 17:51:46,644] {docker.py:276} INFO - 21/05/14 20:51:46 INFO StagingCommitter: Task committer attempt_202105142049356694324178381645058_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356694324178381645058_0004_m_000186_474 : duration 0:00.004s
[2021-05-14 17:51:48,221] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049359001581104031721639_0004_m_000183_471: needsTaskCommit() Task attempt_202105142049359001581104031721639_0004_m_000183_471
[2021-05-14 17:51:48,221] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049359001581104031721639_0004_m_000183_471: needsTaskCommit() Task attempt_202105142049359001581104031721639_0004_m_000183_471: duration 0:00.000s
21/05/14 20:51:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359001581104031721639_0004_m_000183_471
[2021-05-14 17:51:48,222] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4587 bytes result sent to driver
[2021-05-14 17:51:48,223] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (7cd3ddbc35d2, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:48,224] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 2182 ms on 7cd3ddbc35d2 (executor driver) (184/200)
[2021-05-14 17:51:48,224] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
[2021-05-14 17:51:48,231] {docker.py:276} INFO - 21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:48,233] {docker.py:276} INFO - 21/05/14 20:51:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353661937648047946156_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353661937648047946156_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353661937648047946156_0004}; taskId=attempt_202105142049353661937648047946156_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f2c57d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:48,233] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049353661937648047946156_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353661937648047946156_0004_m_000187_475
[2021-05-14 17:51:48,236] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049353661937648047946156_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353661937648047946156_0004_m_000187_475 : duration 0:00.003s
[2021-05-14 17:51:48,498] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049353410161806038701691_0004_m_000184_472: needsTaskCommit() Task attempt_202105142049353410161806038701691_0004_m_000184_472
[2021-05-14 17:51:48,499] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049353410161806038701691_0004_m_000184_472: needsTaskCommit() Task attempt_202105142049353410161806038701691_0004_m_000184_472: duration 0:00.001s
21/05/14 20:51:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353410161806038701691_0004_m_000184_472
[2021-05-14 17:51:48,500] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4587 bytes result sent to driver
[2021-05-14 17:51:48,501] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (7cd3ddbc35d2, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:48,502] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-14 17:51:48,502] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 2174 ms on 7cd3ddbc35d2 (executor driver) (185/200)
[2021-05-14 17:51:48,510] {docker.py:276} INFO - 21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:48,511] {docker.py:276} INFO - 21/05/14 20:51:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:48,512] {docker.py:276} INFO - 21/05/14 20:51:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:48,513] {docker.py:276} INFO - 21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:48,513] {docker.py:276} INFO - 21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355046159870627299089_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355046159870627299089_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355046159870627299089_0004}; taskId=attempt_202105142049355046159870627299089_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c650eac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:48,513] {docker.py:276} INFO - 21/05/14 20:51:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:48,513] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049355046159870627299089_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355046159870627299089_0004_m_000188_476
[2021-05-14 17:51:48,515] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049355046159870627299089_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355046159870627299089_0004_m_000188_476 : duration 0:00.003s
[2021-05-14 17:51:48,704] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049355710282420260267162_0004_m_000185_473: needsTaskCommit() Task attempt_202105142049355710282420260267162_0004_m_000185_473
[2021-05-14 17:51:48,705] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049355710282420260267162_0004_m_000185_473: needsTaskCommit() Task attempt_202105142049355710282420260267162_0004_m_000185_473: duration 0:00.001s
21/05/14 20:51:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355710282420260267162_0004_m_000185_473
[2021-05-14 17:51:48,706] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4587 bytes result sent to driver
[2021-05-14 17:51:48,708] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (7cd3ddbc35d2, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:48,708] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 2218 ms on 7cd3ddbc35d2 (executor driver) (186/200)
[2021-05-14 17:51:48,709] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-14 17:51:48,717] {docker.py:276} INFO - 21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:48,719] {docker.py:276} INFO - 21/05/14 20:51:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353136943701504311856_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353136943701504311856_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353136943701504311856_0004}; taskId=attempt_202105142049353136943701504311856_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d0d1aa5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049353136943701504311856_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353136943701504311856_0004_m_000189_477
[2021-05-14 17:51:48,721] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049353136943701504311856_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353136943701504311856_0004_m_000189_477 : duration 0:00.002s
[2021-05-14 17:51:48,766] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049356694324178381645058_0004_m_000186_474: needsTaskCommit() Task attempt_202105142049356694324178381645058_0004_m_000186_474
21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049356694324178381645058_0004_m_000186_474: needsTaskCommit() Task attempt_202105142049356694324178381645058_0004_m_000186_474: duration 0:00.001s
[2021-05-14 17:51:48,767] {docker.py:276} INFO - 21/05/14 20:51:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356694324178381645058_0004_m_000186_474
[2021-05-14 17:51:48,768] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4544 bytes result sent to driver
[2021-05-14 17:51:48,769] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (7cd3ddbc35d2, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:48,770] {docker.py:276} INFO - 21/05/14 20:51:48 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-14 17:51:48,771] {docker.py:276} INFO - 21/05/14 20:51:48 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 2146 ms on 7cd3ddbc35d2 (executor driver) (187/200)
[2021-05-14 17:51:48,780] {docker.py:276} INFO - 21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:48,782] {docker.py:276} INFO - 21/05/14 20:51:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:48,782] {docker.py:276} INFO - 21/05/14 20:51:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049356110120807506705478_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356110120807506705478_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049356110120807506705478_0004}; taskId=attempt_202105142049356110120807506705478_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@394e2d6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:48 INFO StagingCommitter: Starting: Task committer attempt_202105142049356110120807506705478_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356110120807506705478_0004_m_000190_478
[2021-05-14 17:51:48,785] {docker.py:276} INFO - 21/05/14 20:51:48 INFO StagingCommitter: Task committer attempt_202105142049356110120807506705478_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049356110120807506705478_0004_m_000190_478 : duration 0:00.002s
[2021-05-14 17:51:51,124] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049353661937648047946156_0004_m_000187_475: needsTaskCommit() Task attempt_202105142049353661937648047946156_0004_m_000187_475
21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049353661937648047946156_0004_m_000187_475: needsTaskCommit() Task attempt_202105142049353661937648047946156_0004_m_000187_475: duration 0:00.000s
21/05/14 20:51:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353661937648047946156_0004_m_000187_475
[2021-05-14 17:51:51,126] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4544 bytes result sent to driver
[2021-05-14 17:51:51,127] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (7cd3ddbc35d2, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:51,129] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 2909 ms on 7cd3ddbc35d2 (executor driver) (188/200)
21/05/14 20:51:51 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-14 17:51:51,139] {docker.py:276} INFO - 21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:51,141] {docker.py:276} INFO - 21/05/14 20:51:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:51,141] {docker.py:276} INFO - 21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357887872106799896179_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357887872106799896179_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357887872106799896179_0004}; taskId=attempt_202105142049357887872106799896179_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6be1ea67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049357887872106799896179_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357887872106799896179_0004_m_000191_479
[2021-05-14 17:51:51,144] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049357887872106799896179_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357887872106799896179_0004_m_000191_479 : duration 0:00.003s
[2021-05-14 17:51:51,351] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049355046159870627299089_0004_m_000188_476: needsTaskCommit() Task attempt_202105142049355046159870627299089_0004_m_000188_476
[2021-05-14 17:51:51,352] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049355046159870627299089_0004_m_000188_476: needsTaskCommit() Task attempt_202105142049355046159870627299089_0004_m_000188_476: duration 0:00.001s
21/05/14 20:51:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355046159870627299089_0004_m_000188_476
[2021-05-14 17:51:51,354] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4544 bytes result sent to driver
[2021-05-14 17:51:51,355] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (7cd3ddbc35d2, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:51,356] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 2858 ms on 7cd3ddbc35d2 (executor driver) (189/200)
21/05/14 20:51:51 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-14 17:51:51,365] {docker.py:276} INFO - 21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:51,368] {docker.py:276} INFO - 21/05/14 20:51:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049357313904757813272013_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357313904757813272013_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049357313904757813272013_0004}; taskId=attempt_202105142049357313904757813272013_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14914cd8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:51,368] {docker.py:276} INFO - 21/05/14 20:51:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049357313904757813272013_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357313904757813272013_0004_m_000192_480
[2021-05-14 17:51:51,370] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049357313904757813272013_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049357313904757813272013_0004_m_000192_480 : duration 0:00.002s
[2021-05-14 17:51:51,643] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049356110120807506705478_0004_m_000190_478: needsTaskCommit() Task attempt_202105142049356110120807506705478_0004_m_000190_478
[2021-05-14 17:51:51,644] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049356110120807506705478_0004_m_000190_478: needsTaskCommit() Task attempt_202105142049356110120807506705478_0004_m_000190_478: duration 0:00.001s
21/05/14 20:51:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049356110120807506705478_0004_m_000190_478
[2021-05-14 17:51:51,645] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049353136943701504311856_0004_m_000189_477: needsTaskCommit() Task attempt_202105142049353136943701504311856_0004_m_000189_477
[2021-05-14 17:51:51,646] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049353136943701504311856_0004_m_000189_477: needsTaskCommit() Task attempt_202105142049353136943701504311856_0004_m_000189_477: duration 0:00.001s
[2021-05-14 17:51:51,647] {docker.py:276} INFO - 21/05/14 20:51:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353136943701504311856_0004_m_000189_477
[2021-05-14 17:51:51,647] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4544 bytes result sent to driver
[2021-05-14 17:51:51,647] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (7cd3ddbc35d2, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:51,649] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4544 bytes result sent to driver
[2021-05-14 17:51:51,649] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
21/05/14 20:51:51 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2882 ms on 7cd3ddbc35d2 (executor driver) (190/200)
[2021-05-14 17:51:51,650] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 2944 ms on 7cd3ddbc35d2 (executor driver) (191/200)
[2021-05-14 17:51:51,651] {docker.py:276} INFO - 21/05/14 20:51:51 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (7cd3ddbc35d2, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:51,652] {docker.py:276} INFO - 21/05/14 20:51:51 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-14 17:51:51,660] {docker.py:276} INFO - 21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:51,661] {docker.py:276} INFO - 21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 17:51:51,662] {docker.py:276} INFO - 21/05/14 20:51:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355366152434306575368_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355366152434306575368_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355366152434306575368_0004}; taskId=attempt_202105142049355366152434306575368_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f2014ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_202105142049355366152434306575368_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355366152434306575368_0004_m_000194_482
[2021-05-14 17:51:51,663] {docker.py:276} INFO - 21/05/14 20:51:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514204935955697744074007455_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935955697744074007455_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514204935955697744074007455_0004}; taskId=attempt_20210514204935955697744074007455_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@340d106}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:51,663] {docker.py:276} INFO - 21/05/14 20:51:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:51 INFO StagingCommitter: Starting: Task committer attempt_20210514204935955697744074007455_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935955697744074007455_0004_m_000193_481
[2021-05-14 17:51:51,665] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_202105142049355366152434306575368_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355366152434306575368_0004_m_000194_482 : duration 0:00.003s
[2021-05-14 17:51:51,667] {docker.py:276} INFO - 21/05/14 20:51:51 INFO StagingCommitter: Task committer attempt_20210514204935955697744074007455_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_20210514204935955697744074007455_0004_m_000193_481 : duration 0:00.004s
[2021-05-14 17:51:53,311] {docker.py:276} INFO - 21/05/14 20:51:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049357887872106799896179_0004_m_000191_479: needsTaskCommit() Task attempt_202105142049357887872106799896179_0004_m_000191_479
21/05/14 20:51:53 INFO StagingCommitter: Task committer attempt_202105142049357887872106799896179_0004_m_000191_479: needsTaskCommit() Task attempt_202105142049357887872106799896179_0004_m_000191_479: duration 0:00.000s
21/05/14 20:51:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357887872106799896179_0004_m_000191_479
[2021-05-14 17:51:53,312] {docker.py:276} INFO - 21/05/14 20:51:53 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4544 bytes result sent to driver
[2021-05-14 17:51:53,313] {docker.py:276} INFO - 21/05/14 20:51:53 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (7cd3ddbc35d2, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:53,314] {docker.py:276} INFO - 21/05/14 20:51:53 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-14 17:51:53,315] {docker.py:276} INFO - 21/05/14 20:51:53 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2190 ms on 7cd3ddbc35d2 (executor driver) (192/200)
[2021-05-14 17:51:53,323] {docker.py:276} INFO - 21/05/14 20:51:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:53,325] {docker.py:276} INFO - 21/05/14 20:51:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353495398240679637746_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353495398240679637746_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353495398240679637746_0004}; taskId=attempt_202105142049353495398240679637746_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@412216f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:53 INFO StagingCommitter: Starting: Task committer attempt_202105142049353495398240679637746_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353495398240679637746_0004_m_000195_483
[2021-05-14 17:51:53,328] {docker.py:276} INFO - 21/05/14 20:51:53 INFO StagingCommitter: Task committer attempt_202105142049353495398240679637746_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353495398240679637746_0004_m_000195_483 : duration 0:00.003s
[2021-05-14 17:51:54,364] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Starting: Task committer attempt_202105142049357313904757813272013_0004_m_000192_480: needsTaskCommit() Task attempt_202105142049357313904757813272013_0004_m_000192_480
[2021-05-14 17:51:54,364] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Task committer attempt_202105142049357313904757813272013_0004_m_000192_480: needsTaskCommit() Task attempt_202105142049357313904757813272013_0004_m_000192_480: duration 0:00.000s
21/05/14 20:51:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049357313904757813272013_0004_m_000192_480
[2021-05-14 17:51:54,365] {docker.py:276} INFO - 21/05/14 20:51:54 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4587 bytes result sent to driver
[2021-05-14 17:51:54,366] {docker.py:276} INFO - 21/05/14 20:51:54 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (7cd3ddbc35d2, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:54,366] {docker.py:276} INFO - 21/05/14 20:51:54 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-14 17:51:54,367] {docker.py:276} INFO - 21/05/14 20:51:54 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 3018 ms on 7cd3ddbc35d2 (executor driver) (193/200)
[2021-05-14 17:51:54,374] {docker.py:276} INFO - 21/05/14 20:51:54 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:54,376] {docker.py:276} INFO - 21/05/14 20:51:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049353205595217789383554_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205595217789383554_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049353205595217789383554_0004}; taskId=attempt_202105142049353205595217789383554_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b2d335f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:54 INFO StagingCommitter: Starting: Task committer attempt_202105142049353205595217789383554_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205595217789383554_0004_m_000196_484
[2021-05-14 17:51:54,378] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Task committer attempt_202105142049353205595217789383554_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049353205595217789383554_0004_m_000196_484 : duration 0:00.003s
[2021-05-14 17:51:54,725] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Starting: Task committer attempt_20210514204935955697744074007455_0004_m_000193_481: needsTaskCommit() Task attempt_20210514204935955697744074007455_0004_m_000193_481
[2021-05-14 17:51:54,726] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Task committer attempt_20210514204935955697744074007455_0004_m_000193_481: needsTaskCommit() Task attempt_20210514204935955697744074007455_0004_m_000193_481: duration 0:00.001s
21/05/14 20:51:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514204935955697744074007455_0004_m_000193_481
[2021-05-14 17:51:54,726] {docker.py:276} INFO - 21/05/14 20:51:54 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4587 bytes result sent to driver
[2021-05-14 17:51:54,728] {docker.py:276} INFO - 21/05/14 20:51:54 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (7cd3ddbc35d2, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:54,730] {docker.py:276} INFO - 21/05/14 20:51:54 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 3087 ms on 7cd3ddbc35d2 (executor driver) (194/200)
[2021-05-14 17:51:54,730] {docker.py:276} INFO - 21/05/14 20:51:54 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-14 17:51:54,739] {docker.py:276} INFO - 21/05/14 20:51:54 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:54,740] {docker.py:276} INFO - 21/05/14 20:51:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049359156559143825624736_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359156559143825624736_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049359156559143825624736_0004}; taskId=attempt_202105142049359156559143825624736_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69888a64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:54 INFO StagingCommitter: Starting: Task committer attempt_202105142049359156559143825624736_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359156559143825624736_0004_m_000197_485
[2021-05-14 17:51:54,743] {docker.py:276} INFO - 21/05/14 20:51:54 INFO StagingCommitter: Task committer attempt_202105142049359156559143825624736_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049359156559143825624736_0004_m_000197_485 : duration 0:00.003s
[2021-05-14 17:51:55,245] {docker.py:276} INFO - 21/05/14 20:51:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049355366152434306575368_0004_m_000194_482: needsTaskCommit() Task attempt_202105142049355366152434306575368_0004_m_000194_482
[2021-05-14 17:51:55,246] {docker.py:276} INFO - 21/05/14 20:51:55 INFO StagingCommitter: Task committer attempt_202105142049355366152434306575368_0004_m_000194_482: needsTaskCommit() Task attempt_202105142049355366152434306575368_0004_m_000194_482: duration 0:00.001s
[2021-05-14 17:51:55,246] {docker.py:276} INFO - 21/05/14 20:51:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355366152434306575368_0004_m_000194_482
[2021-05-14 17:51:55,248] {docker.py:276} INFO - 21/05/14 20:51:55 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4587 bytes result sent to driver
[2021-05-14 17:51:55,249] {docker.py:276} INFO - 21/05/14 20:51:55 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (7cd3ddbc35d2, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:55,250] {docker.py:276} INFO - 21/05/14 20:51:55 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-14 17:51:55,250] {docker.py:276} INFO - 21/05/14 20:51:55 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 3604 ms on 7cd3ddbc35d2 (executor driver) (195/200)
[2021-05-14 17:51:55,258] {docker.py:276} INFO - 21/05/14 20:51:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:55,259] {docker.py:276} INFO - 21/05/14 20:51:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 17:51:55,259] {docker.py:276} INFO - 21/05/14 20:51:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 17:51:55,260] {docker.py:276} INFO - 21/05/14 20:51:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:55,260] {docker.py:276} INFO - 21/05/14 20:51:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049355631578803175037957_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355631578803175037957_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049355631578803175037957_0004}; taskId=attempt_202105142049355631578803175037957_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b1b98dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 17:51:55,261] {docker.py:276} INFO - 21/05/14 20:51:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 17:51:55,261] {docker.py:276} INFO - 21/05/14 20:51:55 INFO StagingCommitter: Starting: Task committer attempt_202105142049355631578803175037957_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355631578803175037957_0004_m_000198_486
[2021-05-14 17:51:55,264] {docker.py:276} INFO - 21/05/14 20:51:55 INFO StagingCommitter: Task committer attempt_202105142049355631578803175037957_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049355631578803175037957_0004_m_000198_486 : duration 0:00.003s
[2021-05-14 17:51:56,981] {docker.py:276} INFO - 21/05/14 20:51:56 INFO StagingCommitter: Starting: Task committer attempt_202105142049353495398240679637746_0004_m_000195_483: needsTaskCommit() Task attempt_202105142049353495398240679637746_0004_m_000195_483
[2021-05-14 17:51:56,982] {docker.py:276} INFO - 21/05/14 20:51:57 INFO StagingCommitter: Task committer attempt_202105142049353495398240679637746_0004_m_000195_483: needsTaskCommit() Task attempt_202105142049353495398240679637746_0004_m_000195_483: duration 0:00.001s
21/05/14 20:51:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353495398240679637746_0004_m_000195_483
[2021-05-14 17:51:56,985] {docker.py:276} INFO - 21/05/14 20:51:57 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4587 bytes result sent to driver
[2021-05-14 17:51:56,987] {docker.py:276} INFO - 21/05/14 20:51:57 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (7cd3ddbc35d2, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 17:51:56,988] {docker.py:276} INFO - 21/05/14 20:51:57 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-14 17:51:56,988] {docker.py:276} INFO - 21/05/14 20:51:57 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 3679 ms on 7cd3ddbc35d2 (executor driver) (196/200)
[2021-05-14 17:51:56,997] {docker.py:276} INFO - 21/05/14 20:51:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 20:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 17:51:56,999] {docker.py:276} INFO - 21/05/14 20:51:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 20:51:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 20:51:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 20:51:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142049358995027093350676585_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358995027093350676585_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142049358995027093350676585_0004}; taskId=attempt_202105142049358995027093350676585_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ff08807}; outputPath=file:/home/jovyan/tmp/staging/jovyan/0cf2b82e-aa7d-4150-b888-d63efdec71a6/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 20:51:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 20:51:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049358995027093350676585_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358995027093350676585_0004_m_000199_487
[2021-05-14 17:51:57,001] {docker.py:276} INFO - 21/05/14 20:51:57 INFO StagingCommitter: Task committer attempt_202105142049358995027093350676585_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/0cf2b82e-aa7d-4150-b888-d63efdec71a6/_temporary/0/_temporary/attempt_202105142049358995027093350676585_0004_m_000199_487 : duration 0:00.003s
[2021-05-14 17:51:57,345] {docker.py:276} INFO - 21/05/14 20:51:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049359156559143825624736_0004_m_000197_485: needsTaskCommit() Task attempt_202105142049359156559143825624736_0004_m_000197_485
21/05/14 20:51:57 INFO StagingCommitter: Task committer attempt_202105142049359156559143825624736_0004_m_000197_485: needsTaskCommit() Task attempt_202105142049359156559143825624736_0004_m_000197_485: duration 0:00.000s
21/05/14 20:51:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049359156559143825624736_0004_m_000197_485
[2021-05-14 17:51:57,348] {docker.py:276} INFO - 21/05/14 20:51:57 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4544 bytes result sent to driver
[2021-05-14 17:51:57,349] {docker.py:276} INFO - 21/05/14 20:51:57 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 2625 ms on 7cd3ddbc35d2 (executor driver) (197/200)
[2021-05-14 17:51:57,413] {docker.py:276} INFO - 21/05/14 20:51:57 INFO StagingCommitter: Starting: Task committer attempt_202105142049355631578803175037957_0004_m_000198_486: needsTaskCommit() Task attempt_202105142049355631578803175037957_0004_m_000198_486
[2021-05-14 17:51:57,414] {docker.py:276} INFO - 21/05/14 20:51:57 INFO StagingCommitter: Task committer attempt_202105142049355631578803175037957_0004_m_000198_486: needsTaskCommit() Task attempt_202105142049355631578803175037957_0004_m_000198_486: duration 0:00.001s
21/05/14 20:51:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049355631578803175037957_0004_m_000198_486
[2021-05-14 17:51:57,415] {docker.py:276} INFO - 21/05/14 20:51:57 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4544 bytes result sent to driver
[2021-05-14 17:51:57,416] {docker.py:276} INFO - 21/05/14 20:51:57 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2170 ms on 7cd3ddbc35d2 (executor driver) (198/200)
[2021-05-14 17:51:58,068] {docker.py:276} INFO - 21/05/14 20:51:58 INFO StagingCommitter: Starting: Task committer attempt_202105142049353205595217789383554_0004_m_000196_484: needsTaskCommit() Task attempt_202105142049353205595217789383554_0004_m_000196_484
[2021-05-14 17:51:58,069] {docker.py:276} INFO - 21/05/14 20:51:58 INFO StagingCommitter: Task committer attempt_202105142049353205595217789383554_0004_m_000196_484: needsTaskCommit() Task attempt_202105142049353205595217789383554_0004_m_000196_484: duration 0:00.000s
[2021-05-14 17:51:58,069] {docker.py:276} INFO - 21/05/14 20:51:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049353205595217789383554_0004_m_000196_484
[2021-05-14 17:51:58,070] {docker.py:276} INFO - 21/05/14 20:51:58 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4544 bytes result sent to driver
[2021-05-14 17:51:58,071] {docker.py:276} INFO - 21/05/14 20:51:58 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 3710 ms on 7cd3ddbc35d2 (executor driver) (199/200)
[2021-05-14 17:52:00,487] {docker.py:276} INFO - 21/05/14 20:52:00 INFO StagingCommitter: Starting: Task committer attempt_202105142049358995027093350676585_0004_m_000199_487: needsTaskCommit() Task attempt_202105142049358995027093350676585_0004_m_000199_487
[2021-05-14 17:52:00,489] {docker.py:276} INFO - 21/05/14 20:52:00 INFO StagingCommitter: Task committer attempt_202105142049358995027093350676585_0004_m_000199_487: needsTaskCommit() Task attempt_202105142049358995027093350676585_0004_m_000199_487: duration 0:00.001s
21/05/14 20:52:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142049358995027093350676585_0004_m_000199_487
[2021-05-14 17:52:00,490] {docker.py:276} INFO - 21/05/14 20:52:00 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4544 bytes result sent to driver
[2021-05-14 17:52:00,492] {docker.py:276} INFO - 21/05/14 20:52:00 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 3509 ms on 7cd3ddbc35d2 (executor driver) (200/200)
21/05/14 20:52:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-14 17:52:00,493] {docker.py:276} INFO - 21/05/14 20:52:00 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 125.111 s
[2021-05-14 17:52:00,493] {docker.py:276} INFO - 21/05/14 20:52:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 20:52:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-14 17:52:00,494] {docker.py:276} INFO - 21/05/14 20:52:00 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 145.538273 s
[2021-05-14 17:52:00,496] {docker.py:276} INFO - 21/05/14 20:52:00 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105142049341510091530566747650_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 17:52:00,514] {docker.py:276} INFO - 21/05/14 20:52:00 WARN AbstractS3ACommitter: Task committer attempt_202105142049341510091530566747650_0000_m_000000_0: No pending uploads to commit
[2021-05-14 17:52:01,028] {docker.py:276} INFO - 21/05/14 20:52:01 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 20:52:01 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 17:52:01,204] {docker.py:276} INFO - 21/05/14 20:52:01 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.177s
21/05/14 20:52:01 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.178s
[2021-05-14 17:52:01,205] {docker.py:276} INFO - 21/05/14 20:52:01 INFO AbstractS3ACommitter: Task committer attempt_202105142049341510091530566747650_0000_m_000000_0: commitJob((no job ID)): duration 0:00.710s
[2021-05-14 17:52:01,707] {docker.py:276} INFO - 21/05/14 20:52:01 INFO FileFormatWriter: Write Job 0cf2b82e-aa7d-4150-b888-d63efdec71a6 committed.
[2021-05-14 17:52:01,717] {docker.py:276} INFO - 21/05/14 20:52:01 INFO FileFormatWriter: Finished processing stats for write job 0cf2b82e-aa7d-4150-b888-d63efdec71a6.
[2021-05-14 17:52:01,819] {docker.py:276} INFO - 21/05/14 20:52:01 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 17:52:01,835] {docker.py:276} INFO - 21/05/14 20:52:01 INFO SparkUI: Stopped Spark web UI at http://7cd3ddbc35d2:4040
[2021-05-14 17:52:01,856] {docker.py:276} INFO - 21/05/14 20:52:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 17:52:01,874] {docker.py:276} INFO - 21/05/14 20:52:01 INFO MemoryStore: MemoryStore cleared
[2021-05-14 17:52:01,875] {docker.py:276} INFO - 21/05/14 20:52:01 INFO BlockManager: BlockManager stopped
[2021-05-14 17:52:01,879] {docker.py:276} INFO - 21/05/14 20:52:01 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 17:52:01,883] {docker.py:276} INFO - 21/05/14 20:52:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 17:52:01,892] {docker.py:276} INFO - 21/05/14 20:52:01 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 17:52:01,893] {docker.py:276} INFO - 21/05/14 20:52:01 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 17:52:01,894] {docker.py:276} INFO - 21/05/14 20:52:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-6983241b-30f6-45e4-9a84-9c675ab0fab8
[2021-05-14 17:52:01,897] {docker.py:276} INFO - 21/05/14 20:52:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0dd93f5-4922-4ffb-aebb-5f19e710a40e
[2021-05-14 17:52:01,899] {docker.py:276} INFO - 21/05/14 20:52:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0dd93f5-4922-4ffb-aebb-5f19e710a40e/pyspark-d228ac1a-7f99-4963-be5b-9be069413712
[2021-05-14 17:52:01,905] {docker.py:276} INFO - 21/05/14 20:52:01 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 17:52:01,906] {docker.py:276} INFO - 21/05/14 20:52:01 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-14 17:52:01,906] {docker.py:276} INFO - 21/05/14 20:52:01 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 17:52:02,151] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210514T204759, start_date=20210514T204852, end_date=20210514T205202
[2021-05-14 17:52:02,203] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-14 17:52:02,246] {local_task_job.py:146} INFO - Task exited with return code 0
