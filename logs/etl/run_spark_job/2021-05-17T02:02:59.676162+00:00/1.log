[2021-05-16 23:03:18,763] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T02:02:59.676162+00:00 [queued]>
[2021-05-16 23:03:18,769] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-17T02:02:59.676162+00:00 [queued]>
[2021-05-16 23:03:18,769] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 23:03:18,769] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-16 23:03:18,769] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 23:03:18,774] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-17T02:02:59.676162+00:00
[2021-05-16 23:03:18,777] {standard_task_runner.py:52} INFO - Started process 33126 to run task
[2021-05-16 23:03:18,784] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-17T02:02:59.676162+00:00', '--job-id', '862', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp20q5csqk', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpgx0lwxg8']
[2021-05-16 23:03:18,785] {standard_task_runner.py:77} INFO - Job 862: Subtask run_spark_job
[2021-05-16 23:03:18,815] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-17T02:02:59.676162+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-16 23:03:18,839] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-17T02:02:59.676162+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-17T02:02:59.676162+00:00
[2021-05-16 23:03:18,841] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-16 23:03:21,617] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-16 23:03:21,620] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-16 23:03:21,620] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-16 23:03:21,624] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-16 23:03:23,516] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-16 23:03:24,052] {docker.py:276} INFO - 21/05/17 02:03:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-16 23:03:26,366] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-16 23:03:26,367] {docker.py:276} INFO - 
[2021-05-16 23:03:26,383] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SparkContext: Running Spark version 3.1.1
[2021-05-16 23:03:26,456] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceUtils: ==============================================================
[2021-05-16 23:03:26,457] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-16 23:03:26,458] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceUtils: ==============================================================
[2021-05-16 23:03:26,460] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SparkContext: Submitted application: spark.py
[2021-05-16 23:03:26,492] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-16 23:03:26,508] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceProfile: Limiting resource is cpu
[2021-05-16 23:03:26,509] {docker.py:276} INFO - 21/05/17 02:03:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-16 23:03:26,592] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-16 23:03:26,593] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-16 23:03:26,594] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SecurityManager: Changing view acls groups to:
[2021-05-16 23:03:26,594] {docker.py:276} INFO - 21/05/17 02:03:26 INFO SecurityManager: Changing modify acls groups to: 
21/05/17 02:03:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-16 23:03:27,003] {docker.py:276} INFO - 21/05/17 02:03:27 INFO Utils: Successfully started service 'sparkDriver' on port 37855.
[2021-05-16 23:03:27,054] {docker.py:276} INFO - 21/05/17 02:03:27 INFO SparkEnv: Registering MapOutputTracker
[2021-05-16 23:03:27,101] {docker.py:276} INFO - 21/05/17 02:03:27 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-16 23:03:27,138] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-16 23:03:27,139] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-16 23:03:27,146] {docker.py:276} INFO - 21/05/17 02:03:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-16 23:03:27,164] {docker.py:276} INFO - 21/05/17 02:03:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f3673142-31b2-4400-8fed-2b0d075e0c02
[2021-05-16 23:03:27,193] {docker.py:276} INFO - 21/05/17 02:03:27 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-16 23:03:27,216] {docker.py:276} INFO - 21/05/17 02:03:27 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-16 23:03:27,505] {docker.py:276} INFO - 21/05/17 02:03:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-16 23:03:27,591] {docker.py:276} INFO - 21/05/17 02:03:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e60b22510068:4040
[2021-05-16 23:03:27,843] {docker.py:276} INFO - 21/05/17 02:03:27 INFO Executor: Starting executor ID driver on host e60b22510068
[2021-05-16 23:03:27,886] {docker.py:276} INFO - 21/05/17 02:03:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36045.
[2021-05-16 23:03:27,887] {docker.py:276} INFO - 21/05/17 02:03:27 INFO NettyBlockTransferService: Server created on e60b22510068:36045
[2021-05-16 23:03:27,890] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-16 23:03:27,903] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e60b22510068, 36045, None)
[2021-05-16 23:03:27,911] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManagerMasterEndpoint: Registering block manager e60b22510068:36045 with 934.4 MiB RAM, BlockManagerId(driver, e60b22510068, 36045, None)
[2021-05-16 23:03:27,914] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e60b22510068, 36045, None)
[2021-05-16 23:03:27,916] {docker.py:276} INFO - 21/05/17 02:03:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e60b22510068, 36045, None)
[2021-05-16 23:03:28,492] {docker.py:276} INFO - 21/05/17 02:03:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-16 23:03:28,493] {docker.py:276} INFO - 21/05/17 02:03:28 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-16 23:03:29,536] {docker.py:276} INFO - 21/05/17 02:03:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-16 23:03:29,581] {docker.py:276} INFO - 21/05/17 02:03:29 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2021-05-16 23:03:29,582] {docker.py:276} INFO - 21/05/17 02:03:29 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-16 23:03:37,302] {docker.py:276} INFO - 21/05/17 02:03:37 INFO InMemoryFileIndex: It took 597 ms to list leaf files for 3 paths.
[2021-05-16 23:03:37,921] {docker.py:276} INFO - 21/05/17 02:03:37 INFO InMemoryFileIndex: It took 523 ms to list leaf files for 3 paths.
[2021-05-16 23:03:40,537] {docker.py:276} INFO - 21/05/17 02:03:40 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 23:03:40,543] {docker.py:276} INFO - 21/05/17 02:03:40 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-16 23:03:40,548] {docker.py:276} INFO - 21/05/17 02:03:40 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 23:03:41,399] {docker.py:276} INFO - 21/05/17 02:03:41 INFO CodeGenerator: Code generated in 321.4831 ms
[2021-05-16 23:03:41,497] {docker.py:276} INFO - 21/05/17 02:03:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-16 23:03:41,600] {docker.py:276} INFO - 21/05/17 02:03:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-16 23:03:41,605] {docker.py:276} INFO - 21/05/17 02:03:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e60b22510068:36045 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 23:03:41,609] {docker.py:276} INFO - 21/05/17 02:03:41 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 23:03:41,624] {docker.py:276} INFO - 21/05/17 02:03:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 23:03:41,828] {docker.py:276} INFO - 21/05/17 02:03:41 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 23:03:41,854] {docker.py:276} INFO - 21/05/17 02:03:41 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-05-16 23:03:41,855] {docker.py:276} INFO - 21/05/17 02:03:41 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 23:03:41,856] {docker.py:276} INFO - 21/05/17 02:03:41 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 23:03:41,859] {docker.py:276} INFO - 21/05/17 02:03:41 INFO DAGScheduler: Missing parents: List()
[2021-05-16 23:03:41,865] {docker.py:276} INFO - 21/05/17 02:03:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 23:03:41,967] {docker.py:276} INFO - 21/05/17 02:03:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-16 23:03:41,984] {docker.py:276} INFO - 21/05/17 02:03:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-16 23:03:41,986] {docker.py:276} INFO - 21/05/17 02:03:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e60b22510068:36045 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-16 23:03:41,988] {docker.py:276} INFO - 21/05/17 02:03:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-16 23:03:42,009] {docker.py:276} INFO - 21/05/17 02:03:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-16 23:03:42,011] {docker.py:276} INFO - 21/05/17 02:03:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-05-16 23:03:42,115] {docker.py:276} INFO - 21/05/17 02:03:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e60b22510068, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:42,140] {docker.py:276} INFO - 21/05/17 02:03:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-16 23:03:42,347] {docker.py:276} INFO - 21/05/17 02:03:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_23_03_12/from_1621215016_to_1621215192.csv, range: 0-10296, partition values: [empty row]
[2021-05-16 23:03:42,380] {docker.py:276} INFO - 21/05/17 02:03:42 INFO CodeGenerator: Code generated in 22.0969 ms
[2021-05-16 23:03:42,814] {docker.py:276} INFO - 21/05/17 02:03:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1564 bytes result sent to driver
[2021-05-16 23:03:42,826] {docker.py:276} INFO - 21/05/17 02:03:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 724 ms on e60b22510068 (executor driver) (1/1)
[2021-05-16 23:03:42,831] {docker.py:276} INFO - 21/05/17 02:03:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-16 23:03:42,838] {docker.py:276} INFO - 21/05/17 02:03:42 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.951 s
[2021-05-16 23:03:42,844] {docker.py:276} INFO - 21/05/17 02:03:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 23:03:42,845] {docker.py:276} INFO - 21/05/17 02:03:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-16 23:03:42,850] {docker.py:276} INFO - 21/05/17 02:03:42 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.021579 s
[2021-05-16 23:03:42,897] {docker.py:276} INFO - 21/05/17 02:03:42 INFO CodeGenerator: Code generated in 21.0783 ms
[2021-05-16 23:03:42,987] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 23:03:42,988] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 23:03:42,989] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 23:03:43,022] {docker.py:276} INFO - 21/05/17 02:03:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-16 23:03:43,046] {docker.py:276} INFO - 21/05/17 02:03:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-16 23:03:43,055] {docker.py:276} INFO - 21/05/17 02:03:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e60b22510068:36045 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 23:03:43,057] {docker.py:276} INFO - 21/05/17 02:03:43 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 23:03:43,059] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 23:03:43,074] {docker.py:276} INFO - 21/05/17 02:03:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on e60b22510068:36045 in memory (size: 5.4 KiB, free: 934.3 MiB)
[2021-05-16 23:03:43,758] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 23:03:43,758] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 23:03:43,759] {docker.py:276} INFO - 21/05/17 02:03:43 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-16 23:03:44,400] {docker.py:276} INFO - 21/05/17 02:03:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:44,404] {docker.py:276} INFO - 21/05/17 02:03:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:44,405] {docker.py:276} INFO - 21/05/17 02:03:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448565622101638569140_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448565622101638569140_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448565622101638569140_0000}; taskId=attempt_202105170203448565622101638569140_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61e4873f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:44,406] {docker.py:276} INFO - 21/05/17 02:03:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:44,438] {docker.py:276} INFO - 21/05/17 02:03:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 23:03:44,530] {docker.py:276} INFO - 21/05/17 02:03:44 INFO CodeGenerator: Code generated in 59.6894 ms
[2021-05-16 23:03:44,533] {docker.py:276} INFO - 21/05/17 02:03:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 23:03:44,580] {docker.py:276} INFO - 21/05/17 02:03:44 INFO CodeGenerator: Code generated in 38.971 ms
[2021-05-16 23:03:44,586] {docker.py:276} INFO - 21/05/17 02:03:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-16 23:03:44,612] {docker.py:276} INFO - 21/05/17 02:03:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-16 23:03:44,614] {docker.py:276} INFO - 21/05/17 02:03:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e60b22510068:36045 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 23:03:44,615] {docker.py:276} INFO - 21/05/17 02:03:44 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 23:03:44,620] {docker.py:276} INFO - 21/05/17 02:03:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 23:03:44,692] {docker.py:276} INFO - 21/05/17 02:03:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e60b22510068:36045 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 23:03:44,754] {docker.py:276} INFO - 21/05/17 02:03:44 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 23:03:44,759] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Registering RDD 13 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-16 23:03:44,763] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/17 02:03:44 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 23:03:44,763] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2021-05-16 23:03:44,765] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2021-05-16 23:03:44,769] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 23:03:44,786] {docker.py:276} INFO - 21/05/17 02:03:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-16 23:03:44,810] {docker.py:276} INFO - 21/05/17 02:03:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-16 23:03:44,812] {docker.py:276} INFO - 21/05/17 02:03:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e60b22510068:36045 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-16 23:03:44,813] {docker.py:276} INFO - 21/05/17 02:03:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
[2021-05-16 23:03:44,815] {docker.py:276} INFO - 21/05/17 02:03:44 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
21/05/17 02:03:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
[2021-05-16 23:03:44,818] {docker.py:276} INFO - 21/05/17 02:03:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e60b22510068, executor driver, partition 0, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:44,819] {docker.py:276} INFO - 21/05/17 02:03:44 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e60b22510068, executor driver, partition 1, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:44,820] {docker.py:276} INFO - 21/05/17 02:03:44 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (e60b22510068, executor driver, partition 2, PROCESS_LOCAL, 4894 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:44,821] {docker.py:276} INFO - 21/05/17 02:03:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2021-05-16 23:03:44,833] {docker.py:276} INFO - 21/05/17 02:03:44 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2021-05-16 23:03:44,842] {docker.py:276} INFO - 21/05/17 02:03:44 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2021-05-16 23:03:44,886] {docker.py:276} INFO - 21/05/17 02:03:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e60b22510068:36045 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 23:03:44,976] {docker.py:276} INFO - 21/05/17 02:03:44 INFO CodeGenerator: Code generated in 27.8452 ms
[2021-05-16 23:03:45,011] {docker.py:276} INFO - 21/05/17 02:03:45 INFO CodeGenerator: Code generated in 12.9521 ms
[2021-05-16 23:03:45,044] {docker.py:276} INFO - 21/05/17 02:03:45 INFO CodeGenerator: Code generated in 22.2275 ms
[2021-05-16 23:03:45,069] {docker.py:276} INFO - 21/05/17 02:03:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_23_03_12/from_1621215016_to_1621215192.csv, range: 0-10282, partition values: [empty row]
[2021-05-16 23:03:45,071] {docker.py:276} INFO - 21/05/17 02:03:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_23_03_12/from_1621215016_to_1621215192.csv, range: 0-10296, partition values: [empty row]
[2021-05-16 23:03:45,075] {docker.py:276} INFO - 21/05/17 02:03:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_23_03_12/from_1621215016_to_1621215192.csv, range: 0-10235, partition values: [empty row]
[2021-05-16 23:03:45,627] {docker.py:276} INFO - 21/05/17 02:03:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2722 bytes result sent to driver
[2021-05-16 23:03:45,631] {docker.py:276} INFO - 21/05/17 02:03:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 816 ms on e60b22510068 (executor driver) (1/3)
[2021-05-16 23:03:46,179] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2679 bytes result sent to driver
[2021-05-16 23:03:46,210] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1393 ms on e60b22510068 (executor driver) (2/3)
[2021-05-16 23:03:46,211] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2679 bytes result sent to driver
[2021-05-16 23:03:46,213] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1395 ms on e60b22510068 (executor driver) (3/3)
[2021-05-16 23:03:46,214] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-16 23:03:46,215] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.440 s
[2021-05-16 23:03:46,216] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: looking for newly runnable stages
[2021-05-16 23:03:46,218] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: running: Set()
[2021-05-16 23:03:46,218] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2021-05-16 23:03:46,219] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: failed: Set()
[2021-05-16 23:03:46,223] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 23:03:46,278] {docker.py:276} INFO - 21/05/17 02:03:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-16 23:03:46,280] {docker.py:276} INFO - 21/05/17 02:03:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-16 23:03:46,281] {docker.py:276} INFO - 21/05/17 02:03:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on e60b22510068:36045 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-16 23:03:46,282] {docker.py:276} INFO - 21/05/17 02:03:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
[2021-05-16 23:03:46,284] {docker.py:276} INFO - 21/05/17 02:03:46 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/17 02:03:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2021-05-16 23:03:46,294] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (e60b22510068, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:46,296] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (e60b22510068, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:46,297] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (e60b22510068, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:46,298] {docker.py:276} INFO - 21/05/17 02:03:46 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (e60b22510068, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:46,299] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2021-05-16 23:03:46,300] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2021-05-16 23:03:46,300] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2021-05-16 23:03:46,303] {docker.py:276} INFO - 21/05/17 02:03:46 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)
[2021-05-16 23:03:46,473] {docker.py:276} INFO - 21/05/17 02:03:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on e60b22510068:36045 in memory (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-16 23:03:46,485] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Getting 3 (549.0 B) non-empty blocks including 3 (549.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:46,493] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 40 ms
[2021-05-16 23:03:46,497] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Getting 3 (707.0 B) non-empty blocks including 3 (707.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
[2021-05-16 23:03:46,497] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:46,498] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:46,498] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 45 ms
[2021-05-16 23:03:46,499] {docker.py:276} INFO - 21/05/17 02:03:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-16 23:03:46,519] {docker.py:276} INFO - 21/05/17 02:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:46,520] {docker.py:276} INFO - 21/05/17 02:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:46,520] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,521] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443368840421948208685_0002_m_000005_7, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443368840421948208685_0002_m_000005_7}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443368840421948208685_0002}; taskId=attempt_202105170203443368840421948208685_0002_m_000005_7, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59c64a64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:46,523] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203443368840421948208685_0002_m_000005_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443368840421948208685_0002_m_000005_7
[2021-05-16 23:03:46,529] {docker.py:276} INFO - 21/05/17 02:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:46,530] {docker.py:276} INFO - 21/05/17 02:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:46,530] {docker.py:276} INFO - 21/05/17 02:03:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:46,531] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,532] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444887341192673018019_0002_m_000001_4, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444887341192673018019_0002_m_000001_4}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444887341192673018019_0002}; taskId=attempt_202105170203444887341192673018019_0002_m_000001_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7444b225}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,532] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,534] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,534] {docker.py:276} INFO - 21/05/17 02:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:46,535] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448640382900077363644_0002_m_000003_6, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448640382900077363644_0002_m_000003_6}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448640382900077363644_0002}; taskId=attempt_202105170203448640382900077363644_0002_m_000003_6, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2987634d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:46,535] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203444887341192673018019_0002_m_000001_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444887341192673018019_0002_m_000001_4
[2021-05-16 23:03:46,536] {docker.py:276} INFO - 21/05/17 02:03:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444806689486724463857_0002_m_000002_5, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444806689486724463857_0002_m_000002_5}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444806689486724463857_0002}; taskId=attempt_202105170203444806689486724463857_0002_m_000002_5, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26027ec1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:46,537] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203444806689486724463857_0002_m_000002_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444806689486724463857_0002_m_000002_5
[2021-05-16 23:03:46,539] {docker.py:276} INFO - 21/05/17 02:03:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:46,539] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203448640382900077363644_0002_m_000003_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448640382900077363644_0002_m_000003_6
[2021-05-16 23:03:46,550] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Task committer attempt_202105170203448640382900077363644_0002_m_000003_6: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448640382900077363644_0002_m_000003_6 : duration 0:00.011s
[2021-05-16 23:03:46,556] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Task committer attempt_202105170203443368840421948208685_0002_m_000005_7: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443368840421948208685_0002_m_000005_7 : duration 0:00.034s
[2021-05-16 23:03:46,580] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Task committer attempt_202105170203444806689486724463857_0002_m_000002_5: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444806689486724463857_0002_m_000002_5 : duration 0:00.043s
[2021-05-16 23:03:46,580] {docker.py:276} INFO - 21/05/17 02:03:46 INFO StagingCommitter: Task committer attempt_202105170203444887341192673018019_0002_m_000001_4: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444887341192673018019_0002_m_000001_4 : duration 0:00.046s
[2021-05-16 23:03:48,481] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203444887341192673018019_0002_m_000001_4: needsTaskCommit() Task attempt_202105170203444887341192673018019_0002_m_000001_4
[2021-05-16 23:03:48,481] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203444887341192673018019_0002_m_000001_4: needsTaskCommit() Task attempt_202105170203444887341192673018019_0002_m_000001_4: duration 0:00.001s
[2021-05-16 23:03:48,483] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444887341192673018019_0002_m_000001_4
[2021-05-16 23:03:48,497] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 4630 bytes result sent to driver
[2021-05-16 23:03:48,498] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (e60b22510068, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:48,499] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 2210 ms on e60b22510068 (executor driver) (1/200)
[2021-05-16 23:03:48,500] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)
[2021-05-16 23:03:48,515] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:48,516] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:48,519] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:48,519] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:48,520] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,520] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441371608187739588885_0002_m_000006_8, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441371608187739588885_0002_m_000006_8}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441371608187739588885_0002}; taskId=attempt_202105170203441371608187739588885_0002_m_000006_8, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a3fc798}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,521] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:48,521] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203441371608187739588885_0002_m_000006_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441371608187739588885_0002_m_000006_8
[2021-05-16 23:03:48,527] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203441371608187739588885_0002_m_000006_8: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441371608187739588885_0002_m_000006_8 : duration 0:00.007s
[2021-05-16 23:03:48,538] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203443368840421948208685_0002_m_000005_7: needsTaskCommit() Task attempt_202105170203443368840421948208685_0002_m_000005_7
[2021-05-16 23:03:48,540] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203443368840421948208685_0002_m_000005_7: needsTaskCommit() Task attempt_202105170203443368840421948208685_0002_m_000005_7: duration 0:00.008s
[2021-05-16 23:03:48,541] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443368840421948208685_0002_m_000005_7
[2021-05-16 23:03:48,543] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Finished task 5.0 in stage 2.0 (TID 7). 4587 bytes result sent to driver
[2021-05-16 23:03:48,544] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9) (e60b22510068, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:48,545] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Running task 7.0 in stage 2.0 (TID 9)
[2021-05-16 23:03:48,546] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 2251 ms on e60b22510068 (executor driver) (2/200)
[2021-05-16 23:03:48,562] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:48,563] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:48,566] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:48,566] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:48,567] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,568] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445893625819405591223_0002_m_000007_9, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445893625819405591223_0002_m_000007_9}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445893625819405591223_0002}; taskId=attempt_202105170203445893625819405591223_0002_m_000007_9, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4835eb4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,568] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:48,569] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203445893625819405591223_0002_m_000007_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445893625819405591223_0002_m_000007_9
[2021-05-16 23:03:48,575] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203445893625819405591223_0002_m_000007_9: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445893625819405591223_0002_m_000007_9 : duration 0:00.007s
[2021-05-16 23:03:48,596] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203448640382900077363644_0002_m_000003_6: needsTaskCommit() Task attempt_202105170203448640382900077363644_0002_m_000003_6
[2021-05-16 23:03:48,597] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203448640382900077363644_0002_m_000003_6: needsTaskCommit() Task attempt_202105170203448640382900077363644_0002_m_000003_6: duration 0:00.001s
[2021-05-16 23:03:48,597] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448640382900077363644_0002_m_000003_6
[2021-05-16 23:03:48,599] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 4587 bytes result sent to driver
[2021-05-16 23:03:48,600] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 10) (e60b22510068, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:48,601] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 2307 ms on e60b22510068 (executor driver) (3/200)
[2021-05-16 23:03:48,603] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Running task 8.0 in stage 2.0 (TID 10)
[2021-05-16 23:03:48,631] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:48,631] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:48,634] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:48,635] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,635] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447206851833275715155_0002_m_000008_10, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447206851833275715155_0002_m_000008_10}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447206851833275715155_0002}; taskId=attempt_202105170203447206851833275715155_0002_m_000008_10, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31c804f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:48,636] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203447206851833275715155_0002_m_000008_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447206851833275715155_0002_m_000008_10
[2021-05-16 23:03:48,641] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203447206851833275715155_0002_m_000008_10: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447206851833275715155_0002_m_000008_10 : duration 0:00.006s
[2021-05-16 23:03:48,755] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203444806689486724463857_0002_m_000002_5: needsTaskCommit() Task attempt_202105170203444806689486724463857_0002_m_000002_5
[2021-05-16 23:03:48,756] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203444806689486724463857_0002_m_000002_5: needsTaskCommit() Task attempt_202105170203444806689486724463857_0002_m_000002_5: duration 0:00.002s
21/05/17 02:03:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444806689486724463857_0002_m_000002_5
[2021-05-16 23:03:48,758] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 4587 bytes result sent to driver
[2021-05-16 23:03:48,761] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 11) (e60b22510068, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:48,762] {docker.py:276} INFO - 21/05/17 02:03:48 INFO Executor: Running task 9.0 in stage 2.0 (TID 11)
[2021-05-16 23:03:48,763] {docker.py:276} INFO - 21/05/17 02:03:48 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 2470 ms on e60b22510068 (executor driver) (4/200)
[2021-05-16 23:03:48,785] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:48,786] {docker.py:276} INFO - 21/05/17 02:03:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:48,789] {docker.py:276} INFO - 21/05/17 02:03:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:48,789] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,790] {docker.py:276} INFO - 21/05/17 02:03:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448800926863618899032_0002_m_000009_11, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448800926863618899032_0002_m_000009_11}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448800926863618899032_0002}; taskId=attempt_202105170203448800926863618899032_0002_m_000009_11, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24132e96}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:48,790] {docker.py:276} INFO - 21/05/17 02:03:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:48,791] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203448800926863618899032_0002_m_000009_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448800926863618899032_0002_m_000009_11
[2021-05-16 23:03:48,797] {docker.py:276} INFO - 21/05/17 02:03:48 INFO StagingCommitter: Task committer attempt_202105170203448800926863618899032_0002_m_000009_11: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448800926863618899032_0002_m_000009_11 : duration 0:00.006s
[2021-05-16 23:03:50,320] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203445893625819405591223_0002_m_000007_9: needsTaskCommit() Task attempt_202105170203445893625819405591223_0002_m_000007_9
[2021-05-16 23:03:50,321] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203445893625819405591223_0002_m_000007_9: needsTaskCommit() Task attempt_202105170203445893625819405591223_0002_m_000007_9: duration 0:00.001s
21/05/17 02:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445893625819405591223_0002_m_000007_9
[2021-05-16 23:03:50,323] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203441371608187739588885_0002_m_000006_8: needsTaskCommit() Task attempt_202105170203441371608187739588885_0002_m_000006_8
[2021-05-16 23:03:50,324] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203441371608187739588885_0002_m_000006_8: needsTaskCommit() Task attempt_202105170203441371608187739588885_0002_m_000006_8: duration 0:00.001s
[2021-05-16 23:03:50,325] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441371608187739588885_0002_m_000006_8
[2021-05-16 23:03:50,326] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Finished task 7.0 in stage 2.0 (TID 9). 4587 bytes result sent to driver
[2021-05-16 23:03:50,327] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203447206851833275715155_0002_m_000008_10: needsTaskCommit() Task attempt_202105170203447206851833275715155_0002_m_000008_10
[2021-05-16 23:03:50,327] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203447206851833275715155_0002_m_000008_10: needsTaskCommit() Task attempt_202105170203447206851833275715155_0002_m_000008_10: duration 0:00.001s
21/05/17 02:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447206851833275715155_0002_m_000008_10
[2021-05-16 23:03:50,328] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 12) (e60b22510068, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:50,331] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 9) in 1789 ms on e60b22510068 (executor driver) (5/200)
[2021-05-16 23:03:50,333] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Finished task 8.0 in stage 2.0 (TID 10). 4587 bytes result sent to driver
[2021-05-16 23:03:50,337] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 13) (e60b22510068, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:50,338] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Running task 10.0 in stage 2.0 (TID 12)
[2021-05-16 23:03:50,339] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 10) in 1741 ms on e60b22510068 (executor driver) (6/200)
[2021-05-16 23:03:50,339] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Running task 11.0 in stage 2.0 (TID 13)
[2021-05-16 23:03:50,340] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Finished task 6.0 in stage 2.0 (TID 8). 4587 bytes result sent to driver
[2021-05-16 23:03:50,341] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 14) (e60b22510068, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:50,342] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 1847 ms on e60b22510068 (executor driver) (7/200)
[2021-05-16 23:03:50,343] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Running task 13.0 in stage 2.0 (TID 14)
[2021-05-16 23:03:50,372] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:50,373] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:50,374] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Getting 2 (399.0 B) non-empty blocks including 2 (399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:50,376] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:50,377] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:50,377] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:50,380] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,381] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:50,381] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-16 23:03:50,381] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:50,382] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:50,382] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,382] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445287982009048797317_0002_m_000010_12, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445287982009048797317_0002_m_000010_12}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445287982009048797317_0002}; taskId=attempt_202105170203445287982009048797317_0002_m_000010_12, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c47e2b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,383] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:50,383] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203445287982009048797317_0002_m_000010_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445287982009048797317_0002_m_000010_12
[2021-05-16 23:03:50,384] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:50,386] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:50,386] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441347359875081530475_0002_m_000013_14, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441347359875081530475_0002_m_000013_14}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441347359875081530475_0002}; taskId=attempt_202105170203441347359875081530475_0002_m_000013_14, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e9f3645}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,387] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203445287982009048797317_0002_m_000010_12: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445287982009048797317_0002_m_000010_12 : duration 0:00.006s
[2021-05-16 23:03:50,390] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:50,390] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203441347359875081530475_0002_m_000013_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441347359875081530475_0002_m_000013_14
[2021-05-16 23:03:50,399] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,402] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203441347359875081530475_0002_m_000013_14: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441347359875081530475_0002_m_000013_14 : duration 0:00.011s
[2021-05-16 23:03:50,402] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441630956180611126965_0002_m_000011_13, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441630956180611126965_0002_m_000011_13}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441630956180611126965_0002}; taskId=attempt_202105170203441630956180611126965_0002_m_000011_13, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d3d9a27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,403] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:50,403] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203441630956180611126965_0002_m_000011_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441630956180611126965_0002_m_000011_13
[2021-05-16 23:03:50,410] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203441630956180611126965_0002_m_000011_13: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441630956180611126965_0002_m_000011_13 : duration 0:00.007s
[2021-05-16 23:03:50,492] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203448800926863618899032_0002_m_000009_11: needsTaskCommit() Task attempt_202105170203448800926863618899032_0002_m_000009_11
[2021-05-16 23:03:50,492] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203448800926863618899032_0002_m_000009_11: needsTaskCommit() Task attempt_202105170203448800926863618899032_0002_m_000009_11: duration 0:00.002s
[2021-05-16 23:03:50,493] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448800926863618899032_0002_m_000009_11
[2021-05-16 23:03:50,497] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Finished task 9.0 in stage 2.0 (TID 11). 4587 bytes result sent to driver
[2021-05-16 23:03:50,497] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 15) (e60b22510068, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:50,498] {docker.py:276} INFO - 21/05/17 02:03:50 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 11) in 1739 ms on e60b22510068 (executor driver) (8/200)
[2021-05-16 23:03:50,498] {docker.py:276} INFO - 21/05/17 02:03:50 INFO Executor: Running task 14.0 in stage 2.0 (TID 15)
[2021-05-16 23:03:50,522] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:50,525] {docker.py:276} INFO - 21/05/17 02:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-16 23:03:50,527] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:50,528] {docker.py:276} INFO - 21/05/17 02:03:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:50,528] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,529] {docker.py:276} INFO - 21/05/17 02:03:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443360178245639449383_0002_m_000014_15, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443360178245639449383_0002_m_000014_15}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443360178245639449383_0002}; taskId=attempt_202105170203443360178245639449383_0002_m_000014_15, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f29c57d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:50,530] {docker.py:276} INFO - 21/05/17 02:03:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:50,530] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203443360178245639449383_0002_m_000014_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443360178245639449383_0002_m_000014_15
[2021-05-16 23:03:50,534] {docker.py:276} INFO - 21/05/17 02:03:50 INFO StagingCommitter: Task committer attempt_202105170203443360178245639449383_0002_m_000014_15: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443360178245639449383_0002_m_000014_15 : duration 0:00.004s
[2021-05-16 23:03:52,102] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203445287982009048797317_0002_m_000010_12: needsTaskCommit() Task attempt_202105170203445287982009048797317_0002_m_000010_12
[2021-05-16 23:03:52,103] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203445287982009048797317_0002_m_000010_12: needsTaskCommit() Task attempt_202105170203445287982009048797317_0002_m_000010_12: duration 0:00.003s
21/05/17 02:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445287982009048797317_0002_m_000010_12
[2021-05-16 23:03:52,109] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Finished task 10.0 in stage 2.0 (TID 12). 4587 bytes result sent to driver
21/05/17 02:03:52 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 16) (e60b22510068, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:52,110] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Running task 15.0 in stage 2.0 (TID 16)
[2021-05-16 23:03:52,111] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 12) in 1786 ms on e60b22510068 (executor driver) (9/200)
[2021-05-16 23:03:52,114] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203441630956180611126965_0002_m_000011_13: needsTaskCommit() Task attempt_202105170203441630956180611126965_0002_m_000011_13
[2021-05-16 23:03:52,115] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203441630956180611126965_0002_m_000011_13: needsTaskCommit() Task attempt_202105170203441630956180611126965_0002_m_000011_13: duration 0:00.000s
21/05/17 02:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441630956180611126965_0002_m_000011_13
[2021-05-16 23:03:52,116] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Finished task 11.0 in stage 2.0 (TID 13). 4587 bytes result sent to driver
[2021-05-16 23:03:52,117] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 17) (e60b22510068, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:52,119] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 13) in 1785 ms on e60b22510068 (executor driver) (10/200)
[2021-05-16 23:03:52,120] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Running task 16.0 in stage 2.0 (TID 17)
[2021-05-16 23:03:52,128] {docker.py:276} INFO - 21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:52,131] {docker.py:276} INFO - 21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:52,132] {docker.py:276} INFO - 21/05/17 02:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:52,133] {docker.py:276} INFO - 21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:52,133] {docker.py:276} INFO - 21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441860704577597799383_0002_m_000015_16, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441860704577597799383_0002_m_000015_16}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441860704577597799383_0002}; taskId=attempt_202105170203441860704577597799383_0002_m_000015_16, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ad11e5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:52,133] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203441860704577597799383_0002_m_000015_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441860704577597799383_0002_m_000015_16
[2021-05-16 23:03:52,135] {docker.py:276} INFO - 21/05/17 02:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:52,137] {docker.py:276} INFO - 21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448545628105399853120_0002_m_000016_17, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448545628105399853120_0002_m_000016_17}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448545628105399853120_0002}; taskId=attempt_202105170203448545628105399853120_0002_m_000016_17, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d4833d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:52,138] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203448545628105399853120_0002_m_000016_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448545628105399853120_0002_m_000016_17
[2021-05-16 23:03:52,141] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203441860704577597799383_0002_m_000015_16: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441860704577597799383_0002_m_000015_16 : duration 0:00.007s
[2021-05-16 23:03:52,145] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203448545628105399853120_0002_m_000016_17: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448545628105399853120_0002_m_000016_17 : duration 0:00.008s
[2021-05-16 23:03:52,191] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203441347359875081530475_0002_m_000013_14: needsTaskCommit() Task attempt_202105170203441347359875081530475_0002_m_000013_14
[2021-05-16 23:03:52,192] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203441347359875081530475_0002_m_000013_14: needsTaskCommit() Task attempt_202105170203441347359875081530475_0002_m_000013_14: duration 0:00.001s
[2021-05-16 23:03:52,192] {docker.py:276} INFO - 21/05/17 02:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441347359875081530475_0002_m_000013_14
[2021-05-16 23:03:52,195] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Finished task 13.0 in stage 2.0 (TID 14). 4587 bytes result sent to driver
[2021-05-16 23:03:52,196] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 18) (e60b22510068, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:52,198] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 14) in 1858 ms on e60b22510068 (executor driver) (11/200)
[2021-05-16 23:03:52,198] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Running task 17.0 in stage 2.0 (TID 18)
[2021-05-16 23:03:52,207] {docker.py:276} INFO - 21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:52,210] {docker.py:276} INFO - 21/05/17 02:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:52,210] {docker.py:276} INFO - 21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443748471133788880621_0002_m_000017_18, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443748471133788880621_0002_m_000017_18}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443748471133788880621_0002}; taskId=attempt_202105170203443748471133788880621_0002_m_000017_18, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43e59289}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:52,211] {docker.py:276} INFO - 21/05/17 02:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203443748471133788880621_0002_m_000017_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443748471133788880621_0002_m_000017_18
[2021-05-16 23:03:52,215] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203443748471133788880621_0002_m_000017_18: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443748471133788880621_0002_m_000017_18 : duration 0:00.004s
[2021-05-16 23:03:52,274] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203443360178245639449383_0002_m_000014_15: needsTaskCommit() Task attempt_202105170203443360178245639449383_0002_m_000014_15
[2021-05-16 23:03:52,282] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203443360178245639449383_0002_m_000014_15: needsTaskCommit() Task attempt_202105170203443360178245639449383_0002_m_000014_15: duration 0:00.002s
21/05/17 02:03:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443360178245639449383_0002_m_000014_15
[2021-05-16 23:03:52,283] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Finished task 14.0 in stage 2.0 (TID 15). 4587 bytes result sent to driver
[2021-05-16 23:03:52,283] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 19) (e60b22510068, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:52,284] {docker.py:276} INFO - 21/05/17 02:03:52 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 15) in 1785 ms on e60b22510068 (executor driver) (12/200)
[2021-05-16 23:03:52,284] {docker.py:276} INFO - 21/05/17 02:03:52 INFO Executor: Running task 18.0 in stage 2.0 (TID 19)
[2021-05-16 23:03:52,301] {docker.py:276} INFO - 21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:52,301] {docker.py:276} INFO - 21/05/17 02:03:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:52,303] {docker.py:276} INFO - 21/05/17 02:03:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:52,304] {docker.py:276} INFO - 21/05/17 02:03:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444473789960713185708_0002_m_000018_19, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444473789960713185708_0002_m_000018_19}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444473789960713185708_0002}; taskId=attempt_202105170203444473789960713185708_0002_m_000018_19, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74635326}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:52,304] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203444473789960713185708_0002_m_000018_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444473789960713185708_0002_m_000018_19
[2021-05-16 23:03:52,309] {docker.py:276} INFO - 21/05/17 02:03:52 INFO StagingCommitter: Task committer attempt_202105170203444473789960713185708_0002_m_000018_19: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444473789960713185708_0002_m_000018_19 : duration 0:00.005s
[2021-05-16 23:03:53,832] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203441860704577597799383_0002_m_000015_16: needsTaskCommit() Task attempt_202105170203441860704577597799383_0002_m_000015_16
[2021-05-16 23:03:53,833] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Task committer attempt_202105170203441860704577597799383_0002_m_000015_16: needsTaskCommit() Task attempt_202105170203441860704577597799383_0002_m_000015_16: duration 0:00.002s
[2021-05-16 23:03:53,834] {docker.py:276} INFO - 21/05/17 02:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441860704577597799383_0002_m_000015_16
[2021-05-16 23:03:53,836] {docker.py:276} INFO - 21/05/17 02:03:53 INFO Executor: Finished task 15.0 in stage 2.0 (TID 16). 4587 bytes result sent to driver
[2021-05-16 23:03:53,837] {docker.py:276} INFO - 21/05/17 02:03:53 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 20) (e60b22510068, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:53,838] {docker.py:276} INFO - 21/05/17 02:03:53 INFO Executor: Running task 19.0 in stage 2.0 (TID 20)
[2021-05-16 23:03:53,839] {docker.py:276} INFO - 21/05/17 02:03:53 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 16) in 1732 ms on e60b22510068 (executor driver) (13/200)
[2021-05-16 23:03:53,854] {docker.py:276} INFO - 21/05/17 02:03:53 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:03:53,858] {docker.py:276} INFO - 21/05/17 02:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:53,858] {docker.py:276} INFO - 21/05/17 02:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446118451995445973155_0002_m_000019_20, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446118451995445973155_0002_m_000019_20}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446118451995445973155_0002}; taskId=attempt_202105170203446118451995445973155_0002_m_000019_20, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c3eb913}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:53,859] {docker.py:276} INFO - 21/05/17 02:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203446118451995445973155_0002_m_000019_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446118451995445973155_0002_m_000019_20
[2021-05-16 23:03:53,863] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Task committer attempt_202105170203446118451995445973155_0002_m_000019_20: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446118451995445973155_0002_m_000019_20 : duration 0:00.005s
[2021-05-16 23:03:53,928] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203448545628105399853120_0002_m_000016_17: needsTaskCommit() Task attempt_202105170203448545628105399853120_0002_m_000016_17
[2021-05-16 23:03:53,929] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Task committer attempt_202105170203448545628105399853120_0002_m_000016_17: needsTaskCommit() Task attempt_202105170203448545628105399853120_0002_m_000016_17: duration 0:00.001s
21/05/17 02:03:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448545628105399853120_0002_m_000016_17
[2021-05-16 23:03:53,931] {docker.py:276} INFO - 21/05/17 02:03:53 INFO Executor: Finished task 16.0 in stage 2.0 (TID 17). 4587 bytes result sent to driver
[2021-05-16 23:03:53,932] {docker.py:276} INFO - 21/05/17 02:03:53 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 21) (e60b22510068, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:53,933] {docker.py:276} INFO - 21/05/17 02:03:53 INFO Executor: Running task 20.0 in stage 2.0 (TID 21)
[2021-05-16 23:03:53,934] {docker.py:276} INFO - 21/05/17 02:03:53 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 17) in 1819 ms on e60b22510068 (executor driver) (14/200)
[2021-05-16 23:03:53,943] {docker.py:276} INFO - 21/05/17 02:03:53 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:53,944] {docker.py:276} INFO - 21/05/17 02:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:53,945] {docker.py:276} INFO - 21/05/17 02:03:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:53,946] {docker.py:276} INFO - 21/05/17 02:03:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:53,946] {docker.py:276} INFO - 21/05/17 02:03:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441626249153268863877_0002_m_000020_21, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441626249153268863877_0002_m_000020_21}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441626249153268863877_0002}; taskId=attempt_202105170203441626249153268863877_0002_m_000020_21, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@383266e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:53,946] {docker.py:276} INFO - 21/05/17 02:03:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:53,947] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203441626249153268863877_0002_m_000020_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441626249153268863877_0002_m_000020_21
[2021-05-16 23:03:53,952] {docker.py:276} INFO - 21/05/17 02:03:53 INFO StagingCommitter: Task committer attempt_202105170203441626249153268863877_0002_m_000020_21: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441626249153268863877_0002_m_000020_21 : duration 0:00.006s
[2021-05-16 23:03:54,000] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203443748471133788880621_0002_m_000017_18: needsTaskCommit() Task attempt_202105170203443748471133788880621_0002_m_000017_18
[2021-05-16 23:03:54,000] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Task committer attempt_202105170203443748471133788880621_0002_m_000017_18: needsTaskCommit() Task attempt_202105170203443748471133788880621_0002_m_000017_18: duration 0:00.002s
[2021-05-16 23:03:54,001] {docker.py:276} INFO - 21/05/17 02:03:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443748471133788880621_0002_m_000017_18
[2021-05-16 23:03:54,003] {docker.py:276} INFO - 21/05/17 02:03:54 INFO Executor: Finished task 17.0 in stage 2.0 (TID 18). 4587 bytes result sent to driver
[2021-05-16 23:03:54,005] {docker.py:276} INFO - 21/05/17 02:03:54 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 22) (e60b22510068, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:54,006] {docker.py:276} INFO - 21/05/17 02:03:54 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 18) in 1812 ms on e60b22510068 (executor driver) (15/200)
[2021-05-16 23:03:54,007] {docker.py:276} INFO - 21/05/17 02:03:54 INFO Executor: Running task 21.0 in stage 2.0 (TID 22)
[2021-05-16 23:03:54,012] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203444473789960713185708_0002_m_000018_19: needsTaskCommit() Task attempt_202105170203444473789960713185708_0002_m_000018_19
[2021-05-16 23:03:54,012] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Task committer attempt_202105170203444473789960713185708_0002_m_000018_19: needsTaskCommit() Task attempt_202105170203444473789960713185708_0002_m_000018_19: duration 0:00.000s
[2021-05-16 23:03:54,013] {docker.py:276} INFO - 21/05/17 02:03:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444473789960713185708_0002_m_000018_19
[2021-05-16 23:03:54,013] {docker.py:276} INFO - 21/05/17 02:03:54 INFO Executor: Finished task 18.0 in stage 2.0 (TID 19). 4587 bytes result sent to driver
[2021-05-16 23:03:54,016] {docker.py:276} INFO - 21/05/17 02:03:54 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 23) (e60b22510068, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:54,017] {docker.py:276} INFO - 21/05/17 02:03:54 INFO Executor: Running task 22.0 in stage 2.0 (TID 23)
[2021-05-16 23:03:54,017] {docker.py:276} INFO - 21/05/17 02:03:54 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 19) in 1741 ms on e60b22510068 (executor driver) (16/200)
[2021-05-16 23:03:54,023] {docker.py:276} INFO - 21/05/17 02:03:54 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:54,023] {docker.py:276} INFO - 21/05/17 02:03:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:54,025] {docker.py:276} INFO - 21/05/17 02:03:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:03:54,026] {docker.py:276} INFO - 21/05/17 02:03:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:54,026] {docker.py:276} INFO - 21/05/17 02:03:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:54,027] {docker.py:276} INFO - 21/05/17 02:03:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447089342317234692562_0002_m_000021_22, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447089342317234692562_0002_m_000021_22}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447089342317234692562_0002}; taskId=attempt_202105170203447089342317234692562_0002_m_000021_22, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52ec2fcd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:54,027] {docker.py:276} INFO - 21/05/17 02:03:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:54,028] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203447089342317234692562_0002_m_000021_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447089342317234692562_0002_m_000021_22
[2021-05-16 23:03:54,030] {docker.py:276} INFO - 21/05/17 02:03:54 INFO ShuffleBlockFetcherIterator: Getting 3 (672.0 B) non-empty blocks including 3 (672.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:54,031] {docker.py:276} INFO - 21/05/17 02:03:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:03:54,034] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Task committer attempt_202105170203447089342317234692562_0002_m_000021_22: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447089342317234692562_0002_m_000021_22 : duration 0:00.007s
[2021-05-16 23:03:54,038] {docker.py:276} INFO - 21/05/17 02:03:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:54,038] {docker.py:276} INFO - 21/05/17 02:03:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:54,039] {docker.py:276} INFO - 21/05/17 02:03:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447724123567640140612_0002_m_000022_23, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447724123567640140612_0002_m_000022_23}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447724123567640140612_0002}; taskId=attempt_202105170203447724123567640140612_0002_m_000022_23, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71b0dee3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:54,040] {docker.py:276} INFO - 21/05/17 02:03:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:54,040] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203447724123567640140612_0002_m_000022_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447724123567640140612_0002_m_000022_23
[2021-05-16 23:03:54,048] {docker.py:276} INFO - 21/05/17 02:03:54 INFO StagingCommitter: Task committer attempt_202105170203447724123567640140612_0002_m_000022_23: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447724123567640140612_0002_m_000022_23 : duration 0:00.008s
[2021-05-16 23:03:55,538] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203446118451995445973155_0002_m_000019_20: needsTaskCommit() Task attempt_202105170203446118451995445973155_0002_m_000019_20
[2021-05-16 23:03:55,539] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203446118451995445973155_0002_m_000019_20: needsTaskCommit() Task attempt_202105170203446118451995445973155_0002_m_000019_20: duration 0:00.002s
21/05/17 02:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446118451995445973155_0002_m_000019_20
[2021-05-16 23:03:55,542] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Finished task 19.0 in stage 2.0 (TID 20). 4587 bytes result sent to driver
[2021-05-16 23:03:55,542] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 24) (e60b22510068, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:55,544] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Running task 23.0 in stage 2.0 (TID 24)
21/05/17 02:03:55 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 20) in 1709 ms on e60b22510068 (executor driver) (17/200)
[2021-05-16 23:03:55,554] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:55,556] {docker.py:276} INFO - 21/05/17 02:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:55,557] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,557] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445799184517920111064_0002_m_000023_24, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445799184517920111064_0002_m_000023_24}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445799184517920111064_0002}; taskId=attempt_202105170203445799184517920111064_0002_m_000023_24, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@249b2044}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,558] {docker.py:276} INFO - 21/05/17 02:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:55,558] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203445799184517920111064_0002_m_000023_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445799184517920111064_0002_m_000023_24
[2021-05-16 23:03:55,562] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203445799184517920111064_0002_m_000023_24: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445799184517920111064_0002_m_000023_24 : duration 0:00.005s
[2021-05-16 23:03:55,673] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203441626249153268863877_0002_m_000020_21: needsTaskCommit() Task attempt_202105170203441626249153268863877_0002_m_000020_21
[2021-05-16 23:03:55,674] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203441626249153268863877_0002_m_000020_21: needsTaskCommit() Task attempt_202105170203441626249153268863877_0002_m_000020_21: duration 0:00.002s
[2021-05-16 23:03:55,674] {docker.py:276} INFO - 21/05/17 02:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441626249153268863877_0002_m_000020_21
[2021-05-16 23:03:55,676] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Finished task 20.0 in stage 2.0 (TID 21). 4587 bytes result sent to driver
[2021-05-16 23:03:55,678] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 25) (e60b22510068, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:55,679] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Running task 24.0 in stage 2.0 (TID 25)
21/05/17 02:03:55 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 21) in 1749 ms on e60b22510068 (executor driver) (18/200)
[2021-05-16 23:03:55,690] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Getting 3 (645.0 B) non-empty blocks including 3 (645.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:55,692] {docker.py:276} INFO - 21/05/17 02:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:55,693] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444458910771368073829_0002_m_000024_25, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444458910771368073829_0002_m_000024_25}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444458910771368073829_0002}; taskId=attempt_202105170203444458910771368073829_0002_m_000024_25, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@711c8b93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:55,693] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203444458910771368073829_0002_m_000024_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444458910771368073829_0002_m_000024_25
[2021-05-16 23:03:55,697] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203444458910771368073829_0002_m_000024_25: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444458910771368073829_0002_m_000024_25 : duration 0:00.004s
[2021-05-16 23:03:55,729] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203447724123567640140612_0002_m_000022_23: needsTaskCommit() Task attempt_202105170203447724123567640140612_0002_m_000022_23
[2021-05-16 23:03:55,729] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203447724123567640140612_0002_m_000022_23: needsTaskCommit() Task attempt_202105170203447724123567640140612_0002_m_000022_23: duration 0:00.001s
21/05/17 02:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447724123567640140612_0002_m_000022_23
[2021-05-16 23:03:55,731] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Finished task 22.0 in stage 2.0 (TID 23). 4587 bytes result sent to driver
[2021-05-16 23:03:55,732] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 26) (e60b22510068, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:55,733] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 23) in 1720 ms on e60b22510068 (executor driver) (19/200)
[2021-05-16 23:03:55,734] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Running task 25.0 in stage 2.0 (TID 26)
[2021-05-16 23:03:55,740] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203447089342317234692562_0002_m_000021_22: needsTaskCommit() Task attempt_202105170203447089342317234692562_0002_m_000021_22
[2021-05-16 23:03:55,741] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203447089342317234692562_0002_m_000021_22: needsTaskCommit() Task attempt_202105170203447089342317234692562_0002_m_000021_22: duration 0:00.000s
[2021-05-16 23:03:55,741] {docker.py:276} INFO - 21/05/17 02:03:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447089342317234692562_0002_m_000021_22
[2021-05-16 23:03:55,742] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Finished task 21.0 in stage 2.0 (TID 22). 4587 bytes result sent to driver
[2021-05-16 23:03:55,744] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 27) (e60b22510068, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:55,744] {docker.py:276} INFO - 21/05/17 02:03:55 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 22) in 1743 ms on e60b22510068 (executor driver) (20/200)
[2021-05-16 23:03:55,745] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:55,746] {docker.py:276} INFO - 21/05/17 02:03:55 INFO Executor: Running task 26.0 in stage 2.0 (TID 27)
[2021-05-16 23:03:55,746] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:03:55,748] {docker.py:276} INFO - 21/05/17 02:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:55,749] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,750] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445555806665221590376_0002_m_000025_26, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445555806665221590376_0002_m_000025_26}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445555806665221590376_0002}; taskId=attempt_202105170203445555806665221590376_0002_m_000025_26, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34739024}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,751] {docker.py:276} INFO - 21/05/17 02:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:55,751] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203445555806665221590376_0002_m_000025_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445555806665221590376_0002_m_000025_26
[2021-05-16 23:03:55,759] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203445555806665221590376_0002_m_000025_26: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445555806665221590376_0002_m_000025_26 : duration 0:00.009s
[2021-05-16 23:03:55,774] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Getting 2 (435.0 B) non-empty blocks including 2 (435.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:55,775] {docker.py:276} INFO - 21/05/17 02:03:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2021-05-16 23:03:55,776] {docker.py:276} INFO - 21/05/17 02:03:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,777] {docker.py:276} INFO - 21/05/17 02:03:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445786722598060000161_0002_m_000026_27, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445786722598060000161_0002_m_000026_27}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445786722598060000161_0002}; taskId=attempt_202105170203445786722598060000161_0002_m_000026_27, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16c01b35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:55,777] {docker.py:276} INFO - 21/05/17 02:03:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203445786722598060000161_0002_m_000026_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445786722598060000161_0002_m_000026_27
[2021-05-16 23:03:55,781] {docker.py:276} INFO - 21/05/17 02:03:55 INFO StagingCommitter: Task committer attempt_202105170203445786722598060000161_0002_m_000026_27: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445786722598060000161_0002_m_000026_27 : duration 0:00.004s
[2021-05-16 23:03:57,283] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203445799184517920111064_0002_m_000023_24: needsTaskCommit() Task attempt_202105170203445799184517920111064_0002_m_000023_24
[2021-05-16 23:03:57,284] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203445799184517920111064_0002_m_000023_24: needsTaskCommit() Task attempt_202105170203445799184517920111064_0002_m_000023_24: duration 0:00.001s
[2021-05-16 23:03:57,285] {docker.py:276} INFO - 21/05/17 02:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445799184517920111064_0002_m_000023_24
[2021-05-16 23:03:57,287] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Finished task 23.0 in stage 2.0 (TID 24). 4544 bytes result sent to driver
[2021-05-16 23:03:57,289] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 28) (e60b22510068, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:57,290] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 24) in 1715 ms on e60b22510068 (executor driver) (21/200)
[2021-05-16 23:03:57,292] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Running task 27.0 in stage 2.0 (TID 28)
[2021-05-16 23:03:57,313] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Getting 1 (171.0 B) non-empty blocks including 1 (171.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:57,314] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:57,316] {docker.py:276} INFO - 21/05/17 02:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:57,316] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:57,316] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442996613718676926661_0002_m_000027_28, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442996613718676926661_0002_m_000027_28}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442996613718676926661_0002}; taskId=attempt_202105170203442996613718676926661_0002_m_000027_28, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fe430f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:57,317] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203442996613718676926661_0002_m_000027_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442996613718676926661_0002_m_000027_28
[2021-05-16 23:03:57,321] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203442996613718676926661_0002_m_000027_28: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442996613718676926661_0002_m_000027_28 : duration 0:00.005s
[2021-05-16 23:03:57,331] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203444458910771368073829_0002_m_000024_25: needsTaskCommit() Task attempt_202105170203444458910771368073829_0002_m_000024_25
[2021-05-16 23:03:57,332] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203444458910771368073829_0002_m_000024_25: needsTaskCommit() Task attempt_202105170203444458910771368073829_0002_m_000024_25: duration 0:00.001s
21/05/17 02:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444458910771368073829_0002_m_000024_25
[2021-05-16 23:03:57,333] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Finished task 24.0 in stage 2.0 (TID 25). 4587 bytes result sent to driver
[2021-05-16 23:03:57,335] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 29) (e60b22510068, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:57,336] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 25) in 1626 ms on e60b22510068 (executor driver) (22/200)
[2021-05-16 23:03:57,337] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Running task 28.0 in stage 2.0 (TID 29)
[2021-05-16 23:03:57,346] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:57,348] {docker.py:276} INFO - 21/05/17 02:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:57,348] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344296441718485094732_0002_m_000028_29, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344296441718485094732_0002_m_000028_29}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344296441718485094732_0002}; taskId=attempt_20210517020344296441718485094732_0002_m_000028_29, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f30817e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_20210517020344296441718485094732_0002_m_000028_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344296441718485094732_0002_m_000028_29
[2021-05-16 23:03:57,352] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_20210517020344296441718485094732_0002_m_000028_29: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344296441718485094732_0002_m_000028_29 : duration 0:00.003s
[2021-05-16 23:03:57,443] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203445786722598060000161_0002_m_000026_27: needsTaskCommit() Task attempt_202105170203445786722598060000161_0002_m_000026_27
[2021-05-16 23:03:57,444] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203445786722598060000161_0002_m_000026_27: needsTaskCommit() Task attempt_202105170203445786722598060000161_0002_m_000026_27: duration 0:00.001s
[2021-05-16 23:03:57,444] {docker.py:276} INFO - 21/05/17 02:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445786722598060000161_0002_m_000026_27
[2021-05-16 23:03:57,445] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Finished task 26.0 in stage 2.0 (TID 27). 4587 bytes result sent to driver
[2021-05-16 23:03:57,447] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 30) (e60b22510068, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:57,449] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 27) in 1672 ms on e60b22510068 (executor driver) (23/200)
[2021-05-16 23:03:57,450] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Running task 29.0 in stage 2.0 (TID 30)
[2021-05-16 23:03:57,458] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203445555806665221590376_0002_m_000025_26: needsTaskCommit() Task attempt_202105170203445555806665221590376_0002_m_000025_26
[2021-05-16 23:03:57,459] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203445555806665221590376_0002_m_000025_26: needsTaskCommit() Task attempt_202105170203445555806665221590376_0002_m_000025_26: duration 0:00.000s
[2021-05-16 23:03:57,459] {docker.py:276} INFO - 21/05/17 02:03:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445555806665221590376_0002_m_000025_26
[2021-05-16 23:03:57,460] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:57,461] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Finished task 25.0 in stage 2.0 (TID 26). 4587 bytes result sent to driver
[2021-05-16 23:03:57,462] {docker.py:276} INFO - 21/05/17 02:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:57,462] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203449155730000852673440_0002_m_000029_30, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449155730000852673440_0002_m_000029_30}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203449155730000852673440_0002}; taskId=attempt_202105170203449155730000852673440_0002_m_000029_30, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73fd710}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:57,463] {docker.py:276} INFO - 21/05/17 02:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203449155730000852673440_0002_m_000029_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449155730000852673440_0002_m_000029_30
[2021-05-16 23:03:57,464] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 31) (e60b22510068, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:57,465] {docker.py:276} INFO - 21/05/17 02:03:57 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 26) in 1701 ms on e60b22510068 (executor driver) (24/200)
[2021-05-16 23:03:57,466] {docker.py:276} INFO - 21/05/17 02:03:57 INFO Executor: Running task 30.0 in stage 2.0 (TID 31)
[2021-05-16 23:03:57,468] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203449155730000852673440_0002_m_000029_30: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449155730000852673440_0002_m_000029_30 : duration 0:00.006s
[2021-05-16 23:03:57,478] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:57,481] {docker.py:276} INFO - 21/05/17 02:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:57,483] {docker.py:276} INFO - 21/05/17 02:03:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:57,485] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:57,489] {docker.py:276} INFO - 21/05/17 02:03:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446935367096376058214_0002_m_000030_31, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935367096376058214_0002_m_000030_31}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446935367096376058214_0002}; taskId=attempt_202105170203446935367096376058214_0002_m_000030_31, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@171d7ff5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203446935367096376058214_0002_m_000030_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935367096376058214_0002_m_000030_31
[2021-05-16 23:03:57,492] {docker.py:276} INFO - 21/05/17 02:03:57 INFO StagingCommitter: Task committer attempt_202105170203446935367096376058214_0002_m_000030_31: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935367096376058214_0002_m_000030_31 : duration 0:00.002s
[2021-05-16 23:03:58,974] {docker.py:276} INFO - 21/05/17 02:03:58 INFO StagingCommitter: Starting: Task committer attempt_202105170203442996613718676926661_0002_m_000027_28: needsTaskCommit() Task attempt_202105170203442996613718676926661_0002_m_000027_28
21/05/17 02:03:58 INFO StagingCommitter: Task committer attempt_202105170203442996613718676926661_0002_m_000027_28: needsTaskCommit() Task attempt_202105170203442996613718676926661_0002_m_000027_28: duration 0:00.002s
[2021-05-16 23:03:58,974] {docker.py:276} INFO - 21/05/17 02:03:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442996613718676926661_0002_m_000027_28
[2021-05-16 23:03:58,976] {docker.py:276} INFO - 21/05/17 02:03:58 INFO Executor: Finished task 27.0 in stage 2.0 (TID 28). 4587 bytes result sent to driver
[2021-05-16 23:03:58,978] {docker.py:276} INFO - 21/05/17 02:03:58 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 32) (e60b22510068, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:58,979] {docker.py:276} INFO - 21/05/17 02:03:58 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 28) in 1694 ms on e60b22510068 (executor driver) (25/200)
[2021-05-16 23:03:58,980] {docker.py:276} INFO - 21/05/17 02:03:58 INFO Executor: Running task 31.0 in stage 2.0 (TID 32)
[2021-05-16 23:03:58,990] {docker.py:276} INFO - 21/05/17 02:03:58 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:58,992] {docker.py:276} INFO - 21/05/17 02:03:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:58,993] {docker.py:276} INFO - 21/05/17 02:03:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344918166182258631298_0002_m_000031_32, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344918166182258631298_0002_m_000031_32}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344918166182258631298_0002}; taskId=attempt_20210517020344918166182258631298_0002_m_000031_32, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a98aa9a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:58 INFO StagingCommitter: Starting: Task committer attempt_20210517020344918166182258631298_0002_m_000031_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344918166182258631298_0002_m_000031_32
[2021-05-16 23:03:58,995] {docker.py:276} INFO - 21/05/17 02:03:58 INFO StagingCommitter: Task committer attempt_20210517020344918166182258631298_0002_m_000031_32: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344918166182258631298_0002_m_000031_32 : duration 0:00.003s
[2021-05-16 23:03:59,016] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_20210517020344296441718485094732_0002_m_000028_29: needsTaskCommit() Task attempt_20210517020344296441718485094732_0002_m_000028_29
[2021-05-16 23:03:59,017] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_20210517020344296441718485094732_0002_m_000028_29: needsTaskCommit() Task attempt_20210517020344296441718485094732_0002_m_000028_29: duration 0:00.002s
[2021-05-16 23:03:59,018] {docker.py:276} INFO - 21/05/17 02:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344296441718485094732_0002_m_000028_29
[2021-05-16 23:03:59,020] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Finished task 28.0 in stage 2.0 (TID 29). 4544 bytes result sent to driver
[2021-05-16 23:03:59,022] {docker.py:276} INFO - 21/05/17 02:03:59 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 33) (e60b22510068, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:59,023] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Running task 32.0 in stage 2.0 (TID 33)
21/05/17 02:03:59 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 29) in 1690 ms on e60b22510068 (executor driver) (26/200)
[2021-05-16 23:03:59,039] {docker.py:276} INFO - 21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:03:59,040] {docker.py:276} INFO - 21/05/17 02:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:59,041] {docker.py:276} INFO - 21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447083873556336599575_0002_m_000032_33, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447083873556336599575_0002_m_000032_33}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447083873556336599575_0002}; taskId=attempt_202105170203447083873556336599575_0002_m_000032_33, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f03b1ab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203447083873556336599575_0002_m_000032_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447083873556336599575_0002_m_000032_33
[2021-05-16 23:03:59,044] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_202105170203447083873556336599575_0002_m_000032_33: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447083873556336599575_0002_m_000032_33 : duration 0:00.003s
[2021-05-16 23:03:59,146] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203449155730000852673440_0002_m_000029_30: needsTaskCommit() Task attempt_202105170203449155730000852673440_0002_m_000029_30
[2021-05-16 23:03:59,147] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_202105170203449155730000852673440_0002_m_000029_30: needsTaskCommit() Task attempt_202105170203449155730000852673440_0002_m_000029_30: duration 0:00.003s
21/05/17 02:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203449155730000852673440_0002_m_000029_30
[2021-05-16 23:03:59,151] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Finished task 29.0 in stage 2.0 (TID 30). 4587 bytes result sent to driver
[2021-05-16 23:03:59,152] {docker.py:276} INFO - 21/05/17 02:03:59 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 34) (e60b22510068, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:59,154] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Running task 33.0 in stage 2.0 (TID 34)
[2021-05-16 23:03:59,155] {docker.py:276} INFO - 21/05/17 02:03:59 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 30) in 1709 ms on e60b22510068 (executor driver) (27/200)
[2021-05-16 23:03:59,169] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203446935367096376058214_0002_m_000030_31: needsTaskCommit() Task attempt_202105170203446935367096376058214_0002_m_000030_31
[2021-05-16 23:03:59,170] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_202105170203446935367096376058214_0002_m_000030_31: needsTaskCommit() Task attempt_202105170203446935367096376058214_0002_m_000030_31: duration 0:00.000s
21/05/17 02:03:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446935367096376058214_0002_m_000030_31
[2021-05-16 23:03:59,171] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Finished task 30.0 in stage 2.0 (TID 31). 4587 bytes result sent to driver
[2021-05-16 23:03:59,174] {docker.py:276} INFO - 21/05/17 02:03:59 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 35) (e60b22510068, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:03:59,176] {docker.py:276} INFO - 21/05/17 02:03:59 INFO Executor: Running task 34.0 in stage 2.0 (TID 35)
21/05/17 02:03:59 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 31) in 1716 ms on e60b22510068 (executor driver) (28/200)
[2021-05-16 23:03:59,178] {docker.py:276} INFO - 21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:03:59,178] {docker.py:276} INFO - 21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-16 23:03:59,180] {docker.py:276} INFO - 21/05/17 02:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:03:59,181] {docker.py:276} INFO - 21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:59,181] {docker.py:276} INFO - 21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444983334470134161074_0002_m_000033_34, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444983334470134161074_0002_m_000033_34}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444983334470134161074_0002}; taskId=attempt_202105170203444983334470134161074_0002_m_000033_34, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20f8c328}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:59,182] {docker.py:276} INFO - 21/05/17 02:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:03:59,182] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203444983334470134161074_0002_m_000033_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444983334470134161074_0002_m_000033_34
[2021-05-16 23:03:59,185] {docker.py:276} INFO - 21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:03:59,187] {docker.py:276} INFO - 21/05/17 02:03:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:03:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_202105170203444983334470134161074_0002_m_000033_34: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444983334470134161074_0002_m_000033_34 : duration 0:00.005s
[2021-05-16 23:03:59,188] {docker.py:276} INFO - 21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:03:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443067314730306388137_0002_m_000034_35, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443067314730306388137_0002_m_000034_35}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443067314730306388137_0002}; taskId=attempt_202105170203443067314730306388137_0002_m_000034_35, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23983cfd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:03:59,188] {docker.py:276} INFO - 21/05/17 02:03:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:03:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203443067314730306388137_0002_m_000034_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443067314730306388137_0002_m_000034_35
[2021-05-16 23:03:59,191] {docker.py:276} INFO - 21/05/17 02:03:59 INFO StagingCommitter: Task committer attempt_202105170203443067314730306388137_0002_m_000034_35: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443067314730306388137_0002_m_000034_35 : duration 0:00.003s
[2021-05-16 23:04:00,698] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_20210517020344918166182258631298_0002_m_000031_32: needsTaskCommit() Task attempt_20210517020344918166182258631298_0002_m_000031_32
[2021-05-16 23:04:00,699] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_20210517020344918166182258631298_0002_m_000031_32: needsTaskCommit() Task attempt_20210517020344918166182258631298_0002_m_000031_32: duration 0:00.001s
[2021-05-16 23:04:00,699] {docker.py:276} INFO - 21/05/17 02:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344918166182258631298_0002_m_000031_32
[2021-05-16 23:04:00,700] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Finished task 31.0 in stage 2.0 (TID 32). 4587 bytes result sent to driver
[2021-05-16 23:04:00,702] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 36) (e60b22510068, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:00,703] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 32) in 1728 ms on e60b22510068 (executor driver) (29/200)
[2021-05-16 23:04:00,704] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Running task 35.0 in stage 2.0 (TID 36)
[2021-05-16 23:04:00,713] {docker.py:276} INFO - 21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:00,715] {docker.py:276} INFO - 21/05/17 02:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:00,715] {docker.py:276} INFO - 21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444066889434745935014_0002_m_000035_36, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444066889434745935014_0002_m_000035_36}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444066889434745935014_0002}; taskId=attempt_202105170203444066889434745935014_0002_m_000035_36, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c951a2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203444066889434745935014_0002_m_000035_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444066889434745935014_0002_m_000035_36
[2021-05-16 23:04:00,718] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203444066889434745935014_0002_m_000035_36: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444066889434745935014_0002_m_000035_36 : duration 0:00.003s
[2021-05-16 23:04:00,756] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203447083873556336599575_0002_m_000032_33: needsTaskCommit() Task attempt_202105170203447083873556336599575_0002_m_000032_33
[2021-05-16 23:04:00,757] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203447083873556336599575_0002_m_000032_33: needsTaskCommit() Task attempt_202105170203447083873556336599575_0002_m_000032_33: duration 0:00.001s
21/05/17 02:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447083873556336599575_0002_m_000032_33
[2021-05-16 23:04:00,758] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Finished task 32.0 in stage 2.0 (TID 33). 4587 bytes result sent to driver
[2021-05-16 23:04:00,760] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 37) (e60b22510068, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:00,760] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Running task 36.0 in stage 2.0 (TID 37)
[2021-05-16 23:04:00,761] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 33) in 1742 ms on e60b22510068 (executor driver) (30/200)
[2021-05-16 23:04:00,770] {docker.py:276} INFO - 21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Getting 2 (435.0 B) non-empty blocks including 2 (435.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:00,770] {docker.py:276} INFO - 21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:00,773] {docker.py:276} INFO - 21/05/17 02:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:00,774] {docker.py:276} INFO - 21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344450244770395595063_0002_m_000036_37, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344450244770395595063_0002_m_000036_37}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344450244770395595063_0002}; taskId=attempt_20210517020344450244770395595063_0002_m_000036_37, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@650a56a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:00,774] {docker.py:276} INFO - 21/05/17 02:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:00,774] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_20210517020344450244770395595063_0002_m_000036_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344450244770395595063_0002_m_000036_37
[2021-05-16 23:04:00,776] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_20210517020344450244770395595063_0002_m_000036_37: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344450244770395595063_0002_m_000036_37 : duration 0:00.003s
[2021-05-16 23:04:00,837] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203444983334470134161074_0002_m_000033_34: needsTaskCommit() Task attempt_202105170203444983334470134161074_0002_m_000033_34
[2021-05-16 23:04:00,838] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203444983334470134161074_0002_m_000033_34: needsTaskCommit() Task attempt_202105170203444983334470134161074_0002_m_000033_34: duration 0:00.001s
21/05/17 02:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444983334470134161074_0002_m_000033_34
[2021-05-16 23:04:00,839] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Finished task 33.0 in stage 2.0 (TID 34). 4587 bytes result sent to driver
[2021-05-16 23:04:00,840] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 38) (e60b22510068, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:00,842] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Running task 37.0 in stage 2.0 (TID 38)
21/05/17 02:04:00 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 34) in 1692 ms on e60b22510068 (executor driver) (31/200)
[2021-05-16 23:04:00,850] {docker.py:276} INFO - 21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:00,853] {docker.py:276} INFO - 21/05/17 02:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:00,854] {docker.py:276} INFO - 21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445088395498924378355_0002_m_000037_38, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445088395498924378355_0002_m_000037_38}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445088395498924378355_0002}; taskId=attempt_202105170203445088395498924378355_0002_m_000037_38, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f6188d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203445088395498924378355_0002_m_000037_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445088395498924378355_0002_m_000037_38
[2021-05-16 23:04:00,858] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203445088395498924378355_0002_m_000037_38: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445088395498924378355_0002_m_000037_38 : duration 0:00.004s
[2021-05-16 23:04:00,919] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203443067314730306388137_0002_m_000034_35: needsTaskCommit() Task attempt_202105170203443067314730306388137_0002_m_000034_35
[2021-05-16 23:04:00,920] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203443067314730306388137_0002_m_000034_35: needsTaskCommit() Task attempt_202105170203443067314730306388137_0002_m_000034_35: duration 0:00.001s
[2021-05-16 23:04:00,920] {docker.py:276} INFO - 21/05/17 02:04:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443067314730306388137_0002_m_000034_35
[2021-05-16 23:04:00,922] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Finished task 34.0 in stage 2.0 (TID 35). 4544 bytes result sent to driver
[2021-05-16 23:04:00,923] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 39) (e60b22510068, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:00,924] {docker.py:276} INFO - 21/05/17 02:04:00 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 35) in 1752 ms on e60b22510068 (executor driver) (32/200)
[2021-05-16 23:04:00,925] {docker.py:276} INFO - 21/05/17 02:04:00 INFO Executor: Running task 38.0 in stage 2.0 (TID 39)
[2021-05-16 23:04:00,933] {docker.py:276} INFO - 21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:00,935] {docker.py:276} INFO - 21/05/17 02:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448481303187698579605_0002_m_000038_39, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448481303187698579605_0002_m_000038_39}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448481303187698579605_0002}; taskId=attempt_202105170203448481303187698579605_0002_m_000038_39, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bf4849c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:00,936] {docker.py:276} INFO - 21/05/17 02:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203448481303187698579605_0002_m_000038_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448481303187698579605_0002_m_000038_39
[2021-05-16 23:04:00,938] {docker.py:276} INFO - 21/05/17 02:04:00 INFO StagingCommitter: Task committer attempt_202105170203448481303187698579605_0002_m_000038_39: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448481303187698579605_0002_m_000038_39 : duration 0:00.002s
[2021-05-16 23:04:02,454] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_20210517020344450244770395595063_0002_m_000036_37: needsTaskCommit() Task attempt_20210517020344450244770395595063_0002_m_000036_37
[2021-05-16 23:04:02,455] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_20210517020344450244770395595063_0002_m_000036_37: needsTaskCommit() Task attempt_20210517020344450244770395595063_0002_m_000036_37: duration 0:00.002s
[2021-05-16 23:04:02,456] {docker.py:276} INFO - 21/05/17 02:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344450244770395595063_0002_m_000036_37
[2021-05-16 23:04:02,458] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Finished task 36.0 in stage 2.0 (TID 37). 4587 bytes result sent to driver
[2021-05-16 23:04:02,460] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 40) (e60b22510068, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:02,461] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Running task 39.0 in stage 2.0 (TID 40)
[2021-05-16 23:04:02,462] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 37) in 1704 ms on e60b22510068 (executor driver) (33/200)
[2021-05-16 23:04:02,475] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203444066889434745935014_0002_m_000035_36: needsTaskCommit() Task attempt_202105170203444066889434745935014_0002_m_000035_36
[2021-05-16 23:04:02,475] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203444066889434745935014_0002_m_000035_36: needsTaskCommit() Task attempt_202105170203444066889434745935014_0002_m_000035_36: duration 0:00.001s
21/05/17 02:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444066889434745935014_0002_m_000035_36
[2021-05-16 23:04:02,476] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Finished task 35.0 in stage 2.0 (TID 36). 4587 bytes result sent to driver
[2021-05-16 23:04:02,478] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 41) (e60b22510068, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:02,479] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 36) in 1780 ms on e60b22510068 (executor driver) (34/200)
[2021-05-16 23:04:02,480] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Running task 40.0 in stage 2.0 (TID 41)
[2021-05-16 23:04:02,480] {docker.py:276} INFO - 21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:02,482] {docker.py:276} INFO - 21/05/17 02:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:02,483] {docker.py:276} INFO - 21/05/17 02:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:02,483] {docker.py:276} INFO - 21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:02,484] {docker.py:276} INFO - 21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446360982657636052341_0002_m_000039_40, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446360982657636052341_0002_m_000039_40}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446360982657636052341_0002}; taskId=attempt_202105170203446360982657636052341_0002_m_000039_40, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cd5f423}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:02,484] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203446360982657636052341_0002_m_000039_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446360982657636052341_0002_m_000039_40
[2021-05-16 23:04:02,487] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203446360982657636052341_0002_m_000039_40: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446360982657636052341_0002_m_000039_40 : duration 0:00.003s
[2021-05-16 23:04:02,494] {docker.py:276} INFO - 21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-16 23:04:02,499] {docker.py:276} INFO - 21/05/17 02:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:02,500] {docker.py:276} INFO - 21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446464979678616871427_0002_m_000040_41, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446464979678616871427_0002_m_000040_41}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446464979678616871427_0002}; taskId=attempt_202105170203446464979678616871427_0002_m_000040_41, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cfc2570}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:02,500] {docker.py:276} INFO - 21/05/17 02:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203446464979678616871427_0002_m_000040_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446464979678616871427_0002_m_000040_41
[2021-05-16 23:04:02,502] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203446464979678616871427_0002_m_000040_41: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446464979678616871427_0002_m_000040_41 : duration 0:00.003s
[2021-05-16 23:04:02,535] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203445088395498924378355_0002_m_000037_38: needsTaskCommit() Task attempt_202105170203445088395498924378355_0002_m_000037_38
[2021-05-16 23:04:02,536] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203445088395498924378355_0002_m_000037_38: needsTaskCommit() Task attempt_202105170203445088395498924378355_0002_m_000037_38: duration 0:00.001s
21/05/17 02:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445088395498924378355_0002_m_000037_38
[2021-05-16 23:04:02,539] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Finished task 37.0 in stage 2.0 (TID 38). 4587 bytes result sent to driver
[2021-05-16 23:04:02,540] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 42) (e60b22510068, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:02,542] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 38) in 1703 ms on e60b22510068 (executor driver) (35/200)
[2021-05-16 23:04:02,542] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Running task 41.0 in stage 2.0 (TID 42)
[2021-05-16 23:04:02,552] {docker.py:276} INFO - 21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:02,554] {docker.py:276} INFO - 21/05/17 02:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445932741996032340808_0002_m_000041_42, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445932741996032340808_0002_m_000041_42}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445932741996032340808_0002}; taskId=attempt_202105170203445932741996032340808_0002_m_000041_42, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e3afbc6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:02,555] {docker.py:276} INFO - 21/05/17 02:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203445932741996032340808_0002_m_000041_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445932741996032340808_0002_m_000041_42
[2021-05-16 23:04:02,557] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203445932741996032340808_0002_m_000041_42: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445932741996032340808_0002_m_000041_42 : duration 0:00.003s
[2021-05-16 23:04:02,642] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203448481303187698579605_0002_m_000038_39: needsTaskCommit() Task attempt_202105170203448481303187698579605_0002_m_000038_39
[2021-05-16 23:04:02,643] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203448481303187698579605_0002_m_000038_39: needsTaskCommit() Task attempt_202105170203448481303187698579605_0002_m_000038_39: duration 0:00.000s
21/05/17 02:04:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448481303187698579605_0002_m_000038_39
[2021-05-16 23:04:02,644] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Finished task 38.0 in stage 2.0 (TID 39). 4587 bytes result sent to driver
[2021-05-16 23:04:02,646] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 43) (e60b22510068, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:02,647] {docker.py:276} INFO - 21/05/17 02:04:02 INFO Executor: Running task 42.0 in stage 2.0 (TID 43)
[2021-05-16 23:04:02,648] {docker.py:276} INFO - 21/05/17 02:04:02 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 39) in 1727 ms on e60b22510068 (executor driver) (36/200)
[2021-05-16 23:04:02,656] {docker.py:276} INFO - 21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:02,658] {docker.py:276} INFO - 21/05/17 02:04:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:02,658] {docker.py:276} INFO - 21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445618775558222046684_0002_m_000042_43, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445618775558222046684_0002_m_000042_43}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445618775558222046684_0002}; taskId=attempt_202105170203445618775558222046684_0002_m_000042_43, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ce6130b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203445618775558222046684_0002_m_000042_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445618775558222046684_0002_m_000042_43
[2021-05-16 23:04:02,661] {docker.py:276} INFO - 21/05/17 02:04:02 INFO StagingCommitter: Task committer attempt_202105170203445618775558222046684_0002_m_000042_43: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445618775558222046684_0002_m_000042_43 : duration 0:00.003s
[2021-05-16 23:04:04,143] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203446360982657636052341_0002_m_000039_40: needsTaskCommit() Task attempt_202105170203446360982657636052341_0002_m_000039_40
[2021-05-16 23:04:04,144] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203446360982657636052341_0002_m_000039_40: needsTaskCommit() Task attempt_202105170203446360982657636052341_0002_m_000039_40: duration 0:00.002s
[2021-05-16 23:04:04,144] {docker.py:276} INFO - 21/05/17 02:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446360982657636052341_0002_m_000039_40
[2021-05-16 23:04:04,147] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Finished task 39.0 in stage 2.0 (TID 40). 4544 bytes result sent to driver
[2021-05-16 23:04:04,149] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 44) (e60b22510068, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:04,150] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Running task 43.0 in stage 2.0 (TID 44)
[2021-05-16 23:04:04,151] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 40) in 1693 ms on e60b22510068 (executor driver) (37/200)
[2021-05-16 23:04:04,171] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Getting 2 (399.0 B) non-empty blocks including 2 (399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:04,171] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:04,173] {docker.py:276} INFO - 21/05/17 02:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:04,173] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447463907270189739220_0002_m_000043_44, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447463907270189739220_0002_m_000043_44}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447463907270189739220_0002}; taskId=attempt_202105170203447463907270189739220_0002_m_000043_44, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@390f6817}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:04,174] {docker.py:276} INFO - 21/05/17 02:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203447463907270189739220_0002_m_000043_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447463907270189739220_0002_m_000043_44
[2021-05-16 23:04:04,177] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203447463907270189739220_0002_m_000043_44: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447463907270189739220_0002_m_000043_44 : duration 0:00.002s
[2021-05-16 23:04:04,218] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203445932741996032340808_0002_m_000041_42: needsTaskCommit() Task attempt_202105170203445932741996032340808_0002_m_000041_42
[2021-05-16 23:04:04,218] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203445932741996032340808_0002_m_000041_42: needsTaskCommit() Task attempt_202105170203445932741996032340808_0002_m_000041_42: duration 0:00.000s
21/05/17 02:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445932741996032340808_0002_m_000041_42
[2021-05-16 23:04:04,220] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Finished task 41.0 in stage 2.0 (TID 42). 4587 bytes result sent to driver
[2021-05-16 23:04:04,221] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 45) (e60b22510068, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:04,222] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Running task 44.0 in stage 2.0 (TID 45)
[2021-05-16 23:04:04,222] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 42) in 1685 ms on e60b22510068 (executor driver) (38/200)
[2021-05-16 23:04:04,228] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203446464979678616871427_0002_m_000040_41: needsTaskCommit() Task attempt_202105170203446464979678616871427_0002_m_000040_41
[2021-05-16 23:04:04,229] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203446464979678616871427_0002_m_000040_41: needsTaskCommit() Task attempt_202105170203446464979678616871427_0002_m_000040_41: duration 0:00.001s
21/05/17 02:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446464979678616871427_0002_m_000040_41
[2021-05-16 23:04:04,230] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Finished task 40.0 in stage 2.0 (TID 41). 4587 bytes result sent to driver
[2021-05-16 23:04:04,231] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 46) (e60b22510068, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:04,232] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Running task 45.0 in stage 2.0 (TID 46)
[2021-05-16 23:04:04,233] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 41) in 1757 ms on e60b22510068 (executor driver) (39/200)
[2021-05-16 23:04:04,237] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:04,237] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:04,240] {docker.py:276} INFO - 21/05/17 02:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:04,240] {docker.py:276} INFO - 21/05/17 02:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:04,240] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:04,241] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442066229508291246463_0002_m_000044_45, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442066229508291246463_0002_m_000044_45}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442066229508291246463_0002}; taskId=attempt_202105170203442066229508291246463_0002_m_000044_45, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5feca9e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:04,241] {docker.py:276} INFO - 21/05/17 02:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:04,242] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203442066229508291246463_0002_m_000044_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442066229508291246463_0002_m_000044_45
[2021-05-16 23:04:04,242] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:04,243] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:04,244] {docker.py:276} INFO - 21/05/17 02:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:04,245] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:04,245] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442815861445050283982_0002_m_000045_46, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442815861445050283982_0002_m_000045_46}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442815861445050283982_0002}; taskId=attempt_202105170203442815861445050283982_0002_m_000045_46, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f2c0ffe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:04,246] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203442815861445050283982_0002_m_000045_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442815861445050283982_0002_m_000045_46
[2021-05-16 23:04:04,247] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203442066229508291246463_0002_m_000044_45: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442066229508291246463_0002_m_000044_45 : duration 0:00.004s
[2021-05-16 23:04:04,248] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203442815861445050283982_0002_m_000045_46: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442815861445050283982_0002_m_000045_46 : duration 0:00.003s
[2021-05-16 23:04:04,349] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203445618775558222046684_0002_m_000042_43: needsTaskCommit() Task attempt_202105170203445618775558222046684_0002_m_000042_43
[2021-05-16 23:04:04,351] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203445618775558222046684_0002_m_000042_43: needsTaskCommit() Task attempt_202105170203445618775558222046684_0002_m_000042_43: duration 0:00.001s
21/05/17 02:04:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445618775558222046684_0002_m_000042_43
[2021-05-16 23:04:04,353] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Finished task 42.0 in stage 2.0 (TID 43). 4587 bytes result sent to driver
[2021-05-16 23:04:04,355] {docker.py:276} INFO - 21/05/17 02:04:04 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 47) (e60b22510068, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:04,358] {docker.py:276} INFO - 21/05/17 02:04:04 INFO Executor: Running task 46.0 in stage 2.0 (TID 47)
21/05/17 02:04:04 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 43) in 1713 ms on e60b22510068 (executor driver) (40/200)
[2021-05-16 23:04:04,367] {docker.py:276} INFO - 21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:04,369] {docker.py:276} INFO - 21/05/17 02:04:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:04,369] {docker.py:276} INFO - 21/05/17 02:04:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445045515160701712952_0002_m_000046_47, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445045515160701712952_0002_m_000046_47}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445045515160701712952_0002}; taskId=attempt_202105170203445045515160701712952_0002_m_000046_47, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1496e44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203445045515160701712952_0002_m_000046_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445045515160701712952_0002_m_000046_47
[2021-05-16 23:04:04,372] {docker.py:276} INFO - 21/05/17 02:04:04 INFO StagingCommitter: Task committer attempt_202105170203445045515160701712952_0002_m_000046_47: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445045515160701712952_0002_m_000046_47 : duration 0:00.003s
[2021-05-16 23:04:05,832] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203447463907270189739220_0002_m_000043_44: needsTaskCommit() Task attempt_202105170203447463907270189739220_0002_m_000043_44
[2021-05-16 23:04:05,833] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_202105170203447463907270189739220_0002_m_000043_44: needsTaskCommit() Task attempt_202105170203447463907270189739220_0002_m_000043_44: duration 0:00.002s
21/05/17 02:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447463907270189739220_0002_m_000043_44
[2021-05-16 23:04:05,835] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Finished task 43.0 in stage 2.0 (TID 44). 4587 bytes result sent to driver
[2021-05-16 23:04:05,837] {docker.py:276} INFO - 21/05/17 02:04:05 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 48) (e60b22510068, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:05,838] {docker.py:276} INFO - 21/05/17 02:04:05 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 44) in 1692 ms on e60b22510068 (executor driver) (41/200)
[2021-05-16 23:04:05,839] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Running task 47.0 in stage 2.0 (TID 48)
[2021-05-16 23:04:05,849] {docker.py:276} INFO - 21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:05,851] {docker.py:276} INFO - 21/05/17 02:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444975908501109468323_0002_m_000047_48, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444975908501109468323_0002_m_000047_48}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444975908501109468323_0002}; taskId=attempt_202105170203444975908501109468323_0002_m_000047_48, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b8331c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:05,852] {docker.py:276} INFO - 21/05/17 02:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203444975908501109468323_0002_m_000047_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444975908501109468323_0002_m_000047_48
[2021-05-16 23:04:05,854] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_202105170203444975908501109468323_0002_m_000047_48: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444975908501109468323_0002_m_000047_48 : duration 0:00.003s
[2021-05-16 23:04:05,899] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203442066229508291246463_0002_m_000044_45: needsTaskCommit() Task attempt_202105170203442066229508291246463_0002_m_000044_45
[2021-05-16 23:04:05,900] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_202105170203442066229508291246463_0002_m_000044_45: needsTaskCommit() Task attempt_202105170203442066229508291246463_0002_m_000044_45: duration 0:00.001s
21/05/17 02:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442066229508291246463_0002_m_000044_45
[2021-05-16 23:04:05,901] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Finished task 44.0 in stage 2.0 (TID 45). 4544 bytes result sent to driver
[2021-05-16 23:04:05,903] {docker.py:276} INFO - 21/05/17 02:04:05 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 49) (e60b22510068, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:05,904] {docker.py:276} INFO - 21/05/17 02:04:05 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 45) in 1685 ms on e60b22510068 (executor driver) (42/200)
[2021-05-16 23:04:05,905] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Running task 48.0 in stage 2.0 (TID 49)
[2021-05-16 23:04:05,915] {docker.py:276} INFO - 21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:05,916] {docker.py:276} INFO - 21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:05,917] {docker.py:276} INFO - 21/05/17 02:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:05,918] {docker.py:276} INFO - 21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:05,918] {docker.py:276} INFO - 21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344987160886677942630_0002_m_000048_49, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344987160886677942630_0002_m_000048_49}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344987160886677942630_0002}; taskId=attempt_20210517020344987160886677942630_0002_m_000048_49, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74bd9092}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:05,919] {docker.py:276} INFO - 21/05/17 02:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:05,919] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_20210517020344987160886677942630_0002_m_000048_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344987160886677942630_0002_m_000048_49
[2021-05-16 23:04:05,922] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_20210517020344987160886677942630_0002_m_000048_49: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344987160886677942630_0002_m_000048_49 : duration 0:00.003s
[2021-05-16 23:04:05,964] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203442815861445050283982_0002_m_000045_46: needsTaskCommit() Task attempt_202105170203442815861445050283982_0002_m_000045_46
[2021-05-16 23:04:05,965] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_202105170203442815861445050283982_0002_m_000045_46: needsTaskCommit() Task attempt_202105170203442815861445050283982_0002_m_000045_46: duration 0:00.002s
[2021-05-16 23:04:05,965] {docker.py:276} INFO - 21/05/17 02:04:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442815861445050283982_0002_m_000045_46
[2021-05-16 23:04:05,967] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Finished task 45.0 in stage 2.0 (TID 46). 4544 bytes result sent to driver
[2021-05-16 23:04:05,969] {docker.py:276} INFO - 21/05/17 02:04:05 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 50) (e60b22510068, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:05,970] {docker.py:276} INFO - 21/05/17 02:04:05 INFO Executor: Running task 49.0 in stage 2.0 (TID 50)
21/05/17 02:04:05 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 46) in 1741 ms on e60b22510068 (executor driver) (43/200)
[2021-05-16 23:04:05,982] {docker.py:276} INFO - 21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:05,984] {docker.py:276} INFO - 21/05/17 02:04:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203449198928635360341761_0002_m_000049_50, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449198928635360341761_0002_m_000049_50}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203449198928635360341761_0002}; taskId=attempt_202105170203449198928635360341761_0002_m_000049_50, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6acf13b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203449198928635360341761_0002_m_000049_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449198928635360341761_0002_m_000049_50
[2021-05-16 23:04:05,987] {docker.py:276} INFO - 21/05/17 02:04:05 INFO StagingCommitter: Task committer attempt_202105170203449198928635360341761_0002_m_000049_50: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449198928635360341761_0002_m_000049_50 : duration 0:00.003s
[2021-05-16 23:04:06,074] {docker.py:276} INFO - 21/05/17 02:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105170203445045515160701712952_0002_m_000046_47: needsTaskCommit() Task attempt_202105170203445045515160701712952_0002_m_000046_47
[2021-05-16 23:04:06,075] {docker.py:276} INFO - 21/05/17 02:04:06 INFO StagingCommitter: Task committer attempt_202105170203445045515160701712952_0002_m_000046_47: needsTaskCommit() Task attempt_202105170203445045515160701712952_0002_m_000046_47: duration 0:00.001s
21/05/17 02:04:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445045515160701712952_0002_m_000046_47
[2021-05-16 23:04:06,077] {docker.py:276} INFO - 21/05/17 02:04:06 INFO Executor: Finished task 46.0 in stage 2.0 (TID 47). 4544 bytes result sent to driver
[2021-05-16 23:04:06,079] {docker.py:276} INFO - 21/05/17 02:04:06 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 51) (e60b22510068, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:06,080] {docker.py:276} INFO - 21/05/17 02:04:06 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 47) in 1728 ms on e60b22510068 (executor driver) (44/200)
21/05/17 02:04:06 INFO Executor: Running task 50.0 in stage 2.0 (TID 51)
[2021-05-16 23:04:06,102] {docker.py:276} INFO - 21/05/17 02:04:06 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:06,104] {docker.py:276} INFO - 21/05/17 02:04:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:06,104] {docker.py:276} INFO - 21/05/17 02:04:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:06,104] {docker.py:276} INFO - 21/05/17 02:04:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446681916621352690611_0002_m_000050_51, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446681916621352690611_0002_m_000050_51}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446681916621352690611_0002}; taskId=attempt_202105170203446681916621352690611_0002_m_000050_51, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c8f69b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:06 INFO StagingCommitter: Starting: Task committer attempt_202105170203446681916621352690611_0002_m_000050_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446681916621352690611_0002_m_000050_51
[2021-05-16 23:04:06,108] {docker.py:276} INFO - 21/05/17 02:04:06 INFO StagingCommitter: Task committer attempt_202105170203446681916621352690611_0002_m_000050_51: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446681916621352690611_0002_m_000050_51 : duration 0:00.003s
[2021-05-16 23:04:07,532] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203444975908501109468323_0002_m_000047_48: needsTaskCommit() Task attempt_202105170203444975908501109468323_0002_m_000047_48
[2021-05-16 23:04:07,533] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203444975908501109468323_0002_m_000047_48: needsTaskCommit() Task attempt_202105170203444975908501109468323_0002_m_000047_48: duration 0:00.001s
21/05/17 02:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444975908501109468323_0002_m_000047_48
[2021-05-16 23:04:07,536] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Finished task 47.0 in stage 2.0 (TID 48). 4587 bytes result sent to driver
[2021-05-16 23:04:07,538] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 52) (e60b22510068, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:07,539] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Running task 51.0 in stage 2.0 (TID 52)
21/05/17 02:04:07 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 48) in 1705 ms on e60b22510068 (executor driver) (45/200)
[2021-05-16 23:04:07,550] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:07,551] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:07,552] {docker.py:276} INFO - 21/05/17 02:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:07,553] {docker.py:276} INFO - 21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446255648658878726334_0002_m_000051_52, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446255648658878726334_0002_m_000051_52}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446255648658878726334_0002}; taskId=attempt_202105170203446255648658878726334_0002_m_000051_52, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e2d4126}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:07,553] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203446255648658878726334_0002_m_000051_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446255648658878726334_0002_m_000051_52
[2021-05-16 23:04:07,556] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203446255648658878726334_0002_m_000051_52: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446255648658878726334_0002_m_000051_52 : duration 0:00.003s
[2021-05-16 23:04:07,586] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344987160886677942630_0002_m_000048_49: needsTaskCommit() Task attempt_20210517020344987160886677942630_0002_m_000048_49
[2021-05-16 23:04:07,586] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_20210517020344987160886677942630_0002_m_000048_49: needsTaskCommit() Task attempt_20210517020344987160886677942630_0002_m_000048_49: duration 0:00.000s
21/05/17 02:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344987160886677942630_0002_m_000048_49
[2021-05-16 23:04:07,587] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Finished task 48.0 in stage 2.0 (TID 49). 4587 bytes result sent to driver
[2021-05-16 23:04:07,588] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 53) (e60b22510068, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:07,589] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 49) in 1689 ms on e60b22510068 (executor driver) (46/200)
[2021-05-16 23:04:07,590] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Running task 52.0 in stage 2.0 (TID 53)
[2021-05-16 23:04:07,598] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:07,600] {docker.py:276} INFO - 21/05/17 02:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447452439981645790146_0002_m_000052_53, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447452439981645790146_0002_m_000052_53}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447452439981645790146_0002}; taskId=attempt_202105170203447452439981645790146_0002_m_000052_53, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7634fad4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203447452439981645790146_0002_m_000052_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447452439981645790146_0002_m_000052_53
[2021-05-16 23:04:07,603] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203447452439981645790146_0002_m_000052_53: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447452439981645790146_0002_m_000052_53 : duration 0:00.003s
[2021-05-16 23:04:07,717] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203449198928635360341761_0002_m_000049_50: needsTaskCommit() Task attempt_202105170203449198928635360341761_0002_m_000049_50
[2021-05-16 23:04:07,718] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203449198928635360341761_0002_m_000049_50: needsTaskCommit() Task attempt_202105170203449198928635360341761_0002_m_000049_50: duration 0:00.001s
21/05/17 02:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203449198928635360341761_0002_m_000049_50
[2021-05-16 23:04:07,720] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Finished task 49.0 in stage 2.0 (TID 50). 4587 bytes result sent to driver
[2021-05-16 23:04:07,722] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 54) (e60b22510068, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:07,723] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 50) in 1756 ms on e60b22510068 (executor driver) (47/200)
21/05/17 02:04:07 INFO Executor: Running task 53.0 in stage 2.0 (TID 54)
[2021-05-16 23:04:07,733] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Getting 2 (435.0 B) non-empty blocks including 2 (435.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:07,734] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:07,735] {docker.py:276} INFO - 21/05/17 02:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:07,736] {docker.py:276} INFO - 21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:07,736] {docker.py:276} INFO - 21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444438291352597425769_0002_m_000053_54, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444438291352597425769_0002_m_000053_54}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444438291352597425769_0002}; taskId=attempt_202105170203444438291352597425769_0002_m_000053_54, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25f9989f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:07,737] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203444438291352597425769_0002_m_000053_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444438291352597425769_0002_m_000053_54
[2021-05-16 23:04:07,740] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203444438291352597425769_0002_m_000053_54: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444438291352597425769_0002_m_000053_54 : duration 0:00.003s
[2021-05-16 23:04:07,837] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203446681916621352690611_0002_m_000050_51: needsTaskCommit() Task attempt_202105170203446681916621352690611_0002_m_000050_51
[2021-05-16 23:04:07,838] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203446681916621352690611_0002_m_000050_51: needsTaskCommit() Task attempt_202105170203446681916621352690611_0002_m_000050_51: duration 0:00.001s
21/05/17 02:04:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446681916621352690611_0002_m_000050_51
[2021-05-16 23:04:07,840] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Finished task 50.0 in stage 2.0 (TID 51). 4587 bytes result sent to driver
[2021-05-16 23:04:07,841] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 55) (e60b22510068, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:07,843] {docker.py:276} INFO - 21/05/17 02:04:07 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 51) in 1766 ms on e60b22510068 (executor driver) (48/200)
[2021-05-16 23:04:07,843] {docker.py:276} INFO - 21/05/17 02:04:07 INFO Executor: Running task 54.0 in stage 2.0 (TID 55)
[2021-05-16 23:04:07,854] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:07,855] {docker.py:276} INFO - 21/05/17 02:04:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:07,856] {docker.py:276} INFO - 21/05/17 02:04:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:07,857] {docker.py:276} INFO - 21/05/17 02:04:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443959256009203285442_0002_m_000054_55, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443959256009203285442_0002_m_000054_55}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443959256009203285442_0002}; taskId=attempt_202105170203443959256009203285442_0002_m_000054_55, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74474adc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203443959256009203285442_0002_m_000054_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443959256009203285442_0002_m_000054_55
[2021-05-16 23:04:07,860] {docker.py:276} INFO - 21/05/17 02:04:07 INFO StagingCommitter: Task committer attempt_202105170203443959256009203285442_0002_m_000054_55: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443959256009203285442_0002_m_000054_55 : duration 0:00.004s
[2021-05-16 23:04:09,234] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203446255648658878726334_0002_m_000051_52: needsTaskCommit() Task attempt_202105170203446255648658878726334_0002_m_000051_52
[2021-05-16 23:04:09,235] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203446255648658878726334_0002_m_000051_52: needsTaskCommit() Task attempt_202105170203446255648658878726334_0002_m_000051_52: duration 0:00.001s
21/05/17 02:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446255648658878726334_0002_m_000051_52
[2021-05-16 23:04:09,237] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Finished task 51.0 in stage 2.0 (TID 52). 4544 bytes result sent to driver
[2021-05-16 23:04:09,239] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 56) (e60b22510068, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:09,240] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 52) in 1704 ms on e60b22510068 (executor driver) (49/200)
[2021-05-16 23:04:09,241] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Running task 55.0 in stage 2.0 (TID 56)
[2021-05-16 23:04:09,246] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203447452439981645790146_0002_m_000052_53: needsTaskCommit() Task attempt_202105170203447452439981645790146_0002_m_000052_53
[2021-05-16 23:04:09,247] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203447452439981645790146_0002_m_000052_53: needsTaskCommit() Task attempt_202105170203447452439981645790146_0002_m_000052_53: duration 0:00.000s
21/05/17 02:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447452439981645790146_0002_m_000052_53
[2021-05-16 23:04:09,248] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Finished task 52.0 in stage 2.0 (TID 53). 4544 bytes result sent to driver
[2021-05-16 23:04:09,249] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 57) (e60b22510068, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:09,250] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 53) in 1665 ms on e60b22510068 (executor driver) (50/200)
[2021-05-16 23:04:09,251] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Running task 56.0 in stage 2.0 (TID 57)
[2021-05-16 23:04:09,262] {docker.py:276} INFO - 21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:09,263] {docker.py:276} INFO - 21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:09,264] {docker.py:276} INFO - 21/05/17 02:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:09,265] {docker.py:276} INFO - 21/05/17 02:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:09,265] {docker.py:276} INFO - 21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344747955887570490828_0002_m_000055_56, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344747955887570490828_0002_m_000055_56}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344747955887570490828_0002}; taskId=attempt_20210517020344747955887570490828_0002_m_000055_56, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@87f7c44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:09,265] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_20210517020344747955887570490828_0002_m_000055_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344747955887570490828_0002_m_000055_56
[2021-05-16 23:04:09,273] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_20210517020344747955887570490828_0002_m_000055_56: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344747955887570490828_0002_m_000055_56 : duration 0:00.008s
[2021-05-16 23:04:09,274] {docker.py:276} INFO - 21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:09,276] {docker.py:276} INFO - 21/05/17 02:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:09,277] {docker.py:276} INFO - 21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:09,277] {docker.py:276} INFO - 21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445912584119287469577_0002_m_000056_57, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445912584119287469577_0002_m_000056_57}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445912584119287469577_0002}; taskId=attempt_202105170203445912584119287469577_0002_m_000056_57, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1007c078}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:09,277] {docker.py:276} INFO - 21/05/17 02:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203445912584119287469577_0002_m_000056_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445912584119287469577_0002_m_000056_57
[2021-05-16 23:04:09,280] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203445912584119287469577_0002_m_000056_57: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445912584119287469577_0002_m_000056_57 : duration 0:00.002s
[2021-05-16 23:04:09,460] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203444438291352597425769_0002_m_000053_54: needsTaskCommit() Task attempt_202105170203444438291352597425769_0002_m_000053_54
[2021-05-16 23:04:09,461] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203444438291352597425769_0002_m_000053_54: needsTaskCommit() Task attempt_202105170203444438291352597425769_0002_m_000053_54: duration 0:00.002s
21/05/17 02:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444438291352597425769_0002_m_000053_54
[2021-05-16 23:04:09,464] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Finished task 53.0 in stage 2.0 (TID 54). 4587 bytes result sent to driver
[2021-05-16 23:04:09,466] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 58) (e60b22510068, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:09,467] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Running task 57.0 in stage 2.0 (TID 58)
21/05/17 02:04:09 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 54) in 1747 ms on e60b22510068 (executor driver) (51/200)
[2021-05-16 23:04:09,477] {docker.py:276} INFO - 21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:09,479] {docker.py:276} INFO - 21/05/17 02:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:09,479] {docker.py:276} INFO - 21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442928952867385019604_0002_m_000057_58, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442928952867385019604_0002_m_000057_58}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442928952867385019604_0002}; taskId=attempt_202105170203442928952867385019604_0002_m_000057_58, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b155a9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:09,479] {docker.py:276} INFO - 21/05/17 02:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:09,480] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203442928952867385019604_0002_m_000057_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442928952867385019604_0002_m_000057_58
[2021-05-16 23:04:09,482] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203442928952867385019604_0002_m_000057_58: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442928952867385019604_0002_m_000057_58 : duration 0:00.003s
[2021-05-16 23:04:09,491] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203443959256009203285442_0002_m_000054_55: needsTaskCommit() Task attempt_202105170203443959256009203285442_0002_m_000054_55
[2021-05-16 23:04:09,492] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203443959256009203285442_0002_m_000054_55: needsTaskCommit() Task attempt_202105170203443959256009203285442_0002_m_000054_55: duration 0:00.000s
21/05/17 02:04:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443959256009203285442_0002_m_000054_55
[2021-05-16 23:04:09,492] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Finished task 54.0 in stage 2.0 (TID 55). 4587 bytes result sent to driver
[2021-05-16 23:04:09,494] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 59) (e60b22510068, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:09,495] {docker.py:276} INFO - 21/05/17 02:04:09 INFO Executor: Running task 58.0 in stage 2.0 (TID 59)
[2021-05-16 23:04:09,495] {docker.py:276} INFO - 21/05/17 02:04:09 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 55) in 1656 ms on e60b22510068 (executor driver) (52/200)
[2021-05-16 23:04:09,504] {docker.py:276} INFO - 21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:09,506] {docker.py:276} INFO - 21/05/17 02:04:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448043100873596560584_0002_m_000058_59, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448043100873596560584_0002_m_000058_59}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448043100873596560584_0002}; taskId=attempt_202105170203448043100873596560584_0002_m_000058_59, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75412aca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:09 INFO StagingCommitter: Starting: Task committer attempt_202105170203448043100873596560584_0002_m_000058_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448043100873596560584_0002_m_000058_59
[2021-05-16 23:04:09,508] {docker.py:276} INFO - 21/05/17 02:04:09 INFO StagingCommitter: Task committer attempt_202105170203448043100873596560584_0002_m_000058_59: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448043100873596560584_0002_m_000058_59 : duration 0:00.002s
[2021-05-16 23:04:10,935] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Starting: Task committer attempt_20210517020344747955887570490828_0002_m_000055_56: needsTaskCommit() Task attempt_20210517020344747955887570490828_0002_m_000055_56
[2021-05-16 23:04:10,935] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Task committer attempt_20210517020344747955887570490828_0002_m_000055_56: needsTaskCommit() Task attempt_20210517020344747955887570490828_0002_m_000055_56: duration 0:00.000s
21/05/17 02:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344747955887570490828_0002_m_000055_56
[2021-05-16 23:04:10,936] {docker.py:276} INFO - 21/05/17 02:04:10 INFO Executor: Finished task 55.0 in stage 2.0 (TID 56). 4587 bytes result sent to driver
[2021-05-16 23:04:10,938] {docker.py:276} INFO - 21/05/17 02:04:10 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 60) (e60b22510068, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 02:04:10 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 56) in 1702 ms on e60b22510068 (executor driver) (53/200)
21/05/17 02:04:10 INFO Executor: Running task 59.0 in stage 2.0 (TID 60)
[2021-05-16 23:04:10,947] {docker.py:276} INFO - 21/05/17 02:04:10 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:10,949] {docker.py:276} INFO - 21/05/17 02:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:10,950] {docker.py:276} INFO - 21/05/17 02:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441505316280481277209_0002_m_000059_60, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441505316280481277209_0002_m_000059_60}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441505316280481277209_0002}; taskId=attempt_202105170203441505316280481277209_0002_m_000059_60, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c53f61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:10,950] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105170203441505316280481277209_0002_m_000059_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441505316280481277209_0002_m_000059_60
[2021-05-16 23:04:10,953] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Task committer attempt_202105170203441505316280481277209_0002_m_000059_60: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441505316280481277209_0002_m_000059_60 : duration 0:00.004s
[2021-05-16 23:04:10,958] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105170203445912584119287469577_0002_m_000056_57: needsTaskCommit() Task attempt_202105170203445912584119287469577_0002_m_000056_57
[2021-05-16 23:04:10,958] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Task committer attempt_202105170203445912584119287469577_0002_m_000056_57: needsTaskCommit() Task attempt_202105170203445912584119287469577_0002_m_000056_57: duration 0:00.000s
21/05/17 02:04:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445912584119287469577_0002_m_000056_57
[2021-05-16 23:04:10,959] {docker.py:276} INFO - 21/05/17 02:04:10 INFO Executor: Finished task 56.0 in stage 2.0 (TID 57). 4587 bytes result sent to driver
[2021-05-16 23:04:10,960] {docker.py:276} INFO - 21/05/17 02:04:10 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 61) (e60b22510068, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:10,961] {docker.py:276} INFO - 21/05/17 02:04:10 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 57) in 1714 ms on e60b22510068 (executor driver) (54/200)
[2021-05-16 23:04:10,962] {docker.py:276} INFO - 21/05/17 02:04:10 INFO Executor: Running task 60.0 in stage 2.0 (TID 61)
[2021-05-16 23:04:10,970] {docker.py:276} INFO - 21/05/17 02:04:10 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:10,972] {docker.py:276} INFO - 21/05/17 02:04:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447997116500173857691_0002_m_000060_61, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447997116500173857691_0002_m_000060_61}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447997116500173857691_0002}; taskId=attempt_202105170203447997116500173857691_0002_m_000060_61, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b760751}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:10,972] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Starting: Task committer attempt_202105170203447997116500173857691_0002_m_000060_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447997116500173857691_0002_m_000060_61
[2021-05-16 23:04:10,975] {docker.py:276} INFO - 21/05/17 02:04:10 INFO StagingCommitter: Task committer attempt_202105170203447997116500173857691_0002_m_000060_61: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447997116500173857691_0002_m_000060_61 : duration 0:00.003s
[2021-05-16 23:04:11,197] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105170203442928952867385019604_0002_m_000057_58: needsTaskCommit() Task attempt_202105170203442928952867385019604_0002_m_000057_58
[2021-05-16 23:04:11,197] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Task committer attempt_202105170203442928952867385019604_0002_m_000057_58: needsTaskCommit() Task attempt_202105170203442928952867385019604_0002_m_000057_58: duration 0:00.000s
[2021-05-16 23:04:11,198] {docker.py:276} INFO - 21/05/17 02:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442928952867385019604_0002_m_000057_58
[2021-05-16 23:04:11,199] {docker.py:276} INFO - 21/05/17 02:04:11 INFO Executor: Finished task 57.0 in stage 2.0 (TID 58). 4544 bytes result sent to driver
[2021-05-16 23:04:11,200] {docker.py:276} INFO - 21/05/17 02:04:11 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 62) (e60b22510068, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:11,201] {docker.py:276} INFO - 21/05/17 02:04:11 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 58) in 1739 ms on e60b22510068 (executor driver) (55/200)
[2021-05-16 23:04:11,202] {docker.py:276} INFO - 21/05/17 02:04:11 INFO Executor: Running task 62.0 in stage 2.0 (TID 62)
[2021-05-16 23:04:11,203] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105170203448043100873596560584_0002_m_000058_59: needsTaskCommit() Task attempt_202105170203448043100873596560584_0002_m_000058_59
[2021-05-16 23:04:11,204] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Task committer attempt_202105170203448043100873596560584_0002_m_000058_59: needsTaskCommit() Task attempt_202105170203448043100873596560584_0002_m_000058_59: duration 0:00.000s
21/05/17 02:04:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448043100873596560584_0002_m_000058_59
[2021-05-16 23:04:11,205] {docker.py:276} INFO - 21/05/17 02:04:11 INFO Executor: Finished task 58.0 in stage 2.0 (TID 59). 4544 bytes result sent to driver
[2021-05-16 23:04:11,206] {docker.py:276} INFO - 21/05/17 02:04:11 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 63) (e60b22510068, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:11,207] {docker.py:276} INFO - 21/05/17 02:04:11 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 59) in 1715 ms on e60b22510068 (executor driver) (56/200)
[2021-05-16 23:04:11,207] {docker.py:276} INFO - 21/05/17 02:04:11 INFO Executor: Running task 63.0 in stage 2.0 (TID 63)
[2021-05-16 23:04:11,213] {docker.py:276} INFO - 21/05/17 02:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (668.0 B) non-empty blocks including 3 (668.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:11,215] {docker.py:276} INFO - 21/05/17 02:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445447924504082030729_0002_m_000062_62, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445447924504082030729_0002_m_000062_62}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445447924504082030729_0002}; taskId=attempt_202105170203445447924504082030729_0002_m_000062_62, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fb1e287}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:11,215] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105170203445447924504082030729_0002_m_000062_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445447924504082030729_0002_m_000062_62
[2021-05-16 23:04:11,216] {docker.py:276} INFO - 21/05/17 02:04:11 INFO ShuffleBlockFetcherIterator: Getting 3 (624.0 B) non-empty blocks including 3 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:11,218] {docker.py:276} INFO - 21/05/17 02:04:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:11,218] {docker.py:276} INFO - 21/05/17 02:04:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447279834894228081418_0002_m_000063_63, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447279834894228081418_0002_m_000063_63}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447279834894228081418_0002}; taskId=attempt_202105170203447279834894228081418_0002_m_000063_63, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ad87732}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:11,219] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Task committer attempt_202105170203445447924504082030729_0002_m_000062_62: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445447924504082030729_0002_m_000062_62 : duration 0:00.004s
21/05/17 02:04:11 INFO StagingCommitter: Starting: Task committer attempt_202105170203447279834894228081418_0002_m_000063_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447279834894228081418_0002_m_000063_63
[2021-05-16 23:04:11,222] {docker.py:276} INFO - 21/05/17 02:04:11 INFO StagingCommitter: Task committer attempt_202105170203447279834894228081418_0002_m_000063_63: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447279834894228081418_0002_m_000063_63 : duration 0:00.004s
[2021-05-16 23:04:12,639] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203441505316280481277209_0002_m_000059_60: needsTaskCommit() Task attempt_202105170203441505316280481277209_0002_m_000059_60
[2021-05-16 23:04:12,640] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203441505316280481277209_0002_m_000059_60: needsTaskCommit() Task attempt_202105170203441505316280481277209_0002_m_000059_60: duration 0:00.001s
[2021-05-16 23:04:12,640] {docker.py:276} INFO - 21/05/17 02:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441505316280481277209_0002_m_000059_60
[2021-05-16 23:04:12,641] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Finished task 59.0 in stage 2.0 (TID 60). 4544 bytes result sent to driver
[2021-05-16 23:04:12,643] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 64) (e60b22510068, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:12,645] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Running task 64.0 in stage 2.0 (TID 64)
21/05/17 02:04:12 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 60) in 1710 ms on e60b22510068 (executor driver) (57/200)
[2021-05-16 23:04:12,661] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:12,663] {docker.py:276} INFO - 21/05/17 02:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447075694262435205518_0002_m_000064_64, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447075694262435205518_0002_m_000064_64}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447075694262435205518_0002}; taskId=attempt_202105170203447075694262435205518_0002_m_000064_64, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68aef97f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:12,664] {docker.py:276} INFO - 21/05/17 02:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:12,664] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203447075694262435205518_0002_m_000064_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447075694262435205518_0002_m_000064_64
[2021-05-16 23:04:12,666] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203447075694262435205518_0002_m_000064_64: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447075694262435205518_0002_m_000064_64 : duration 0:00.004s
[2021-05-16 23:04:12,684] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203447997116500173857691_0002_m_000060_61: needsTaskCommit() Task attempt_202105170203447997116500173857691_0002_m_000060_61
[2021-05-16 23:04:12,685] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203447997116500173857691_0002_m_000060_61: needsTaskCommit() Task attempt_202105170203447997116500173857691_0002_m_000060_61: duration 0:00.001s
[2021-05-16 23:04:12,685] {docker.py:276} INFO - 21/05/17 02:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447997116500173857691_0002_m_000060_61
[2021-05-16 23:04:12,686] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Finished task 60.0 in stage 2.0 (TID 61). 4587 bytes result sent to driver
[2021-05-16 23:04:12,687] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 65) (e60b22510068, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:12,689] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 61) in 1731 ms on e60b22510068 (executor driver) (58/200)
21/05/17 02:04:12 INFO Executor: Running task 65.0 in stage 2.0 (TID 65)
[2021-05-16 23:04:12,696] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:12,696] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:12,698] {docker.py:276} INFO - 21/05/17 02:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:12,699] {docker.py:276} INFO - 21/05/17 02:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:12,699] {docker.py:276} INFO - 21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:12,700] {docker.py:276} INFO - 21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448966706842556428224_0002_m_000065_65, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448966706842556428224_0002_m_000065_65}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448966706842556428224_0002}; taskId=attempt_202105170203448966706842556428224_0002_m_000065_65, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@200f306c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:12,700] {docker.py:276} INFO - 21/05/17 02:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:12,700] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203448966706842556428224_0002_m_000065_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448966706842556428224_0002_m_000065_65
[2021-05-16 23:04:12,703] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203448966706842556428224_0002_m_000065_65: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448966706842556428224_0002_m_000065_65 : duration 0:00.003s
[2021-05-16 23:04:12,886] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203447279834894228081418_0002_m_000063_63: needsTaskCommit() Task attempt_202105170203447279834894228081418_0002_m_000063_63
[2021-05-16 23:04:12,887] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203447279834894228081418_0002_m_000063_63: needsTaskCommit() Task attempt_202105170203447279834894228081418_0002_m_000063_63: duration 0:00.001s
21/05/17 02:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447279834894228081418_0002_m_000063_63
[2021-05-16 23:04:12,888] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203445447924504082030729_0002_m_000062_62: needsTaskCommit() Task attempt_202105170203445447924504082030729_0002_m_000062_62
[2021-05-16 23:04:12,889] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203445447924504082030729_0002_m_000062_62: needsTaskCommit() Task attempt_202105170203445447924504082030729_0002_m_000062_62: duration 0:00.001s
21/05/17 02:04:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445447924504082030729_0002_m_000062_62
[2021-05-16 23:04:12,889] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Finished task 63.0 in stage 2.0 (TID 63). 4587 bytes result sent to driver
[2021-05-16 23:04:12,891] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 66) (e60b22510068, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:12,892] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 63) in 1689 ms on e60b22510068 (executor driver) (59/200)
21/05/17 02:04:12 INFO Executor: Running task 66.0 in stage 2.0 (TID 66)
[2021-05-16 23:04:12,893] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Finished task 62.0 in stage 2.0 (TID 62). 4587 bytes result sent to driver
[2021-05-16 23:04:12,894] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 67) (e60b22510068, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:12,896] {docker.py:276} INFO - 21/05/17 02:04:12 INFO Executor: Running task 67.0 in stage 2.0 (TID 67)
[2021-05-16 23:04:12,896] {docker.py:276} INFO - 21/05/17 02:04:12 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 62) in 1697 ms on e60b22510068 (executor driver) (60/200)
[2021-05-16 23:04:12,904] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:12,906] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:12,906] {docker.py:276} INFO - 21/05/17 02:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:12,908] {docker.py:276} INFO - 21/05/17 02:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:12,908] {docker.py:276} INFO - 21/05/17 02:04:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:12,909] {docker.py:276} INFO - 21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:12,909] {docker.py:276} INFO - 21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441459364053689381575_0002_m_000066_66, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441459364053689381575_0002_m_000066_66}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441459364053689381575_0002}; taskId=attempt_202105170203441459364053689381575_0002_m_000066_66, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27dca76c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:12,909] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203441459364053689381575_0002_m_000066_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441459364053689381575_0002_m_000066_66
[2021-05-16 23:04:12,910] {docker.py:276} INFO - 21/05/17 02:04:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447709847257683163347_0002_m_000067_67, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709847257683163347_0002_m_000067_67}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447709847257683163347_0002}; taskId=attempt_202105170203447709847257683163347_0002_m_000067_67, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@401bfddf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:12,910] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Starting: Task committer attempt_202105170203447709847257683163347_0002_m_000067_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709847257683163347_0002_m_000067_67
[2021-05-16 23:04:12,913] {docker.py:276} INFO - 21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203441459364053689381575_0002_m_000066_66: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441459364053689381575_0002_m_000066_66 : duration 0:00.004s
21/05/17 02:04:12 INFO StagingCommitter: Task committer attempt_202105170203447709847257683163347_0002_m_000067_67: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709847257683163347_0002_m_000067_67 : duration 0:00.003s
[2021-05-16 23:04:14,339] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203447075694262435205518_0002_m_000064_64: needsTaskCommit() Task attempt_202105170203447075694262435205518_0002_m_000064_64
[2021-05-16 23:04:14,340] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203447075694262435205518_0002_m_000064_64: needsTaskCommit() Task attempt_202105170203447075694262435205518_0002_m_000064_64: duration 0:00.001s
21/05/17 02:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447075694262435205518_0002_m_000064_64
[2021-05-16 23:04:14,344] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Finished task 64.0 in stage 2.0 (TID 64). 4587 bytes result sent to driver
[2021-05-16 23:04:14,346] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 68) (e60b22510068, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:14,347] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 64) in 1706 ms on e60b22510068 (executor driver) (61/200)
[2021-05-16 23:04:14,348] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Running task 69.0 in stage 2.0 (TID 68)
[2021-05-16 23:04:14,358] {docker.py:276} INFO - 21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Getting 3 (585.0 B) non-empty blocks including 3 (585.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:14,361] {docker.py:276} INFO - 21/05/17 02:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:14,361] {docker.py:276} INFO - 21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448178715402080564642_0002_m_000069_68, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448178715402080564642_0002_m_000069_68}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448178715402080564642_0002}; taskId=attempt_202105170203448178715402080564642_0002_m_000069_68, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a885fb4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203448178715402080564642_0002_m_000069_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448178715402080564642_0002_m_000069_68
[2021-05-16 23:04:14,364] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203448178715402080564642_0002_m_000069_68: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448178715402080564642_0002_m_000069_68 : duration 0:00.002s
[2021-05-16 23:04:14,409] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203448966706842556428224_0002_m_000065_65: needsTaskCommit() Task attempt_202105170203448966706842556428224_0002_m_000065_65
[2021-05-16 23:04:14,410] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203448966706842556428224_0002_m_000065_65: needsTaskCommit() Task attempt_202105170203448966706842556428224_0002_m_000065_65: duration 0:00.001s
[2021-05-16 23:04:14,410] {docker.py:276} INFO - 21/05/17 02:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448966706842556428224_0002_m_000065_65
[2021-05-16 23:04:14,414] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Finished task 65.0 in stage 2.0 (TID 65). 4544 bytes result sent to driver
[2021-05-16 23:04:14,415] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Starting task 70.0 in stage 2.0 (TID 69) (e60b22510068, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:14,416] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Finished task 65.0 in stage 2.0 (TID 65) in 1731 ms on e60b22510068 (executor driver) (62/200)
[2021-05-16 23:04:14,417] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Running task 70.0 in stage 2.0 (TID 69)
[2021-05-16 23:04:14,427] {docker.py:276} INFO - 21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:14,429] {docker.py:276} INFO - 21/05/17 02:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:14,429] {docker.py:276} INFO - 21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:14,430] {docker.py:276} INFO - 21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444959326412942099675_0002_m_000070_69, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444959326412942099675_0002_m_000070_69}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444959326412942099675_0002}; taskId=attempt_202105170203444959326412942099675_0002_m_000070_69, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e6e1588}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203444959326412942099675_0002_m_000070_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444959326412942099675_0002_m_000070_69
[2021-05-16 23:04:14,433] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203444959326412942099675_0002_m_000070_69: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444959326412942099675_0002_m_000070_69 : duration 0:00.003s
[2021-05-16 23:04:14,596] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203441459364053689381575_0002_m_000066_66: needsTaskCommit() Task attempt_202105170203441459364053689381575_0002_m_000066_66
[2021-05-16 23:04:14,597] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203441459364053689381575_0002_m_000066_66: needsTaskCommit() Task attempt_202105170203441459364053689381575_0002_m_000066_66: duration 0:00.001s
[2021-05-16 23:04:14,598] {docker.py:276} INFO - 21/05/17 02:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441459364053689381575_0002_m_000066_66
[2021-05-16 23:04:14,601] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Finished task 66.0 in stage 2.0 (TID 66). 4544 bytes result sent to driver
[2021-05-16 23:04:14,603] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Starting task 71.0 in stage 2.0 (TID 70) (e60b22510068, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:14,604] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Finished task 66.0 in stage 2.0 (TID 66) in 1715 ms on e60b22510068 (executor driver) (63/200)
[2021-05-16 23:04:14,606] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Running task 71.0 in stage 2.0 (TID 70)
[2021-05-16 23:04:14,615] {docker.py:276} INFO - 21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:14,617] {docker.py:276} INFO - 21/05/17 02:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442088118614361701806_0002_m_000071_70, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442088118614361701806_0002_m_000071_70}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442088118614361701806_0002}; taskId=attempt_202105170203442088118614361701806_0002_m_000071_70, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f7987b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203442088118614361701806_0002_m_000071_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442088118614361701806_0002_m_000071_70
[2021-05-16 23:04:14,621] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203442088118614361701806_0002_m_000071_70: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442088118614361701806_0002_m_000071_70 : duration 0:00.003s
[2021-05-16 23:04:14,754] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_202105170203447709847257683163347_0002_m_000067_67: needsTaskCommit() Task attempt_202105170203447709847257683163347_0002_m_000067_67
21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_202105170203447709847257683163347_0002_m_000067_67: needsTaskCommit() Task attempt_202105170203447709847257683163347_0002_m_000067_67: duration 0:00.000s
[2021-05-16 23:04:14,754] {docker.py:276} INFO - 21/05/17 02:04:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447709847257683163347_0002_m_000067_67
[2021-05-16 23:04:14,756] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Finished task 67.0 in stage 2.0 (TID 67). 4544 bytes result sent to driver
[2021-05-16 23:04:14,757] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Starting task 73.0 in stage 2.0 (TID 71) (e60b22510068, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:14,758] {docker.py:276} INFO - 21/05/17 02:04:14 INFO TaskSetManager: Finished task 67.0 in stage 2.0 (TID 67) in 1865 ms on e60b22510068 (executor driver) (64/200)
[2021-05-16 23:04:14,758] {docker.py:276} INFO - 21/05/17 02:04:14 INFO Executor: Running task 73.0 in stage 2.0 (TID 71)
[2021-05-16 23:04:14,766] {docker.py:276} INFO - 21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:14,768] {docker.py:276} INFO - 21/05/17 02:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:14,769] {docker.py:276} INFO - 21/05/17 02:04:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344842643338408287569_0002_m_000073_71, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344842643338408287569_0002_m_000073_71}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344842643338408287569_0002}; taskId=attempt_20210517020344842643338408287569_0002_m_000073_71, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ddcce91}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:14,769] {docker.py:276} INFO - 21/05/17 02:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:14,769] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Starting: Task committer attempt_20210517020344842643338408287569_0002_m_000073_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344842643338408287569_0002_m_000073_71
[2021-05-16 23:04:14,773] {docker.py:276} INFO - 21/05/17 02:04:14 INFO StagingCommitter: Task committer attempt_20210517020344842643338408287569_0002_m_000073_71: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344842643338408287569_0002_m_000073_71 : duration 0:00.004s
[2021-05-16 23:04:16,105] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203444959326412942099675_0002_m_000070_69: needsTaskCommit() Task attempt_202105170203444959326412942099675_0002_m_000070_69
[2021-05-16 23:04:16,106] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203444959326412942099675_0002_m_000070_69: needsTaskCommit() Task attempt_202105170203444959326412942099675_0002_m_000070_69: duration 0:00.001s
21/05/17 02:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444959326412942099675_0002_m_000070_69
[2021-05-16 23:04:16,108] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Finished task 70.0 in stage 2.0 (TID 69). 4587 bytes result sent to driver
[2021-05-16 23:04:16,109] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Starting task 74.0 in stage 2.0 (TID 72) (e60b22510068, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:16,111] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Finished task 70.0 in stage 2.0 (TID 69) in 1698 ms on e60b22510068 (executor driver) (65/200)
[2021-05-16 23:04:16,111] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Running task 74.0 in stage 2.0 (TID 72)
[2021-05-16 23:04:16,115] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203448178715402080564642_0002_m_000069_68: needsTaskCommit() Task attempt_202105170203448178715402080564642_0002_m_000069_68
[2021-05-16 23:04:16,116] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203448178715402080564642_0002_m_000069_68: needsTaskCommit() Task attempt_202105170203448178715402080564642_0002_m_000069_68: duration 0:00.001s
[2021-05-16 23:04:16,116] {docker.py:276} INFO - 21/05/17 02:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448178715402080564642_0002_m_000069_68
[2021-05-16 23:04:16,117] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Finished task 69.0 in stage 2.0 (TID 68). 4587 bytes result sent to driver
[2021-05-16 23:04:16,119] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Starting task 75.0 in stage 2.0 (TID 73) (e60b22510068, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:16,120] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Running task 75.0 in stage 2.0 (TID 73)
21/05/17 02:04:16 INFO TaskSetManager: Finished task 69.0 in stage 2.0 (TID 68) in 1776 ms on e60b22510068 (executor driver) (66/200)
[2021-05-16 23:04:16,125] {docker.py:276} INFO - 21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Getting 3 (549.0 B) non-empty blocks including 3 (549.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:16,127] {docker.py:276} INFO - 21/05/17 02:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448037187917594232227_0002_m_000074_72, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448037187917594232227_0002_m_000074_72}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448037187917594232227_0002}; taskId=attempt_202105170203448037187917594232227_0002_m_000074_72, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@610f6a19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:16,128] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203448037187917594232227_0002_m_000074_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448037187917594232227_0002_m_000074_72
[2021-05-16 23:04:16,129] {docker.py:276} INFO - 21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:16,131] {docker.py:276} INFO - 21/05/17 02:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448866647100113317614_0002_m_000075_73, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448866647100113317614_0002_m_000075_73}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448866647100113317614_0002}; taskId=attempt_202105170203448866647100113317614_0002_m_000075_73, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44c33a5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:16,132] {docker.py:276} INFO - 21/05/17 02:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:16,132] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203448866647100113317614_0002_m_000075_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448866647100113317614_0002_m_000075_73
[2021-05-16 23:04:16,133] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203448037187917594232227_0002_m_000074_72: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448037187917594232227_0002_m_000074_72 : duration 0:00.005s
[2021-05-16 23:04:16,137] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203448866647100113317614_0002_m_000075_73: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448866647100113317614_0002_m_000075_73 : duration 0:00.004s
[2021-05-16 23:04:16,353] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203442088118614361701806_0002_m_000071_70: needsTaskCommit() Task attempt_202105170203442088118614361701806_0002_m_000071_70
[2021-05-16 23:04:16,354] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203442088118614361701806_0002_m_000071_70: needsTaskCommit() Task attempt_202105170203442088118614361701806_0002_m_000071_70: duration 0:00.001s
21/05/17 02:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442088118614361701806_0002_m_000071_70
[2021-05-16 23:04:16,355] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Finished task 71.0 in stage 2.0 (TID 70). 4587 bytes result sent to driver
[2021-05-16 23:04:16,357] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Starting task 76.0 in stage 2.0 (TID 74) (e60b22510068, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:16,358] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Finished task 71.0 in stage 2.0 (TID 70) in 1757 ms on e60b22510068 (executor driver) (67/200)
21/05/17 02:04:16 INFO Executor: Running task 76.0 in stage 2.0 (TID 74)
[2021-05-16 23:04:16,368] {docker.py:276} INFO - 21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:16,370] {docker.py:276} INFO - 21/05/17 02:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:16,370] {docker.py:276} INFO - 21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447926056728267708682_0002_m_000076_74, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447926056728267708682_0002_m_000076_74}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447926056728267708682_0002}; taskId=attempt_202105170203447926056728267708682_0002_m_000076_74, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b9dcc85}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203447926056728267708682_0002_m_000076_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447926056728267708682_0002_m_000076_74
[2021-05-16 23:04:16,373] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203447926056728267708682_0002_m_000076_74: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447926056728267708682_0002_m_000076_74 : duration 0:00.003s
[2021-05-16 23:04:16,426] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_20210517020344842643338408287569_0002_m_000073_71: needsTaskCommit() Task attempt_20210517020344842643338408287569_0002_m_000073_71
[2021-05-16 23:04:16,427] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_20210517020344842643338408287569_0002_m_000073_71: needsTaskCommit() Task attempt_20210517020344842643338408287569_0002_m_000073_71: duration 0:00.002s
21/05/17 02:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344842643338408287569_0002_m_000073_71
[2021-05-16 23:04:16,429] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Finished task 73.0 in stage 2.0 (TID 71). 4587 bytes result sent to driver
[2021-05-16 23:04:16,431] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 75) (e60b22510068, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:16,432] {docker.py:276} INFO - 21/05/17 02:04:16 INFO Executor: Running task 77.0 in stage 2.0 (TID 75)
[2021-05-16 23:04:16,432] {docker.py:276} INFO - 21/05/17 02:04:16 INFO TaskSetManager: Finished task 73.0 in stage 2.0 (TID 71) in 1678 ms on e60b22510068 (executor driver) (68/200)
[2021-05-16 23:04:16,442] {docker.py:276} INFO - 21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:16,444] {docker.py:276} INFO - 21/05/17 02:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446526902910236425958_0002_m_000077_75, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446526902910236425958_0002_m_000077_75}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446526902910236425958_0002}; taskId=attempt_202105170203446526902910236425958_0002_m_000077_75, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1392210}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:16,444] {docker.py:276} INFO - 21/05/17 02:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:16,444] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Starting: Task committer attempt_202105170203446526902910236425958_0002_m_000077_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446526902910236425958_0002_m_000077_75
[2021-05-16 23:04:16,447] {docker.py:276} INFO - 21/05/17 02:04:16 INFO StagingCommitter: Task committer attempt_202105170203446526902910236425958_0002_m_000077_75: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446526902910236425958_0002_m_000077_75 : duration 0:00.003s
[2021-05-16 23:04:17,806] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105170203448037187917594232227_0002_m_000074_72: needsTaskCommit() Task attempt_202105170203448037187917594232227_0002_m_000074_72
21/05/17 02:04:17 INFO StagingCommitter: Task committer attempt_202105170203448037187917594232227_0002_m_000074_72: needsTaskCommit() Task attempt_202105170203448037187917594232227_0002_m_000074_72: duration 0:00.000s
21/05/17 02:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448037187917594232227_0002_m_000074_72
[2021-05-16 23:04:17,808] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105170203448866647100113317614_0002_m_000075_73: needsTaskCommit() Task attempt_202105170203448866647100113317614_0002_m_000075_73
21/05/17 02:04:17 INFO Executor: Finished task 74.0 in stage 2.0 (TID 72). 4544 bytes result sent to driver
[2021-05-16 23:04:17,810] {docker.py:276} INFO - 21/05/17 02:04:17 INFO TaskSetManager: Starting task 78.0 in stage 2.0 (TID 76) (e60b22510068, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:17,810] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Task committer attempt_202105170203448866647100113317614_0002_m_000075_73: needsTaskCommit() Task attempt_202105170203448866647100113317614_0002_m_000075_73: duration 0:00.003s
21/05/17 02:04:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448866647100113317614_0002_m_000075_73
[2021-05-16 23:04:17,811] {docker.py:276} INFO - 21/05/17 02:04:17 INFO Executor: Running task 78.0 in stage 2.0 (TID 76)
[2021-05-16 23:04:17,812] {docker.py:276} INFO - 21/05/17 02:04:17 INFO TaskSetManager: Finished task 74.0 in stage 2.0 (TID 72) in 1704 ms on e60b22510068 (executor driver) (69/200)
[2021-05-16 23:04:17,813] {docker.py:276} INFO - 21/05/17 02:04:17 INFO Executor: Finished task 75.0 in stage 2.0 (TID 73). 4544 bytes result sent to driver
[2021-05-16 23:04:17,815] {docker.py:276} INFO - 21/05/17 02:04:17 INFO TaskSetManager: Starting task 79.0 in stage 2.0 (TID 77) (e60b22510068, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:17,816] {docker.py:276} INFO - 21/05/17 02:04:17 INFO TaskSetManager: Finished task 75.0 in stage 2.0 (TID 73) in 1699 ms on e60b22510068 (executor driver) (70/200)
[2021-05-16 23:04:17,817] {docker.py:276} INFO - 21/05/17 02:04:17 INFO Executor: Running task 79.0 in stage 2.0 (TID 77)
[2021-05-16 23:04:17,826] {docker.py:276} INFO - 21/05/17 02:04:17 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 02:04:17 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:17,826] {docker.py:276} INFO - 21/05/17 02:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:17,827] {docker.py:276} INFO - 21/05/17 02:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:17,828] {docker.py:276} INFO - 21/05/17 02:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:17,828] {docker.py:276} INFO - 21/05/17 02:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:17,829] {docker.py:276} INFO - 21/05/17 02:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441849968458471347005_0002_m_000078_76, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441849968458471347005_0002_m_000078_76}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441849968458471347005_0002}; taskId=attempt_202105170203441849968458471347005_0002_m_000078_76, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@516ea72b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:17,829] {docker.py:276} INFO - 21/05/17 02:04:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:17,830] {docker.py:276} INFO - 21/05/17 02:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105170203441849968458471347005_0002_m_000078_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441849968458471347005_0002_m_000078_76
[2021-05-16 23:04:17,830] {docker.py:276} INFO - 21/05/17 02:04:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:17,831] {docker.py:276} INFO - 21/05/17 02:04:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447709667933397235730_0002_m_000079_77, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709667933397235730_0002_m_000079_77}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447709667933397235730_0002}; taskId=attempt_202105170203447709667933397235730_0002_m_000079_77, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d5c3a4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:17,831] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Starting: Task committer attempt_202105170203447709667933397235730_0002_m_000079_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709667933397235730_0002_m_000079_77
[2021-05-16 23:04:17,833] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Task committer attempt_202105170203441849968458471347005_0002_m_000078_76: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441849968458471347005_0002_m_000078_76 : duration 0:00.004s
[2021-05-16 23:04:17,835] {docker.py:276} INFO - 21/05/17 02:04:17 INFO StagingCommitter: Task committer attempt_202105170203447709667933397235730_0002_m_000079_77: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447709667933397235730_0002_m_000079_77 : duration 0:00.005s
[2021-05-16 23:04:18,080] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105170203447926056728267708682_0002_m_000076_74: needsTaskCommit() Task attempt_202105170203447926056728267708682_0002_m_000076_74
[2021-05-16 23:04:18,082] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Task committer attempt_202105170203447926056728267708682_0002_m_000076_74: needsTaskCommit() Task attempt_202105170203447926056728267708682_0002_m_000076_74: duration 0:00.001s
21/05/17 02:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447926056728267708682_0002_m_000076_74
[2021-05-16 23:04:18,083] {docker.py:276} INFO - 21/05/17 02:04:18 INFO Executor: Finished task 76.0 in stage 2.0 (TID 74). 4544 bytes result sent to driver
[2021-05-16 23:04:18,084] {docker.py:276} INFO - 21/05/17 02:04:18 INFO TaskSetManager: Starting task 80.0 in stage 2.0 (TID 78) (e60b22510068, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:18,087] {docker.py:276} INFO - 21/05/17 02:04:18 INFO Executor: Running task 80.0 in stage 2.0 (TID 78)
21/05/17 02:04:18 INFO TaskSetManager: Finished task 76.0 in stage 2.0 (TID 74) in 1731 ms on e60b22510068 (executor driver) (71/200)
[2021-05-16 23:04:18,107] {docker.py:276} INFO - 21/05/17 02:04:18 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:18,109] {docker.py:276} INFO - 21/05/17 02:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446302784810090062546_0002_m_000080_78, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446302784810090062546_0002_m_000080_78}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446302784810090062546_0002}; taskId=attempt_202105170203446302784810090062546_0002_m_000080_78, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d5fbf43}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:18,109] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105170203446302784810090062546_0002_m_000080_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446302784810090062546_0002_m_000080_78
[2021-05-16 23:04:18,112] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Task committer attempt_202105170203446302784810090062546_0002_m_000080_78: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446302784810090062546_0002_m_000080_78 : duration 0:00.003s
[2021-05-16 23:04:18,185] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105170203446526902910236425958_0002_m_000077_75: needsTaskCommit() Task attempt_202105170203446526902910236425958_0002_m_000077_75
[2021-05-16 23:04:18,185] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Task committer attempt_202105170203446526902910236425958_0002_m_000077_75: needsTaskCommit() Task attempt_202105170203446526902910236425958_0002_m_000077_75: duration 0:00.000s
21/05/17 02:04:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446526902910236425958_0002_m_000077_75
[2021-05-16 23:04:18,187] {docker.py:276} INFO - 21/05/17 02:04:18 INFO Executor: Finished task 77.0 in stage 2.0 (TID 75). 4587 bytes result sent to driver
[2021-05-16 23:04:18,189] {docker.py:276} INFO - 21/05/17 02:04:18 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 79) (e60b22510068, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:18,190] {docker.py:276} INFO - 21/05/17 02:04:18 INFO TaskSetManager: Finished task 77.0 in stage 2.0 (TID 75) in 1761 ms on e60b22510068 (executor driver) (72/200)
[2021-05-16 23:04:18,190] {docker.py:276} INFO - 21/05/17 02:04:18 INFO Executor: Running task 81.0 in stage 2.0 (TID 79)
[2021-05-16 23:04:18,198] {docker.py:276} INFO - 21/05/17 02:04:18 INFO ShuffleBlockFetcherIterator: Getting 3 (668.0 B) non-empty blocks including 3 (668.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:18,199] {docker.py:276} INFO - 21/05/17 02:04:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:18,200] {docker.py:276} INFO - 21/05/17 02:04:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447122163992420008472_0002_m_000081_79, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447122163992420008472_0002_m_000081_79}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447122163992420008472_0002}; taskId=attempt_202105170203447122163992420008472_0002_m_000081_79, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c717863}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:18,201] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Starting: Task committer attempt_202105170203447122163992420008472_0002_m_000081_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447122163992420008472_0002_m_000081_79
[2021-05-16 23:04:18,203] {docker.py:276} INFO - 21/05/17 02:04:18 INFO StagingCommitter: Task committer attempt_202105170203447122163992420008472_0002_m_000081_79: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447122163992420008472_0002_m_000081_79 : duration 0:00.003s
[2021-05-16 23:04:19,546] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203447709667933397235730_0002_m_000079_77: needsTaskCommit() Task attempt_202105170203447709667933397235730_0002_m_000079_77
[2021-05-16 23:04:19,548] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203447709667933397235730_0002_m_000079_77: needsTaskCommit() Task attempt_202105170203447709667933397235730_0002_m_000079_77: duration 0:00.002s
[2021-05-16 23:04:19,548] {docker.py:276} INFO - 21/05/17 02:04:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447709667933397235730_0002_m_000079_77
[2021-05-16 23:04:19,552] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Finished task 79.0 in stage 2.0 (TID 77). 4587 bytes result sent to driver
[2021-05-16 23:04:19,554] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 80) (e60b22510068, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:19,555] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Finished task 79.0 in stage 2.0 (TID 77) in 1742 ms on e60b22510068 (executor driver) (73/200)
[2021-05-16 23:04:19,556] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Running task 82.0 in stage 2.0 (TID 80)
[2021-05-16 23:04:19,565] {docker.py:276} INFO - 21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:19,566] {docker.py:276} INFO - 21/05/17 02:04:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:19,566] {docker.py:276} INFO - 21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:19,567] {docker.py:276} INFO - 21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344186999220234291065_0002_m_000082_80, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344186999220234291065_0002_m_000082_80}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344186999220234291065_0002}; taskId=attempt_20210517020344186999220234291065_0002_m_000082_80, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44541ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:19,567] {docker.py:276} INFO - 21/05/17 02:04:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:19,568] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_20210517020344186999220234291065_0002_m_000082_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344186999220234291065_0002_m_000082_80
[2021-05-16 23:04:19,571] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_20210517020344186999220234291065_0002_m_000082_80: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344186999220234291065_0002_m_000082_80 : duration 0:00.003s
[2021-05-16 23:04:19,597] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203441849968458471347005_0002_m_000078_76: needsTaskCommit() Task attempt_202105170203441849968458471347005_0002_m_000078_76
[2021-05-16 23:04:19,598] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203441849968458471347005_0002_m_000078_76: needsTaskCommit() Task attempt_202105170203441849968458471347005_0002_m_000078_76: duration 0:00.001s
21/05/17 02:04:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441849968458471347005_0002_m_000078_76
[2021-05-16 23:04:19,599] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Finished task 78.0 in stage 2.0 (TID 76). 4587 bytes result sent to driver
[2021-05-16 23:04:19,600] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Starting task 83.0 in stage 2.0 (TID 81) (e60b22510068, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:19,601] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Running task 83.0 in stage 2.0 (TID 81)
[2021-05-16 23:04:19,602] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Finished task 78.0 in stage 2.0 (TID 76) in 1796 ms on e60b22510068 (executor driver) (74/200)
[2021-05-16 23:04:19,610] {docker.py:276} INFO - 21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:19,611] {docker.py:276} INFO - 21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:19,612] {docker.py:276} INFO - 21/05/17 02:04:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:19,613] {docker.py:276} INFO - 21/05/17 02:04:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:19,613] {docker.py:276} INFO - 21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444520855913537056129_0002_m_000083_81, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444520855913537056129_0002_m_000083_81}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444520855913537056129_0002}; taskId=attempt_202105170203444520855913537056129_0002_m_000083_81, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27275071}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203444520855913537056129_0002_m_000083_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444520855913537056129_0002_m_000083_81
[2021-05-16 23:04:19,616] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203444520855913537056129_0002_m_000083_81: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444520855913537056129_0002_m_000083_81 : duration 0:00.002s
[2021-05-16 23:04:19,806] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203446302784810090062546_0002_m_000080_78: needsTaskCommit() Task attempt_202105170203446302784810090062546_0002_m_000080_78
[2021-05-16 23:04:19,807] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203446302784810090062546_0002_m_000080_78: needsTaskCommit() Task attempt_202105170203446302784810090062546_0002_m_000080_78: duration 0:00.001s
21/05/17 02:04:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446302784810090062546_0002_m_000080_78
[2021-05-16 23:04:19,808] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Finished task 80.0 in stage 2.0 (TID 78). 4587 bytes result sent to driver
[2021-05-16 23:04:19,810] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Starting task 85.0 in stage 2.0 (TID 82) (e60b22510068, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:19,810] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Running task 85.0 in stage 2.0 (TID 82)
[2021-05-16 23:04:19,811] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Finished task 80.0 in stage 2.0 (TID 78) in 1729 ms on e60b22510068 (executor driver) (75/200)
[2021-05-16 23:04:19,820] {docker.py:276} INFO - 21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Getting 3 (629.0 B) non-empty blocks including 3 (629.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:19,822] {docker.py:276} INFO - 21/05/17 02:04:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448296379057416253294_0002_m_000085_82, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448296379057416253294_0002_m_000085_82}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448296379057416253294_0002}; taskId=attempt_202105170203448296379057416253294_0002_m_000085_82, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@808b5d3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:19,822] {docker.py:276} INFO - 21/05/17 02:04:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203448296379057416253294_0002_m_000085_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448296379057416253294_0002_m_000085_82
[2021-05-16 23:04:19,826] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203448296379057416253294_0002_m_000085_82: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448296379057416253294_0002_m_000085_82 : duration 0:00.003s
[2021-05-16 23:04:19,948] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203447122163992420008472_0002_m_000081_79: needsTaskCommit() Task attempt_202105170203447122163992420008472_0002_m_000081_79
[2021-05-16 23:04:19,949] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203447122163992420008472_0002_m_000081_79: needsTaskCommit() Task attempt_202105170203447122163992420008472_0002_m_000081_79: duration 0:00.001s
21/05/17 02:04:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447122163992420008472_0002_m_000081_79
[2021-05-16 23:04:19,951] {docker.py:276} INFO - 21/05/17 02:04:19 INFO Executor: Finished task 81.0 in stage 2.0 (TID 79). 4544 bytes result sent to driver
[2021-05-16 23:04:19,954] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 83) (e60b22510068, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:19,955] {docker.py:276} INFO - 21/05/17 02:04:19 INFO TaskSetManager: Finished task 81.0 in stage 2.0 (TID 79) in 1768 ms on e60b22510068 (executor driver) (76/200)
21/05/17 02:04:19 INFO Executor: Running task 86.0 in stage 2.0 (TID 83)
[2021-05-16 23:04:19,965] {docker.py:276} INFO - 21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:19,967] {docker.py:276} INFO - 21/05/17 02:04:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442933158426892405265_0002_m_000086_83, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442933158426892405265_0002_m_000086_83}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442933158426892405265_0002}; taskId=attempt_202105170203442933158426892405265_0002_m_000086_83, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bc0783d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:19 INFO StagingCommitter: Starting: Task committer attempt_202105170203442933158426892405265_0002_m_000086_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442933158426892405265_0002_m_000086_83
[2021-05-16 23:04:19,970] {docker.py:276} INFO - 21/05/17 02:04:19 INFO StagingCommitter: Task committer attempt_202105170203442933158426892405265_0002_m_000086_83: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442933158426892405265_0002_m_000086_83 : duration 0:00.003s
[2021-05-16 23:04:21,254] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_20210517020344186999220234291065_0002_m_000082_80: needsTaskCommit() Task attempt_20210517020344186999220234291065_0002_m_000082_80
[2021-05-16 23:04:21,255] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_20210517020344186999220234291065_0002_m_000082_80: needsTaskCommit() Task attempt_20210517020344186999220234291065_0002_m_000082_80: duration 0:00.001s
21/05/17 02:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344186999220234291065_0002_m_000082_80
[2021-05-16 23:04:21,257] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Finished task 82.0 in stage 2.0 (TID 80). 4544 bytes result sent to driver
[2021-05-16 23:04:21,260] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Starting task 87.0 in stage 2.0 (TID 84) (e60b22510068, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:21,260] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Running task 87.0 in stage 2.0 (TID 84)
21/05/17 02:04:21 INFO TaskSetManager: Finished task 82.0 in stage 2.0 (TID 80) in 1707 ms on e60b22510068 (executor driver) (77/200)
[2021-05-16 23:04:21,269] {docker.py:276} INFO - 21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (624.0 B) non-empty blocks including 3 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:21,271] {docker.py:276} INFO - 21/05/17 02:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:21,271] {docker.py:276} INFO - 21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445327604345446169538_0002_m_000087_84, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445327604345446169538_0002_m_000087_84}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445327604345446169538_0002}; taskId=attempt_202105170203445327604345446169538_0002_m_000087_84, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31c1b51c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203445327604345446169538_0002_m_000087_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445327604345446169538_0002_m_000087_84
[2021-05-16 23:04:21,274] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203445327604345446169538_0002_m_000087_84: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445327604345446169538_0002_m_000087_84 : duration 0:00.003s
[2021-05-16 23:04:21,303] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203444520855913537056129_0002_m_000083_81: needsTaskCommit() Task attempt_202105170203444520855913537056129_0002_m_000083_81
[2021-05-16 23:04:21,304] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203444520855913537056129_0002_m_000083_81: needsTaskCommit() Task attempt_202105170203444520855913537056129_0002_m_000083_81: duration 0:00.001s
21/05/17 02:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444520855913537056129_0002_m_000083_81
[2021-05-16 23:04:21,306] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Finished task 83.0 in stage 2.0 (TID 81). 4544 bytes result sent to driver
[2021-05-16 23:04:21,307] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Starting task 88.0 in stage 2.0 (TID 85) (e60b22510068, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:21,309] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Finished task 83.0 in stage 2.0 (TID 81) in 1711 ms on e60b22510068 (executor driver) (78/200)
21/05/17 02:04:21 INFO Executor: Running task 88.0 in stage 2.0 (TID 85)
[2021-05-16 23:04:21,327] {docker.py:276} INFO - 21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (650.0 B) non-empty blocks including 3 (650.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:21,336] {docker.py:276} INFO - 21/05/17 02:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:21,337] {docker.py:276} INFO - 21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443522444232563791759_0002_m_000088_85, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443522444232563791759_0002_m_000088_85}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443522444232563791759_0002}; taskId=attempt_202105170203443522444232563791759_0002_m_000088_85, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bb55b38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203443522444232563791759_0002_m_000088_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443522444232563791759_0002_m_000088_85
[2021-05-16 23:04:21,339] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203443522444232563791759_0002_m_000088_85: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443522444232563791759_0002_m_000088_85 : duration 0:00.003s
[2021-05-16 23:04:21,476] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203448296379057416253294_0002_m_000085_82: needsTaskCommit() Task attempt_202105170203448296379057416253294_0002_m_000085_82
[2021-05-16 23:04:21,477] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203448296379057416253294_0002_m_000085_82: needsTaskCommit() Task attempt_202105170203448296379057416253294_0002_m_000085_82: duration 0:00.001s
21/05/17 02:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448296379057416253294_0002_m_000085_82
[2021-05-16 23:04:21,478] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Finished task 85.0 in stage 2.0 (TID 82). 4587 bytes result sent to driver
[2021-05-16 23:04:21,480] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 86) (e60b22510068, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:21,481] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Finished task 85.0 in stage 2.0 (TID 82) in 1674 ms on e60b22510068 (executor driver) (79/200)
[2021-05-16 23:04:21,481] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Running task 89.0 in stage 2.0 (TID 86)
[2021-05-16 23:04:21,491] {docker.py:276} INFO - 21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:21,493] {docker.py:276} INFO - 21/05/17 02:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:21,494] {docker.py:276} INFO - 21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442508700475383243101_0002_m_000089_86, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442508700475383243101_0002_m_000089_86}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442508700475383243101_0002}; taskId=attempt_202105170203442508700475383243101_0002_m_000089_86, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55067dae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:21,494] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203442508700475383243101_0002_m_000089_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442508700475383243101_0002_m_000089_86
[2021-05-16 23:04:21,498] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203442508700475383243101_0002_m_000089_86: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442508700475383243101_0002_m_000089_86 : duration 0:00.004s
[2021-05-16 23:04:21,665] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203442933158426892405265_0002_m_000086_83: needsTaskCommit() Task attempt_202105170203442933158426892405265_0002_m_000086_83
[2021-05-16 23:04:21,666] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203442933158426892405265_0002_m_000086_83: needsTaskCommit() Task attempt_202105170203442933158426892405265_0002_m_000086_83: duration 0:00.000s
21/05/17 02:04:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442933158426892405265_0002_m_000086_83
[2021-05-16 23:04:21,670] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Finished task 86.0 in stage 2.0 (TID 83). 4587 bytes result sent to driver
21/05/17 02:04:21 INFO TaskSetManager: Starting task 90.0 in stage 2.0 (TID 87) (e60b22510068, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:21,671] {docker.py:276} INFO - 21/05/17 02:04:21 INFO Executor: Running task 90.0 in stage 2.0 (TID 87)
[2021-05-16 23:04:21,671] {docker.py:276} INFO - 21/05/17 02:04:21 INFO TaskSetManager: Finished task 86.0 in stage 2.0 (TID 83) in 1719 ms on e60b22510068 (executor driver) (80/200)
[2021-05-16 23:04:21,680] {docker.py:276} INFO - 21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Getting 3 (694.0 B) non-empty blocks including 3 (694.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:21,681] {docker.py:276} INFO - 21/05/17 02:04:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:21,682] {docker.py:276} INFO - 21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:21,682] {docker.py:276} INFO - 21/05/17 02:04:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441853202987478095585_0002_m_000090_87, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441853202987478095585_0002_m_000090_87}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441853202987478095585_0002}; taskId=attempt_202105170203441853202987478095585_0002_m_000090_87, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f84fe04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:21,683] {docker.py:276} INFO - 21/05/17 02:04:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:21,683] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Starting: Task committer attempt_202105170203441853202987478095585_0002_m_000090_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441853202987478095585_0002_m_000090_87
[2021-05-16 23:04:21,685] {docker.py:276} INFO - 21/05/17 02:04:21 INFO StagingCommitter: Task committer attempt_202105170203441853202987478095585_0002_m_000090_87: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441853202987478095585_0002_m_000090_87 : duration 0:00.003s
[2021-05-16 23:04:23,019] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203445327604345446169538_0002_m_000087_84: needsTaskCommit() Task attempt_202105170203445327604345446169538_0002_m_000087_84
[2021-05-16 23:04:23,020] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203445327604345446169538_0002_m_000087_84: needsTaskCommit() Task attempt_202105170203445327604345446169538_0002_m_000087_84: duration 0:00.001s
[2021-05-16 23:04:23,021] {docker.py:276} INFO - 21/05/17 02:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445327604345446169538_0002_m_000087_84
[2021-05-16 23:04:23,022] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Finished task 87.0 in stage 2.0 (TID 84). 4587 bytes result sent to driver
[2021-05-16 23:04:23,023] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203443522444232563791759_0002_m_000088_85: needsTaskCommit() Task attempt_202105170203443522444232563791759_0002_m_000088_85
[2021-05-16 23:04:23,024] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203443522444232563791759_0002_m_000088_85: needsTaskCommit() Task attempt_202105170203443522444232563791759_0002_m_000088_85: duration 0:00.000s
[2021-05-16 23:04:23,024] {docker.py:276} INFO - 21/05/17 02:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443522444232563791759_0002_m_000088_85
[2021-05-16 23:04:23,027] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Finished task 88.0 in stage 2.0 (TID 85). 4587 bytes result sent to driver
[2021-05-16 23:04:23,028] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Starting task 91.0 in stage 2.0 (TID 88) (e60b22510068, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:23,029] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Finished task 87.0 in stage 2.0 (TID 84) in 1773 ms on e60b22510068 (executor driver) (81/200)
[2021-05-16 23:04:23,030] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Finished task 88.0 in stage 2.0 (TID 85) in 1725 ms on e60b22510068 (executor driver) (82/200)
[2021-05-16 23:04:23,030] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Starting task 94.0 in stage 2.0 (TID 89) (e60b22510068, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:23,031] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Running task 94.0 in stage 2.0 (TID 89)
[2021-05-16 23:04:23,033] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Running task 91.0 in stage 2.0 (TID 88)
[2021-05-16 23:04:23,040] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:23,042] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:23,042] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:23,043] {docker.py:276} INFO - 21/05/17 02:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:23,044] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:23,044] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447835085448285989944_0002_m_000094_89, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447835085448285989944_0002_m_000094_89}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447835085448285989944_0002}; taskId=attempt_202105170203447835085448285989944_0002_m_000094_89, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fc79c4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203447835085448285989944_0002_m_000094_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447835085448285989944_0002_m_000094_89
[2021-05-16 23:04:23,044] {docker.py:276} INFO - 21/05/17 02:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:23,045] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:23,045] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447096962734360149718_0002_m_000091_88, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447096962734360149718_0002_m_000091_88}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447096962734360149718_0002}; taskId=attempt_202105170203447096962734360149718_0002_m_000091_88, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c64ad27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:23,046] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203447096962734360149718_0002_m_000091_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447096962734360149718_0002_m_000091_88
[2021-05-16 23:04:23,047] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203447835085448285989944_0002_m_000094_89: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447835085448285989944_0002_m_000094_89 : duration 0:00.004s
[2021-05-16 23:04:23,049] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203447096962734360149718_0002_m_000091_88: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447096962734360149718_0002_m_000091_88 : duration 0:00.003s
[2021-05-16 23:04:23,196] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203442508700475383243101_0002_m_000089_86: needsTaskCommit() Task attempt_202105170203442508700475383243101_0002_m_000089_86
21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203442508700475383243101_0002_m_000089_86: needsTaskCommit() Task attempt_202105170203442508700475383243101_0002_m_000089_86: duration 0:00.000s
[2021-05-16 23:04:23,196] {docker.py:276} INFO - 21/05/17 02:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442508700475383243101_0002_m_000089_86
[2021-05-16 23:04:23,198] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Finished task 89.0 in stage 2.0 (TID 86). 4544 bytes result sent to driver
[2021-05-16 23:04:23,199] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 90) (e60b22510068, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:23,200] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Running task 95.0 in stage 2.0 (TID 90)
21/05/17 02:04:23 INFO TaskSetManager: Finished task 89.0 in stage 2.0 (TID 86) in 1723 ms on e60b22510068 (executor driver) (83/200)
[2021-05-16 23:04:23,208] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:23,211] {docker.py:276} INFO - 21/05/17 02:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:23,212] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447914364460458291285_0002_m_000095_90, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447914364460458291285_0002_m_000095_90}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447914364460458291285_0002}; taskId=attempt_202105170203447914364460458291285_0002_m_000095_90, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b0caf5f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:23,212] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203447914364460458291285_0002_m_000095_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447914364460458291285_0002_m_000095_90
[2021-05-16 23:04:23,215] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203447914364460458291285_0002_m_000095_90: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447914364460458291285_0002_m_000095_90 : duration 0:00.004s
[2021-05-16 23:04:23,351] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203441853202987478095585_0002_m_000090_87: needsTaskCommit() Task attempt_202105170203441853202987478095585_0002_m_000090_87
[2021-05-16 23:04:23,352] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203441853202987478095585_0002_m_000090_87: needsTaskCommit() Task attempt_202105170203441853202987478095585_0002_m_000090_87: duration 0:00.002s
[2021-05-16 23:04:23,353] {docker.py:276} INFO - 21/05/17 02:04:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441853202987478095585_0002_m_000090_87
[2021-05-16 23:04:23,355] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Finished task 90.0 in stage 2.0 (TID 87). 4544 bytes result sent to driver
[2021-05-16 23:04:23,357] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Starting task 96.0 in stage 2.0 (TID 91) (e60b22510068, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:23,358] {docker.py:276} INFO - 21/05/17 02:04:23 INFO Executor: Running task 96.0 in stage 2.0 (TID 91)
[2021-05-16 23:04:23,359] {docker.py:276} INFO - 21/05/17 02:04:23 INFO TaskSetManager: Finished task 90.0 in stage 2.0 (TID 87) in 1694 ms on e60b22510068 (executor driver) (84/200)
[2021-05-16 23:04:23,369] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:23,369] {docker.py:276} INFO - 21/05/17 02:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:23,371] {docker.py:276} INFO - 21/05/17 02:04:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:23,371] {docker.py:276} INFO - 21/05/17 02:04:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:23,372] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:23,372] {docker.py:276} INFO - 21/05/17 02:04:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446851215599165104437_0002_m_000096_91, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446851215599165104437_0002_m_000096_91}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446851215599165104437_0002}; taskId=attempt_202105170203446851215599165104437_0002_m_000096_91, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34bd9a03}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:23,373] {docker.py:276} INFO - 21/05/17 02:04:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:23,373] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Starting: Task committer attempt_202105170203446851215599165104437_0002_m_000096_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446851215599165104437_0002_m_000096_91
[2021-05-16 23:04:23,375] {docker.py:276} INFO - 21/05/17 02:04:23 INFO StagingCommitter: Task committer attempt_202105170203446851215599165104437_0002_m_000096_91: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446851215599165104437_0002_m_000096_91 : duration 0:00.003s
[2021-05-16 23:04:24,686] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203447835085448285989944_0002_m_000094_89: needsTaskCommit() Task attempt_202105170203447835085448285989944_0002_m_000094_89
[2021-05-16 23:04:24,687] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203447835085448285989944_0002_m_000094_89: needsTaskCommit() Task attempt_202105170203447835085448285989944_0002_m_000094_89: duration 0:00.001s
[2021-05-16 23:04:24,688] {docker.py:276} INFO - 21/05/17 02:04:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447835085448285989944_0002_m_000094_89
[2021-05-16 23:04:24,690] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Finished task 94.0 in stage 2.0 (TID 89). 4544 bytes result sent to driver
[2021-05-16 23:04:24,692] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Starting task 97.0 in stage 2.0 (TID 92) (e60b22510068, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:24,693] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203447096962734360149718_0002_m_000091_88: needsTaskCommit() Task attempt_202105170203447096962734360149718_0002_m_000091_88
[2021-05-16 23:04:24,694] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Finished task 94.0 in stage 2.0 (TID 89) in 1666 ms on e60b22510068 (executor driver) (85/200)
21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203447096962734360149718_0002_m_000091_88: needsTaskCommit() Task attempt_202105170203447096962734360149718_0002_m_000091_88: duration 0:00.001s
[2021-05-16 23:04:24,697] {docker.py:276} INFO - 21/05/17 02:04:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447096962734360149718_0002_m_000091_88
[2021-05-16 23:04:24,698] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Running task 97.0 in stage 2.0 (TID 92)
[2021-05-16 23:04:24,698] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Finished task 91.0 in stage 2.0 (TID 88). 4544 bytes result sent to driver
[2021-05-16 23:04:24,699] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Starting task 98.0 in stage 2.0 (TID 93) (e60b22510068, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:24,699] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Finished task 91.0 in stage 2.0 (TID 88) in 1673 ms on e60b22510068 (executor driver) (86/200)
[2021-05-16 23:04:24,699] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Running task 98.0 in stage 2.0 (TID 93)
[2021-05-16 23:04:24,715] {docker.py:276} INFO - 21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:24,716] {docker.py:276} INFO - 21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:24,717] {docker.py:276} INFO - 21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:24,717] {docker.py:276} INFO - 21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:24,718] {docker.py:276} INFO - 21/05/17 02:04:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443086044687057866913_0002_m_000098_93, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443086044687057866913_0002_m_000098_93}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443086044687057866913_0002}; taskId=attempt_202105170203443086044687057866913_0002_m_000098_93, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d47d849}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203443086044687057866913_0002_m_000098_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443086044687057866913_0002_m_000098_93
[2021-05-16 23:04:24,719] {docker.py:276} INFO - 21/05/17 02:04:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:24,720] {docker.py:276} INFO - 21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445426670019021878408_0002_m_000097_92, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445426670019021878408_0002_m_000097_92}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445426670019021878408_0002}; taskId=attempt_202105170203445426670019021878408_0002_m_000097_92, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31ccb272}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203445426670019021878408_0002_m_000097_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445426670019021878408_0002_m_000097_92
[2021-05-16 23:04:24,722] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203443086044687057866913_0002_m_000098_93: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443086044687057866913_0002_m_000098_93 : duration 0:00.004s
[2021-05-16 23:04:24,723] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203445426670019021878408_0002_m_000097_92: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445426670019021878408_0002_m_000097_92 : duration 0:00.003s
[2021-05-16 23:04:24,903] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203447914364460458291285_0002_m_000095_90: needsTaskCommit() Task attempt_202105170203447914364460458291285_0002_m_000095_90
[2021-05-16 23:04:24,904] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203447914364460458291285_0002_m_000095_90: needsTaskCommit() Task attempt_202105170203447914364460458291285_0002_m_000095_90: duration 0:00.001s
[2021-05-16 23:04:24,904] {docker.py:276} INFO - 21/05/17 02:04:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447914364460458291285_0002_m_000095_90
[2021-05-16 23:04:24,906] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Finished task 95.0 in stage 2.0 (TID 90). 4587 bytes result sent to driver
[2021-05-16 23:04:24,907] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Starting task 99.0 in stage 2.0 (TID 94) (e60b22510068, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:24,909] {docker.py:276} INFO - 21/05/17 02:04:24 INFO TaskSetManager: Finished task 95.0 in stage 2.0 (TID 90) in 1712 ms on e60b22510068 (executor driver) (87/200)
[2021-05-16 23:04:24,909] {docker.py:276} INFO - 21/05/17 02:04:24 INFO Executor: Running task 99.0 in stage 2.0 (TID 94)
[2021-05-16 23:04:24,918] {docker.py:276} INFO - 21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:24,919] {docker.py:276} INFO - 21/05/17 02:04:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:24,920] {docker.py:276} INFO - 21/05/17 02:04:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447969545312012417511_0002_m_000099_94, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447969545312012417511_0002_m_000099_94}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447969545312012417511_0002}; taskId=attempt_202105170203447969545312012417511_0002_m_000099_94, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@98c8f25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:24,920] {docker.py:276} INFO - 21/05/17 02:04:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:24,920] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Starting: Task committer attempt_202105170203447969545312012417511_0002_m_000099_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447969545312012417511_0002_m_000099_94
[2021-05-16 23:04:24,923] {docker.py:276} INFO - 21/05/17 02:04:24 INFO StagingCommitter: Task committer attempt_202105170203447969545312012417511_0002_m_000099_94: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447969545312012417511_0002_m_000099_94 : duration 0:00.002s
[2021-05-16 23:04:25,058] {docker.py:276} INFO - 21/05/17 02:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105170203446851215599165104437_0002_m_000096_91: needsTaskCommit() Task attempt_202105170203446851215599165104437_0002_m_000096_91
21/05/17 02:04:25 INFO StagingCommitter: Task committer attempt_202105170203446851215599165104437_0002_m_000096_91: needsTaskCommit() Task attempt_202105170203446851215599165104437_0002_m_000096_91: duration 0:00.000s
21/05/17 02:04:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446851215599165104437_0002_m_000096_91
[2021-05-16 23:04:25,061] {docker.py:276} INFO - 21/05/17 02:04:25 INFO Executor: Finished task 96.0 in stage 2.0 (TID 91). 4587 bytes result sent to driver
[2021-05-16 23:04:25,062] {docker.py:276} INFO - 21/05/17 02:04:25 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 95) (e60b22510068, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:25,063] {docker.py:276} INFO - 21/05/17 02:04:25 INFO TaskSetManager: Finished task 96.0 in stage 2.0 (TID 91) in 1708 ms on e60b22510068 (executor driver) (88/200)
[2021-05-16 23:04:25,064] {docker.py:276} INFO - 21/05/17 02:04:25 INFO Executor: Running task 100.0 in stage 2.0 (TID 95)
[2021-05-16 23:04:25,073] {docker.py:276} INFO - 21/05/17 02:04:25 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:25,074] {docker.py:276} INFO - 21/05/17 02:04:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:25,075] {docker.py:276} INFO - 21/05/17 02:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:25,076] {docker.py:276} INFO - 21/05/17 02:04:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446870048916669287319_0002_m_000100_95, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446870048916669287319_0002_m_000100_95}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446870048916669287319_0002}; taskId=attempt_202105170203446870048916669287319_0002_m_000100_95, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e7dbc1e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:25,076] {docker.py:276} INFO - 21/05/17 02:04:25 INFO StagingCommitter: Starting: Task committer attempt_202105170203446870048916669287319_0002_m_000100_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446870048916669287319_0002_m_000100_95
[2021-05-16 23:04:25,078] {docker.py:276} INFO - 21/05/17 02:04:25 INFO StagingCommitter: Task committer attempt_202105170203446870048916669287319_0002_m_000100_95: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446870048916669287319_0002_m_000100_95 : duration 0:00.003s
[2021-05-16 23:04:26,403] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203443086044687057866913_0002_m_000098_93: needsTaskCommit() Task attempt_202105170203443086044687057866913_0002_m_000098_93
[2021-05-16 23:04:26,404] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203443086044687057866913_0002_m_000098_93: needsTaskCommit() Task attempt_202105170203443086044687057866913_0002_m_000098_93: duration 0:00.002s
21/05/17 02:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443086044687057866913_0002_m_000098_93
[2021-05-16 23:04:26,407] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Finished task 98.0 in stage 2.0 (TID 93). 4587 bytes result sent to driver
[2021-05-16 23:04:26,408] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 96) (e60b22510068, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:26,410] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Running task 101.0 in stage 2.0 (TID 96)
21/05/17 02:04:26 INFO TaskSetManager: Finished task 98.0 in stage 2.0 (TID 93) in 1715 ms on e60b22510068 (executor driver) (89/200)
[2021-05-16 23:04:26,421] {docker.py:276} INFO - 21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:26,424] {docker.py:276} INFO - 21/05/17 02:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:26,424] {docker.py:276} INFO - 21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:26,424] {docker.py:276} INFO - 21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446133534243202346538_0002_m_000101_96, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446133534243202346538_0002_m_000101_96}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446133534243202346538_0002}; taskId=attempt_202105170203446133534243202346538_0002_m_000101_96, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@656b3bc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:26,425] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203446133534243202346538_0002_m_000101_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446133534243202346538_0002_m_000101_96
[2021-05-16 23:04:26,427] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203446133534243202346538_0002_m_000101_96: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446133534243202346538_0002_m_000101_96 : duration 0:00.003s
[2021-05-16 23:04:26,438] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203445426670019021878408_0002_m_000097_92: needsTaskCommit() Task attempt_202105170203445426670019021878408_0002_m_000097_92
[2021-05-16 23:04:26,438] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203445426670019021878408_0002_m_000097_92: needsTaskCommit() Task attempt_202105170203445426670019021878408_0002_m_000097_92: duration 0:00.000s
[2021-05-16 23:04:26,439] {docker.py:276} INFO - 21/05/17 02:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445426670019021878408_0002_m_000097_92
[2021-05-16 23:04:26,439] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Finished task 97.0 in stage 2.0 (TID 92). 4587 bytes result sent to driver
[2021-05-16 23:04:26,440] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Starting task 102.0 in stage 2.0 (TID 97) (e60b22510068, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:26,441] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Running task 102.0 in stage 2.0 (TID 97)
[2021-05-16 23:04:26,441] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Finished task 97.0 in stage 2.0 (TID 92) in 1753 ms on e60b22510068 (executor driver) (90/200)
[2021-05-16 23:04:26,449] {docker.py:276} INFO - 21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Getting 1 (171.0 B) non-empty blocks including 1 (171.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:26,451] {docker.py:276} INFO - 21/05/17 02:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447000774867767506730_0002_m_000102_97, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447000774867767506730_0002_m_000102_97}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447000774867767506730_0002}; taskId=attempt_202105170203447000774867767506730_0002_m_000102_97, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33c09e28}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:26,451] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203447000774867767506730_0002_m_000102_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447000774867767506730_0002_m_000102_97
[2021-05-16 23:04:26,454] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203447000774867767506730_0002_m_000102_97: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447000774867767506730_0002_m_000102_97 : duration 0:00.003s
[2021-05-16 23:04:26,593] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203447969545312012417511_0002_m_000099_94: needsTaskCommit() Task attempt_202105170203447969545312012417511_0002_m_000099_94
[2021-05-16 23:04:26,594] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203447969545312012417511_0002_m_000099_94: needsTaskCommit() Task attempt_202105170203447969545312012417511_0002_m_000099_94: duration 0:00.001s
21/05/17 02:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447969545312012417511_0002_m_000099_94
[2021-05-16 23:04:26,595] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Finished task 99.0 in stage 2.0 (TID 94). 4544 bytes result sent to driver
[2021-05-16 23:04:26,597] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Starting task 103.0 in stage 2.0 (TID 98) (e60b22510068, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:26,598] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Running task 103.0 in stage 2.0 (TID 98)
[2021-05-16 23:04:26,599] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Finished task 99.0 in stage 2.0 (TID 94) in 1693 ms on e60b22510068 (executor driver) (91/200)
[2021-05-16 23:04:26,607] {docker.py:276} INFO - 21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:26,609] {docker.py:276} INFO - 21/05/17 02:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446218423450213665860_0002_m_000103_98, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446218423450213665860_0002_m_000103_98}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446218423450213665860_0002}; taskId=attempt_202105170203446218423450213665860_0002_m_000103_98, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38b9e4e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:26,609] {docker.py:276} INFO - 21/05/17 02:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:26,610] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203446218423450213665860_0002_m_000103_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446218423450213665860_0002_m_000103_98
[2021-05-16 23:04:26,613] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203446218423450213665860_0002_m_000103_98: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446218423450213665860_0002_m_000103_98 : duration 0:00.003s
[2021-05-16 23:04:26,736] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203446870048916669287319_0002_m_000100_95: needsTaskCommit() Task attempt_202105170203446870048916669287319_0002_m_000100_95
[2021-05-16 23:04:26,737] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203446870048916669287319_0002_m_000100_95: needsTaskCommit() Task attempt_202105170203446870048916669287319_0002_m_000100_95: duration 0:00.000s
21/05/17 02:04:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446870048916669287319_0002_m_000100_95
[2021-05-16 23:04:26,739] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Finished task 100.0 in stage 2.0 (TID 95). 4544 bytes result sent to driver
[2021-05-16 23:04:26,741] {docker.py:276} INFO - 21/05/17 02:04:26 INFO TaskSetManager: Starting task 104.0 in stage 2.0 (TID 99) (e60b22510068, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:26,742] {docker.py:276} INFO - 21/05/17 02:04:26 INFO Executor: Running task 104.0 in stage 2.0 (TID 99)
21/05/17 02:04:26 INFO TaskSetManager: Finished task 100.0 in stage 2.0 (TID 95) in 1682 ms on e60b22510068 (executor driver) (92/200)
[2021-05-16 23:04:26,752] {docker.py:276} INFO - 21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:26,755] {docker.py:276} INFO - 21/05/17 02:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:26,755] {docker.py:276} INFO - 21/05/17 02:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:26,756] {docker.py:276} INFO - 21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443029112058464191860_0002_m_000104_99, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443029112058464191860_0002_m_000104_99}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443029112058464191860_0002}; taskId=attempt_202105170203443029112058464191860_0002_m_000104_99, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e54a1d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:26,756] {docker.py:276} INFO - 21/05/17 02:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:26 INFO StagingCommitter: Starting: Task committer attempt_202105170203443029112058464191860_0002_m_000104_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443029112058464191860_0002_m_000104_99
[2021-05-16 23:04:26,759] {docker.py:276} INFO - 21/05/17 02:04:26 INFO StagingCommitter: Task committer attempt_202105170203443029112058464191860_0002_m_000104_99: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443029112058464191860_0002_m_000104_99 : duration 0:00.003s
[2021-05-16 23:04:28,111] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203447000774867767506730_0002_m_000102_97: needsTaskCommit() Task attempt_202105170203447000774867767506730_0002_m_000102_97
[2021-05-16 23:04:28,112] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203447000774867767506730_0002_m_000102_97: needsTaskCommit() Task attempt_202105170203447000774867767506730_0002_m_000102_97: duration 0:00.000s
21/05/17 02:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447000774867767506730_0002_m_000102_97
[2021-05-16 23:04:28,113] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Finished task 102.0 in stage 2.0 (TID 97). 4587 bytes result sent to driver
[2021-05-16 23:04:28,115] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Starting task 105.0 in stage 2.0 (TID 100) (e60b22510068, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:28,116] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Running task 105.0 in stage 2.0 (TID 100)
[2021-05-16 23:04:28,117] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Finished task 102.0 in stage 2.0 (TID 97) in 1644 ms on e60b22510068 (executor driver) (93/200)
[2021-05-16 23:04:28,126] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:28,127] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:28,129] {docker.py:276} INFO - 21/05/17 02:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,130] {docker.py:276} INFO - 21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445518940227414983402_0002_m_000105_100, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445518940227414983402_0002_m_000105_100}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445518940227414983402_0002}; taskId=attempt_202105170203445518940227414983402_0002_m_000105_100, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@576a81bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,130] {docker.py:276} INFO - 21/05/17 02:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:28,131] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203445518940227414983402_0002_m_000105_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445518940227414983402_0002_m_000105_100
[2021-05-16 23:04:28,135] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203445518940227414983402_0002_m_000105_100: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445518940227414983402_0002_m_000105_100 : duration 0:00.004s
[2021-05-16 23:04:28,144] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203446133534243202346538_0002_m_000101_96: needsTaskCommit() Task attempt_202105170203446133534243202346538_0002_m_000101_96
[2021-05-16 23:04:28,144] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203446133534243202346538_0002_m_000101_96: needsTaskCommit() Task attempt_202105170203446133534243202346538_0002_m_000101_96: duration 0:00.001s
21/05/17 02:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446133534243202346538_0002_m_000101_96
[2021-05-16 23:04:28,145] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Finished task 101.0 in stage 2.0 (TID 96). 4587 bytes result sent to driver
[2021-05-16 23:04:28,147] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Starting task 106.0 in stage 2.0 (TID 101) (e60b22510068, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:28,148] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Running task 106.0 in stage 2.0 (TID 101)
[2021-05-16 23:04:28,148] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Finished task 101.0 in stage 2.0 (TID 96) in 1707 ms on e60b22510068 (executor driver) (94/200)
[2021-05-16 23:04:28,156] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:28,157] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:28,158] {docker.py:276} INFO - 21/05/17 02:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,159] {docker.py:276} INFO - 21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444716754443410936493_0002_m_000106_101, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444716754443410936493_0002_m_000106_101}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444716754443410936493_0002}; taskId=attempt_202105170203444716754443410936493_0002_m_000106_101, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d48c06d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,159] {docker.py:276} INFO - 21/05/17 02:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:28,160] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203444716754443410936493_0002_m_000106_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444716754443410936493_0002_m_000106_101
[2021-05-16 23:04:28,163] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203444716754443410936493_0002_m_000106_101: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444716754443410936493_0002_m_000106_101 : duration 0:00.003s
[2021-05-16 23:04:28,301] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203446218423450213665860_0002_m_000103_98: needsTaskCommit() Task attempt_202105170203446218423450213665860_0002_m_000103_98
[2021-05-16 23:04:28,301] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203446218423450213665860_0002_m_000103_98: needsTaskCommit() Task attempt_202105170203446218423450213665860_0002_m_000103_98: duration 0:00.001s
21/05/17 02:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446218423450213665860_0002_m_000103_98
[2021-05-16 23:04:28,303] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Finished task 103.0 in stage 2.0 (TID 98). 4587 bytes result sent to driver
[2021-05-16 23:04:28,304] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Starting task 107.0 in stage 2.0 (TID 102) (e60b22510068, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:28,305] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Running task 107.0 in stage 2.0 (TID 102)
21/05/17 02:04:28 INFO TaskSetManager: Finished task 103.0 in stage 2.0 (TID 98) in 1676 ms on e60b22510068 (executor driver) (95/200)
[2021-05-16 23:04:28,313] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:28,316] {docker.py:276} INFO - 21/05/17 02:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:28,316] {docker.py:276} INFO - 21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,317] {docker.py:276} INFO - 21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445774712642765999890_0002_m_000107_102, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445774712642765999890_0002_m_000107_102}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445774712642765999890_0002}; taskId=attempt_202105170203445774712642765999890_0002_m_000107_102, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76fb747c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203445774712642765999890_0002_m_000107_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445774712642765999890_0002_m_000107_102
[2021-05-16 23:04:28,320] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203445774712642765999890_0002_m_000107_102: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445774712642765999890_0002_m_000107_102 : duration 0:00.004s
[2021-05-16 23:04:28,464] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203443029112058464191860_0002_m_000104_99: needsTaskCommit() Task attempt_202105170203443029112058464191860_0002_m_000104_99
[2021-05-16 23:04:28,465] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203443029112058464191860_0002_m_000104_99: needsTaskCommit() Task attempt_202105170203443029112058464191860_0002_m_000104_99: duration 0:00.001s
21/05/17 02:04:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443029112058464191860_0002_m_000104_99
[2021-05-16 23:04:28,467] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Finished task 104.0 in stage 2.0 (TID 99). 4587 bytes result sent to driver
[2021-05-16 23:04:28,468] {docker.py:276} INFO - 21/05/17 02:04:28 INFO TaskSetManager: Starting task 108.0 in stage 2.0 (TID 103) (e60b22510068, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:28,469] {docker.py:276} INFO - 21/05/17 02:04:28 INFO Executor: Running task 108.0 in stage 2.0 (TID 103)
21/05/17 02:04:28 INFO TaskSetManager: Finished task 104.0 in stage 2.0 (TID 99) in 1696 ms on e60b22510068 (executor driver) (96/200)
[2021-05-16 23:04:28,479] {docker.py:276} INFO - 21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:28,481] {docker.py:276} INFO - 21/05/17 02:04:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444524607641584404533_0002_m_000108_103, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444524607641584404533_0002_m_000108_103}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444524607641584404533_0002}; taskId=attempt_202105170203444524607641584404533_0002_m_000108_103, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74b0b0b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:28,481] {docker.py:276} INFO - 21/05/17 02:04:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:28 INFO StagingCommitter: Starting: Task committer attempt_202105170203444524607641584404533_0002_m_000108_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444524607641584404533_0002_m_000108_103
[2021-05-16 23:04:28,484] {docker.py:276} INFO - 21/05/17 02:04:28 INFO StagingCommitter: Task committer attempt_202105170203444524607641584404533_0002_m_000108_103: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444524607641584404533_0002_m_000108_103 : duration 0:00.003s
[2021-05-16 23:04:29,517] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203445774712642765999890_0002_m_000107_102: needsTaskCommit() Task attempt_202105170203445774712642765999890_0002_m_000107_102
21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203445774712642765999890_0002_m_000107_102: needsTaskCommit() Task attempt_202105170203445774712642765999890_0002_m_000107_102: duration 0:00.001s
[2021-05-16 23:04:29,517] {docker.py:276} INFO - 21/05/17 02:04:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445774712642765999890_0002_m_000107_102
[2021-05-16 23:04:29,519] {docker.py:276} INFO - 21/05/17 02:04:29 INFO Executor: Finished task 107.0 in stage 2.0 (TID 102). 4544 bytes result sent to driver
[2021-05-16 23:04:29,519] {docker.py:276} INFO - 21/05/17 02:04:29 INFO TaskSetManager: Starting task 109.0 in stage 2.0 (TID 104) (e60b22510068, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:29,520] {docker.py:276} INFO - 21/05/17 02:04:29 INFO TaskSetManager: Finished task 107.0 in stage 2.0 (TID 102) in 1217 ms on e60b22510068 (executor driver) (97/200)
[2021-05-16 23:04:29,521] {docker.py:276} INFO - 21/05/17 02:04:29 INFO Executor: Running task 109.0 in stage 2.0 (TID 104)
[2021-05-16 23:04:29,529] {docker.py:276} INFO - 21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:29,531] {docker.py:276} INFO - 21/05/17 02:04:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:29,531] {docker.py:276} INFO - 21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441050383337193738765_0002_m_000109_104, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441050383337193738765_0002_m_000109_104}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441050383337193738765_0002}; taskId=attempt_202105170203441050383337193738765_0002_m_000109_104, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cb9e62d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203441050383337193738765_0002_m_000109_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441050383337193738765_0002_m_000109_104
[2021-05-16 23:04:29,534] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203441050383337193738765_0002_m_000109_104: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441050383337193738765_0002_m_000109_104 : duration 0:00.003s
[2021-05-16 23:04:29,811] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203445518940227414983402_0002_m_000105_100: needsTaskCommit() Task attempt_202105170203445518940227414983402_0002_m_000105_100
[2021-05-16 23:04:29,812] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203445518940227414983402_0002_m_000105_100: needsTaskCommit() Task attempt_202105170203445518940227414983402_0002_m_000105_100: duration 0:00.001s
[2021-05-16 23:04:29,812] {docker.py:276} INFO - 21/05/17 02:04:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445518940227414983402_0002_m_000105_100
[2021-05-16 23:04:29,815] {docker.py:276} INFO - 21/05/17 02:04:29 INFO Executor: Finished task 105.0 in stage 2.0 (TID 100). 4544 bytes result sent to driver
[2021-05-16 23:04:29,817] {docker.py:276} INFO - 21/05/17 02:04:29 INFO TaskSetManager: Starting task 110.0 in stage 2.0 (TID 105) (e60b22510068, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:29,818] {docker.py:276} INFO - 21/05/17 02:04:29 INFO Executor: Running task 110.0 in stage 2.0 (TID 105)
21/05/17 02:04:29 INFO TaskSetManager: Finished task 105.0 in stage 2.0 (TID 100) in 1705 ms on e60b22510068 (executor driver) (98/200)
[2021-05-16 23:04:29,823] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203444716754443410936493_0002_m_000106_101: needsTaskCommit() Task attempt_202105170203444716754443410936493_0002_m_000106_101
[2021-05-16 23:04:29,823] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203444716754443410936493_0002_m_000106_101: needsTaskCommit() Task attempt_202105170203444716754443410936493_0002_m_000106_101: duration 0:00.000s
21/05/17 02:04:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444716754443410936493_0002_m_000106_101
[2021-05-16 23:04:29,824] {docker.py:276} INFO - 21/05/17 02:04:29 INFO Executor: Finished task 106.0 in stage 2.0 (TID 101). 4544 bytes result sent to driver
[2021-05-16 23:04:29,825] {docker.py:276} INFO - 21/05/17 02:04:29 INFO TaskSetManager: Starting task 111.0 in stage 2.0 (TID 106) (e60b22510068, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:29,826] {docker.py:276} INFO - 21/05/17 02:04:29 INFO TaskSetManager: Finished task 106.0 in stage 2.0 (TID 101) in 1682 ms on e60b22510068 (executor driver) (99/200)
21/05/17 02:04:29 INFO Executor: Running task 111.0 in stage 2.0 (TID 106)
[2021-05-16 23:04:29,830] {docker.py:276} INFO - 21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:29,831] {docker.py:276} INFO - 21/05/17 02:04:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:29,832] {docker.py:276} INFO - 21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447560040091809383114_0002_m_000110_105, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447560040091809383114_0002_m_000110_105}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447560040091809383114_0002}; taskId=attempt_202105170203447560040091809383114_0002_m_000110_105, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@705cb7a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203447560040091809383114_0002_m_000110_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447560040091809383114_0002_m_000110_105
[2021-05-16 23:04:29,835] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203447560040091809383114_0002_m_000110_105: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447560040091809383114_0002_m_000110_105 : duration 0:00.003s
[2021-05-16 23:04:29,836] {docker.py:276} INFO - 21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:29,838] {docker.py:276} INFO - 21/05/17 02:04:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444429959145524053464_0002_m_000111_106, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444429959145524053464_0002_m_000111_106}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444429959145524053464_0002}; taskId=attempt_202105170203444429959145524053464_0002_m_000111_106, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@773183e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:29,838] {docker.py:276} INFO - 21/05/17 02:04:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:29 INFO StagingCommitter: Starting: Task committer attempt_202105170203444429959145524053464_0002_m_000111_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444429959145524053464_0002_m_000111_106
[2021-05-16 23:04:29,840] {docker.py:276} INFO - 21/05/17 02:04:29 INFO StagingCommitter: Task committer attempt_202105170203444429959145524053464_0002_m_000111_106: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444429959145524053464_0002_m_000111_106 : duration 0:00.003s
[2021-05-16 23:04:30,657] {docker.py:276} INFO - 21/05/17 02:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105170203444524607641584404533_0002_m_000108_103: needsTaskCommit() Task attempt_202105170203444524607641584404533_0002_m_000108_103
[2021-05-16 23:04:30,659] {docker.py:276} INFO - 21/05/17 02:04:30 INFO StagingCommitter: Task committer attempt_202105170203444524607641584404533_0002_m_000108_103: needsTaskCommit() Task attempt_202105170203444524607641584404533_0002_m_000108_103: duration 0:00.001s
21/05/17 02:04:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444524607641584404533_0002_m_000108_103
[2021-05-16 23:04:30,659] {docker.py:276} INFO - 21/05/17 02:04:30 INFO Executor: Finished task 108.0 in stage 2.0 (TID 103). 4587 bytes result sent to driver
[2021-05-16 23:04:30,661] {docker.py:276} INFO - 21/05/17 02:04:30 INFO TaskSetManager: Starting task 112.0 in stage 2.0 (TID 107) (e60b22510068, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:30,662] {docker.py:276} INFO - 21/05/17 02:04:30 INFO Executor: Running task 112.0 in stage 2.0 (TID 107)
[2021-05-16 23:04:30,663] {docker.py:276} INFO - 21/05/17 02:04:30 INFO TaskSetManager: Finished task 108.0 in stage 2.0 (TID 103) in 2197 ms on e60b22510068 (executor driver) (100/200)
[2021-05-16 23:04:30,673] {docker.py:276} INFO - 21/05/17 02:04:30 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:30,675] {docker.py:276} INFO - 21/05/17 02:04:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:30,675] {docker.py:276} INFO - 21/05/17 02:04:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:30,676] {docker.py:276} INFO - 21/05/17 02:04:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442162175927310058030_0002_m_000112_107, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442162175927310058030_0002_m_000112_107}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442162175927310058030_0002}; taskId=attempt_202105170203442162175927310058030_0002_m_000112_107, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4dbebba3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:30,676] {docker.py:276} INFO - 21/05/17 02:04:30 INFO StagingCommitter: Starting: Task committer attempt_202105170203442162175927310058030_0002_m_000112_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442162175927310058030_0002_m_000112_107
[2021-05-16 23:04:30,679] {docker.py:276} INFO - 21/05/17 02:04:30 INFO StagingCommitter: Task committer attempt_202105170203442162175927310058030_0002_m_000112_107: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442162175927310058030_0002_m_000112_107 : duration 0:00.002s
[2021-05-16 23:04:31,248] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203441050383337193738765_0002_m_000109_104: needsTaskCommit() Task attempt_202105170203441050383337193738765_0002_m_000109_104
[2021-05-16 23:04:31,250] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203441050383337193738765_0002_m_000109_104: needsTaskCommit() Task attempt_202105170203441050383337193738765_0002_m_000109_104: duration 0:00.002s
21/05/17 02:04:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441050383337193738765_0002_m_000109_104
[2021-05-16 23:04:31,252] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Finished task 109.0 in stage 2.0 (TID 104). 4587 bytes result sent to driver
[2021-05-16 23:04:31,254] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Starting task 113.0 in stage 2.0 (TID 108) (e60b22510068, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:31,255] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Finished task 109.0 in stage 2.0 (TID 104) in 1738 ms on e60b22510068 (executor driver) (101/200)
[2021-05-16 23:04:31,256] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Running task 113.0 in stage 2.0 (TID 108)
[2021-05-16 23:04:31,265] {docker.py:276} INFO - 21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:31,267] {docker.py:276} INFO - 21/05/17 02:04:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442969827307177847073_0002_m_000113_108, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442969827307177847073_0002_m_000113_108}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442969827307177847073_0002}; taskId=attempt_202105170203442969827307177847073_0002_m_000113_108, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31d709c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:31,268] {docker.py:276} INFO - 21/05/17 02:04:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:31,268] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203442969827307177847073_0002_m_000113_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442969827307177847073_0002_m_000113_108
[2021-05-16 23:04:31,271] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203442969827307177847073_0002_m_000113_108: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442969827307177847073_0002_m_000113_108 : duration 0:00.003s
[2021-05-16 23:04:31,513] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203444429959145524053464_0002_m_000111_106: needsTaskCommit() Task attempt_202105170203444429959145524053464_0002_m_000111_106
[2021-05-16 23:04:31,514] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203444429959145524053464_0002_m_000111_106: needsTaskCommit() Task attempt_202105170203444429959145524053464_0002_m_000111_106: duration 0:00.001s
21/05/17 02:04:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444429959145524053464_0002_m_000111_106
[2021-05-16 23:04:31,515] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Finished task 111.0 in stage 2.0 (TID 106). 4587 bytes result sent to driver
[2021-05-16 23:04:31,517] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Starting task 114.0 in stage 2.0 (TID 109) (e60b22510068, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:31,518] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Finished task 111.0 in stage 2.0 (TID 106) in 1695 ms on e60b22510068 (executor driver) (102/200)
[2021-05-16 23:04:31,520] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Running task 114.0 in stage 2.0 (TID 109)
[2021-05-16 23:04:31,528] {docker.py:276} INFO - 21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Getting 3 (585.0 B) non-empty blocks including 3 (585.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:31,530] {docker.py:276} INFO - 21/05/17 02:04:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:31,530] {docker.py:276} INFO - 21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441599858841328111866_0002_m_000114_109, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441599858841328111866_0002_m_000114_109}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441599858841328111866_0002}; taskId=attempt_202105170203441599858841328111866_0002_m_000114_109, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44341bdb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:31,530] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203441599858841328111866_0002_m_000114_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441599858841328111866_0002_m_000114_109
[2021-05-16 23:04:31,532] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203447560040091809383114_0002_m_000110_105: needsTaskCommit() Task attempt_202105170203447560040091809383114_0002_m_000110_105
[2021-05-16 23:04:31,532] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203441599858841328111866_0002_m_000114_109: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441599858841328111866_0002_m_000114_109 : duration 0:00.003s
[2021-05-16 23:04:31,533] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203447560040091809383114_0002_m_000110_105: needsTaskCommit() Task attempt_202105170203447560040091809383114_0002_m_000110_105: duration 0:00.001s
21/05/17 02:04:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447560040091809383114_0002_m_000110_105
[2021-05-16 23:04:31,534] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Finished task 110.0 in stage 2.0 (TID 105). 4587 bytes result sent to driver
[2021-05-16 23:04:31,537] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Starting task 115.0 in stage 2.0 (TID 110) (e60b22510068, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:31,538] {docker.py:276} INFO - 21/05/17 02:04:31 INFO Executor: Running task 115.0 in stage 2.0 (TID 110)
[2021-05-16 23:04:31,538] {docker.py:276} INFO - 21/05/17 02:04:31 INFO TaskSetManager: Finished task 110.0 in stage 2.0 (TID 105) in 1723 ms on e60b22510068 (executor driver) (103/200)
[2021-05-16 23:04:31,544] {docker.py:276} INFO - 21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Getting 3 (709.0 B) non-empty blocks including 3 (709.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:31,546] {docker.py:276} INFO - 21/05/17 02:04:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:31,547] {docker.py:276} INFO - 21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441957568605780047486_0002_m_000115_110, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441957568605780047486_0002_m_000115_110}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441957568605780047486_0002}; taskId=attempt_202105170203441957568605780047486_0002_m_000115_110, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1602025a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:31,547] {docker.py:276} INFO - 21/05/17 02:04:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:31,547] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Starting: Task committer attempt_202105170203441957568605780047486_0002_m_000115_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441957568605780047486_0002_m_000115_110
[2021-05-16 23:04:31,549] {docker.py:276} INFO - 21/05/17 02:04:31 INFO StagingCommitter: Task committer attempt_202105170203441957568605780047486_0002_m_000115_110: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441957568605780047486_0002_m_000115_110 : duration 0:00.003s
[2021-05-16 23:04:32,343] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Starting: Task committer attempt_202105170203442162175927310058030_0002_m_000112_107: needsTaskCommit() Task attempt_202105170203442162175927310058030_0002_m_000112_107
[2021-05-16 23:04:32,344] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Task committer attempt_202105170203442162175927310058030_0002_m_000112_107: needsTaskCommit() Task attempt_202105170203442162175927310058030_0002_m_000112_107: duration 0:00.001s
21/05/17 02:04:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442162175927310058030_0002_m_000112_107
[2021-05-16 23:04:32,345] {docker.py:276} INFO - 21/05/17 02:04:32 INFO Executor: Finished task 112.0 in stage 2.0 (TID 107). 4544 bytes result sent to driver
[2021-05-16 23:04:32,346] {docker.py:276} INFO - 21/05/17 02:04:32 INFO TaskSetManager: Starting task 116.0 in stage 2.0 (TID 111) (e60b22510068, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:32,348] {docker.py:276} INFO - 21/05/17 02:04:32 INFO TaskSetManager: Finished task 112.0 in stage 2.0 (TID 107) in 1690 ms on e60b22510068 (executor driver) (104/200)
[2021-05-16 23:04:32,349] {docker.py:276} INFO - 21/05/17 02:04:32 INFO Executor: Running task 116.0 in stage 2.0 (TID 111)
[2021-05-16 23:04:32,358] {docker.py:276} INFO - 21/05/17 02:04:32 INFO ShuffleBlockFetcherIterator: Getting 2 (479.0 B) non-empty blocks including 2 (479.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:32,359] {docker.py:276} INFO - 21/05/17 02:04:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441117656974783456507_0002_m_000116_111, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441117656974783456507_0002_m_000116_111}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441117656974783456507_0002}; taskId=attempt_202105170203441117656974783456507_0002_m_000116_111, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4563652f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:32,360] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Starting: Task committer attempt_202105170203441117656974783456507_0002_m_000116_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441117656974783456507_0002_m_000116_111
[2021-05-16 23:04:32,362] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Task committer attempt_202105170203441117656974783456507_0002_m_000116_111: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441117656974783456507_0002_m_000116_111 : duration 0:00.003s
[2021-05-16 23:04:32,987] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Starting: Task committer attempt_202105170203442969827307177847073_0002_m_000113_108: needsTaskCommit() Task attempt_202105170203442969827307177847073_0002_m_000113_108
[2021-05-16 23:04:32,988] {docker.py:276} INFO - 21/05/17 02:04:32 INFO StagingCommitter: Task committer attempt_202105170203442969827307177847073_0002_m_000113_108: needsTaskCommit() Task attempt_202105170203442969827307177847073_0002_m_000113_108: duration 0:00.001s
[2021-05-16 23:04:32,989] {docker.py:276} INFO - 21/05/17 02:04:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442969827307177847073_0002_m_000113_108
[2021-05-16 23:04:32,991] {docker.py:276} INFO - 21/05/17 02:04:32 INFO Executor: Finished task 113.0 in stage 2.0 (TID 108). 4544 bytes result sent to driver
[2021-05-16 23:04:32,992] {docker.py:276} INFO - 21/05/17 02:04:32 INFO TaskSetManager: Starting task 117.0 in stage 2.0 (TID 112) (e60b22510068, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:32,994] {docker.py:276} INFO - 21/05/17 02:04:33 INFO TaskSetManager: Finished task 113.0 in stage 2.0 (TID 108) in 1742 ms on e60b22510068 (executor driver) (105/200)
[2021-05-16 23:04:32,994] {docker.py:276} INFO - 21/05/17 02:04:33 INFO Executor: Running task 117.0 in stage 2.0 (TID 112)
[2021-05-16 23:04:33,003] {docker.py:276} INFO - 21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:33,005] {docker.py:276} INFO - 21/05/17 02:04:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,005] {docker.py:276} INFO - 21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444106332274423590460_0002_m_000117_112, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444106332274423590460_0002_m_000117_112}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444106332274423590460_0002}; taskId=attempt_202105170203444106332274423590460_0002_m_000117_112, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67f3f1b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,006] {docker.py:276} INFO - 21/05/17 02:04:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:33 INFO StagingCommitter: Starting: Task committer attempt_202105170203444106332274423590460_0002_m_000117_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444106332274423590460_0002_m_000117_112
[2021-05-16 23:04:33,009] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Task committer attempt_202105170203444106332274423590460_0002_m_000117_112: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444106332274423590460_0002_m_000117_112 : duration 0:00.003s
[2021-05-16 23:04:33,197] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Starting: Task committer attempt_202105170203441599858841328111866_0002_m_000114_109: needsTaskCommit() Task attempt_202105170203441599858841328111866_0002_m_000114_109
[2021-05-16 23:04:33,198] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Task committer attempt_202105170203441599858841328111866_0002_m_000114_109: needsTaskCommit() Task attempt_202105170203441599858841328111866_0002_m_000114_109: duration 0:00.002s
[2021-05-16 23:04:33,199] {docker.py:276} INFO - 21/05/17 02:04:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441599858841328111866_0002_m_000114_109
[2021-05-16 23:04:33,200] {docker.py:276} INFO - 21/05/17 02:04:33 INFO Executor: Finished task 114.0 in stage 2.0 (TID 109). 4544 bytes result sent to driver
[2021-05-16 23:04:33,201] {docker.py:276} INFO - 21/05/17 02:04:33 INFO TaskSetManager: Starting task 118.0 in stage 2.0 (TID 113) (e60b22510068, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:33,202] {docker.py:276} INFO - 21/05/17 02:04:33 INFO TaskSetManager: Finished task 114.0 in stage 2.0 (TID 109) in 1687 ms on e60b22510068 (executor driver) (106/200)
[2021-05-16 23:04:33,203] {docker.py:276} INFO - 21/05/17 02:04:33 INFO Executor: Running task 118.0 in stage 2.0 (TID 113)
[2021-05-16 23:04:33,212] {docker.py:276} INFO - 21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Getting 3 (624.0 B) non-empty blocks including 3 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:33,212] {docker.py:276} INFO - 21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:33,214] {docker.py:276} INFO - 21/05/17 02:04:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:33,215] {docker.py:276} INFO - 21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,215] {docker.py:276} INFO - 21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448433203034557046601_0002_m_000118_113, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448433203034557046601_0002_m_000118_113}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448433203034557046601_0002}; taskId=attempt_202105170203448433203034557046601_0002_m_000118_113, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25e6e68e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,215] {docker.py:276} INFO - 21/05/17 02:04:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:33,215] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Starting: Task committer attempt_202105170203448433203034557046601_0002_m_000118_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448433203034557046601_0002_m_000118_113
[2021-05-16 23:04:33,225] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Task committer attempt_202105170203448433203034557046601_0002_m_000118_113: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448433203034557046601_0002_m_000118_113 : duration 0:00.010s
[2021-05-16 23:04:33,297] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Starting: Task committer attempt_202105170203441957568605780047486_0002_m_000115_110: needsTaskCommit() Task attempt_202105170203441957568605780047486_0002_m_000115_110
[2021-05-16 23:04:33,297] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Task committer attempt_202105170203441957568605780047486_0002_m_000115_110: needsTaskCommit() Task attempt_202105170203441957568605780047486_0002_m_000115_110: duration 0:00.001s
21/05/17 02:04:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441957568605780047486_0002_m_000115_110
[2021-05-16 23:04:33,298] {docker.py:276} INFO - 21/05/17 02:04:33 INFO Executor: Finished task 115.0 in stage 2.0 (TID 110). 4587 bytes result sent to driver
[2021-05-16 23:04:33,300] {docker.py:276} INFO - 21/05/17 02:04:33 INFO TaskSetManager: Starting task 119.0 in stage 2.0 (TID 114) (e60b22510068, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:33,300] {docker.py:276} INFO - 21/05/17 02:04:33 INFO Executor: Running task 119.0 in stage 2.0 (TID 114)
[2021-05-16 23:04:33,301] {docker.py:276} INFO - 21/05/17 02:04:33 INFO TaskSetManager: Finished task 115.0 in stage 2.0 (TID 110) in 1768 ms on e60b22510068 (executor driver) (107/200)
[2021-05-16 23:04:33,318] {docker.py:276} INFO - 21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:33,320] {docker.py:276} INFO - 21/05/17 02:04:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:33,321] {docker.py:276} INFO - 21/05/17 02:04:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:33,321] {docker.py:276} INFO - 21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,321] {docker.py:276} INFO - 21/05/17 02:04:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443255674651868065913_0002_m_000119_114, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443255674651868065913_0002_m_000119_114}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443255674651868065913_0002}; taskId=attempt_202105170203443255674651868065913_0002_m_000119_114, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a19e721}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:33,322] {docker.py:276} INFO - 21/05/17 02:04:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:33 INFO StagingCommitter: Starting: Task committer attempt_202105170203443255674651868065913_0002_m_000119_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443255674651868065913_0002_m_000119_114
[2021-05-16 23:04:33,324] {docker.py:276} INFO - 21/05/17 02:04:33 INFO StagingCommitter: Task committer attempt_202105170203443255674651868065913_0002_m_000119_114: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443255674651868065913_0002_m_000119_114 : duration 0:00.003s
[2021-05-16 23:04:34,058] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203441117656974783456507_0002_m_000116_111: needsTaskCommit() Task attempt_202105170203441117656974783456507_0002_m_000116_111
[2021-05-16 23:04:34,059] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203441117656974783456507_0002_m_000116_111: needsTaskCommit() Task attempt_202105170203441117656974783456507_0002_m_000116_111: duration 0:00.002s
21/05/17 02:04:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441117656974783456507_0002_m_000116_111
[2021-05-16 23:04:34,061] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Finished task 116.0 in stage 2.0 (TID 111). 4587 bytes result sent to driver
[2021-05-16 23:04:34,062] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Starting task 120.0 in stage 2.0 (TID 115) (e60b22510068, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:34,063] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Finished task 116.0 in stage 2.0 (TID 111) in 1720 ms on e60b22510068 (executor driver) (108/200)
[2021-05-16 23:04:34,064] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Running task 120.0 in stage 2.0 (TID 115)
[2021-05-16 23:04:34,074] {docker.py:276} INFO - 21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Getting 2 (440.0 B) non-empty blocks including 2 (440.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:34,076] {docker.py:276} INFO - 21/05/17 02:04:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:34,076] {docker.py:276} INFO - 21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448869128795920270076_0002_m_000120_115, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448869128795920270076_0002_m_000120_115}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448869128795920270076_0002}; taskId=attempt_202105170203448869128795920270076_0002_m_000120_115, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54131676}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203448869128795920270076_0002_m_000120_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448869128795920270076_0002_m_000120_115
[2021-05-16 23:04:34,080] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203448869128795920270076_0002_m_000120_115: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448869128795920270076_0002_m_000120_115 : duration 0:00.003s
[2021-05-16 23:04:34,554] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203443255674651868065913_0002_m_000119_114: needsTaskCommit() Task attempt_202105170203443255674651868065913_0002_m_000119_114
[2021-05-16 23:04:34,556] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203443255674651868065913_0002_m_000119_114: needsTaskCommit() Task attempt_202105170203443255674651868065913_0002_m_000119_114: duration 0:00.001s
21/05/17 02:04:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443255674651868065913_0002_m_000119_114
[2021-05-16 23:04:34,557] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Finished task 119.0 in stage 2.0 (TID 114). 4587 bytes result sent to driver
[2021-05-16 23:04:34,558] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Starting task 121.0 in stage 2.0 (TID 116) (e60b22510068, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:34,559] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Running task 121.0 in stage 2.0 (TID 116)
[2021-05-16 23:04:34,560] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Finished task 119.0 in stage 2.0 (TID 114) in 1262 ms on e60b22510068 (executor driver) (109/200)
[2021-05-16 23:04:34,569] {docker.py:276} INFO - 21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Getting 2 (435.0 B) non-empty blocks including 2 (435.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:34,569] {docker.py:276} INFO - 21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:34,571] {docker.py:276} INFO - 21/05/17 02:04:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:34,571] {docker.py:276} INFO - 21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444018327271274200990_0002_m_000121_116, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444018327271274200990_0002_m_000121_116}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444018327271274200990_0002}; taskId=attempt_202105170203444018327271274200990_0002_m_000121_116, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a17591f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:34,571] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203444018327271274200990_0002_m_000121_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444018327271274200990_0002_m_000121_116
[2021-05-16 23:04:34,574] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203444018327271274200990_0002_m_000121_116: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444018327271274200990_0002_m_000121_116 : duration 0:00.003s
[2021-05-16 23:04:34,683] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203444106332274423590460_0002_m_000117_112: needsTaskCommit() Task attempt_202105170203444106332274423590460_0002_m_000117_112
[2021-05-16 23:04:34,683] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203444106332274423590460_0002_m_000117_112: needsTaskCommit() Task attempt_202105170203444106332274423590460_0002_m_000117_112: duration 0:00.000s
21/05/17 02:04:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444106332274423590460_0002_m_000117_112
[2021-05-16 23:04:34,685] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Finished task 117.0 in stage 2.0 (TID 112). 4587 bytes result sent to driver
[2021-05-16 23:04:34,687] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Starting task 122.0 in stage 2.0 (TID 117) (e60b22510068, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:34,689] {docker.py:276} INFO - 21/05/17 02:04:34 INFO TaskSetManager: Finished task 117.0 in stage 2.0 (TID 112) in 1698 ms on e60b22510068 (executor driver) (110/200)
[2021-05-16 23:04:34,689] {docker.py:276} INFO - 21/05/17 02:04:34 INFO Executor: Running task 122.0 in stage 2.0 (TID 117)
[2021-05-16 23:04:34,699] {docker.py:276} INFO - 21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Getting 2 (456.0 B) non-empty blocks including 2 (456.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:34,700] {docker.py:276} INFO - 21/05/17 02:04:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443515509405756105823_0002_m_000122_117, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443515509405756105823_0002_m_000122_117}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443515509405756105823_0002}; taskId=attempt_202105170203443515509405756105823_0002_m_000122_117, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7663de64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:34,701] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Starting: Task committer attempt_202105170203443515509405756105823_0002_m_000122_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443515509405756105823_0002_m_000122_117
[2021-05-16 23:04:34,703] {docker.py:276} INFO - 21/05/17 02:04:34 INFO StagingCommitter: Task committer attempt_202105170203443515509405756105823_0002_m_000122_117: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443515509405756105823_0002_m_000122_117 : duration 0:00.003s
[2021-05-16 23:04:35,010] {docker.py:276} INFO - 21/05/17 02:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105170203448433203034557046601_0002_m_000118_113: needsTaskCommit() Task attempt_202105170203448433203034557046601_0002_m_000118_113
[2021-05-16 23:04:35,012] {docker.py:276} INFO - 21/05/17 02:04:35 INFO StagingCommitter: Task committer attempt_202105170203448433203034557046601_0002_m_000118_113: needsTaskCommit() Task attempt_202105170203448433203034557046601_0002_m_000118_113: duration 0:00.002s
[2021-05-16 23:04:35,013] {docker.py:276} INFO - 21/05/17 02:04:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448433203034557046601_0002_m_000118_113
[2021-05-16 23:04:35,014] {docker.py:276} INFO - 21/05/17 02:04:35 INFO Executor: Finished task 118.0 in stage 2.0 (TID 113). 4587 bytes result sent to driver
[2021-05-16 23:04:35,016] {docker.py:276} INFO - 21/05/17 02:04:35 INFO TaskSetManager: Starting task 123.0 in stage 2.0 (TID 118) (e60b22510068, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:35,018] {docker.py:276} INFO - 21/05/17 02:04:35 INFO TaskSetManager: Finished task 118.0 in stage 2.0 (TID 113) in 1819 ms on e60b22510068 (executor driver) (111/200)
21/05/17 02:04:35 INFO Executor: Running task 123.0 in stage 2.0 (TID 118)
[2021-05-16 23:04:35,028] {docker.py:276} INFO - 21/05/17 02:04:35 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:35,029] {docker.py:276} INFO - 21/05/17 02:04:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442474778844697718590_0002_m_000123_118, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442474778844697718590_0002_m_000123_118}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442474778844697718590_0002}; taskId=attempt_202105170203442474778844697718590_0002_m_000123_118, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31f2aa46}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105170203442474778844697718590_0002_m_000123_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442474778844697718590_0002_m_000123_118
[2021-05-16 23:04:35,032] {docker.py:276} INFO - 21/05/17 02:04:35 INFO StagingCommitter: Task committer attempt_202105170203442474778844697718590_0002_m_000123_118: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442474778844697718590_0002_m_000123_118 : duration 0:00.003s
[2021-05-16 23:04:35,773] {docker.py:276} INFO - 21/05/17 02:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105170203448869128795920270076_0002_m_000120_115: needsTaskCommit() Task attempt_202105170203448869128795920270076_0002_m_000120_115
21/05/17 02:04:35 INFO StagingCommitter: Task committer attempt_202105170203448869128795920270076_0002_m_000120_115: needsTaskCommit() Task attempt_202105170203448869128795920270076_0002_m_000120_115: duration 0:00.001s
[2021-05-16 23:04:35,774] {docker.py:276} INFO - 21/05/17 02:04:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448869128795920270076_0002_m_000120_115
[2021-05-16 23:04:35,775] {docker.py:276} INFO - 21/05/17 02:04:35 INFO Executor: Finished task 120.0 in stage 2.0 (TID 115). 4544 bytes result sent to driver
[2021-05-16 23:04:35,777] {docker.py:276} INFO - 21/05/17 02:04:35 INFO TaskSetManager: Starting task 124.0 in stage 2.0 (TID 119) (e60b22510068, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:35,777] {docker.py:276} INFO - 21/05/17 02:04:35 INFO Executor: Running task 124.0 in stage 2.0 (TID 119)
[2021-05-16 23:04:35,778] {docker.py:276} INFO - 21/05/17 02:04:35 INFO TaskSetManager: Finished task 120.0 in stage 2.0 (TID 115) in 1718 ms on e60b22510068 (executor driver) (112/200)
[2021-05-16 23:04:35,787] {docker.py:276} INFO - 21/05/17 02:04:35 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:35,789] {docker.py:276} INFO - 21/05/17 02:04:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:35,790] {docker.py:276} INFO - 21/05/17 02:04:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446615990959463843226_0002_m_000124_119, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446615990959463843226_0002_m_000124_119}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446615990959463843226_0002}; taskId=attempt_202105170203446615990959463843226_0002_m_000124_119, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d08f833}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:35 INFO StagingCommitter: Starting: Task committer attempt_202105170203446615990959463843226_0002_m_000124_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446615990959463843226_0002_m_000124_119
[2021-05-16 23:04:35,792] {docker.py:276} INFO - 21/05/17 02:04:35 INFO StagingCommitter: Task committer attempt_202105170203446615990959463843226_0002_m_000124_119: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446615990959463843226_0002_m_000124_119 : duration 0:00.003s
[2021-05-16 23:04:36,360] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105170203444018327271274200990_0002_m_000121_116: needsTaskCommit() Task attempt_202105170203444018327271274200990_0002_m_000121_116
[2021-05-16 23:04:36,361] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_202105170203444018327271274200990_0002_m_000121_116: needsTaskCommit() Task attempt_202105170203444018327271274200990_0002_m_000121_116: duration 0:00.001s
21/05/17 02:04:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444018327271274200990_0002_m_000121_116
[2021-05-16 23:04:36,365] {docker.py:276} INFO - 21/05/17 02:04:36 INFO Executor: Finished task 121.0 in stage 2.0 (TID 116). 4544 bytes result sent to driver
[2021-05-16 23:04:36,366] {docker.py:276} INFO - 21/05/17 02:04:36 INFO TaskSetManager: Starting task 125.0 in stage 2.0 (TID 120) (e60b22510068, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:36,367] {docker.py:276} INFO - 21/05/17 02:04:36 INFO TaskSetManager: Finished task 121.0 in stage 2.0 (TID 116) in 1811 ms on e60b22510068 (executor driver) (113/200)
[2021-05-16 23:04:36,368] {docker.py:276} INFO - 21/05/17 02:04:36 INFO Executor: Running task 125.0 in stage 2.0 (TID 120)
[2021-05-16 23:04:36,376] {docker.py:276} INFO - 21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Getting 3 (642.0 B) non-empty blocks including 3 (642.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:36,378] {docker.py:276} INFO - 21/05/17 02:04:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:36,378] {docker.py:276} INFO - 21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444868741616074555940_0002_m_000125_120, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444868741616074555940_0002_m_000125_120}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444868741616074555940_0002}; taskId=attempt_202105170203444868741616074555940_0002_m_000125_120, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@105a99d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:36,378] {docker.py:276} INFO - 21/05/17 02:04:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:36,379] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105170203444868741616074555940_0002_m_000125_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444868741616074555940_0002_m_000125_120
[2021-05-16 23:04:36,381] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_202105170203444868741616074555940_0002_m_000125_120: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444868741616074555940_0002_m_000125_120 : duration 0:00.003s
[2021-05-16 23:04:36,434] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105170203443515509405756105823_0002_m_000122_117: needsTaskCommit() Task attempt_202105170203443515509405756105823_0002_m_000122_117
[2021-05-16 23:04:36,435] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_202105170203443515509405756105823_0002_m_000122_117: needsTaskCommit() Task attempt_202105170203443515509405756105823_0002_m_000122_117: duration 0:00.001s
21/05/17 02:04:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443515509405756105823_0002_m_000122_117
[2021-05-16 23:04:36,436] {docker.py:276} INFO - 21/05/17 02:04:36 INFO Executor: Finished task 122.0 in stage 2.0 (TID 117). 4544 bytes result sent to driver
[2021-05-16 23:04:36,437] {docker.py:276} INFO - 21/05/17 02:04:36 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 121) (e60b22510068, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:36,439] {docker.py:276} INFO - 21/05/17 02:04:36 INFO TaskSetManager: Finished task 122.0 in stage 2.0 (TID 117) in 1755 ms on e60b22510068 (executor driver) (114/200)
21/05/17 02:04:36 INFO Executor: Running task 126.0 in stage 2.0 (TID 121)
[2021-05-16 23:04:36,457] {docker.py:276} INFO - 21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:36,459] {docker.py:276} INFO - 21/05/17 02:04:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:36,459] {docker.py:276} INFO - 21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445582687474331086729_0002_m_000126_121, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445582687474331086729_0002_m_000126_121}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445582687474331086729_0002}; taskId=attempt_202105170203445582687474331086729_0002_m_000126_121, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b006f0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:36,459] {docker.py:276} INFO - 21/05/17 02:04:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:36,459] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105170203445582687474331086729_0002_m_000126_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445582687474331086729_0002_m_000126_121
[2021-05-16 23:04:36,461] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_202105170203445582687474331086729_0002_m_000126_121: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445582687474331086729_0002_m_000126_121 : duration 0:00.003s
[2021-05-16 23:04:36,743] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_202105170203442474778844697718590_0002_m_000123_118: needsTaskCommit() Task attempt_202105170203442474778844697718590_0002_m_000123_118
[2021-05-16 23:04:36,744] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_202105170203442474778844697718590_0002_m_000123_118: needsTaskCommit() Task attempt_202105170203442474778844697718590_0002_m_000123_118: duration 0:00.001s
21/05/17 02:04:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442474778844697718590_0002_m_000123_118
[2021-05-16 23:04:36,746] {docker.py:276} INFO - 21/05/17 02:04:36 INFO Executor: Finished task 123.0 in stage 2.0 (TID 118). 4587 bytes result sent to driver
[2021-05-16 23:04:36,748] {docker.py:276} INFO - 21/05/17 02:04:36 INFO TaskSetManager: Starting task 127.0 in stage 2.0 (TID 122) (e60b22510068, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:36,750] {docker.py:276} INFO - 21/05/17 02:04:36 INFO Executor: Running task 127.0 in stage 2.0 (TID 122)
21/05/17 02:04:36 INFO TaskSetManager: Finished task 123.0 in stage 2.0 (TID 118) in 1736 ms on e60b22510068 (executor driver) (115/200)
[2021-05-16 23:04:36,759] {docker.py:276} INFO - 21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:36,761] {docker.py:276} INFO - 21/05/17 02:04:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344728648817820708440_0002_m_000127_122, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344728648817820708440_0002_m_000127_122}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344728648817820708440_0002}; taskId=attempt_20210517020344728648817820708440_0002_m_000127_122, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11460229}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:36,762] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Starting: Task committer attempt_20210517020344728648817820708440_0002_m_000127_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344728648817820708440_0002_m_000127_122
[2021-05-16 23:04:36,765] {docker.py:276} INFO - 21/05/17 02:04:36 INFO StagingCommitter: Task committer attempt_20210517020344728648817820708440_0002_m_000127_122: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344728648817820708440_0002_m_000127_122 : duration 0:00.004s
[2021-05-16 23:04:37,470] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105170203446615990959463843226_0002_m_000124_119: needsTaskCommit() Task attempt_202105170203446615990959463843226_0002_m_000124_119
[2021-05-16 23:04:37,471] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Task committer attempt_202105170203446615990959463843226_0002_m_000124_119: needsTaskCommit() Task attempt_202105170203446615990959463843226_0002_m_000124_119: duration 0:00.001s
21/05/17 02:04:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446615990959463843226_0002_m_000124_119
[2021-05-16 23:04:37,473] {docker.py:276} INFO - 21/05/17 02:04:37 INFO Executor: Finished task 124.0 in stage 2.0 (TID 119). 4587 bytes result sent to driver
[2021-05-16 23:04:37,475] {docker.py:276} INFO - 21/05/17 02:04:37 INFO TaskSetManager: Starting task 128.0 in stage 2.0 (TID 123) (e60b22510068, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:37,477] {docker.py:276} INFO - 21/05/17 02:04:37 INFO TaskSetManager: Finished task 124.0 in stage 2.0 (TID 119) in 1703 ms on e60b22510068 (executor driver) (116/200)
[2021-05-16 23:04:37,478] {docker.py:276} INFO - 21/05/17 02:04:37 INFO Executor: Running task 128.0 in stage 2.0 (TID 123)
[2021-05-16 23:04:37,487] {docker.py:276} INFO - 21/05/17 02:04:37 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:37,489] {docker.py:276} INFO - 21/05/17 02:04:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441770007617269319128_0002_m_000128_123, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441770007617269319128_0002_m_000128_123}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441770007617269319128_0002}; taskId=attempt_202105170203441770007617269319128_0002_m_000128_123, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c17289d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105170203441770007617269319128_0002_m_000128_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441770007617269319128_0002_m_000128_123
[2021-05-16 23:04:37,492] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Task committer attempt_202105170203441770007617269319128_0002_m_000128_123: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441770007617269319128_0002_m_000128_123 : duration 0:00.003s
[2021-05-16 23:04:37,686] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105170203445582687474331086729_0002_m_000126_121: needsTaskCommit() Task attempt_202105170203445582687474331086729_0002_m_000126_121
[2021-05-16 23:04:37,687] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Task committer attempt_202105170203445582687474331086729_0002_m_000126_121: needsTaskCommit() Task attempt_202105170203445582687474331086729_0002_m_000126_121: duration 0:00.001s
21/05/17 02:04:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445582687474331086729_0002_m_000126_121
[2021-05-16 23:04:37,687] {docker.py:276} INFO - 21/05/17 02:04:37 INFO Executor: Finished task 126.0 in stage 2.0 (TID 121). 4587 bytes result sent to driver
[2021-05-16 23:04:37,689] {docker.py:276} INFO - 21/05/17 02:04:37 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 124) (e60b22510068, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:37,690] {docker.py:276} INFO - 21/05/17 02:04:37 INFO TaskSetManager: Finished task 126.0 in stage 2.0 (TID 121) in 1254 ms on e60b22510068 (executor driver) (117/200)
[2021-05-16 23:04:37,691] {docker.py:276} INFO - 21/05/17 02:04:37 INFO Executor: Running task 129.0 in stage 2.0 (TID 124)
[2021-05-16 23:04:37,699] {docker.py:276} INFO - 21/05/17 02:04:37 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:37,699] {docker.py:276} INFO - 21/05/17 02:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:37,701] {docker.py:276} INFO - 21/05/17 02:04:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:37,701] {docker.py:276} INFO - 21/05/17 02:04:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:37,702] {docker.py:276} INFO - 21/05/17 02:04:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446565140174498041018_0002_m_000129_124, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446565140174498041018_0002_m_000129_124}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446565140174498041018_0002}; taskId=attempt_202105170203446565140174498041018_0002_m_000129_124, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72a83866}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:37,702] {docker.py:276} INFO - 21/05/17 02:04:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:37,702] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Starting: Task committer attempt_202105170203446565140174498041018_0002_m_000129_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446565140174498041018_0002_m_000129_124
[2021-05-16 23:04:37,704] {docker.py:276} INFO - 21/05/17 02:04:37 INFO StagingCommitter: Task committer attempt_202105170203446565140174498041018_0002_m_000129_124: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446565140174498041018_0002_m_000129_124 : duration 0:00.003s
[2021-05-16 23:04:38,063] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Starting: Task committer attempt_202105170203444868741616074555940_0002_m_000125_120: needsTaskCommit() Task attempt_202105170203444868741616074555940_0002_m_000125_120
[2021-05-16 23:04:38,064] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Task committer attempt_202105170203444868741616074555940_0002_m_000125_120: needsTaskCommit() Task attempt_202105170203444868741616074555940_0002_m_000125_120: duration 0:00.001s
[2021-05-16 23:04:38,064] {docker.py:276} INFO - 21/05/17 02:04:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444868741616074555940_0002_m_000125_120
[2021-05-16 23:04:38,065] {docker.py:276} INFO - 21/05/17 02:04:38 INFO Executor: Finished task 125.0 in stage 2.0 (TID 120). 4587 bytes result sent to driver
[2021-05-16 23:04:38,067] {docker.py:276} INFO - 21/05/17 02:04:38 INFO TaskSetManager: Starting task 130.0 in stage 2.0 (TID 125) (e60b22510068, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:38,068] {docker.py:276} INFO - 21/05/17 02:04:38 INFO Executor: Running task 130.0 in stage 2.0 (TID 125)
[2021-05-16 23:04:38,069] {docker.py:276} INFO - 21/05/17 02:04:38 INFO TaskSetManager: Finished task 125.0 in stage 2.0 (TID 120) in 1706 ms on e60b22510068 (executor driver) (118/200)
[2021-05-16 23:04:38,078] {docker.py:276} INFO - 21/05/17 02:04:38 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:38,079] {docker.py:276} INFO - 21/05/17 02:04:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441902795271010058151_0002_m_000130_125, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441902795271010058151_0002_m_000130_125}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441902795271010058151_0002}; taskId=attempt_202105170203441902795271010058151_0002_m_000130_125, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77939f87}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:38,080] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Starting: Task committer attempt_202105170203441902795271010058151_0002_m_000130_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441902795271010058151_0002_m_000130_125
[2021-05-16 23:04:38,083] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Task committer attempt_202105170203441902795271010058151_0002_m_000130_125: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441902795271010058151_0002_m_000130_125 : duration 0:00.003s
[2021-05-16 23:04:38,481] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Starting: Task committer attempt_20210517020344728648817820708440_0002_m_000127_122: needsTaskCommit() Task attempt_20210517020344728648817820708440_0002_m_000127_122
21/05/17 02:04:38 INFO StagingCommitter: Task committer attempt_20210517020344728648817820708440_0002_m_000127_122: needsTaskCommit() Task attempt_20210517020344728648817820708440_0002_m_000127_122: duration 0:00.000s
21/05/17 02:04:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344728648817820708440_0002_m_000127_122
[2021-05-16 23:04:38,483] {docker.py:276} INFO - 21/05/17 02:04:38 INFO Executor: Finished task 127.0 in stage 2.0 (TID 122). 4544 bytes result sent to driver
[2021-05-16 23:04:38,485] {docker.py:276} INFO - 21/05/17 02:04:38 INFO TaskSetManager: Starting task 131.0 in stage 2.0 (TID 126) (e60b22510068, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:38,486] {docker.py:276} INFO - 21/05/17 02:04:38 INFO TaskSetManager: Finished task 127.0 in stage 2.0 (TID 122) in 1740 ms on e60b22510068 (executor driver) (119/200)
[2021-05-16 23:04:38,487] {docker.py:276} INFO - 21/05/17 02:04:38 INFO Executor: Running task 131.0 in stage 2.0 (TID 126)
[2021-05-16 23:04:38,497] {docker.py:276} INFO - 21/05/17 02:04:38 INFO ShuffleBlockFetcherIterator: Getting 3 (588.0 B) non-empty blocks including 3 (588.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:38,499] {docker.py:276} INFO - 21/05/17 02:04:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441019460164068693689_0002_m_000131_126, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441019460164068693689_0002_m_000131_126}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441019460164068693689_0002}; taskId=attempt_202105170203441019460164068693689_0002_m_000131_126, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a201861}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:38,499] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Starting: Task committer attempt_202105170203441019460164068693689_0002_m_000131_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441019460164068693689_0002_m_000131_126
[2021-05-16 23:04:38,502] {docker.py:276} INFO - 21/05/17 02:04:38 INFO StagingCommitter: Task committer attempt_202105170203441019460164068693689_0002_m_000131_126: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441019460164068693689_0002_m_000131_126 : duration 0:00.004s
[2021-05-16 23:04:39,259] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203441770007617269319128_0002_m_000128_123: needsTaskCommit() Task attempt_202105170203441770007617269319128_0002_m_000128_123
[2021-05-16 23:04:39,259] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203441770007617269319128_0002_m_000128_123: needsTaskCommit() Task attempt_202105170203441770007617269319128_0002_m_000128_123: duration 0:00.000s
21/05/17 02:04:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441770007617269319128_0002_m_000128_123
[2021-05-16 23:04:39,261] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Finished task 128.0 in stage 2.0 (TID 123). 4544 bytes result sent to driver
[2021-05-16 23:04:39,263] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Starting task 132.0 in stage 2.0 (TID 127) (e60b22510068, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:39,264] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Finished task 128.0 in stage 2.0 (TID 123) in 1791 ms on e60b22510068 (executor driver) (120/200)
[2021-05-16 23:04:39,267] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Running task 132.0 in stage 2.0 (TID 127)
[2021-05-16 23:04:39,278] {docker.py:276} INFO - 21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:39,280] {docker.py:276} INFO - 21/05/17 02:04:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442867979927751234293_0002_m_000132_127, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442867979927751234293_0002_m_000132_127}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442867979927751234293_0002}; taskId=attempt_202105170203442867979927751234293_0002_m_000132_127, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63c0efbb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203442867979927751234293_0002_m_000132_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442867979927751234293_0002_m_000132_127
[2021-05-16 23:04:39,283] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203442867979927751234293_0002_m_000132_127: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442867979927751234293_0002_m_000132_127 : duration 0:00.002s
[2021-05-16 23:04:39,449] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203446565140174498041018_0002_m_000129_124: needsTaskCommit() Task attempt_202105170203446565140174498041018_0002_m_000129_124
[2021-05-16 23:04:39,450] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203446565140174498041018_0002_m_000129_124: needsTaskCommit() Task attempt_202105170203446565140174498041018_0002_m_000129_124: duration 0:00.000s
[2021-05-16 23:04:39,451] {docker.py:276} INFO - 21/05/17 02:04:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446565140174498041018_0002_m_000129_124
[2021-05-16 23:04:39,452] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Finished task 129.0 in stage 2.0 (TID 124). 4544 bytes result sent to driver
[2021-05-16 23:04:39,454] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Starting task 133.0 in stage 2.0 (TID 128) (e60b22510068, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:39,455] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Finished task 129.0 in stage 2.0 (TID 124) in 1769 ms on e60b22510068 (executor driver) (121/200)
[2021-05-16 23:04:39,457] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Running task 133.0 in stage 2.0 (TID 128)
[2021-05-16 23:04:39,466] {docker.py:276} INFO - 21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:39,467] {docker.py:276} INFO - 21/05/17 02:04:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:39,468] {docker.py:276} INFO - 21/05/17 02:04:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:39,468] {docker.py:276} INFO - 21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:39,469] {docker.py:276} INFO - 21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444764427924699656343_0002_m_000133_128, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444764427924699656343_0002_m_000133_128}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444764427924699656343_0002}; taskId=attempt_202105170203444764427924699656343_0002_m_000133_128, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@487c5562}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:39,469] {docker.py:276} INFO - 21/05/17 02:04:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:39,469] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203444764427924699656343_0002_m_000133_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444764427924699656343_0002_m_000133_128
[2021-05-16 23:04:39,472] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203444764427924699656343_0002_m_000133_128: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444764427924699656343_0002_m_000133_128 : duration 0:00.003s
[2021-05-16 23:04:39,778] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203441902795271010058151_0002_m_000130_125: needsTaskCommit() Task attempt_202105170203441902795271010058151_0002_m_000130_125
[2021-05-16 23:04:39,779] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203441902795271010058151_0002_m_000130_125: needsTaskCommit() Task attempt_202105170203441902795271010058151_0002_m_000130_125: duration 0:00.001s
21/05/17 02:04:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441902795271010058151_0002_m_000130_125
[2021-05-16 23:04:39,780] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Finished task 130.0 in stage 2.0 (TID 125). 4587 bytes result sent to driver
[2021-05-16 23:04:39,782] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Starting task 134.0 in stage 2.0 (TID 129) (e60b22510068, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:39,783] {docker.py:276} INFO - 21/05/17 02:04:39 INFO Executor: Running task 134.0 in stage 2.0 (TID 129)
[2021-05-16 23:04:39,784] {docker.py:276} INFO - 21/05/17 02:04:39 INFO TaskSetManager: Finished task 130.0 in stage 2.0 (TID 125) in 1719 ms on e60b22510068 (executor driver) (122/200)
[2021-05-16 23:04:39,795] {docker.py:276} INFO - 21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:39,797] {docker.py:276} INFO - 21/05/17 02:04:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:39,798] {docker.py:276} INFO - 21/05/17 02:04:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203449008779955208319077_0002_m_000134_129, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449008779955208319077_0002_m_000134_129}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203449008779955208319077_0002}; taskId=attempt_202105170203449008779955208319077_0002_m_000134_129, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d119faa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:39 INFO StagingCommitter: Starting: Task committer attempt_202105170203449008779955208319077_0002_m_000134_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449008779955208319077_0002_m_000134_129
[2021-05-16 23:04:39,800] {docker.py:276} INFO - 21/05/17 02:04:39 INFO StagingCommitter: Task committer attempt_202105170203449008779955208319077_0002_m_000134_129: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449008779955208319077_0002_m_000134_129 : duration 0:00.003s
[2021-05-16 23:04:40,243] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105170203441019460164068693689_0002_m_000131_126: needsTaskCommit() Task attempt_202105170203441019460164068693689_0002_m_000131_126
[2021-05-16 23:04:40,243] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Task committer attempt_202105170203441019460164068693689_0002_m_000131_126: needsTaskCommit() Task attempt_202105170203441019460164068693689_0002_m_000131_126: duration 0:00.000s
21/05/17 02:04:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441019460164068693689_0002_m_000131_126
[2021-05-16 23:04:40,244] {docker.py:276} INFO - 21/05/17 02:04:40 INFO Executor: Finished task 131.0 in stage 2.0 (TID 126). 4587 bytes result sent to driver
[2021-05-16 23:04:40,247] {docker.py:276} INFO - 21/05/17 02:04:40 INFO TaskSetManager: Starting task 135.0 in stage 2.0 (TID 130) (e60b22510068, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:40,248] {docker.py:276} INFO - 21/05/17 02:04:40 INFO TaskSetManager: Finished task 131.0 in stage 2.0 (TID 126) in 1765 ms on e60b22510068 (executor driver) (123/200)
[2021-05-16 23:04:40,250] {docker.py:276} INFO - 21/05/17 02:04:40 INFO Executor: Running task 135.0 in stage 2.0 (TID 130)
[2021-05-16 23:04:40,260] {docker.py:276} INFO - 21/05/17 02:04:40 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:40,262] {docker.py:276} INFO - 21/05/17 02:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:40,263] {docker.py:276} INFO - 21/05/17 02:04:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447542150353892547181_0002_m_000135_130, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447542150353892547181_0002_m_000135_130}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447542150353892547181_0002}; taskId=attempt_202105170203447542150353892547181_0002_m_000135_130, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@393cb666}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:40,263] {docker.py:276} INFO - 21/05/17 02:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105170203447542150353892547181_0002_m_000135_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447542150353892547181_0002_m_000135_130
[2021-05-16 23:04:40,266] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Task committer attempt_202105170203447542150353892547181_0002_m_000135_130: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447542150353892547181_0002_m_000135_130 : duration 0:00.003s
[2021-05-16 23:04:40,948] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105170203442867979927751234293_0002_m_000132_127: needsTaskCommit() Task attempt_202105170203442867979927751234293_0002_m_000132_127
[2021-05-16 23:04:40,948] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Task committer attempt_202105170203442867979927751234293_0002_m_000132_127: needsTaskCommit() Task attempt_202105170203442867979927751234293_0002_m_000132_127: duration 0:00.000s
[2021-05-16 23:04:40,949] {docker.py:276} INFO - 21/05/17 02:04:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442867979927751234293_0002_m_000132_127
[2021-05-16 23:04:40,954] {docker.py:276} INFO - 21/05/17 02:04:40 INFO Executor: Finished task 132.0 in stage 2.0 (TID 127). 4587 bytes result sent to driver
[2021-05-16 23:04:40,956] {docker.py:276} INFO - 21/05/17 02:04:40 INFO TaskSetManager: Starting task 136.0 in stage 2.0 (TID 131) (e60b22510068, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:40,957] {docker.py:276} INFO - 21/05/17 02:04:40 INFO Executor: Running task 136.0 in stage 2.0 (TID 131)
[2021-05-16 23:04:40,959] {docker.py:276} INFO - 21/05/17 02:04:40 INFO TaskSetManager: Finished task 132.0 in stage 2.0 (TID 127) in 1697 ms on e60b22510068 (executor driver) (124/200)
[2021-05-16 23:04:40,967] {docker.py:276} INFO - 21/05/17 02:04:40 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:40,969] {docker.py:276} INFO - 21/05/17 02:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445687540726791710411_0002_m_000136_131, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445687540726791710411_0002_m_000136_131}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445687540726791710411_0002}; taskId=attempt_202105170203445687540726791710411_0002_m_000136_131, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32f269e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:40,969] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Starting: Task committer attempt_202105170203445687540726791710411_0002_m_000136_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445687540726791710411_0002_m_000136_131
[2021-05-16 23:04:40,972] {docker.py:276} INFO - 21/05/17 02:04:40 INFO StagingCommitter: Task committer attempt_202105170203445687540726791710411_0002_m_000136_131: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445687540726791710411_0002_m_000136_131 : duration 0:00.002s
[2021-05-16 23:04:41,148] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105170203444764427924699656343_0002_m_000133_128: needsTaskCommit() Task attempt_202105170203444764427924699656343_0002_m_000133_128
[2021-05-16 23:04:41,148] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Task committer attempt_202105170203444764427924699656343_0002_m_000133_128: needsTaskCommit() Task attempt_202105170203444764427924699656343_0002_m_000133_128: duration 0:00.001s
21/05/17 02:04:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444764427924699656343_0002_m_000133_128
[2021-05-16 23:04:41,150] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Finished task 133.0 in stage 2.0 (TID 128). 4587 bytes result sent to driver
[2021-05-16 23:04:41,151] {docker.py:276} INFO - 21/05/17 02:04:41 INFO TaskSetManager: Starting task 138.0 in stage 2.0 (TID 132) (e60b22510068, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:41,152] {docker.py:276} INFO - 21/05/17 02:04:41 INFO TaskSetManager: Finished task 133.0 in stage 2.0 (TID 128) in 1700 ms on e60b22510068 (executor driver) (125/200)
[2021-05-16 23:04:41,152] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Running task 138.0 in stage 2.0 (TID 132)
[2021-05-16 23:04:41,159] {docker.py:276} INFO - 21/05/17 02:04:41 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:41,161] {docker.py:276} INFO - 21/05/17 02:04:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:41,162] {docker.py:276} INFO - 21/05/17 02:04:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344667898958395207026_0002_m_000138_132, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344667898958395207026_0002_m_000138_132}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344667898958395207026_0002}; taskId=attempt_20210517020344667898958395207026_0002_m_000138_132, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75104fd1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:41,162] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Starting: Task committer attempt_20210517020344667898958395207026_0002_m_000138_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344667898958395207026_0002_m_000138_132
[2021-05-16 23:04:41,165] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Task committer attempt_20210517020344667898958395207026_0002_m_000138_132: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344667898958395207026_0002_m_000138_132 : duration 0:00.003s
[2021-05-16 23:04:41,464] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105170203449008779955208319077_0002_m_000134_129: needsTaskCommit() Task attempt_202105170203449008779955208319077_0002_m_000134_129
[2021-05-16 23:04:41,465] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Task committer attempt_202105170203449008779955208319077_0002_m_000134_129: needsTaskCommit() Task attempt_202105170203449008779955208319077_0002_m_000134_129: duration 0:00.001s
21/05/17 02:04:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203449008779955208319077_0002_m_000134_129
[2021-05-16 23:04:41,467] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Finished task 134.0 in stage 2.0 (TID 129). 4544 bytes result sent to driver
[2021-05-16 23:04:41,468] {docker.py:276} INFO - 21/05/17 02:04:41 INFO TaskSetManager: Starting task 139.0 in stage 2.0 (TID 133) (e60b22510068, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:41,469] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Running task 139.0 in stage 2.0 (TID 133)
[2021-05-16 23:04:41,470] {docker.py:276} INFO - 21/05/17 02:04:41 INFO TaskSetManager: Finished task 134.0 in stage 2.0 (TID 129) in 1690 ms on e60b22510068 (executor driver) (126/200)
[2021-05-16 23:04:41,479] {docker.py:276} INFO - 21/05/17 02:04:41 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:41,480] {docker.py:276} INFO - 21/05/17 02:04:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:04:41,482] {docker.py:276} INFO - 21/05/17 02:04:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:41,482] {docker.py:276} INFO - 21/05/17 02:04:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446935036215280538609_0002_m_000139_133, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935036215280538609_0002_m_000139_133}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446935036215280538609_0002}; taskId=attempt_202105170203446935036215280538609_0002_m_000139_133, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14065570}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105170203446935036215280538609_0002_m_000139_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935036215280538609_0002_m_000139_133
[2021-05-16 23:04:41,485] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Task committer attempt_202105170203446935036215280538609_0002_m_000139_133: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446935036215280538609_0002_m_000139_133 : duration 0:00.003s
[2021-05-16 23:04:41,980] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Starting: Task committer attempt_202105170203447542150353892547181_0002_m_000135_130: needsTaskCommit() Task attempt_202105170203447542150353892547181_0002_m_000135_130
[2021-05-16 23:04:41,980] {docker.py:276} INFO - 21/05/17 02:04:41 INFO StagingCommitter: Task committer attempt_202105170203447542150353892547181_0002_m_000135_130: needsTaskCommit() Task attempt_202105170203447542150353892547181_0002_m_000135_130: duration 0:00.000s
21/05/17 02:04:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447542150353892547181_0002_m_000135_130
[2021-05-16 23:04:41,981] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Finished task 135.0 in stage 2.0 (TID 130). 4544 bytes result sent to driver
[2021-05-16 23:04:41,981] {docker.py:276} INFO - 21/05/17 02:04:41 INFO TaskSetManager: Starting task 140.0 in stage 2.0 (TID 134) (e60b22510068, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:41,982] {docker.py:276} INFO - 21/05/17 02:04:41 INFO Executor: Running task 140.0 in stage 2.0 (TID 134)
[2021-05-16 23:04:41,983] {docker.py:276} INFO - 21/05/17 02:04:42 INFO TaskSetManager: Finished task 135.0 in stage 2.0 (TID 130) in 1739 ms on e60b22510068 (executor driver) (127/200)
[2021-05-16 23:04:41,991] {docker.py:276} INFO - 21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Getting 3 (585.0 B) non-empty blocks including 3 (585.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:41,993] {docker.py:276} INFO - 21/05/17 02:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446771531786422556293_0002_m_000140_134, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446771531786422556293_0002_m_000140_134}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446771531786422556293_0002}; taskId=attempt_202105170203446771531786422556293_0002_m_000140_134, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21235d9b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:41,993] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Starting: Task committer attempt_202105170203446771531786422556293_0002_m_000140_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446771531786422556293_0002_m_000140_134
[2021-05-16 23:04:41,996] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Task committer attempt_202105170203446771531786422556293_0002_m_000140_134: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446771531786422556293_0002_m_000140_134 : duration 0:00.002s
[2021-05-16 23:04:42,136] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Starting: Task committer attempt_202105170203445687540726791710411_0002_m_000136_131: needsTaskCommit() Task attempt_202105170203445687540726791710411_0002_m_000136_131
[2021-05-16 23:04:42,137] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Task committer attempt_202105170203445687540726791710411_0002_m_000136_131: needsTaskCommit() Task attempt_202105170203445687540726791710411_0002_m_000136_131: duration 0:00.001s
[2021-05-16 23:04:42,138] {docker.py:276} INFO - 21/05/17 02:04:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445687540726791710411_0002_m_000136_131
[2021-05-16 23:04:42,139] {docker.py:276} INFO - 21/05/17 02:04:42 INFO Executor: Finished task 136.0 in stage 2.0 (TID 131). 4544 bytes result sent to driver
[2021-05-16 23:04:42,141] {docker.py:276} INFO - 21/05/17 02:04:42 INFO TaskSetManager: Starting task 141.0 in stage 2.0 (TID 135) (e60b22510068, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:42,142] {docker.py:276} INFO - 21/05/17 02:04:42 INFO Executor: Running task 141.0 in stage 2.0 (TID 135)
[2021-05-16 23:04:42,143] {docker.py:276} INFO - 21/05/17 02:04:42 INFO TaskSetManager: Finished task 136.0 in stage 2.0 (TID 131) in 1188 ms on e60b22510068 (executor driver) (128/200)
[2021-05-16 23:04:42,152] {docker.py:276} INFO - 21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:42,154] {docker.py:276} INFO - 21/05/17 02:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442045194968338280361_0002_m_000141_135, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442045194968338280361_0002_m_000141_135}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442045194968338280361_0002}; taskId=attempt_202105170203442045194968338280361_0002_m_000141_135, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65720a5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:42,154] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Starting: Task committer attempt_202105170203442045194968338280361_0002_m_000141_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442045194968338280361_0002_m_000141_135
[2021-05-16 23:04:42,157] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Task committer attempt_202105170203442045194968338280361_0002_m_000141_135: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442045194968338280361_0002_m_000141_135 : duration 0:00.002s
[2021-05-16 23:04:42,828] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Starting: Task committer attempt_20210517020344667898958395207026_0002_m_000138_132: needsTaskCommit() Task attempt_20210517020344667898958395207026_0002_m_000138_132
[2021-05-16 23:04:42,829] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Task committer attempt_20210517020344667898958395207026_0002_m_000138_132: needsTaskCommit() Task attempt_20210517020344667898958395207026_0002_m_000138_132: duration 0:00.000s
21/05/17 02:04:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344667898958395207026_0002_m_000138_132
[2021-05-16 23:04:42,830] {docker.py:276} INFO - 21/05/17 02:04:42 INFO Executor: Finished task 138.0 in stage 2.0 (TID 132). 4587 bytes result sent to driver
[2021-05-16 23:04:42,831] {docker.py:276} INFO - 21/05/17 02:04:42 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 136) (e60b22510068, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:42,832] {docker.py:276} INFO - 21/05/17 02:04:42 INFO Executor: Running task 142.0 in stage 2.0 (TID 136)
21/05/17 02:04:42 INFO TaskSetManager: Finished task 138.0 in stage 2.0 (TID 132) in 1684 ms on e60b22510068 (executor driver) (129/200)
[2021-05-16 23:04:42,840] {docker.py:276} INFO - 21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Getting 3 (567.0 B) non-empty blocks including 3 (567.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:42,841] {docker.py:276} INFO - 21/05/17 02:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:42,842] {docker.py:276} INFO - 21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445832479257818633925_0002_m_000142_136, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445832479257818633925_0002_m_000142_136}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445832479257818633925_0002}; taskId=attempt_202105170203445832479257818633925_0002_m_000142_136, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68197846}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:42,842] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Starting: Task committer attempt_202105170203445832479257818633925_0002_m_000142_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445832479257818633925_0002_m_000142_136
[2021-05-16 23:04:42,845] {docker.py:276} INFO - 21/05/17 02:04:42 INFO StagingCommitter: Task committer attempt_202105170203445832479257818633925_0002_m_000142_136: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445832479257818633925_0002_m_000142_136 : duration 0:00.003s
[2021-05-16 23:04:43,167] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Starting: Task committer attempt_202105170203446935036215280538609_0002_m_000139_133: needsTaskCommit() Task attempt_202105170203446935036215280538609_0002_m_000139_133
[2021-05-16 23:04:43,168] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Task committer attempt_202105170203446935036215280538609_0002_m_000139_133: needsTaskCommit() Task attempt_202105170203446935036215280538609_0002_m_000139_133: duration 0:00.001s
[2021-05-16 23:04:43,168] {docker.py:276} INFO - 21/05/17 02:04:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446935036215280538609_0002_m_000139_133
[2021-05-16 23:04:43,169] {docker.py:276} INFO - 21/05/17 02:04:43 INFO Executor: Finished task 139.0 in stage 2.0 (TID 133). 4587 bytes result sent to driver
[2021-05-16 23:04:43,171] {docker.py:276} INFO - 21/05/17 02:04:43 INFO TaskSetManager: Starting task 143.0 in stage 2.0 (TID 137) (e60b22510068, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:43,172] {docker.py:276} INFO - 21/05/17 02:04:43 INFO TaskSetManager: Finished task 139.0 in stage 2.0 (TID 133) in 1707 ms on e60b22510068 (executor driver) (130/200)
[2021-05-16 23:04:43,172] {docker.py:276} INFO - 21/05/17 02:04:43 INFO Executor: Running task 143.0 in stage 2.0 (TID 137)
[2021-05-16 23:04:43,180] {docker.py:276} INFO - 21/05/17 02:04:43 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:43,182] {docker.py:276} INFO - 21/05/17 02:04:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:43,183] {docker.py:276} INFO - 21/05/17 02:04:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:43,183] {docker.py:276} INFO - 21/05/17 02:04:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448406162888569323675_0002_m_000143_137, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448406162888569323675_0002_m_000143_137}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448406162888569323675_0002}; taskId=attempt_202105170203448406162888569323675_0002_m_000143_137, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dff68b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:43,183] {docker.py:276} INFO - 21/05/17 02:04:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:43,184] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Starting: Task committer attempt_202105170203448406162888569323675_0002_m_000143_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448406162888569323675_0002_m_000143_137
[2021-05-16 23:04:43,185] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Task committer attempt_202105170203448406162888569323675_0002_m_000143_137: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448406162888569323675_0002_m_000143_137 : duration 0:00.003s
[2021-05-16 23:04:43,792] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Starting: Task committer attempt_202105170203442045194968338280361_0002_m_000141_135: needsTaskCommit() Task attempt_202105170203442045194968338280361_0002_m_000141_135
[2021-05-16 23:04:43,793] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Task committer attempt_202105170203442045194968338280361_0002_m_000141_135: needsTaskCommit() Task attempt_202105170203442045194968338280361_0002_m_000141_135: duration 0:00.001s
21/05/17 02:04:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442045194968338280361_0002_m_000141_135
[2021-05-16 23:04:43,796] {docker.py:276} INFO - 21/05/17 02:04:43 INFO Executor: Finished task 141.0 in stage 2.0 (TID 135). 4587 bytes result sent to driver
[2021-05-16 23:04:43,797] {docker.py:276} INFO - 21/05/17 02:04:43 INFO TaskSetManager: Starting task 144.0 in stage 2.0 (TID 138) (e60b22510068, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:43,798] {docker.py:276} INFO - 21/05/17 02:04:43 INFO Executor: Running task 144.0 in stage 2.0 (TID 138)
21/05/17 02:04:43 INFO TaskSetManager: Finished task 141.0 in stage 2.0 (TID 135) in 1660 ms on e60b22510068 (executor driver) (131/200)
[2021-05-16 23:04:43,808] {docker.py:276} INFO - 21/05/17 02:04:43 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:43,810] {docker.py:276} INFO - 21/05/17 02:04:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:43,810] {docker.py:276} INFO - 21/05/17 02:04:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:43,811] {docker.py:276} INFO - 21/05/17 02:04:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:43,811] {docker.py:276} INFO - 21/05/17 02:04:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443843217582775894030_0002_m_000144_138, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443843217582775894030_0002_m_000144_138}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443843217582775894030_0002}; taskId=attempt_202105170203443843217582775894030_0002_m_000144_138, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@455ed516}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:43,811] {docker.py:276} INFO - 21/05/17 02:04:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:43,812] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Starting: Task committer attempt_202105170203443843217582775894030_0002_m_000144_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443843217582775894030_0002_m_000144_138
[2021-05-16 23:04:43,815] {docker.py:276} INFO - 21/05/17 02:04:43 INFO StagingCommitter: Task committer attempt_202105170203443843217582775894030_0002_m_000144_138: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443843217582775894030_0002_m_000144_138 : duration 0:00.003s
[2021-05-16 23:04:44,160] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203446771531786422556293_0002_m_000140_134: needsTaskCommit() Task attempt_202105170203446771531786422556293_0002_m_000140_134
[2021-05-16 23:04:44,161] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203446771531786422556293_0002_m_000140_134: needsTaskCommit() Task attempt_202105170203446771531786422556293_0002_m_000140_134: duration 0:00.001s
21/05/17 02:04:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446771531786422556293_0002_m_000140_134
[2021-05-16 23:04:44,164] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Finished task 140.0 in stage 2.0 (TID 134). 4587 bytes result sent to driver
[2021-05-16 23:04:44,165] {docker.py:276} INFO - 21/05/17 02:04:44 INFO TaskSetManager: Starting task 145.0 in stage 2.0 (TID 139) (e60b22510068, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:44,166] {docker.py:276} INFO - 21/05/17 02:04:44 INFO TaskSetManager: Finished task 140.0 in stage 2.0 (TID 134) in 2187 ms on e60b22510068 (executor driver) (132/200)
[2021-05-16 23:04:44,167] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Running task 145.0 in stage 2.0 (TID 139)
[2021-05-16 23:04:44,176] {docker.py:276} INFO - 21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:44,178] {docker.py:276} INFO - 21/05/17 02:04:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:44,179] {docker.py:276} INFO - 21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447101083204807478212_0002_m_000145_139, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447101083204807478212_0002_m_000145_139}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447101083204807478212_0002}; taskId=attempt_202105170203447101083204807478212_0002_m_000145_139, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ca994cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:44,179] {docker.py:276} INFO - 21/05/17 02:04:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:44,179] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203447101083204807478212_0002_m_000145_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447101083204807478212_0002_m_000145_139
[2021-05-16 23:04:44,182] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203447101083204807478212_0002_m_000145_139: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447101083204807478212_0002_m_000145_139 : duration 0:00.003s
[2021-05-16 23:04:44,508] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203445832479257818633925_0002_m_000142_136: needsTaskCommit() Task attempt_202105170203445832479257818633925_0002_m_000142_136
[2021-05-16 23:04:44,509] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203445832479257818633925_0002_m_000142_136: needsTaskCommit() Task attempt_202105170203445832479257818633925_0002_m_000142_136: duration 0:00.000s
21/05/17 02:04:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445832479257818633925_0002_m_000142_136
[2021-05-16 23:04:44,511] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Finished task 142.0 in stage 2.0 (TID 136). 4544 bytes result sent to driver
[2021-05-16 23:04:44,513] {docker.py:276} INFO - 21/05/17 02:04:44 INFO TaskSetManager: Starting task 146.0 in stage 2.0 (TID 140) (e60b22510068, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:44,516] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Running task 146.0 in stage 2.0 (TID 140)
21/05/17 02:04:44 INFO TaskSetManager: Finished task 142.0 in stage 2.0 (TID 136) in 1685 ms on e60b22510068 (executor driver) (133/200)
[2021-05-16 23:04:44,524] {docker.py:276} INFO - 21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:44,526] {docker.py:276} INFO - 21/05/17 02:04:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442823832834588997551_0002_m_000146_140, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442823832834588997551_0002_m_000146_140}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442823832834588997551_0002}; taskId=attempt_202105170203442823832834588997551_0002_m_000146_140, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f1a288e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:44,526] {docker.py:276} INFO - 21/05/17 02:04:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203442823832834588997551_0002_m_000146_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442823832834588997551_0002_m_000146_140
[2021-05-16 23:04:44,529] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203442823832834588997551_0002_m_000146_140: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442823832834588997551_0002_m_000146_140 : duration 0:00.003s
[2021-05-16 23:04:44,938] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203448406162888569323675_0002_m_000143_137: needsTaskCommit() Task attempt_202105170203448406162888569323675_0002_m_000143_137
[2021-05-16 23:04:44,938] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203448406162888569323675_0002_m_000143_137: needsTaskCommit() Task attempt_202105170203448406162888569323675_0002_m_000143_137: duration 0:00.000s
[2021-05-16 23:04:44,938] {docker.py:276} INFO - 21/05/17 02:04:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448406162888569323675_0002_m_000143_137
[2021-05-16 23:04:44,940] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Finished task 143.0 in stage 2.0 (TID 137). 4544 bytes result sent to driver
[2021-05-16 23:04:44,940] {docker.py:276} INFO - 21/05/17 02:04:44 INFO TaskSetManager: Starting task 147.0 in stage 2.0 (TID 141) (e60b22510068, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:44,941] {docker.py:276} INFO - 21/05/17 02:04:44 INFO TaskSetManager: Finished task 143.0 in stage 2.0 (TID 137) in 1772 ms on e60b22510068 (executor driver) (134/200)
[2021-05-16 23:04:44,942] {docker.py:276} INFO - 21/05/17 02:04:44 INFO Executor: Running task 147.0 in stage 2.0 (TID 141)
[2021-05-16 23:04:44,949] {docker.py:276} INFO - 21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Getting 2 (440.0 B) non-empty blocks including 2 (440.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:44,951] {docker.py:276} INFO - 21/05/17 02:04:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446963995462152174838_0002_m_000147_141, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446963995462152174838_0002_m_000147_141}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446963995462152174838_0002}; taskId=attempt_202105170203446963995462152174838_0002_m_000147_141, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b0b2745}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:44,951] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Starting: Task committer attempt_202105170203446963995462152174838_0002_m_000147_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446963995462152174838_0002_m_000147_141
[2021-05-16 23:04:44,954] {docker.py:276} INFO - 21/05/17 02:04:44 INFO StagingCommitter: Task committer attempt_202105170203446963995462152174838_0002_m_000147_141: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446963995462152174838_0002_m_000147_141 : duration 0:00.003s
[2021-05-16 23:04:45,015] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Starting: Task committer attempt_202105170203443843217582775894030_0002_m_000144_138: needsTaskCommit() Task attempt_202105170203443843217582775894030_0002_m_000144_138
21/05/17 02:04:45 INFO StagingCommitter: Task committer attempt_202105170203443843217582775894030_0002_m_000144_138: needsTaskCommit() Task attempt_202105170203443843217582775894030_0002_m_000144_138: duration 0:00.000s
21/05/17 02:04:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443843217582775894030_0002_m_000144_138
[2021-05-16 23:04:45,018] {docker.py:276} INFO - 21/05/17 02:04:45 INFO Executor: Finished task 144.0 in stage 2.0 (TID 138). 4544 bytes result sent to driver
[2021-05-16 23:04:45,019] {docker.py:276} INFO - 21/05/17 02:04:45 INFO TaskSetManager: Starting task 148.0 in stage 2.0 (TID 142) (e60b22510068, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:45,021] {docker.py:276} INFO - 21/05/17 02:04:45 INFO Executor: Running task 148.0 in stage 2.0 (TID 142)
21/05/17 02:04:45 INFO TaskSetManager: Finished task 144.0 in stage 2.0 (TID 138) in 1225 ms on e60b22510068 (executor driver) (135/200)
[2021-05-16 23:04:45,030] {docker.py:276} INFO - 21/05/17 02:04:45 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:45,032] {docker.py:276} INFO - 21/05/17 02:04:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:45,032] {docker.py:276} INFO - 21/05/17 02:04:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448998632764871056798_0002_m_000148_142, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448998632764871056798_0002_m_000148_142}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448998632764871056798_0002}; taskId=attempt_202105170203448998632764871056798_0002_m_000148_142, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1309ff35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:45,033] {docker.py:276} INFO - 21/05/17 02:04:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:45 INFO StagingCommitter: Starting: Task committer attempt_202105170203448998632764871056798_0002_m_000148_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448998632764871056798_0002_m_000148_142
[2021-05-16 23:04:45,036] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Task committer attempt_202105170203448998632764871056798_0002_m_000148_142: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448998632764871056798_0002_m_000148_142 : duration 0:00.003s
[2021-05-16 23:04:45,837] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Starting: Task committer attempt_202105170203447101083204807478212_0002_m_000145_139: needsTaskCommit() Task attempt_202105170203447101083204807478212_0002_m_000145_139
[2021-05-16 23:04:45,838] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Task committer attempt_202105170203447101083204807478212_0002_m_000145_139: needsTaskCommit() Task attempt_202105170203447101083204807478212_0002_m_000145_139: duration 0:00.001s
21/05/17 02:04:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447101083204807478212_0002_m_000145_139
[2021-05-16 23:04:45,840] {docker.py:276} INFO - 21/05/17 02:04:45 INFO Executor: Finished task 145.0 in stage 2.0 (TID 139). 4544 bytes result sent to driver
[2021-05-16 23:04:45,841] {docker.py:276} INFO - 21/05/17 02:04:45 INFO TaskSetManager: Starting task 149.0 in stage 2.0 (TID 143) (e60b22510068, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:45,843] {docker.py:276} INFO - 21/05/17 02:04:45 INFO TaskSetManager: Finished task 145.0 in stage 2.0 (TID 139) in 1680 ms on e60b22510068 (executor driver) (136/200)
[2021-05-16 23:04:45,844] {docker.py:276} INFO - 21/05/17 02:04:45 INFO Executor: Running task 149.0 in stage 2.0 (TID 143)
[2021-05-16 23:04:45,863] {docker.py:276} INFO - 21/05/17 02:04:45 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:45,864] {docker.py:276} INFO - 21/05/17 02:04:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:45,865] {docker.py:276} INFO - 21/05/17 02:04:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:45,865] {docker.py:276} INFO - 21/05/17 02:04:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445706529129991265601_0002_m_000149_143, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445706529129991265601_0002_m_000149_143}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445706529129991265601_0002}; taskId=attempt_202105170203445706529129991265601_0002_m_000149_143, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@410d71f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:45,865] {docker.py:276} INFO - 21/05/17 02:04:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:45,866] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Starting: Task committer attempt_202105170203445706529129991265601_0002_m_000149_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445706529129991265601_0002_m_000149_143
[2021-05-16 23:04:45,868] {docker.py:276} INFO - 21/05/17 02:04:45 INFO StagingCommitter: Task committer attempt_202105170203445706529129991265601_0002_m_000149_143: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445706529129991265601_0002_m_000149_143 : duration 0:00.003s
[2021-05-16 23:04:46,217] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203442823832834588997551_0002_m_000146_140: needsTaskCommit() Task attempt_202105170203442823832834588997551_0002_m_000146_140
[2021-05-16 23:04:46,228] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_202105170203442823832834588997551_0002_m_000146_140: needsTaskCommit() Task attempt_202105170203442823832834588997551_0002_m_000146_140: duration 0:00.001s
21/05/17 02:04:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442823832834588997551_0002_m_000146_140
[2021-05-16 23:04:46,229] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Finished task 146.0 in stage 2.0 (TID 140). 4587 bytes result sent to driver
[2021-05-16 23:04:46,229] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Starting task 150.0 in stage 2.0 (TID 144) (e60b22510068, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:46,230] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Running task 150.0 in stage 2.0 (TID 144)
[2021-05-16 23:04:46,230] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Finished task 146.0 in stage 2.0 (TID 140) in 1712 ms on e60b22510068 (executor driver) (137/200)
[2021-05-16 23:04:46,232] {docker.py:276} INFO - 21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Getting 2 (458.0 B) non-empty blocks including 2 (458.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:46,232] {docker.py:276} INFO - 21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:46,234] {docker.py:276} INFO - 21/05/17 02:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:46,234] {docker.py:276} INFO - 21/05/17 02:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443374814911225959784_0002_m_000150_144, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443374814911225959784_0002_m_000150_144}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443374814911225959784_0002}; taskId=attempt_202105170203443374814911225959784_0002_m_000150_144, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f9ecb4e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:46,235] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203443374814911225959784_0002_m_000150_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443374814911225959784_0002_m_000150_144
[2021-05-16 23:04:46,237] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_202105170203443374814911225959784_0002_m_000150_144: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443374814911225959784_0002_m_000150_144 : duration 0:00.003s
[2021-05-16 23:04:46,638] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203446963995462152174838_0002_m_000147_141: needsTaskCommit() Task attempt_202105170203446963995462152174838_0002_m_000147_141
[2021-05-16 23:04:46,639] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_202105170203446963995462152174838_0002_m_000147_141: needsTaskCommit() Task attempt_202105170203446963995462152174838_0002_m_000147_141: duration 0:00.001s
[2021-05-16 23:04:46,640] {docker.py:276} INFO - 21/05/17 02:04:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446963995462152174838_0002_m_000147_141
[2021-05-16 23:04:46,641] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Finished task 147.0 in stage 2.0 (TID 141). 4587 bytes result sent to driver
[2021-05-16 23:04:46,643] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Starting task 151.0 in stage 2.0 (TID 145) (e60b22510068, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:46,644] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Finished task 147.0 in stage 2.0 (TID 141) in 1705 ms on e60b22510068 (executor driver) (138/200)
[2021-05-16 23:04:46,644] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Running task 151.0 in stage 2.0 (TID 145)
[2021-05-16 23:04:46,656] {docker.py:276} INFO - 21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:46,656] {docker.py:276} INFO - 21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:46,657] {docker.py:276} INFO - 21/05/17 02:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:46,658] {docker.py:276} INFO - 21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446863856601768627194_0002_m_000151_145, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446863856601768627194_0002_m_000151_145}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446863856601768627194_0002}; taskId=attempt_202105170203446863856601768627194_0002_m_000151_145, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@602e4431}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203446863856601768627194_0002_m_000151_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446863856601768627194_0002_m_000151_145
[2021-05-16 23:04:46,662] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_202105170203446863856601768627194_0002_m_000151_145: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446863856601768627194_0002_m_000151_145 : duration 0:00.004s
[2021-05-16 23:04:46,664] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_202105170203448998632764871056798_0002_m_000148_142: needsTaskCommit() Task attempt_202105170203448998632764871056798_0002_m_000148_142
[2021-05-16 23:04:46,665] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_202105170203448998632764871056798_0002_m_000148_142: needsTaskCommit() Task attempt_202105170203448998632764871056798_0002_m_000148_142: duration 0:00.000s
21/05/17 02:04:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448998632764871056798_0002_m_000148_142
[2021-05-16 23:04:46,665] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Finished task 148.0 in stage 2.0 (TID 142). 4587 bytes result sent to driver
[2021-05-16 23:04:46,666] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Starting task 152.0 in stage 2.0 (TID 146) (e60b22510068, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:46,667] {docker.py:276} INFO - 21/05/17 02:04:46 INFO TaskSetManager: Finished task 148.0 in stage 2.0 (TID 142) in 1650 ms on e60b22510068 (executor driver) (139/200)
[2021-05-16 23:04:46,667] {docker.py:276} INFO - 21/05/17 02:04:46 INFO Executor: Running task 152.0 in stage 2.0 (TID 146)
[2021-05-16 23:04:46,679] {docker.py:276} INFO - 21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:46,681] {docker.py:276} INFO - 21/05/17 02:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:46,681] {docker.py:276} INFO - 21/05/17 02:04:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051702034437906169906304416_0002_m_000152_146, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_2021051702034437906169906304416_0002_m_000152_146}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051702034437906169906304416_0002}; taskId=attempt_2021051702034437906169906304416_0002_m_000152_146, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cb7323f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:46,682] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Starting: Task committer attempt_2021051702034437906169906304416_0002_m_000152_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_2021051702034437906169906304416_0002_m_000152_146
[2021-05-16 23:04:46,683] {docker.py:276} INFO - 21/05/17 02:04:46 INFO StagingCommitter: Task committer attempt_2021051702034437906169906304416_0002_m_000152_146: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_2021051702034437906169906304416_0002_m_000152_146 : duration 0:00.002s
[2021-05-16 23:04:47,452] {docker.py:276} INFO - 21/05/17 02:04:47 INFO StagingCommitter: Starting: Task committer attempt_202105170203443374814911225959784_0002_m_000150_144: needsTaskCommit() Task attempt_202105170203443374814911225959784_0002_m_000150_144
[2021-05-16 23:04:47,452] {docker.py:276} INFO - 21/05/17 02:04:47 INFO StagingCommitter: Task committer attempt_202105170203443374814911225959784_0002_m_000150_144: needsTaskCommit() Task attempt_202105170203443374814911225959784_0002_m_000150_144: duration 0:00.001s
21/05/17 02:04:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443374814911225959784_0002_m_000150_144
[2021-05-16 23:04:47,453] {docker.py:276} INFO - 21/05/17 02:04:47 INFO Executor: Finished task 150.0 in stage 2.0 (TID 144). 4544 bytes result sent to driver
[2021-05-16 23:04:47,455] {docker.py:276} INFO - 21/05/17 02:04:47 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 147) (e60b22510068, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:47,455] {docker.py:276} INFO - 21/05/17 02:04:47 INFO Executor: Running task 153.0 in stage 2.0 (TID 147)
[2021-05-16 23:04:47,456] {docker.py:276} INFO - 21/05/17 02:04:47 INFO TaskSetManager: Finished task 150.0 in stage 2.0 (TID 144) in 1236 ms on e60b22510068 (executor driver) (140/200)
[2021-05-16 23:04:47,463] {docker.py:276} INFO - 21/05/17 02:04:47 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:47,465] {docker.py:276} INFO - 21/05/17 02:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446358792333289970895_0002_m_000153_147, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446358792333289970895_0002_m_000153_147}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446358792333289970895_0002}; taskId=attempt_202105170203446358792333289970895_0002_m_000153_147, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@541a5625}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:47 INFO StagingCommitter: Starting: Task committer attempt_202105170203446358792333289970895_0002_m_000153_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446358792333289970895_0002_m_000153_147
[2021-05-16 23:04:47,468] {docker.py:276} INFO - 21/05/17 02:04:47 INFO StagingCommitter: Task committer attempt_202105170203446358792333289970895_0002_m_000153_147: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446358792333289970895_0002_m_000153_147 : duration 0:00.003s
[2021-05-16 23:04:47,766] {docker.py:276} INFO - 21/05/17 02:04:47 INFO StagingCommitter: Starting: Task committer attempt_202105170203445706529129991265601_0002_m_000149_143: needsTaskCommit() Task attempt_202105170203445706529129991265601_0002_m_000149_143
21/05/17 02:04:47 INFO StagingCommitter: Task committer attempt_202105170203445706529129991265601_0002_m_000149_143: needsTaskCommit() Task attempt_202105170203445706529129991265601_0002_m_000149_143: duration 0:00.000s
[2021-05-16 23:04:47,767] {docker.py:276} INFO - 21/05/17 02:04:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445706529129991265601_0002_m_000149_143
[2021-05-16 23:04:47,767] {docker.py:276} INFO - 21/05/17 02:04:47 INFO Executor: Finished task 149.0 in stage 2.0 (TID 143). 4587 bytes result sent to driver
[2021-05-16 23:04:47,769] {docker.py:276} INFO - 21/05/17 02:04:47 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 148) (e60b22510068, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:47,770] {docker.py:276} INFO - 21/05/17 02:04:47 INFO TaskSetManager: Finished task 149.0 in stage 2.0 (TID 143) in 1931 ms on e60b22510068 (executor driver) (141/200)
21/05/17 02:04:47 INFO Executor: Running task 154.0 in stage 2.0 (TID 148)
[2021-05-16 23:04:47,777] {docker.py:276} INFO - 21/05/17 02:04:47 INFO ShuffleBlockFetcherIterator: Getting 2 (504.0 B) non-empty blocks including 2 (504.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:47,779] {docker.py:276} INFO - 21/05/17 02:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:47,779] {docker.py:276} INFO - 21/05/17 02:04:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442861318565505927010_0002_m_000154_148, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442861318565505927010_0002_m_000154_148}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442861318565505927010_0002}; taskId=attempt_202105170203442861318565505927010_0002_m_000154_148, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f336209}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:47 INFO StagingCommitter: Starting: Task committer attempt_202105170203442861318565505927010_0002_m_000154_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442861318565505927010_0002_m_000154_148
[2021-05-16 23:04:47,781] {docker.py:276} INFO - 21/05/17 02:04:47 INFO StagingCommitter: Task committer attempt_202105170203442861318565505927010_0002_m_000154_148: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442861318565505927010_0002_m_000154_148 : duration 0:00.002s
[2021-05-16 23:04:48,363] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203446863856601768627194_0002_m_000151_145: needsTaskCommit() Task attempt_202105170203446863856601768627194_0002_m_000151_145
21/05/17 02:04:48 INFO StagingCommitter: Task committer attempt_202105170203446863856601768627194_0002_m_000151_145: needsTaskCommit() Task attempt_202105170203446863856601768627194_0002_m_000151_145: duration 0:00.000s
21/05/17 02:04:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446863856601768627194_0002_m_000151_145
[2021-05-16 23:04:48,365] {docker.py:276} INFO - 21/05/17 02:04:48 INFO Executor: Finished task 151.0 in stage 2.0 (TID 145). 4544 bytes result sent to driver
[2021-05-16 23:04:48,367] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Starting task 155.0 in stage 2.0 (TID 149) (e60b22510068, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:48,368] {docker.py:276} INFO - 21/05/17 02:04:48 INFO Executor: Running task 155.0 in stage 2.0 (TID 149)
[2021-05-16 23:04:48,369] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Finished task 151.0 in stage 2.0 (TID 145) in 1728 ms on e60b22510068 (executor driver) (142/200)
[2021-05-16 23:04:48,371] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Starting: Task committer attempt_2021051702034437906169906304416_0002_m_000152_146: needsTaskCommit() Task attempt_2021051702034437906169906304416_0002_m_000152_146
21/05/17 02:04:48 INFO StagingCommitter: Task committer attempt_2021051702034437906169906304416_0002_m_000152_146: needsTaskCommit() Task attempt_2021051702034437906169906304416_0002_m_000152_146: duration 0:00.000s
21/05/17 02:04:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051702034437906169906304416_0002_m_000152_146
[2021-05-16 23:04:48,372] {docker.py:276} INFO - 21/05/17 02:04:48 INFO Executor: Finished task 152.0 in stage 2.0 (TID 146). 4544 bytes result sent to driver
[2021-05-16 23:04:48,374] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Starting task 156.0 in stage 2.0 (TID 150) (e60b22510068, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:48,375] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Finished task 152.0 in stage 2.0 (TID 146) in 1710 ms on e60b22510068 (executor driver) (143/200)
[2021-05-16 23:04:48,375] {docker.py:276} INFO - 21/05/17 02:04:48 INFO Executor: Running task 156.0 in stage 2.0 (TID 150)
[2021-05-16 23:04:48,381] {docker.py:276} INFO - 21/05/17 02:04:48 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:48,383] {docker.py:276} INFO - 21/05/17 02:04:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445431364012998026170_0002_m_000155_149, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445431364012998026170_0002_m_000155_149}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445431364012998026170_0002}; taskId=attempt_202105170203445431364012998026170_0002_m_000155_149, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52643a1a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:48,384] {docker.py:276} INFO - 21/05/17 02:04:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:48,384] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203445431364012998026170_0002_m_000155_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445431364012998026170_0002_m_000155_149
[2021-05-16 23:04:48,384] {docker.py:276} INFO - 21/05/17 02:04:48 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:48,385] {docker.py:276} INFO - 21/05/17 02:04:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:48,386] {docker.py:276} INFO - 21/05/17 02:04:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:48,386] {docker.py:276} INFO - 21/05/17 02:04:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442188128906358070522_0002_m_000156_150, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442188128906358070522_0002_m_000156_150}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442188128906358070522_0002}; taskId=attempt_202105170203442188128906358070522_0002_m_000156_150, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38682ead}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:48,386] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203442188128906358070522_0002_m_000156_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442188128906358070522_0002_m_000156_150
[2021-05-16 23:04:48,387] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Task committer attempt_202105170203445431364012998026170_0002_m_000155_149: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445431364012998026170_0002_m_000155_149 : duration 0:00.004s
[2021-05-16 23:04:48,389] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Task committer attempt_202105170203442188128906358070522_0002_m_000156_150: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442188128906358070522_0002_m_000156_150 : duration 0:00.003s
[2021-05-16 23:04:48,964] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Starting: Task committer attempt_202105170203442861318565505927010_0002_m_000154_148: needsTaskCommit() Task attempt_202105170203442861318565505927010_0002_m_000154_148
[2021-05-16 23:04:48,965] {docker.py:276} INFO - 21/05/17 02:04:48 INFO StagingCommitter: Task committer attempt_202105170203442861318565505927010_0002_m_000154_148: needsTaskCommit() Task attempt_202105170203442861318565505927010_0002_m_000154_148: duration 0:00.001s
21/05/17 02:04:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442861318565505927010_0002_m_000154_148
[2021-05-16 23:04:48,965] {docker.py:276} INFO - 21/05/17 02:04:48 INFO Executor: Finished task 154.0 in stage 2.0 (TID 148). 4544 bytes result sent to driver
[2021-05-16 23:04:48,967] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Starting task 157.0 in stage 2.0 (TID 151) (e60b22510068, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:48,968] {docker.py:276} INFO - 21/05/17 02:04:48 INFO TaskSetManager: Finished task 154.0 in stage 2.0 (TID 148) in 1201 ms on e60b22510068 (executor driver) (144/200)
21/05/17 02:04:48 INFO Executor: Running task 157.0 in stage 2.0 (TID 151)
[2021-05-16 23:04:48,979] {docker.py:276} INFO - 21/05/17 02:04:49 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:48,981] {docker.py:276} INFO - 21/05/17 02:04:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:48,981] {docker.py:276} INFO - 21/05/17 02:04:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445353751569131371380_0002_m_000157_151, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445353751569131371380_0002_m_000157_151}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445353751569131371380_0002}; taskId=attempt_202105170203445353751569131371380_0002_m_000157_151, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38c0daf9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:49 INFO StagingCommitter: Starting: Task committer attempt_202105170203445353751569131371380_0002_m_000157_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445353751569131371380_0002_m_000157_151
[2021-05-16 23:04:48,985] {docker.py:276} INFO - 21/05/17 02:04:49 INFO StagingCommitter: Task committer attempt_202105170203445353751569131371380_0002_m_000157_151: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445353751569131371380_0002_m_000157_151 : duration 0:00.004s
[2021-05-16 23:04:49,167] {docker.py:276} INFO - 21/05/17 02:04:49 INFO StagingCommitter: Starting: Task committer attempt_202105170203446358792333289970895_0002_m_000153_147: needsTaskCommit() Task attempt_202105170203446358792333289970895_0002_m_000153_147
21/05/17 02:04:49 INFO StagingCommitter: Task committer attempt_202105170203446358792333289970895_0002_m_000153_147: needsTaskCommit() Task attempt_202105170203446358792333289970895_0002_m_000153_147: duration 0:00.001s
21/05/17 02:04:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446358792333289970895_0002_m_000153_147
[2021-05-16 23:04:49,170] {docker.py:276} INFO - 21/05/17 02:04:49 INFO Executor: Finished task 153.0 in stage 2.0 (TID 147). 4587 bytes result sent to driver
[2021-05-16 23:04:49,171] {docker.py:276} INFO - 21/05/17 02:04:49 INFO TaskSetManager: Starting task 159.0 in stage 2.0 (TID 152) (e60b22510068, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:49,173] {docker.py:276} INFO - 21/05/17 02:04:49 INFO TaskSetManager: Finished task 153.0 in stage 2.0 (TID 147) in 1720 ms on e60b22510068 (executor driver) (145/200)
21/05/17 02:04:49 INFO Executor: Running task 159.0 in stage 2.0 (TID 152)
[2021-05-16 23:04:49,182] {docker.py:276} INFO - 21/05/17 02:04:49 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:49,182] {docker.py:276} INFO - 21/05/17 02:04:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:49,184] {docker.py:276} INFO - 21/05/17 02:04:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:04:49,184] {docker.py:276} INFO - 21/05/17 02:04:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:49,185] {docker.py:276} INFO - 21/05/17 02:04:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:49,185] {docker.py:276} INFO - 21/05/17 02:04:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444119835620471853511_0002_m_000159_152, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444119835620471853511_0002_m_000159_152}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444119835620471853511_0002}; taskId=attempt_202105170203444119835620471853511_0002_m_000159_152, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@cb91634}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:49,186] {docker.py:276} INFO - 21/05/17 02:04:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:49,186] {docker.py:276} INFO - 21/05/17 02:04:49 INFO StagingCommitter: Starting: Task committer attempt_202105170203444119835620471853511_0002_m_000159_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444119835620471853511_0002_m_000159_152
[2021-05-16 23:04:49,188] {docker.py:276} INFO - 21/05/17 02:04:49 INFO StagingCommitter: Task committer attempt_202105170203444119835620471853511_0002_m_000159_152: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444119835620471853511_0002_m_000159_152 : duration 0:00.003s
[2021-05-16 23:04:50,072] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203445431364012998026170_0002_m_000155_149: needsTaskCommit() Task attempt_202105170203445431364012998026170_0002_m_000155_149
[2021-05-16 23:04:50,073] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203445431364012998026170_0002_m_000155_149: needsTaskCommit() Task attempt_202105170203445431364012998026170_0002_m_000155_149: duration 0:00.001s
21/05/17 02:04:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445431364012998026170_0002_m_000155_149
[2021-05-16 23:04:50,074] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Finished task 155.0 in stage 2.0 (TID 149). 4587 bytes result sent to driver
[2021-05-16 23:04:50,076] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Starting task 160.0 in stage 2.0 (TID 153) (e60b22510068, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:50,077] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Running task 160.0 in stage 2.0 (TID 153)
[2021-05-16 23:04:50,078] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Finished task 155.0 in stage 2.0 (TID 149) in 1713 ms on e60b22510068 (executor driver) (146/200)
[2021-05-16 23:04:50,087] {docker.py:276} INFO - 21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:50,088] {docker.py:276} INFO - 21/05/17 02:04:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446334872832820464411_0002_m_000160_153, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446334872832820464411_0002_m_000160_153}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446334872832820464411_0002}; taskId=attempt_202105170203446334872832820464411_0002_m_000160_153, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3affeb31}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:50,089] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203446334872832820464411_0002_m_000160_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446334872832820464411_0002_m_000160_153
[2021-05-16 23:04:50,092] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203446334872832820464411_0002_m_000160_153: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446334872832820464411_0002_m_000160_153 : duration 0:00.002s
[2021-05-16 23:04:50,104] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203442188128906358070522_0002_m_000156_150: needsTaskCommit() Task attempt_202105170203442188128906358070522_0002_m_000156_150
[2021-05-16 23:04:50,104] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203442188128906358070522_0002_m_000156_150: needsTaskCommit() Task attempt_202105170203442188128906358070522_0002_m_000156_150: duration 0:00.000s
[2021-05-16 23:04:50,105] {docker.py:276} INFO - 21/05/17 02:04:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442188128906358070522_0002_m_000156_150
[2021-05-16 23:04:50,107] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Finished task 156.0 in stage 2.0 (TID 150). 4587 bytes result sent to driver
[2021-05-16 23:04:50,108] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Starting task 161.0 in stage 2.0 (TID 154) (e60b22510068, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:50,108] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Running task 161.0 in stage 2.0 (TID 154)
[2021-05-16 23:04:50,108] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Finished task 156.0 in stage 2.0 (TID 150) in 1737 ms on e60b22510068 (executor driver) (147/200)
[2021-05-16 23:04:50,116] {docker.py:276} INFO - 21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:50,117] {docker.py:276} INFO - 21/05/17 02:04:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:50,118] {docker.py:276} INFO - 21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441657342778318566768_0002_m_000161_154, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441657342778318566768_0002_m_000161_154}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441657342778318566768_0002}; taskId=attempt_202105170203441657342778318566768_0002_m_000161_154, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27c0c585}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:50,118] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203441657342778318566768_0002_m_000161_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441657342778318566768_0002_m_000161_154
[2021-05-16 23:04:50,122] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203441657342778318566768_0002_m_000161_154: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441657342778318566768_0002_m_000161_154 : duration 0:00.004s
[2021-05-16 23:04:50,658] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203445353751569131371380_0002_m_000157_151: needsTaskCommit() Task attempt_202105170203445353751569131371380_0002_m_000157_151
[2021-05-16 23:04:50,659] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203445353751569131371380_0002_m_000157_151: needsTaskCommit() Task attempt_202105170203445353751569131371380_0002_m_000157_151: duration 0:00.001s
21/05/17 02:04:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445353751569131371380_0002_m_000157_151
[2021-05-16 23:04:50,661] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Finished task 157.0 in stage 2.0 (TID 151). 4587 bytes result sent to driver
[2021-05-16 23:04:50,662] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Starting task 162.0 in stage 2.0 (TID 155) (e60b22510068, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:50,664] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Finished task 157.0 in stage 2.0 (TID 151) in 1699 ms on e60b22510068 (executor driver) (148/200)
21/05/17 02:04:50 INFO Executor: Running task 162.0 in stage 2.0 (TID 155)
[2021-05-16 23:04:50,685] {docker.py:276} INFO - 21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:50,685] {docker.py:276} INFO - 21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:50,687] {docker.py:276} INFO - 21/05/17 02:04:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441317364900555979106_0002_m_000162_155, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441317364900555979106_0002_m_000162_155}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441317364900555979106_0002}; taskId=attempt_202105170203441317364900555979106_0002_m_000162_155, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41ad2f98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:50,688] {docker.py:276} INFO - 21/05/17 02:04:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203441317364900555979106_0002_m_000162_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441317364900555979106_0002_m_000162_155
[2021-05-16 23:04:50,691] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203441317364900555979106_0002_m_000162_155: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441317364900555979106_0002_m_000162_155 : duration 0:00.003s
[2021-05-16 23:04:50,912] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203444119835620471853511_0002_m_000159_152: needsTaskCommit() Task attempt_202105170203444119835620471853511_0002_m_000159_152
21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203444119835620471853511_0002_m_000159_152: needsTaskCommit() Task attempt_202105170203444119835620471853511_0002_m_000159_152: duration 0:00.000s
21/05/17 02:04:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444119835620471853511_0002_m_000159_152
[2021-05-16 23:04:50,914] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Finished task 159.0 in stage 2.0 (TID 152). 4544 bytes result sent to driver
[2021-05-16 23:04:50,915] {docker.py:276} INFO - 21/05/17 02:04:50 INFO TaskSetManager: Starting task 163.0 in stage 2.0 (TID 156) (e60b22510068, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:50,917] {docker.py:276} INFO - 21/05/17 02:04:50 INFO Executor: Running task 163.0 in stage 2.0 (TID 156)
21/05/17 02:04:50 INFO TaskSetManager: Finished task 159.0 in stage 2.0 (TID 152) in 1747 ms on e60b22510068 (executor driver) (149/200)
[2021-05-16 23:04:50,925] {docker.py:276} INFO - 21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:50,927] {docker.py:276} INFO - 21/05/17 02:04:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442052518188316219156_0002_m_000163_156, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442052518188316219156_0002_m_000163_156}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442052518188316219156_0002}; taskId=attempt_202105170203442052518188316219156_0002_m_000163_156, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d996111}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:50,927] {docker.py:276} INFO - 21/05/17 02:04:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:50 INFO StagingCommitter: Starting: Task committer attempt_202105170203442052518188316219156_0002_m_000163_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442052518188316219156_0002_m_000163_156
[2021-05-16 23:04:50,930] {docker.py:276} INFO - 21/05/17 02:04:50 INFO StagingCommitter: Task committer attempt_202105170203442052518188316219156_0002_m_000163_156: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442052518188316219156_0002_m_000163_156 : duration 0:00.003s
[2021-05-16 23:04:51,786] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105170203446334872832820464411_0002_m_000160_153: needsTaskCommit() Task attempt_202105170203446334872832820464411_0002_m_000160_153
[2021-05-16 23:04:51,787] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Task committer attempt_202105170203446334872832820464411_0002_m_000160_153: needsTaskCommit() Task attempt_202105170203446334872832820464411_0002_m_000160_153: duration 0:00.000s
21/05/17 02:04:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446334872832820464411_0002_m_000160_153
[2021-05-16 23:04:51,790] {docker.py:276} INFO - 21/05/17 02:04:51 INFO Executor: Finished task 160.0 in stage 2.0 (TID 153). 4544 bytes result sent to driver
[2021-05-16 23:04:51,791] {docker.py:276} INFO - 21/05/17 02:04:51 INFO TaskSetManager: Starting task 164.0 in stage 2.0 (TID 157) (e60b22510068, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:51,792] {docker.py:276} INFO - 21/05/17 02:04:51 INFO TaskSetManager: Finished task 160.0 in stage 2.0 (TID 153) in 1718 ms on e60b22510068 (executor driver) (150/200)
21/05/17 02:04:51 INFO Executor: Running task 164.0 in stage 2.0 (TID 157)
[2021-05-16 23:04:51,795] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105170203441657342778318566768_0002_m_000161_154: needsTaskCommit() Task attempt_202105170203441657342778318566768_0002_m_000161_154
21/05/17 02:04:51 INFO StagingCommitter: Task committer attempt_202105170203441657342778318566768_0002_m_000161_154: needsTaskCommit() Task attempt_202105170203441657342778318566768_0002_m_000161_154: duration 0:00.000s
21/05/17 02:04:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441657342778318566768_0002_m_000161_154
21/05/17 02:04:51 INFO Executor: Finished task 161.0 in stage 2.0 (TID 154). 4544 bytes result sent to driver
[2021-05-16 23:04:51,796] {docker.py:276} INFO - 21/05/17 02:04:51 INFO TaskSetManager: Starting task 165.0 in stage 2.0 (TID 158) (e60b22510068, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:51,797] {docker.py:276} INFO - 21/05/17 02:04:51 INFO TaskSetManager: Finished task 161.0 in stage 2.0 (TID 154) in 1692 ms on e60b22510068 (executor driver) (151/200)
21/05/17 02:04:51 INFO Executor: Running task 165.0 in stage 2.0 (TID 158)
[2021-05-16 23:04:51,806] {docker.py:276} INFO - 21/05/17 02:04:51 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 02:04:51 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 02:04:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443141235380619697591_0002_m_000164_157, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443141235380619697591_0002_m_000164_157}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443141235380619697591_0002}; taskId=attempt_202105170203443141235380619697591_0002_m_000164_157, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47a28e57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105170203443141235380619697591_0002_m_000164_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443141235380619697591_0002_m_000164_157
[2021-05-16 23:04:51,807] {docker.py:276} INFO - 21/05/17 02:04:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:51,807] {docker.py:276} INFO - 21/05/17 02:04:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442263882193868727616_0002_m_000165_158, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442263882193868727616_0002_m_000165_158}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442263882193868727616_0002}; taskId=attempt_202105170203442263882193868727616_0002_m_000165_158, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b274e3e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:51,808] {docker.py:276} INFO - 21/05/17 02:04:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105170203442263882193868727616_0002_m_000165_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442263882193868727616_0002_m_000165_158
[2021-05-16 23:04:51,810] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Task committer attempt_202105170203443141235380619697591_0002_m_000164_157: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443141235380619697591_0002_m_000164_157 : duration 0:00.004s
[2021-05-16 23:04:51,811] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Task committer attempt_202105170203442263882193868727616_0002_m_000165_158: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442263882193868727616_0002_m_000165_158 : duration 0:00.003s
[2021-05-16 23:04:51,970] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Starting: Task committer attempt_202105170203441317364900555979106_0002_m_000162_155: needsTaskCommit() Task attempt_202105170203441317364900555979106_0002_m_000162_155
[2021-05-16 23:04:51,972] {docker.py:276} INFO - 21/05/17 02:04:51 INFO StagingCommitter: Task committer attempt_202105170203441317364900555979106_0002_m_000162_155: needsTaskCommit() Task attempt_202105170203441317364900555979106_0002_m_000162_155: duration 0:00.001s
21/05/17 02:04:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441317364900555979106_0002_m_000162_155
[2021-05-16 23:04:51,973] {docker.py:276} INFO - 21/05/17 02:04:51 INFO Executor: Finished task 162.0 in stage 2.0 (TID 155). 4544 bytes result sent to driver
[2021-05-16 23:04:51,973] {docker.py:276} INFO - 21/05/17 02:04:52 INFO TaskSetManager: Starting task 166.0 in stage 2.0 (TID 159) (e60b22510068, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:51,974] {docker.py:276} INFO - 21/05/17 02:04:52 INFO Executor: Running task 166.0 in stage 2.0 (TID 159)
[2021-05-16 23:04:51,975] {docker.py:276} INFO - 21/05/17 02:04:52 INFO TaskSetManager: Finished task 162.0 in stage 2.0 (TID 155) in 1314 ms on e60b22510068 (executor driver) (152/200)
[2021-05-16 23:04:51,992] {docker.py:276} INFO - 21/05/17 02:04:52 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:51,993] {docker.py:276} INFO - 21/05/17 02:04:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441220575073977704720_0002_m_000166_159, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441220575073977704720_0002_m_000166_159}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441220575073977704720_0002}; taskId=attempt_202105170203441220575073977704720_0002_m_000166_159, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3064462f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:51,994] {docker.py:276} INFO - 21/05/17 02:04:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203441220575073977704720_0002_m_000166_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441220575073977704720_0002_m_000166_159
[2021-05-16 23:04:51,996] {docker.py:276} INFO - 21/05/17 02:04:52 INFO StagingCommitter: Task committer attempt_202105170203441220575073977704720_0002_m_000166_159: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441220575073977704720_0002_m_000166_159 : duration 0:00.003s
[2021-05-16 23:04:52,149] {docker.py:276} INFO - 21/05/17 02:04:52 INFO StagingCommitter: Starting: Task committer attempt_202105170203442052518188316219156_0002_m_000163_156: needsTaskCommit() Task attempt_202105170203442052518188316219156_0002_m_000163_156
[2021-05-16 23:04:52,150] {docker.py:276} INFO - 21/05/17 02:04:52 INFO StagingCommitter: Task committer attempt_202105170203442052518188316219156_0002_m_000163_156: needsTaskCommit() Task attempt_202105170203442052518188316219156_0002_m_000163_156: duration 0:00.000s
21/05/17 02:04:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442052518188316219156_0002_m_000163_156
[2021-05-16 23:04:52,152] {docker.py:276} INFO - 21/05/17 02:04:52 INFO Executor: Finished task 163.0 in stage 2.0 (TID 156). 4587 bytes result sent to driver
[2021-05-16 23:04:52,153] {docker.py:276} INFO - 21/05/17 02:04:52 INFO TaskSetManager: Starting task 167.0 in stage 2.0 (TID 160) (e60b22510068, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:52,154] {docker.py:276} INFO - 21/05/17 02:04:52 INFO Executor: Running task 167.0 in stage 2.0 (TID 160)
21/05/17 02:04:52 INFO TaskSetManager: Finished task 163.0 in stage 2.0 (TID 156) in 1242 ms on e60b22510068 (executor driver) (153/200)
[2021-05-16 23:04:52,165] {docker.py:276} INFO - 21/05/17 02:04:52 INFO ShuffleBlockFetcherIterator: Getting 3 (672.0 B) non-empty blocks including 3 (672.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:52,166] {docker.py:276} INFO - 21/05/17 02:04:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344251535548084961296_0002_m_000167_160, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344251535548084961296_0002_m_000167_160}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344251535548084961296_0002}; taskId=attempt_20210517020344251535548084961296_0002_m_000167_160, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76df029e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:52 INFO StagingCommitter: Starting: Task committer attempt_20210517020344251535548084961296_0002_m_000167_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344251535548084961296_0002_m_000167_160
[2021-05-16 23:04:52,169] {docker.py:276} INFO - 21/05/17 02:04:52 INFO StagingCommitter: Task committer attempt_20210517020344251535548084961296_0002_m_000167_160: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344251535548084961296_0002_m_000167_160 : duration 0:00.002s
[2021-05-16 23:04:53,515] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203442263882193868727616_0002_m_000165_158: needsTaskCommit() Task attempt_202105170203442263882193868727616_0002_m_000165_158
21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_202105170203442263882193868727616_0002_m_000165_158: needsTaskCommit() Task attempt_202105170203442263882193868727616_0002_m_000165_158: duration 0:00.001s
21/05/17 02:04:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442263882193868727616_0002_m_000165_158
[2021-05-16 23:04:53,520] {docker.py:276} INFO - 21/05/17 02:04:53 INFO Executor: Finished task 165.0 in stage 2.0 (TID 158). 4587 bytes result sent to driver
[2021-05-16 23:04:53,522] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Starting task 168.0 in stage 2.0 (TID 161) (e60b22510068, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 02:04:53 INFO Executor: Running task 168.0 in stage 2.0 (TID 161)
[2021-05-16 23:04:53,523] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Finished task 165.0 in stage 2.0 (TID 158) in 1729 ms on e60b22510068 (executor driver) (154/200)
[2021-05-16 23:04:53,531] {docker.py:276} INFO - 21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Getting 3 (585.0 B) non-empty blocks including 3 (585.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:53,532] {docker.py:276} INFO - 21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:53,533] {docker.py:276} INFO - 21/05/17 02:04:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:53,534] {docker.py:276} INFO - 21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:53,534] {docker.py:276} INFO - 21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442549385302496946253_0002_m_000168_161, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442549385302496946253_0002_m_000168_161}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442549385302496946253_0002}; taskId=attempt_202105170203442549385302496946253_0002_m_000168_161, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68b36446}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:53,534] {docker.py:276} INFO - 21/05/17 02:04:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203442549385302496946253_0002_m_000168_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442549385302496946253_0002_m_000168_161
[2021-05-16 23:04:53,537] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_202105170203442549385302496946253_0002_m_000168_161: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442549385302496946253_0002_m_000168_161 : duration 0:00.003s
[2021-05-16 23:04:53,835] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_20210517020344251535548084961296_0002_m_000167_160: needsTaskCommit() Task attempt_20210517020344251535548084961296_0002_m_000167_160
[2021-05-16 23:04:53,836] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_20210517020344251535548084961296_0002_m_000167_160: needsTaskCommit() Task attempt_20210517020344251535548084961296_0002_m_000167_160: duration 0:00.000s
[2021-05-16 23:04:53,836] {docker.py:276} INFO - 21/05/17 02:04:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344251535548084961296_0002_m_000167_160
[2021-05-16 23:04:53,838] {docker.py:276} INFO - 21/05/17 02:04:53 INFO Executor: Finished task 167.0 in stage 2.0 (TID 160). 4544 bytes result sent to driver
[2021-05-16 23:04:53,839] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Starting task 169.0 in stage 2.0 (TID 162) (e60b22510068, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:53,840] {docker.py:276} INFO - 21/05/17 02:04:53 INFO Executor: Running task 169.0 in stage 2.0 (TID 162)
[2021-05-16 23:04:53,841] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Finished task 167.0 in stage 2.0 (TID 160) in 1690 ms on e60b22510068 (executor driver) (155/200)
[2021-05-16 23:04:53,848] {docker.py:276} INFO - 21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:53,850] {docker.py:276} INFO - 21/05/17 02:04:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444934394901508300058_0002_m_000169_162, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444934394901508300058_0002_m_000169_162}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444934394901508300058_0002}; taskId=attempt_202105170203444934394901508300058_0002_m_000169_162, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b42edc5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:53,850] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203444934394901508300058_0002_m_000169_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444934394901508300058_0002_m_000169_162
[2021-05-16 23:04:53,852] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_202105170203444934394901508300058_0002_m_000169_162: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444934394901508300058_0002_m_000169_162 : duration 0:00.002s
[2021-05-16 23:04:53,946] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203443141235380619697591_0002_m_000164_157: needsTaskCommit() Task attempt_202105170203443141235380619697591_0002_m_000164_157
[2021-05-16 23:04:53,946] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_202105170203443141235380619697591_0002_m_000164_157: needsTaskCommit() Task attempt_202105170203443141235380619697591_0002_m_000164_157: duration 0:00.001s
21/05/17 02:04:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443141235380619697591_0002_m_000164_157
[2021-05-16 23:04:53,948] {docker.py:276} INFO - 21/05/17 02:04:53 INFO Executor: Finished task 164.0 in stage 2.0 (TID 157). 4587 bytes result sent to driver
[2021-05-16 23:04:53,949] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Starting task 170.0 in stage 2.0 (TID 163) (e60b22510068, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:53,951] {docker.py:276} INFO - 21/05/17 02:04:53 INFO TaskSetManager: Finished task 164.0 in stage 2.0 (TID 157) in 2163 ms on e60b22510068 (executor driver) (156/200)
[2021-05-16 23:04:53,951] {docker.py:276} INFO - 21/05/17 02:04:53 INFO Executor: Running task 170.0 in stage 2.0 (TID 163)
[2021-05-16 23:04:53,960] {docker.py:276} INFO - 21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:53,962] {docker.py:276} INFO - 21/05/17 02:04:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446663952121755542248_0002_m_000170_163, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446663952121755542248_0002_m_000170_163}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446663952121755542248_0002}; taskId=attempt_202105170203446663952121755542248_0002_m_000170_163, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23cc03a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:53,963] {docker.py:276} INFO - 21/05/17 02:04:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:53 INFO StagingCommitter: Starting: Task committer attempt_202105170203446663952121755542248_0002_m_000170_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446663952121755542248_0002_m_000170_163
[2021-05-16 23:04:53,966] {docker.py:276} INFO - 21/05/17 02:04:53 INFO StagingCommitter: Task committer attempt_202105170203446663952121755542248_0002_m_000170_163: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446663952121755542248_0002_m_000170_163 : duration 0:00.003s
[2021-05-16 23:04:54,141] {docker.py:276} INFO - 21/05/17 02:04:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203441220575073977704720_0002_m_000166_159: needsTaskCommit() Task attempt_202105170203441220575073977704720_0002_m_000166_159
[2021-05-16 23:04:54,142] {docker.py:276} INFO - 21/05/17 02:04:54 INFO StagingCommitter: Task committer attempt_202105170203441220575073977704720_0002_m_000166_159: needsTaskCommit() Task attempt_202105170203441220575073977704720_0002_m_000166_159: duration 0:00.001s
[2021-05-16 23:04:54,142] {docker.py:276} INFO - 21/05/17 02:04:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441220575073977704720_0002_m_000166_159
[2021-05-16 23:04:54,144] {docker.py:276} INFO - 21/05/17 02:04:54 INFO Executor: Finished task 166.0 in stage 2.0 (TID 159). 4587 bytes result sent to driver
[2021-05-16 23:04:54,146] {docker.py:276} INFO - 21/05/17 02:04:54 INFO TaskSetManager: Starting task 171.0 in stage 2.0 (TID 164) (e60b22510068, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:54,148] {docker.py:276} INFO - 21/05/17 02:04:54 INFO TaskSetManager: Finished task 166.0 in stage 2.0 (TID 159) in 2178 ms on e60b22510068 (executor driver) (157/200)
[2021-05-16 23:04:54,149] {docker.py:276} INFO - 21/05/17 02:04:54 INFO Executor: Running task 171.0 in stage 2.0 (TID 164)
[2021-05-16 23:04:54,158] {docker.py:276} INFO - 21/05/17 02:04:54 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:54,160] {docker.py:276} INFO - 21/05/17 02:04:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446496557445427023738_0002_m_000171_164, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446496557445427023738_0002_m_000171_164}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446496557445427023738_0002}; taskId=attempt_202105170203446496557445427023738_0002_m_000171_164, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3be87353}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:54 INFO StagingCommitter: Starting: Task committer attempt_202105170203446496557445427023738_0002_m_000171_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446496557445427023738_0002_m_000171_164
[2021-05-16 23:04:54,164] {docker.py:276} INFO - 21/05/17 02:04:54 INFO StagingCommitter: Task committer attempt_202105170203446496557445427023738_0002_m_000171_164: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446496557445427023738_0002_m_000171_164 : duration 0:00.005s
[2021-05-16 23:04:55,226] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203442549385302496946253_0002_m_000168_161: needsTaskCommit() Task attempt_202105170203442549385302496946253_0002_m_000168_161
[2021-05-16 23:04:55,227] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203442549385302496946253_0002_m_000168_161: needsTaskCommit() Task attempt_202105170203442549385302496946253_0002_m_000168_161: duration 0:00.000s
21/05/17 02:04:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442549385302496946253_0002_m_000168_161
[2021-05-16 23:04:55,228] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Finished task 168.0 in stage 2.0 (TID 161). 4544 bytes result sent to driver
[2021-05-16 23:04:55,229] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Starting task 172.0 in stage 2.0 (TID 165) (e60b22510068, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:55,230] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Finished task 168.0 in stage 2.0 (TID 161) in 1712 ms on e60b22510068 (executor driver) (158/200)
[2021-05-16 23:04:55,232] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Running task 172.0 in stage 2.0 (TID 165)
[2021-05-16 23:04:55,242] {docker.py:276} INFO - 21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Getting 3 (624.0 B) non-empty blocks including 3 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:55,244] {docker.py:276} INFO - 21/05/17 02:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:55,245] {docker.py:276} INFO - 21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448805963842472490100_0002_m_000172_165, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448805963842472490100_0002_m_000172_165}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448805963842472490100_0002}; taskId=attempt_202105170203448805963842472490100_0002_m_000172_165, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39f2e739}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203448805963842472490100_0002_m_000172_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448805963842472490100_0002_m_000172_165
[2021-05-16 23:04:55,247] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203448805963842472490100_0002_m_000172_165: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448805963842472490100_0002_m_000172_165 : duration 0:00.003s
[2021-05-16 23:04:55,409] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203446496557445427023738_0002_m_000171_164: needsTaskCommit() Task attempt_202105170203446496557445427023738_0002_m_000171_164
[2021-05-16 23:04:55,410] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203446496557445427023738_0002_m_000171_164: needsTaskCommit() Task attempt_202105170203446496557445427023738_0002_m_000171_164: duration 0:00.001s
21/05/17 02:04:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446496557445427023738_0002_m_000171_164
[2021-05-16 23:04:55,411] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Finished task 171.0 in stage 2.0 (TID 164). 4544 bytes result sent to driver
[2021-05-16 23:04:55,412] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Starting task 174.0 in stage 2.0 (TID 166) (e60b22510068, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:55,422] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Finished task 171.0 in stage 2.0 (TID 164) in 1278 ms on e60b22510068 (executor driver) (159/200)
[2021-05-16 23:04:55,422] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Running task 174.0 in stage 2.0 (TID 166)
[2021-05-16 23:04:55,430] {docker.py:276} INFO - 21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Getting 3 (549.0 B) non-empty blocks including 3 (549.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:55,432] {docker.py:276} INFO - 21/05/17 02:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445876115110261263145_0002_m_000174_166, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445876115110261263145_0002_m_000174_166}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445876115110261263145_0002}; taskId=attempt_202105170203445876115110261263145_0002_m_000174_166, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7974a9fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:55,432] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203445876115110261263145_0002_m_000174_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445876115110261263145_0002_m_000174_166
[2021-05-16 23:04:55,435] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203445876115110261263145_0002_m_000174_166: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445876115110261263145_0002_m_000174_166 : duration 0:00.004s
[2021-05-16 23:04:55,501] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203444934394901508300058_0002_m_000169_162: needsTaskCommit() Task attempt_202105170203444934394901508300058_0002_m_000169_162
[2021-05-16 23:04:55,501] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203444934394901508300058_0002_m_000169_162: needsTaskCommit() Task attempt_202105170203444934394901508300058_0002_m_000169_162: duration 0:00.000s
21/05/17 02:04:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444934394901508300058_0002_m_000169_162
[2021-05-16 23:04:55,502] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Finished task 169.0 in stage 2.0 (TID 162). 4587 bytes result sent to driver
[2021-05-16 23:04:55,503] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Starting task 175.0 in stage 2.0 (TID 167) (e60b22510068, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:55,503] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Running task 175.0 in stage 2.0 (TID 167)
[2021-05-16 23:04:55,504] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Finished task 169.0 in stage 2.0 (TID 162) in 1667 ms on e60b22510068 (executor driver) (160/200)
[2021-05-16 23:04:55,511] {docker.py:276} INFO - 21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:55,513] {docker.py:276} INFO - 21/05/17 02:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:55,513] {docker.py:276} INFO - 21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447136574935581966856_0002_m_000175_167, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447136574935581966856_0002_m_000175_167}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447136574935581966856_0002}; taskId=attempt_202105170203447136574935581966856_0002_m_000175_167, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65c23d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:55,513] {docker.py:276} INFO - 21/05/17 02:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:55,514] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203447136574935581966856_0002_m_000175_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447136574935581966856_0002_m_000175_167
[2021-05-16 23:04:55,515] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203447136574935581966856_0002_m_000175_167: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447136574935581966856_0002_m_000175_167 : duration 0:00.003s
[2021-05-16 23:04:55,673] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203446663952121755542248_0002_m_000170_163: needsTaskCommit() Task attempt_202105170203446663952121755542248_0002_m_000170_163
[2021-05-16 23:04:55,674] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203446663952121755542248_0002_m_000170_163: needsTaskCommit() Task attempt_202105170203446663952121755542248_0002_m_000170_163: duration 0:00.001s
21/05/17 02:04:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446663952121755542248_0002_m_000170_163
[2021-05-16 23:04:55,676] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Finished task 170.0 in stage 2.0 (TID 163). 4587 bytes result sent to driver
[2021-05-16 23:04:55,677] {docker.py:276} INFO - 21/05/17 02:04:55 INFO TaskSetManager: Starting task 176.0 in stage 2.0 (TID 168) (e60b22510068, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:55,678] {docker.py:276} INFO - 21/05/17 02:04:55 INFO Executor: Running task 176.0 in stage 2.0 (TID 168)
21/05/17 02:04:55 INFO TaskSetManager: Finished task 170.0 in stage 2.0 (TID 163) in 1732 ms on e60b22510068 (executor driver) (161/200)
[2021-05-16 23:04:55,688] {docker.py:276} INFO - 21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:55,690] {docker.py:276} INFO - 21/05/17 02:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448679967163946560198_0002_m_000176_168, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448679967163946560198_0002_m_000176_168}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448679967163946560198_0002}; taskId=attempt_202105170203448679967163946560198_0002_m_000176_168, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@390a8136}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:55 INFO StagingCommitter: Starting: Task committer attempt_202105170203448679967163946560198_0002_m_000176_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448679967163946560198_0002_m_000176_168
[2021-05-16 23:04:55,693] {docker.py:276} INFO - 21/05/17 02:04:55 INFO StagingCommitter: Task committer attempt_202105170203448679967163946560198_0002_m_000176_168: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448679967163946560198_0002_m_000176_168 : duration 0:00.003s
[2021-05-16 23:04:57,136] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203445876115110261263145_0002_m_000174_166: needsTaskCommit() Task attempt_202105170203445876115110261263145_0002_m_000174_166
[2021-05-16 23:04:57,137] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203445876115110261263145_0002_m_000174_166: needsTaskCommit() Task attempt_202105170203445876115110261263145_0002_m_000174_166: duration 0:00.002s
21/05/17 02:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445876115110261263145_0002_m_000174_166
[2021-05-16 23:04:57,139] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Finished task 174.0 in stage 2.0 (TID 166). 4544 bytes result sent to driver
[2021-05-16 23:04:57,141] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Starting task 177.0 in stage 2.0 (TID 169) (e60b22510068, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:57,142] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Running task 177.0 in stage 2.0 (TID 169)
[2021-05-16 23:04:57,142] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Finished task 174.0 in stage 2.0 (TID 166) in 1697 ms on e60b22510068 (executor driver) (162/200)
[2021-05-16 23:04:57,151] {docker.py:276} INFO - 21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:57,153] {docker.py:276} INFO - 21/05/17 02:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446261655346875641055_0002_m_000177_169, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446261655346875641055_0002_m_000177_169}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446261655346875641055_0002}; taskId=attempt_202105170203446261655346875641055_0002_m_000177_169, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23647f73}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203446261655346875641055_0002_m_000177_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446261655346875641055_0002_m_000177_169
[2021-05-16 23:04:57,156] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203446261655346875641055_0002_m_000177_169: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446261655346875641055_0002_m_000177_169 : duration 0:00.003s
[2021-05-16 23:04:57,188] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203447136574935581966856_0002_m_000175_167: needsTaskCommit() Task attempt_202105170203447136574935581966856_0002_m_000175_167
[2021-05-16 23:04:57,190] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203447136574935581966856_0002_m_000175_167: needsTaskCommit() Task attempt_202105170203447136574935581966856_0002_m_000175_167: duration 0:00.001s
[2021-05-16 23:04:57,191] {docker.py:276} INFO - 21/05/17 02:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447136574935581966856_0002_m_000175_167
[2021-05-16 23:04:57,194] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Finished task 175.0 in stage 2.0 (TID 167). 4544 bytes result sent to driver
[2021-05-16 23:04:57,195] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Starting task 178.0 in stage 2.0 (TID 170) (e60b22510068, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:57,196] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Running task 178.0 in stage 2.0 (TID 170)
21/05/17 02:04:57 INFO TaskSetManager: Finished task 175.0 in stage 2.0 (TID 167) in 1661 ms on e60b22510068 (executor driver) (163/200)
[2021-05-16 23:04:57,205] {docker.py:276} INFO - 21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:57,207] {docker.py:276} INFO - 21/05/17 02:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445627892768923667864_0002_m_000178_170, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445627892768923667864_0002_m_000178_170}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445627892768923667864_0002}; taskId=attempt_202105170203445627892768923667864_0002_m_000178_170, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5378c9d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:57,207] {docker.py:276} INFO - 21/05/17 02:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203445627892768923667864_0002_m_000178_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445627892768923667864_0002_m_000178_170
[2021-05-16 23:04:57,211] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203445627892768923667864_0002_m_000178_170: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445627892768923667864_0002_m_000178_170 : duration 0:00.003s
[2021-05-16 23:04:57,349] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203448679967163946560198_0002_m_000176_168: needsTaskCommit() Task attempt_202105170203448679967163946560198_0002_m_000176_168
[2021-05-16 23:04:57,350] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203448679967163946560198_0002_m_000176_168: needsTaskCommit() Task attempt_202105170203448679967163946560198_0002_m_000176_168: duration 0:00.001s
21/05/17 02:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448679967163946560198_0002_m_000176_168
[2021-05-16 23:04:57,351] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Finished task 176.0 in stage 2.0 (TID 168). 4544 bytes result sent to driver
[2021-05-16 23:04:57,352] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Starting task 179.0 in stage 2.0 (TID 171) (e60b22510068, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:57,354] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Finished task 176.0 in stage 2.0 (TID 168) in 1644 ms on e60b22510068 (executor driver) (164/200)
[2021-05-16 23:04:57,355] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Running task 179.0 in stage 2.0 (TID 171)
[2021-05-16 23:04:57,364] {docker.py:276} INFO - 21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:57,366] {docker.py:276} INFO - 21/05/17 02:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:57,367] {docker.py:276} INFO - 21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444178216270201021777_0002_m_000179_171, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444178216270201021777_0002_m_000179_171}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444178216270201021777_0002}; taskId=attempt_202105170203444178216270201021777_0002_m_000179_171, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7547ec15}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:57,367] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203444178216270201021777_0002_m_000179_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444178216270201021777_0002_m_000179_171
[2021-05-16 23:04:57,370] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203444178216270201021777_0002_m_000179_171: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444178216270201021777_0002_m_000179_171 : duration 0:00.004s
[2021-05-16 23:04:57,392] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203448805963842472490100_0002_m_000172_165: needsTaskCommit() Task attempt_202105170203448805963842472490100_0002_m_000172_165
[2021-05-16 23:04:57,393] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203448805963842472490100_0002_m_000172_165: needsTaskCommit() Task attempt_202105170203448805963842472490100_0002_m_000172_165: duration 0:00.000s
21/05/17 02:04:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448805963842472490100_0002_m_000172_165
[2021-05-16 23:04:57,394] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Finished task 172.0 in stage 2.0 (TID 165). 4587 bytes result sent to driver
[2021-05-16 23:04:57,395] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Starting task 180.0 in stage 2.0 (TID 172) (e60b22510068, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:57,395] {docker.py:276} INFO - 21/05/17 02:04:57 INFO Executor: Running task 180.0 in stage 2.0 (TID 172)
[2021-05-16 23:04:57,396] {docker.py:276} INFO - 21/05/17 02:04:57 INFO TaskSetManager: Finished task 172.0 in stage 2.0 (TID 165) in 2135 ms on e60b22510068 (executor driver) (165/200)
[2021-05-16 23:04:57,402] {docker.py:276} INFO - 21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:57,404] {docker.py:276} INFO - 21/05/17 02:04:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:57,405] {docker.py:276} INFO - 21/05/17 02:04:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441210513223294587347_0002_m_000180_172, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441210513223294587347_0002_m_000180_172}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441210513223294587347_0002}; taskId=attempt_202105170203441210513223294587347_0002_m_000180_172, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71600bea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:57,405] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Starting: Task committer attempt_202105170203441210513223294587347_0002_m_000180_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441210513223294587347_0002_m_000180_172
[2021-05-16 23:04:57,408] {docker.py:276} INFO - 21/05/17 02:04:57 INFO StagingCommitter: Task committer attempt_202105170203441210513223294587347_0002_m_000180_172: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441210513223294587347_0002_m_000180_172 : duration 0:00.002s
[2021-05-16 23:04:58,784] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Starting: Task committer attempt_202105170203446261655346875641055_0002_m_000177_169: needsTaskCommit() Task attempt_202105170203446261655346875641055_0002_m_000177_169
[2021-05-16 23:04:58,784] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Task committer attempt_202105170203446261655346875641055_0002_m_000177_169: needsTaskCommit() Task attempt_202105170203446261655346875641055_0002_m_000177_169: duration 0:00.000s
21/05/17 02:04:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446261655346875641055_0002_m_000177_169
[2021-05-16 23:04:58,785] {docker.py:276} INFO - 21/05/17 02:04:58 INFO Executor: Finished task 177.0 in stage 2.0 (TID 169). 4587 bytes result sent to driver
[2021-05-16 23:04:58,785] {docker.py:276} INFO - 21/05/17 02:04:58 INFO TaskSetManager: Starting task 181.0 in stage 2.0 (TID 173) (e60b22510068, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:58,786] {docker.py:276} INFO - 21/05/17 02:04:58 INFO Executor: Running task 181.0 in stage 2.0 (TID 173)
[2021-05-16 23:04:58,787] {docker.py:276} INFO - 21/05/17 02:04:58 INFO TaskSetManager: Finished task 177.0 in stage 2.0 (TID 169) in 1649 ms on e60b22510068 (executor driver) (166/200)
[2021-05-16 23:04:58,803] {docker.py:276} INFO - 21/05/17 02:04:58 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:58,804] {docker.py:276} INFO - 21/05/17 02:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444997946223533299767_0002_m_000181_173, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444997946223533299767_0002_m_000181_173}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444997946223533299767_0002}; taskId=attempt_202105170203444997946223533299767_0002_m_000181_173, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2da5f308}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:04:58,805] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Starting: Task committer attempt_202105170203444997946223533299767_0002_m_000181_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444997946223533299767_0002_m_000181_173
[2021-05-16 23:04:58,808] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Task committer attempt_202105170203444997946223533299767_0002_m_000181_173: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444997946223533299767_0002_m_000181_173 : duration 0:00.003s
[2021-05-16 23:04:58,881] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Starting: Task committer attempt_202105170203445627892768923667864_0002_m_000178_170: needsTaskCommit() Task attempt_202105170203445627892768923667864_0002_m_000178_170
[2021-05-16 23:04:58,882] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Task committer attempt_202105170203445627892768923667864_0002_m_000178_170: needsTaskCommit() Task attempt_202105170203445627892768923667864_0002_m_000178_170: duration 0:00.001s
21/05/17 02:04:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445627892768923667864_0002_m_000178_170
[2021-05-16 23:04:58,884] {docker.py:276} INFO - 21/05/17 02:04:58 INFO Executor: Finished task 178.0 in stage 2.0 (TID 170). 4587 bytes result sent to driver
[2021-05-16 23:04:58,885] {docker.py:276} INFO - 21/05/17 02:04:58 INFO TaskSetManager: Starting task 182.0 in stage 2.0 (TID 174) (e60b22510068, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:58,887] {docker.py:276} INFO - 21/05/17 02:04:58 INFO TaskSetManager: Finished task 178.0 in stage 2.0 (TID 170) in 1694 ms on e60b22510068 (executor driver) (167/200)
[2021-05-16 23:04:58,887] {docker.py:276} INFO - 21/05/17 02:04:58 INFO Executor: Running task 182.0 in stage 2.0 (TID 174)
[2021-05-16 23:04:58,897] {docker.py:276} INFO - 21/05/17 02:04:58 INFO ShuffleBlockFetcherIterator: Getting 2 (378.0 B) non-empty blocks including 2 (378.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:58,899] {docker.py:276} INFO - 21/05/17 02:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447692459797374488806_0002_m_000182_174, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447692459797374488806_0002_m_000182_174}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447692459797374488806_0002}; taskId=attempt_202105170203447692459797374488806_0002_m_000182_174, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25d82a35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:58 INFO StagingCommitter: Starting: Task committer attempt_202105170203447692459797374488806_0002_m_000182_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447692459797374488806_0002_m_000182_174
[2021-05-16 23:04:58,903] {docker.py:276} INFO - 21/05/17 02:04:58 INFO StagingCommitter: Task committer attempt_202105170203447692459797374488806_0002_m_000182_174: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447692459797374488806_0002_m_000182_174 : duration 0:00.003s
[2021-05-16 23:04:59,090] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203441210513223294587347_0002_m_000180_172: needsTaskCommit() Task attempt_202105170203441210513223294587347_0002_m_000180_172
[2021-05-16 23:04:59,092] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Task committer attempt_202105170203441210513223294587347_0002_m_000180_172: needsTaskCommit() Task attempt_202105170203441210513223294587347_0002_m_000180_172: duration 0:00.001s
21/05/17 02:04:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441210513223294587347_0002_m_000180_172
[2021-05-16 23:04:59,093] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203444178216270201021777_0002_m_000179_171: needsTaskCommit() Task attempt_202105170203444178216270201021777_0002_m_000179_171
[2021-05-16 23:04:59,094] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Task committer attempt_202105170203444178216270201021777_0002_m_000179_171: needsTaskCommit() Task attempt_202105170203444178216270201021777_0002_m_000179_171: duration 0:00.001s
21/05/17 02:04:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444178216270201021777_0002_m_000179_171
[2021-05-16 23:04:59,095] {docker.py:276} INFO - 21/05/17 02:04:59 INFO Executor: Finished task 180.0 in stage 2.0 (TID 172). 4587 bytes result sent to driver
[2021-05-16 23:04:59,097] {docker.py:276} INFO - 21/05/17 02:04:59 INFO Executor: Finished task 179.0 in stage 2.0 (TID 171). 4587 bytes result sent to driver
21/05/17 02:04:59 INFO TaskSetManager: Starting task 183.0 in stage 2.0 (TID 175) (e60b22510068, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:04:59,098] {docker.py:276} INFO - 21/05/17 02:04:59 INFO TaskSetManager: Starting task 184.0 in stage 2.0 (TID 176) (e60b22510068, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 02:04:59 INFO Executor: Running task 183.0 in stage 2.0 (TID 175)
[2021-05-16 23:04:59,099] {docker.py:276} INFO - 21/05/17 02:04:59 INFO TaskSetManager: Finished task 180.0 in stage 2.0 (TID 172) in 1706 ms on e60b22510068 (executor driver) (168/200)
[2021-05-16 23:04:59,100] {docker.py:276} INFO - 21/05/17 02:04:59 INFO TaskSetManager: Finished task 179.0 in stage 2.0 (TID 171) in 1750 ms on e60b22510068 (executor driver) (169/200)
[2021-05-16 23:04:59,101] {docker.py:276} INFO - 21/05/17 02:04:59 INFO Executor: Running task 184.0 in stage 2.0 (TID 176)
[2021-05-16 23:04:59,108] {docker.py:276} INFO - 21/05/17 02:04:59 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:04:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:59,110] {docker.py:276} INFO - 21/05/17 02:04:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:04:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344486117194900610805_0002_m_000183_175, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344486117194900610805_0002_m_000183_175}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344486117194900610805_0002}; taskId=attempt_20210517020344486117194900610805_0002_m_000183_175, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48fc9545}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:59,110] {docker.py:276} INFO - 21/05/17 02:04:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:59 INFO StagingCommitter: Starting: Task committer attempt_20210517020344486117194900610805_0002_m_000183_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344486117194900610805_0002_m_000183_175 
21/05/17 02:04:59 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:04:59,110] {docker.py:276} INFO - 21/05/17 02:04:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:04:59,112] {docker.py:276} INFO - 21/05/17 02:04:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:04:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:04:59,113] {docker.py:276} INFO - 21/05/17 02:04:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:04:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444788850376621918907_0002_m_000184_176, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444788850376621918907_0002_m_000184_176}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444788850376621918907_0002}; taskId=attempt_202105170203444788850376621918907_0002_m_000184_176, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1025ff50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:04:59,113] {docker.py:276} INFO - 21/05/17 02:04:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:04:59 INFO StagingCommitter: Starting: Task committer attempt_202105170203444788850376621918907_0002_m_000184_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444788850376621918907_0002_m_000184_176
[2021-05-16 23:04:59,114] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Task committer attempt_20210517020344486117194900610805_0002_m_000183_175: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344486117194900610805_0002_m_000183_175 : duration 0:00.005s
[2021-05-16 23:04:59,117] {docker.py:276} INFO - 21/05/17 02:04:59 INFO StagingCommitter: Task committer attempt_202105170203444788850376621918907_0002_m_000184_176: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444788850376621918907_0002_m_000184_176 : duration 0:00.004s
[2021-05-16 23:05:00,490] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203444997946223533299767_0002_m_000181_173: needsTaskCommit() Task attempt_202105170203444997946223533299767_0002_m_000181_173
[2021-05-16 23:05:00,490] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203444997946223533299767_0002_m_000181_173: needsTaskCommit() Task attempt_202105170203444997946223533299767_0002_m_000181_173: duration 0:00.001s
21/05/17 02:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444997946223533299767_0002_m_000181_173
[2021-05-16 23:05:00,492] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Finished task 181.0 in stage 2.0 (TID 173). 4587 bytes result sent to driver
[2021-05-16 23:05:00,494] {docker.py:276} INFO - 21/05/17 02:05:00 INFO TaskSetManager: Starting task 185.0 in stage 2.0 (TID 177) (e60b22510068, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/17 02:05:00 INFO TaskSetManager: Finished task 181.0 in stage 2.0 (TID 173) in 1710 ms on e60b22510068 (executor driver) (170/200)
[2021-05-16 23:05:00,494] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Running task 185.0 in stage 2.0 (TID 177)
[2021-05-16 23:05:00,501] {docker.py:276} INFO - 21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:00,503] {docker.py:276} INFO - 21/05/17 02:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:00,503] {docker.py:276} INFO - 21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447195311115639001867_0002_m_000185_177, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447195311115639001867_0002_m_000185_177}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447195311115639001867_0002}; taskId=attempt_202105170203447195311115639001867_0002_m_000185_177, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4fa74bbe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203447195311115639001867_0002_m_000185_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447195311115639001867_0002_m_000185_177
[2021-05-16 23:05:00,506] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203447195311115639001867_0002_m_000185_177: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447195311115639001867_0002_m_000185_177 : duration 0:00.003s
[2021-05-16 23:05:00,568] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203447692459797374488806_0002_m_000182_174: needsTaskCommit() Task attempt_202105170203447692459797374488806_0002_m_000182_174
[2021-05-16 23:05:00,568] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203447692459797374488806_0002_m_000182_174: needsTaskCommit() Task attempt_202105170203447692459797374488806_0002_m_000182_174: duration 0:00.000s
21/05/17 02:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447692459797374488806_0002_m_000182_174
[2021-05-16 23:05:00,570] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Finished task 182.0 in stage 2.0 (TID 174). 4544 bytes result sent to driver
[2021-05-16 23:05:00,571] {docker.py:276} INFO - 21/05/17 02:05:00 INFO TaskSetManager: Starting task 186.0 in stage 2.0 (TID 178) (e60b22510068, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:00,572] {docker.py:276} INFO - 21/05/17 02:05:00 INFO TaskSetManager: Finished task 182.0 in stage 2.0 (TID 174) in 1689 ms on e60b22510068 (executor driver) (171/200)
[2021-05-16 23:05:00,573] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Running task 186.0 in stage 2.0 (TID 178)
[2021-05-16 23:05:00,583] {docker.py:276} INFO - 21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:00,584] {docker.py:276} INFO - 21/05/17 02:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447384633595543909219_0002_m_000186_178, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447384633595543909219_0002_m_000186_178}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447384633595543909219_0002}; taskId=attempt_202105170203447384633595543909219_0002_m_000186_178, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c839e40}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:00,585] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203447384633595543909219_0002_m_000186_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447384633595543909219_0002_m_000186_178
[2021-05-16 23:05:00,587] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203447384633595543909219_0002_m_000186_178: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447384633595543909219_0002_m_000186_178 : duration 0:00.003s
[2021-05-16 23:05:00,748] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203444788850376621918907_0002_m_000184_176: needsTaskCommit() Task attempt_202105170203444788850376621918907_0002_m_000184_176
[2021-05-16 23:05:00,749] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203444788850376621918907_0002_m_000184_176: needsTaskCommit() Task attempt_202105170203444788850376621918907_0002_m_000184_176: duration 0:00.000s
21/05/17 02:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444788850376621918907_0002_m_000184_176
[2021-05-16 23:05:00,751] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Finished task 184.0 in stage 2.0 (TID 176). 4544 bytes result sent to driver
21/05/17 02:05:00 INFO TaskSetManager: Starting task 187.0 in stage 2.0 (TID 179) (e60b22510068, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:00,752] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Running task 187.0 in stage 2.0 (TID 179)
21/05/17 02:05:00 INFO TaskSetManager: Finished task 184.0 in stage 2.0 (TID 176) in 1656 ms on e60b22510068 (executor driver) (172/200)
[2021-05-16 23:05:00,760] {docker.py:276} INFO - 21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:00,762] {docker.py:276} INFO - 21/05/17 02:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441931967328747948463_0002_m_000187_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441931967328747948463_0002_m_000187_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441931967328747948463_0002}; taskId=attempt_202105170203441931967328747948463_0002_m_000187_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2353ba5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:00,763] {docker.py:276} INFO - 21/05/17 02:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203441931967328747948463_0002_m_000187_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441931967328747948463_0002_m_000187_179
[2021-05-16 23:05:00,766] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203441931967328747948463_0002_m_000187_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441931967328747948463_0002_m_000187_179 : duration 0:00.004s
[2021-05-16 23:05:00,799] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_20210517020344486117194900610805_0002_m_000183_175: needsTaskCommit() Task attempt_20210517020344486117194900610805_0002_m_000183_175
[2021-05-16 23:05:00,800] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_20210517020344486117194900610805_0002_m_000183_175: needsTaskCommit() Task attempt_20210517020344486117194900610805_0002_m_000183_175: duration 0:00.000s
21/05/17 02:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344486117194900610805_0002_m_000183_175
[2021-05-16 23:05:00,802] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Finished task 183.0 in stage 2.0 (TID 175). 4544 bytes result sent to driver
[2021-05-16 23:05:00,804] {docker.py:276} INFO - 21/05/17 02:05:00 INFO TaskSetManager: Starting task 188.0 in stage 2.0 (TID 180) (e60b22510068, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:00,805] {docker.py:276} INFO - 21/05/17 02:05:00 INFO TaskSetManager: Finished task 183.0 in stage 2.0 (TID 175) in 1712 ms on e60b22510068 (executor driver) (173/200)
[2021-05-16 23:05:00,806] {docker.py:276} INFO - 21/05/17 02:05:00 INFO Executor: Running task 188.0 in stage 2.0 (TID 180)
[2021-05-16 23:05:00,815] {docker.py:276} INFO - 21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:00,817] {docker.py:276} INFO - 21/05/17 02:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:00,817] {docker.py:276} INFO - 21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445605059022459849474_0002_m_000188_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445605059022459849474_0002_m_000188_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445605059022459849474_0002}; taskId=attempt_202105170203445605059022459849474_0002_m_000188_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14ba2263}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:00,817] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Starting: Task committer attempt_202105170203445605059022459849474_0002_m_000188_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445605059022459849474_0002_m_000188_180
[2021-05-16 23:05:00,820] {docker.py:276} INFO - 21/05/17 02:05:00 INFO StagingCommitter: Task committer attempt_202105170203445605059022459849474_0002_m_000188_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445605059022459849474_0002_m_000188_180 : duration 0:00.003s
[2021-05-16 23:05:02,218] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203447195311115639001867_0002_m_000185_177: needsTaskCommit() Task attempt_202105170203447195311115639001867_0002_m_000185_177
[2021-05-16 23:05:02,219] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203447195311115639001867_0002_m_000185_177: needsTaskCommit() Task attempt_202105170203447195311115639001867_0002_m_000185_177: duration 0:00.001s
21/05/17 02:05:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447195311115639001867_0002_m_000185_177
[2021-05-16 23:05:02,221] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Finished task 185.0 in stage 2.0 (TID 177). 4587 bytes result sent to driver
[2021-05-16 23:05:02,222] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Starting task 189.0 in stage 2.0 (TID 181) (e60b22510068, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:02,223] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Finished task 185.0 in stage 2.0 (TID 177) in 1732 ms on e60b22510068 (executor driver) (174/200)
[2021-05-16 23:05:02,224] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Running task 189.0 in stage 2.0 (TID 181)
[2021-05-16 23:05:02,234] {docker.py:276} INFO - 21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:02,235] {docker.py:276} INFO - 21/05/17 02:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344911760160978766744_0002_m_000189_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344911760160978766744_0002_m_000189_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344911760160978766744_0002}; taskId=attempt_20210517020344911760160978766744_0002_m_000189_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d9a44ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_20210517020344911760160978766744_0002_m_000189_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344911760160978766744_0002_m_000189_181
[2021-05-16 23:05:02,238] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_20210517020344911760160978766744_0002_m_000189_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344911760160978766744_0002_m_000189_181 : duration 0:00.002s
[2021-05-16 23:05:02,304] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203447384633595543909219_0002_m_000186_178: needsTaskCommit() Task attempt_202105170203447384633595543909219_0002_m_000186_178
[2021-05-16 23:05:02,305] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203447384633595543909219_0002_m_000186_178: needsTaskCommit() Task attempt_202105170203447384633595543909219_0002_m_000186_178: duration 0:00.001s
21/05/17 02:05:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447384633595543909219_0002_m_000186_178
[2021-05-16 23:05:02,307] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Finished task 186.0 in stage 2.0 (TID 178). 4587 bytes result sent to driver
[2021-05-16 23:05:02,309] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Starting task 190.0 in stage 2.0 (TID 182) (e60b22510068, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:02,310] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Running task 190.0 in stage 2.0 (TID 182)
[2021-05-16 23:05:02,311] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Finished task 186.0 in stage 2.0 (TID 178) in 1741 ms on e60b22510068 (executor driver) (175/200)
[2021-05-16 23:05:02,319] {docker.py:276} INFO - 21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:02,320] {docker.py:276} INFO - 21/05/17 02:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:02,321] {docker.py:276} INFO - 21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445395201027256224851_0002_m_000190_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445395201027256224851_0002_m_000190_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445395201027256224851_0002}; taskId=attempt_202105170203445395201027256224851_0002_m_000190_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a2bfec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203445395201027256224851_0002_m_000190_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445395201027256224851_0002_m_000190_182
[2021-05-16 23:05:02,323] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203445395201027256224851_0002_m_000190_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445395201027256224851_0002_m_000190_182 : duration 0:00.003s
[2021-05-16 23:05:02,494] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203441931967328747948463_0002_m_000187_179: needsTaskCommit() Task attempt_202105170203441931967328747948463_0002_m_000187_179
[2021-05-16 23:05:02,495] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203441931967328747948463_0002_m_000187_179: needsTaskCommit() Task attempt_202105170203441931967328747948463_0002_m_000187_179: duration 0:00.000s
21/05/17 02:05:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441931967328747948463_0002_m_000187_179
[2021-05-16 23:05:02,496] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Finished task 187.0 in stage 2.0 (TID 179). 4587 bytes result sent to driver
[2021-05-16 23:05:02,498] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Starting task 191.0 in stage 2.0 (TID 183) (e60b22510068, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:02,500] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Finished task 187.0 in stage 2.0 (TID 179) in 1751 ms on e60b22510068 (executor driver) (176/200)
21/05/17 02:05:02 INFO Executor: Running task 191.0 in stage 2.0 (TID 183)
[2021-05-16 23:05:02,510] {docker.py:276} INFO - 21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Getting 2 (360.0 B) non-empty blocks including 2 (360.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:02,512] {docker.py:276} INFO - 21/05/17 02:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443017019034985476843_0002_m_000191_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443017019034985476843_0002_m_000191_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443017019034985476843_0002}; taskId=attempt_202105170203443017019034985476843_0002_m_000191_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ef9606}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:02,512] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203443017019034985476843_0002_m_000191_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443017019034985476843_0002_m_000191_183
[2021-05-16 23:05:02,515] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203443017019034985476843_0002_m_000191_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443017019034985476843_0002_m_000191_183 : duration 0:00.002s
[2021-05-16 23:05:02,591] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203445605059022459849474_0002_m_000188_180: needsTaskCommit() Task attempt_202105170203445605059022459849474_0002_m_000188_180
[2021-05-16 23:05:02,592] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203445605059022459849474_0002_m_000188_180: needsTaskCommit() Task attempt_202105170203445605059022459849474_0002_m_000188_180: duration 0:00.001s
21/05/17 02:05:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445605059022459849474_0002_m_000188_180
[2021-05-16 23:05:02,594] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Finished task 188.0 in stage 2.0 (TID 180). 4587 bytes result sent to driver
[2021-05-16 23:05:02,595] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Starting task 192.0 in stage 2.0 (TID 184) (e60b22510068, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:02,596] {docker.py:276} INFO - 21/05/17 02:05:02 INFO TaskSetManager: Finished task 188.0 in stage 2.0 (TID 180) in 1794 ms on e60b22510068 (executor driver) (177/200)
[2021-05-16 23:05:02,597] {docker.py:276} INFO - 21/05/17 02:05:02 INFO Executor: Running task 192.0 in stage 2.0 (TID 184)
[2021-05-16 23:05:02,608] {docker.py:276} INFO - 21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:02,610] {docker.py:276} INFO - 21/05/17 02:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441831819182095250102_0002_m_000192_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441831819182095250102_0002_m_000192_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441831819182095250102_0002}; taskId=attempt_202105170203441831819182095250102_0002_m_000192_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75e02d73}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:02 INFO StagingCommitter: Starting: Task committer attempt_202105170203441831819182095250102_0002_m_000192_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441831819182095250102_0002_m_000192_184
[2021-05-16 23:05:02,612] {docker.py:276} INFO - 21/05/17 02:05:02 INFO StagingCommitter: Task committer attempt_202105170203441831819182095250102_0002_m_000192_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441831819182095250102_0002_m_000192_184 : duration 0:00.002s
[2021-05-16 23:05:04,024] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203445395201027256224851_0002_m_000190_182: needsTaskCommit() Task attempt_202105170203445395201027256224851_0002_m_000190_182
[2021-05-16 23:05:04,025] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_202105170203445395201027256224851_0002_m_000190_182: needsTaskCommit() Task attempt_202105170203445395201027256224851_0002_m_000190_182: duration 0:00.001s
21/05/17 02:05:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445395201027256224851_0002_m_000190_182
[2021-05-16 23:05:04,029] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Finished task 190.0 in stage 2.0 (TID 182). 4544 bytes result sent to driver
[2021-05-16 23:05:04,030] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Starting task 193.0 in stage 2.0 (TID 185) (e60b22510068, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:04,031] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Running task 193.0 in stage 2.0 (TID 185)
21/05/17 02:05:04 INFO TaskSetManager: Finished task 190.0 in stage 2.0 (TID 182) in 1725 ms on e60b22510068 (executor driver) (178/200)
[2021-05-16 23:05:04,042] {docker.py:276} INFO - 21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Getting 2 (417.0 B) non-empty blocks including 2 (417.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:04,043] {docker.py:276} INFO - 21/05/17 02:05:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344226031970112054528_0002_m_000193_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344226031970112054528_0002_m_000193_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344226031970112054528_0002}; taskId=attempt_20210517020344226031970112054528_0002_m_000193_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21faea69}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_20210517020344226031970112054528_0002_m_000193_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344226031970112054528_0002_m_000193_185
[2021-05-16 23:05:04,046] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_20210517020344226031970112054528_0002_m_000193_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344226031970112054528_0002_m_000193_185 : duration 0:00.002s
[2021-05-16 23:05:04,082] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_20210517020344911760160978766744_0002_m_000189_181: needsTaskCommit() Task attempt_20210517020344911760160978766744_0002_m_000189_181
[2021-05-16 23:05:04,082] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_20210517020344911760160978766744_0002_m_000189_181: needsTaskCommit() Task attempt_20210517020344911760160978766744_0002_m_000189_181: duration 0:00.001s
21/05/17 02:05:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344911760160978766744_0002_m_000189_181
[2021-05-16 23:05:04,084] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Finished task 189.0 in stage 2.0 (TID 181). 4544 bytes result sent to driver
[2021-05-16 23:05:04,085] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Starting task 194.0 in stage 2.0 (TID 186) (e60b22510068, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:04,086] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Finished task 189.0 in stage 2.0 (TID 181) in 1867 ms on e60b22510068 (executor driver) (179/200)
21/05/17 02:05:04 INFO Executor: Running task 194.0 in stage 2.0 (TID 186)
[2021-05-16 23:05:04,093] {docker.py:276} INFO - 21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:04,094] {docker.py:276} INFO - 21/05/17 02:05:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:04,095] {docker.py:276} INFO - 21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448291895010331735244_0002_m_000194_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448291895010331735244_0002_m_000194_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448291895010331735244_0002}; taskId=attempt_202105170203448291895010331735244_0002_m_000194_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41df1fc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203448291895010331735244_0002_m_000194_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448291895010331735244_0002_m_000194_186
[2021-05-16 23:05:04,097] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_202105170203448291895010331735244_0002_m_000194_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448291895010331735244_0002_m_000194_186 : duration 0:00.002s
[2021-05-16 23:05:04,320] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203443017019034985476843_0002_m_000191_183: needsTaskCommit() Task attempt_202105170203443017019034985476843_0002_m_000191_183
[2021-05-16 23:05:04,322] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_202105170203443017019034985476843_0002_m_000191_183: needsTaskCommit() Task attempt_202105170203443017019034985476843_0002_m_000191_183: duration 0:00.002s
[2021-05-16 23:05:04,323] {docker.py:276} INFO - 21/05/17 02:05:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443017019034985476843_0002_m_000191_183
[2021-05-16 23:05:04,324] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Finished task 191.0 in stage 2.0 (TID 183). 4544 bytes result sent to driver
[2021-05-16 23:05:04,326] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Starting task 195.0 in stage 2.0 (TID 187) (e60b22510068, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:04,327] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Finished task 191.0 in stage 2.0 (TID 183) in 1831 ms on e60b22510068 (executor driver) (180/200)
[2021-05-16 23:05:04,328] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Running task 195.0 in stage 2.0 (TID 187)
[2021-05-16 23:05:04,338] {docker.py:276} INFO - 21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Getting 3 (606.0 B) non-empty blocks including 3 (606.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:04,340] {docker.py:276} INFO - 21/05/17 02:05:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344559962479498597358_0002_m_000195_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344559962479498597358_0002_m_000195_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344559962479498597358_0002}; taskId=attempt_20210517020344559962479498597358_0002_m_000195_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@474f64af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:04,340] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_20210517020344559962479498597358_0002_m_000195_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344559962479498597358_0002_m_000195_187
[2021-05-16 23:05:04,343] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_20210517020344559962479498597358_0002_m_000195_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344559962479498597358_0002_m_000195_187 : duration 0:00.003s
[2021-05-16 23:05:04,386] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203441831819182095250102_0002_m_000192_184: needsTaskCommit() Task attempt_202105170203441831819182095250102_0002_m_000192_184
21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_202105170203441831819182095250102_0002_m_000192_184: needsTaskCommit() Task attempt_202105170203441831819182095250102_0002_m_000192_184: duration 0:00.000s
21/05/17 02:05:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441831819182095250102_0002_m_000192_184
[2021-05-16 23:05:04,390] {docker.py:276} INFO - 21/05/17 02:05:04 INFO Executor: Finished task 192.0 in stage 2.0 (TID 184). 4544 bytes result sent to driver
[2021-05-16 23:05:04,391] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Starting task 196.0 in stage 2.0 (TID 188) (e60b22510068, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:04,392] {docker.py:276} INFO - 21/05/17 02:05:04 INFO TaskSetManager: Finished task 192.0 in stage 2.0 (TID 184) in 1799 ms on e60b22510068 (executor driver) (181/200)
21/05/17 02:05:04 INFO Executor: Running task 196.0 in stage 2.0 (TID 188)
[2021-05-16 23:05:04,401] {docker.py:276} INFO - 21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Getting 2 (396.0 B) non-empty blocks including 2 (396.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:04,403] {docker.py:276} INFO - 21/05/17 02:05:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444803981524541418063_0002_m_000196_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444803981524541418063_0002_m_000196_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444803981524541418063_0002}; taskId=attempt_202105170203444803981524541418063_0002_m_000196_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e54c751}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:04,403] {docker.py:276} INFO - 21/05/17 02:05:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:04 INFO StagingCommitter: Starting: Task committer attempt_202105170203444803981524541418063_0002_m_000196_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444803981524541418063_0002_m_000196_188
[2021-05-16 23:05:04,406] {docker.py:276} INFO - 21/05/17 02:05:04 INFO StagingCommitter: Task committer attempt_202105170203444803981524541418063_0002_m_000196_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444803981524541418063_0002_m_000196_188 : duration 0:00.003s
[2021-05-16 23:05:05,780] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Starting: Task committer attempt_20210517020344226031970112054528_0002_m_000193_185: needsTaskCommit() Task attempt_20210517020344226031970112054528_0002_m_000193_185
21/05/17 02:05:05 INFO StagingCommitter: Task committer attempt_20210517020344226031970112054528_0002_m_000193_185: needsTaskCommit() Task attempt_20210517020344226031970112054528_0002_m_000193_185: duration 0:00.001s
21/05/17 02:05:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344226031970112054528_0002_m_000193_185
[2021-05-16 23:05:05,781] {docker.py:276} INFO - 21/05/17 02:05:05 INFO Executor: Finished task 193.0 in stage 2.0 (TID 185). 4587 bytes result sent to driver
[2021-05-16 23:05:05,783] {docker.py:276} INFO - 21/05/17 02:05:05 INFO TaskSetManager: Starting task 197.0 in stage 2.0 (TID 189) (e60b22510068, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:05,784] {docker.py:276} INFO - 21/05/17 02:05:05 INFO TaskSetManager: Finished task 193.0 in stage 2.0 (TID 185) in 1757 ms on e60b22510068 (executor driver) (182/200)
[2021-05-16 23:05:05,786] {docker.py:276} INFO - 21/05/17 02:05:05 INFO Executor: Running task 197.0 in stage 2.0 (TID 189)
[2021-05-16 23:05:05,788] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203448291895010331735244_0002_m_000194_186: needsTaskCommit() Task attempt_202105170203448291895010331735244_0002_m_000194_186
[2021-05-16 23:05:05,789] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Task committer attempt_202105170203448291895010331735244_0002_m_000194_186: needsTaskCommit() Task attempt_202105170203448291895010331735244_0002_m_000194_186: duration 0:00.000s
21/05/17 02:05:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448291895010331735244_0002_m_000194_186
[2021-05-16 23:05:05,790] {docker.py:276} INFO - 21/05/17 02:05:05 INFO Executor: Finished task 194.0 in stage 2.0 (TID 186). 4587 bytes result sent to driver
[2021-05-16 23:05:05,792] {docker.py:276} INFO - 21/05/17 02:05:05 INFO TaskSetManager: Starting task 198.0 in stage 2.0 (TID 190) (e60b22510068, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:05,792] {docker.py:276} INFO - 21/05/17 02:05:05 INFO TaskSetManager: Finished task 194.0 in stage 2.0 (TID 186) in 1709 ms on e60b22510068 (executor driver) (183/200)
[2021-05-16 23:05:05,793] {docker.py:276} INFO - 21/05/17 02:05:05 INFO Executor: Running task 198.0 in stage 2.0 (TID 190)
[2021-05-16 23:05:05,798] {docker.py:276} INFO - 21/05/17 02:05:05 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:05,799] {docker.py:276} INFO - 21/05/17 02:05:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:05,800] {docker.py:276} INFO - 21/05/17 02:05:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:05,801] {docker.py:276} INFO - 21/05/17 02:05:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:05,801] {docker.py:276} INFO - 21/05/17 02:05:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447622013947108253935_0002_m_000197_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447622013947108253935_0002_m_000197_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447622013947108253935_0002}; taskId=attempt_202105170203447622013947108253935_0002_m_000197_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@454fc4ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:05,801] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203447622013947108253935_0002_m_000197_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447622013947108253935_0002_m_000197_189
[2021-05-16 23:05:05,803] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Task committer attempt_202105170203447622013947108253935_0002_m_000197_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447622013947108253935_0002_m_000197_189 : duration 0:00.003s
[2021-05-16 23:05:05,804] {docker.py:276} INFO - 21/05/17 02:05:05 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:05,804] {docker.py:276} INFO - 21/05/17 02:05:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:05,806] {docker.py:276} INFO - 21/05/17 02:05:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:05:05,806] {docker.py:276} INFO - 21/05/17 02:05:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:05,807] {docker.py:276} INFO - 21/05/17 02:05:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:05,807] {docker.py:276} INFO - 21/05/17 02:05:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203442610684569695808582_0002_m_000198_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442610684569695808582_0002_m_000198_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203442610684569695808582_0002}; taskId=attempt_202105170203442610684569695808582_0002_m_000198_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26511607}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:05,807] {docker.py:276} INFO - 21/05/17 02:05:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:05,807] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Starting: Task committer attempt_202105170203442610684569695808582_0002_m_000198_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442610684569695808582_0002_m_000198_190
[2021-05-16 23:05:05,809] {docker.py:276} INFO - 21/05/17 02:05:05 INFO StagingCommitter: Task committer attempt_202105170203442610684569695808582_0002_m_000198_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203442610684569695808582_0002_m_000198_190 : duration 0:00.002s
[2021-05-16 23:05:06,044] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Starting: Task committer attempt_20210517020344559962479498597358_0002_m_000195_187: needsTaskCommit() Task attempt_20210517020344559962479498597358_0002_m_000195_187
[2021-05-16 23:05:06,045] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Task committer attempt_20210517020344559962479498597358_0002_m_000195_187: needsTaskCommit() Task attempt_20210517020344559962479498597358_0002_m_000195_187: duration 0:00.001s
[2021-05-16 23:05:06,045] {docker.py:276} INFO - 21/05/17 02:05:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344559962479498597358_0002_m_000195_187
[2021-05-16 23:05:06,046] {docker.py:276} INFO - 21/05/17 02:05:06 INFO Executor: Finished task 195.0 in stage 2.0 (TID 187). 4587 bytes result sent to driver
[2021-05-16 23:05:06,049] {docker.py:276} INFO - 21/05/17 02:05:06 INFO TaskSetManager: Starting task 199.0 in stage 2.0 (TID 191) (e60b22510068, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:06,050] {docker.py:276} INFO - 21/05/17 02:05:06 INFO TaskSetManager: Finished task 195.0 in stage 2.0 (TID 187) in 1726 ms on e60b22510068 (executor driver) (184/200)
[2021-05-16 23:05:06,051] {docker.py:276} INFO - 21/05/17 02:05:06 INFO Executor: Running task 199.0 in stage 2.0 (TID 191)
[2021-05-16 23:05:06,052] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Starting: Task committer attempt_202105170203444803981524541418063_0002_m_000196_188: needsTaskCommit() Task attempt_202105170203444803981524541418063_0002_m_000196_188
[2021-05-16 23:05:06,052] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Task committer attempt_202105170203444803981524541418063_0002_m_000196_188: needsTaskCommit() Task attempt_202105170203444803981524541418063_0002_m_000196_188: duration 0:00.000s
21/05/17 02:05:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444803981524541418063_0002_m_000196_188
[2021-05-16 23:05:06,053] {docker.py:276} INFO - 21/05/17 02:05:06 INFO Executor: Finished task 196.0 in stage 2.0 (TID 188). 4587 bytes result sent to driver
[2021-05-16 23:05:06,056] {docker.py:276} INFO - 21/05/17 02:05:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 192) (e60b22510068, executor driver, partition 0, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:06,057] {docker.py:276} INFO - 21/05/17 02:05:06 INFO TaskSetManager: Finished task 196.0 in stage 2.0 (TID 188) in 1668 ms on e60b22510068 (executor driver) (185/200)
[2021-05-16 23:05:06,057] {docker.py:276} INFO - 21/05/17 02:05:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 192)
[2021-05-16 23:05:06,060] {docker.py:276} INFO - 21/05/17 02:05:06 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:06,061] {docker.py:276} INFO - 21/05/17 02:05:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:06,062] {docker.py:276} INFO - 21/05/17 02:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:06,062] {docker.py:276} INFO - 21/05/17 02:05:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443477136452462391143_0002_m_000199_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443477136452462391143_0002_m_000199_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443477136452462391143_0002}; taskId=attempt_202105170203443477136452462391143_0002_m_000199_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52493e7b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:06 INFO StagingCommitter: Starting: Task committer attempt_202105170203443477136452462391143_0002_m_000199_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443477136452462391143_0002_m_000199_191
[2021-05-16 23:05:06,065] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Task committer attempt_202105170203443477136452462391143_0002_m_000199_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443477136452462391143_0002_m_000199_191 : duration 0:00.003s
[2021-05-16 23:05:06,067] {docker.py:276} INFO - 21/05/17 02:05:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:06,068] {docker.py:276} INFO - 21/05/17 02:05:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:06,069] {docker.py:276} INFO - 21/05/17 02:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 23:05:06,070] {docker.py:276} INFO - 21/05/17 02:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:06,070] {docker.py:276} INFO - 21/05/17 02:05:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:06,070] {docker.py:276} INFO - 21/05/17 02:05:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203447451426322073120127_0002_m_000000_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447451426322073120127_0002_m_000000_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203447451426322073120127_0002}; taskId=attempt_202105170203447451426322073120127_0002_m_000000_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@352dc20d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:06,071] {docker.py:276} INFO - 21/05/17 02:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:06,071] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Starting: Task committer attempt_202105170203447451426322073120127_0002_m_000000_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447451426322073120127_0002_m_000000_192
[2021-05-16 23:05:06,074] {docker.py:276} INFO - 21/05/17 02:05:06 INFO StagingCommitter: Task committer attempt_202105170203447451426322073120127_0002_m_000000_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203447451426322073120127_0002_m_000000_192 : duration 0:00.003s
[2021-05-16 23:05:07,499] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203447622013947108253935_0002_m_000197_189: needsTaskCommit() Task attempt_202105170203447622013947108253935_0002_m_000197_189
[2021-05-16 23:05:07,500] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203447622013947108253935_0002_m_000197_189: needsTaskCommit() Task attempt_202105170203447622013947108253935_0002_m_000197_189: duration 0:00.001s
[2021-05-16 23:05:07,501] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203442610684569695808582_0002_m_000198_190: needsTaskCommit() Task attempt_202105170203442610684569695808582_0002_m_000198_190
[2021-05-16 23:05:07,502] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447622013947108253935_0002_m_000197_189
[2021-05-16 23:05:07,502] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203442610684569695808582_0002_m_000198_190: needsTaskCommit() Task attempt_202105170203442610684569695808582_0002_m_000198_190: duration 0:00.001s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203442610684569695808582_0002_m_000198_190
[2021-05-16 23:05:07,504] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 198.0 in stage 2.0 (TID 190). 4544 bytes result sent to driver
[2021-05-16 23:05:07,505] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 197.0 in stage 2.0 (TID 189). 4544 bytes result sent to driver
[2021-05-16 23:05:07,506] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 193) (e60b22510068, executor driver, partition 4, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,507] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 4.0 in stage 2.0 (TID 193)
[2021-05-16 23:05:07,508] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 194) (e60b22510068, executor driver, partition 12, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,509] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 197.0 in stage 2.0 (TID 189) in 1729 ms on e60b22510068 (executor driver) (186/200)
[2021-05-16 23:05:07,510] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 198.0 in stage 2.0 (TID 190) in 1721 ms on e60b22510068 (executor driver) (187/200)
[2021-05-16 23:05:07,511] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 12.0 in stage 2.0 (TID 194)
[2021-05-16 23:05:07,518] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:07,519] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,522] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,523] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203444697103460750518612_0002_m_000004_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444697103460750518612_0002_m_000004_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203444697103460750518612_0002}; taskId=attempt_202105170203444697103460750518612_0002_m_000004_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@597a414c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:07,523] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203444697103460750518612_0002_m_000004_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444697103460750518612_0002_m_000004_193
[2021-05-16 23:05:07,523] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:07,524] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,524] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203443346512318508453372_0002_m_000012_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443346512318508453372_0002_m_000012_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203443346512318508453372_0002}; taskId=attempt_202105170203443346512318508453372_0002_m_000012_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@265da264}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203443346512318508453372_0002_m_000012_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443346512318508453372_0002_m_000012_194
[2021-05-16 23:05:07,524] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203444697103460750518612_0002_m_000004_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203444697103460750518612_0002_m_000004_193 : duration 0:00.002s
[2021-05-16 23:05:07,525] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203443346512318508453372_0002_m_000012_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203443346512318508453372_0002_m_000012_194 : duration 0:00.003s
[2021-05-16 23:05:07,527] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203444697103460750518612_0002_m_000004_193: needsTaskCommit() Task attempt_202105170203444697103460750518612_0002_m_000004_193
[2021-05-16 23:05:07,527] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203444697103460750518612_0002_m_000004_193: needsTaskCommit() Task attempt_202105170203444697103460750518612_0002_m_000004_193: duration 0:00.001s
[2021-05-16 23:05:07,528] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203444697103460750518612_0002_m_000004_193
[2021-05-16 23:05:07,529] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203443346512318508453372_0002_m_000012_194: needsTaskCommit() Task attempt_202105170203443346512318508453372_0002_m_000012_194
[2021-05-16 23:05:07,529] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 4.0 in stage 2.0 (TID 193). 4458 bytes result sent to driver
[2021-05-16 23:05:07,530] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203443346512318508453372_0002_m_000012_194: needsTaskCommit() Task attempt_202105170203443346512318508453372_0002_m_000012_194: duration 0:00.001s
[2021-05-16 23:05:07,530] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443346512318508453372_0002_m_000012_194
[2021-05-16 23:05:07,531] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 12.0 in stage 2.0 (TID 194). 4458 bytes result sent to driver
21/05/17 02:05:07 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 195) (e60b22510068, executor driver, partition 61, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,531] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 61.0 in stage 2.0 (TID 195)
[2021-05-16 23:05:07,531] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 193) in 26 ms on e60b22510068 (executor driver) (188/200)
[2021-05-16 23:05:07,533] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 196) (e60b22510068, executor driver, partition 68, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,533] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 194) in 26 ms on e60b22510068 (executor driver) (189/200)
[2021-05-16 23:05:07,534] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 68.0 in stage 2.0 (TID 196)
[2021-05-16 23:05:07,538] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,573] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344108357639009298587_0002_m_000061_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344108357639009298587_0002_m_000061_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344108357639009298587_0002}; taskId=attempt_20210517020344108357639009298587_0002_m_000061_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1796a9fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:07,574] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344108357639009298587_0002_m_000061_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344108357639009298587_0002_m_000061_195
[2021-05-16 23:05:07,577] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 23:05:07,579] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 23:05:07,581] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,581] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344238123646698963778_0002_m_000068_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344238123646698963778_0002_m_000068_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344238123646698963778_0002}; taskId=attempt_20210517020344238123646698963778_0002_m_000068_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@184acb16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,582] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:07,582] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344108357639009298587_0002_m_000061_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344108357639009298587_0002_m_000061_195 : duration 0:00.008s
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344238123646698963778_0002_m_000068_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344238123646698963778_0002_m_000068_196
[2021-05-16 23:05:07,585] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344108357639009298587_0002_m_000061_195: needsTaskCommit() Task attempt_20210517020344108357639009298587_0002_m_000061_195
[2021-05-16 23:05:07,585] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344108357639009298587_0002_m_000061_195: needsTaskCommit() Task attempt_20210517020344108357639009298587_0002_m_000061_195: duration 0:00.001s
[2021-05-16 23:05:07,586] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344108357639009298587_0002_m_000061_195
[2021-05-16 23:05:07,586] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 61.0 in stage 2.0 (TID 195). 4501 bytes result sent to driver
[2021-05-16 23:05:07,587] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 72.0 in stage 2.0 (TID 197) (e60b22510068, executor driver, partition 72, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,589] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 72.0 in stage 2.0 (TID 197)
[2021-05-16 23:05:07,589] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 195) in 60 ms on e60b22510068 (executor driver) (190/200)
[2021-05-16 23:05:07,596] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,596] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344238123646698963778_0002_m_000068_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344238123646698963778_0002_m_000068_196 : duration 0:00.015s
[2021-05-16 23:05:07,598] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:07,598] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,599] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210517020344979629827180229987_0002_m_000072_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344979629827180229987_0002_m_000072_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210517020344979629827180229987_0002}; taskId=attempt_20210517020344979629827180229987_0002_m_000072_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4443a2d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 23:05:07,599] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344979629827180229987_0002_m_000072_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344979629827180229987_0002_m_000072_197
[2021-05-16 23:05:07,601] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344238123646698963778_0002_m_000068_196: needsTaskCommit() Task attempt_20210517020344238123646698963778_0002_m_000068_196
[2021-05-16 23:05:07,609] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344979629827180229987_0002_m_000072_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_20210517020344979629827180229987_0002_m_000072_197 : duration 0:00.006s
21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344238123646698963778_0002_m_000068_196: needsTaskCommit() Task attempt_20210517020344238123646698963778_0002_m_000068_196: duration 0:00.008s
[2021-05-16 23:05:07,609] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344238123646698963778_0002_m_000068_196
[2021-05-16 23:05:07,610] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 68.0 in stage 2.0 (TID 196). 4501 bytes result sent to driver
[2021-05-16 23:05:07,610] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 84.0 in stage 2.0 (TID 198) (e60b22510068, executor driver, partition 84, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,611] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 84.0 in stage 2.0 (TID 198)
[2021-05-16 23:05:07,612] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 68.0 in stage 2.0 (TID 196) in 79 ms on e60b22510068 (executor driver) (191/200)
[2021-05-16 23:05:07,613] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_20210517020344979629827180229987_0002_m_000072_197: needsTaskCommit() Task attempt_20210517020344979629827180229987_0002_m_000072_197
[2021-05-16 23:05:07,614] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_20210517020344979629827180229987_0002_m_000072_197: needsTaskCommit() Task attempt_20210517020344979629827180229987_0002_m_000072_197: duration 0:00.000s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210517020344979629827180229987_0002_m_000072_197
[2021-05-16 23:05:07,615] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 72.0 in stage 2.0 (TID 197). 4458 bytes result sent to driver
[2021-05-16 23:05:07,616] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 199) (e60b22510068, executor driver, partition 92, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,617] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 72.0 in stage 2.0 (TID 197) in 30 ms on e60b22510068 (executor driver) (192/200)
[2021-05-16 23:05:07,617] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 92.0 in stage 2.0 (TID 199)
[2021-05-16 23:05:07,623] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,624] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:07,625] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,625] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203445809323921410110042_0002_m_000084_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445809323921410110042_0002_m_000084_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203445809323921410110042_0002}; taskId=attempt_202105170203445809323921410110042_0002_m_000084_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28ad254b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,626] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203445809323921410110042_0002_m_000084_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445809323921410110042_0002_m_000084_198
[2021-05-16 23:05:07,627] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,628] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203448818260704065775274_0002_m_000092_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448818260704065775274_0002_m_000092_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203448818260704065775274_0002}; taskId=attempt_202105170203448818260704065775274_0002_m_000092_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2baaf48d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,629] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203448818260704065775274_0002_m_000092_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448818260704065775274_0002_m_000092_199
[2021-05-16 23:05:07,632] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203445809323921410110042_0002_m_000084_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203445809323921410110042_0002_m_000084_198 : duration 0:00.007s
[2021-05-16 23:05:07,633] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203448818260704065775274_0002_m_000092_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203448818260704065775274_0002_m_000092_199 : duration 0:00.004s
[2021-05-16 23:05:07,635] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203448818260704065775274_0002_m_000092_199: needsTaskCommit() Task attempt_202105170203448818260704065775274_0002_m_000092_199
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203445809323921410110042_0002_m_000084_198: needsTaskCommit() Task attempt_202105170203445809323921410110042_0002_m_000084_198
[2021-05-16 23:05:07,636] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203448818260704065775274_0002_m_000092_199: needsTaskCommit() Task attempt_202105170203448818260704065775274_0002_m_000092_199: duration 0:00.000s
21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203445809323921410110042_0002_m_000084_198: needsTaskCommit() Task attempt_202105170203445809323921410110042_0002_m_000084_198: duration 0:00.000s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203445809323921410110042_0002_m_000084_198
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203448818260704065775274_0002_m_000092_199
[2021-05-16 23:05:07,637] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 92.0 in stage 2.0 (TID 199). 4458 bytes result sent to driver
[2021-05-16 23:05:07,638] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 93.0 in stage 2.0 (TID 200) (e60b22510068, executor driver, partition 93, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,639] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 92.0 in stage 2.0 (TID 199) in 24 ms on e60b22510068 (executor driver) (193/200)
[2021-05-16 23:05:07,640] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 93.0 in stage 2.0 (TID 200)
[2021-05-16 23:05:07,641] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 84.0 in stage 2.0 (TID 198). 4458 bytes result sent to driver
[2021-05-16 23:05:07,642] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 137.0 in stage 2.0 (TID 201) (e60b22510068, executor driver, partition 137, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,642] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 84.0 in stage 2.0 (TID 198) in 32 ms on e60b22510068 (executor driver) (194/200)
[2021-05-16 23:05:07,649] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,649] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 137.0 in stage 2.0 (TID 201)
[2021-05-16 23:05:07,651] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,651] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203449879312464498609_0002_m_000093_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449879312464498609_0002_m_000093_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203449879312464498609_0002}; taskId=attempt_202105170203449879312464498609_0002_m_000093_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d8f0511}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,652] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203449879312464498609_0002_m_000093_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449879312464498609_0002_m_000093_200
[2021-05-16 23:05:07,658] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,675] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203449879312464498609_0002_m_000093_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203449879312464498609_0002_m_000093_200 : duration 0:00.024s
[2021-05-16 23:05:07,677] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203446049684069577389504_0002_m_000137_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446049684069577389504_0002_m_000137_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203446049684069577389504_0002}; taskId=attempt_202105170203446049684069577389504_0002_m_000137_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27dc761}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,677] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203446049684069577389504_0002_m_000137_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446049684069577389504_0002_m_000137_201
[2021-05-16 23:05:07,679] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203449879312464498609_0002_m_000093_200: needsTaskCommit() Task attempt_202105170203449879312464498609_0002_m_000093_200
[2021-05-16 23:05:07,679] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203449879312464498609_0002_m_000093_200: needsTaskCommit() Task attempt_202105170203449879312464498609_0002_m_000093_200: duration 0:00.001s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203449879312464498609_0002_m_000093_200
[2021-05-16 23:05:07,680] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203446049684069577389504_0002_m_000137_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203446049684069577389504_0002_m_000137_201 : duration 0:00.004s
21/05/17 02:05:07 INFO Executor: Finished task 93.0 in stage 2.0 (TID 200). 4458 bytes result sent to driver
[2021-05-16 23:05:07,682] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 158.0 in stage 2.0 (TID 202) (e60b22510068, executor driver, partition 158, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,682] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203446049684069577389504_0002_m_000137_201: needsTaskCommit() Task attempt_202105170203446049684069577389504_0002_m_000137_201
21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203446049684069577389504_0002_m_000137_201: needsTaskCommit() Task attempt_202105170203446049684069577389504_0002_m_000137_201: duration 0:00.000s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203446049684069577389504_0002_m_000137_201
[2021-05-16 23:05:07,683] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 137.0 in stage 2.0 (TID 201). 4458 bytes result sent to driver
[2021-05-16 23:05:07,684] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 158.0 in stage 2.0 (TID 202)
[2021-05-16 23:05:07,685] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 93.0 in stage 2.0 (TID 200) in 46 ms on e60b22510068 (executor driver) (195/200)
[2021-05-16 23:05:07,685] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 137.0 in stage 2.0 (TID 201) in 44 ms on e60b22510068 (executor driver) (196/200)
[2021-05-16 23:05:07,686] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Starting task 173.0 in stage 2.0 (TID 203) (e60b22510068, executor driver, partition 173, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 23:05:07,687] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Running task 173.0 in stage 2.0 (TID 203)
[2021-05-16 23:05:07,693] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,694] {docker.py:276} INFO - 21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/17 02:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 23:05:07,695] {docker.py:276} INFO - 21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/17 02:05:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/17 02:05:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 23:05:07,696] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441285247890042234284_0002_m_000173_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441285247890042234284_0002_m_000173_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441285247890042234284_0002}; taskId=attempt_202105170203441285247890042234284_0002_m_000173_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d07d885}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203441285247890042234284_0002_m_000173_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441285247890042234284_0002_m_000173_203 
21/05/17 02:05:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105170203441891518083977950475_0002_m_000158_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441891518083977950475_0002_m_000158_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105170203441891518083977950475_0002}; taskId=attempt_202105170203441891518083977950475_0002_m_000158_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@656df1fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/141adbf7-3975-4a60-aa82-2fdc58c51d8f/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:07,696] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203441891518083977950475_0002_m_000158_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441891518083977950475_0002_m_000158_202
[2021-05-16 23:05:07,699] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203441891518083977950475_0002_m_000158_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441891518083977950475_0002_m_000158_202 : duration 0:00.003s
[2021-05-16 23:05:07,700] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203441285247890042234284_0002_m_000173_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/141adbf7-3975-4a60-aa82-2fdc58c51d8f/_temporary/0/_temporary/attempt_202105170203441285247890042234284_0002_m_000173_203 : duration 0:00.004s
[2021-05-16 23:05:07,701] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203441891518083977950475_0002_m_000158_202: needsTaskCommit() Task attempt_202105170203441891518083977950475_0002_m_000158_202
[2021-05-16 23:05:07,702] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203441285247890042234284_0002_m_000173_203: needsTaskCommit() Task attempt_202105170203441285247890042234284_0002_m_000173_203
[2021-05-16 23:05:07,702] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203441891518083977950475_0002_m_000158_202: needsTaskCommit() Task attempt_202105170203441891518083977950475_0002_m_000158_202: duration 0:00.000s
[2021-05-16 23:05:07,703] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441891518083977950475_0002_m_000158_202
21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203441285247890042234284_0002_m_000173_203: needsTaskCommit() Task attempt_202105170203441285247890042234284_0002_m_000173_203: duration 0:00.000s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203441285247890042234284_0002_m_000173_203
[2021-05-16 23:05:07,703] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 173.0 in stage 2.0 (TID 203). 4458 bytes result sent to driver
[2021-05-16 23:05:07,703] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 158.0 in stage 2.0 (TID 202). 4458 bytes result sent to driver
[2021-05-16 23:05:07,706] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 158.0 in stage 2.0 (TID 202) in 25 ms on e60b22510068 (executor driver) (197/200)
[2021-05-16 23:05:07,707] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 173.0 in stage 2.0 (TID 203) in 21 ms on e60b22510068 (executor driver) (198/200)
[2021-05-16 23:05:07,760] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203443477136452462391143_0002_m_000199_191: needsTaskCommit() Task attempt_202105170203443477136452462391143_0002_m_000199_191
[2021-05-16 23:05:07,761] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203443477136452462391143_0002_m_000199_191: needsTaskCommit() Task attempt_202105170203443477136452462391143_0002_m_000199_191: duration 0:00.001s
[2021-05-16 23:05:07,761] {docker.py:276} INFO - 21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203443477136452462391143_0002_m_000199_191
[2021-05-16 23:05:07,762] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 199.0 in stage 2.0 (TID 191). 4587 bytes result sent to driver
[2021-05-16 23:05:07,763] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 199.0 in stage 2.0 (TID 191) in 1717 ms on e60b22510068 (executor driver) (199/200)
[2021-05-16 23:05:07,798] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Starting: Task committer attempt_202105170203447451426322073120127_0002_m_000000_192: needsTaskCommit() Task attempt_202105170203447451426322073120127_0002_m_000000_192
[2021-05-16 23:05:07,799] {docker.py:276} INFO - 21/05/17 02:05:07 INFO StagingCommitter: Task committer attempt_202105170203447451426322073120127_0002_m_000000_192: needsTaskCommit() Task attempt_202105170203447451426322073120127_0002_m_000000_192: duration 0:00.001s
21/05/17 02:05:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105170203447451426322073120127_0002_m_000000_192
[2021-05-16 23:05:07,800] {docker.py:276} INFO - 21/05/17 02:05:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 192). 4544 bytes result sent to driver
[2021-05-16 23:05:07,802] {docker.py:276} INFO - 21/05/17 02:05:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 192) in 1748 ms on e60b22510068 (executor driver) (200/200)
[2021-05-16 23:05:07,803] {docker.py:276} INFO - 21/05/17 02:05:07 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 81.536 s
21/05/17 02:05:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-16 23:05:07,805] {docker.py:276} INFO - 21/05/17 02:05:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/17 02:05:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-16 23:05:07,806] {docker.py:276} INFO - 21/05/17 02:05:07 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 83.147441 s
[2021-05-16 23:05:07,809] {docker.py:276} INFO - 21/05/17 02:05:07 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105170203448565622101638569140_0000_m_000000_0: commitJob((no job ID))
[2021-05-16 23:05:07,827] {docker.py:276} INFO - 21/05/17 02:05:07 WARN AbstractS3ACommitter: Task committer attempt_202105170203448565622101638569140_0000_m_000000_0: No pending uploads to commit
[2021-05-16 23:05:08,328] {docker.py:276} INFO - 21/05/17 02:05:08 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/17 02:05:08 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-16 23:05:08,535] {docker.py:276} INFO - 21/05/17 02:05:08 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.207s
21/05/17 02:05:08 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.209s
[2021-05-16 23:05:08,537] {docker.py:276} INFO - 21/05/17 02:05:08 INFO AbstractS3ACommitter: Task committer attempt_202105170203448565622101638569140_0000_m_000000_0: commitJob((no job ID)): duration 0:00.729s
[2021-05-16 23:05:09,049] {docker.py:276} INFO - 21/05/17 02:05:09 INFO FileFormatWriter: Write Job 141adbf7-3975-4a60-aa82-2fdc58c51d8f committed.
[2021-05-16 23:05:09,060] {docker.py:276} INFO - 21/05/17 02:05:09 INFO FileFormatWriter: Finished processing stats for write job 141adbf7-3975-4a60-aa82-2fdc58c51d8f.
[2021-05-16 23:05:09,175] {docker.py:276} INFO - 21/05/17 02:05:09 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-16 23:05:09,189] {docker.py:276} INFO - 21/05/17 02:05:09 INFO SparkUI: Stopped Spark web UI at http://e60b22510068:4040
[2021-05-16 23:05:09,211] {docker.py:276} INFO - 21/05/17 02:05:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-16 23:05:09,227] {docker.py:276} INFO - 21/05/17 02:05:09 INFO MemoryStore: MemoryStore cleared
[2021-05-16 23:05:09,228] {docker.py:276} INFO - 21/05/17 02:05:09 INFO BlockManager: BlockManager stopped
[2021-05-16 23:05:09,231] {docker.py:276} INFO - 21/05/17 02:05:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-16 23:05:09,235] {docker.py:276} INFO - 21/05/17 02:05:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-16 23:05:09,243] {docker.py:276} INFO - 21/05/17 02:05:09 INFO SparkContext: Successfully stopped SparkContext
[2021-05-16 23:05:09,244] {docker.py:276} INFO - 21/05/17 02:05:09 INFO ShutdownHookManager: Shutdown hook called
[2021-05-16 23:05:09,245] {docker.py:276} INFO - 21/05/17 02:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-e2b66934-a82a-4902-9349-d27d0254561d
[2021-05-16 23:05:09,248] {docker.py:276} INFO - 21/05/17 02:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-25d1f086-d91e-4390-bab7-ae217a841b91
[2021-05-16 23:05:09,250] {docker.py:276} INFO - 21/05/17 02:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-25d1f086-d91e-4390-bab7-ae217a841b91/pyspark-d956b609-4d7c-45fc-93b4-a31b557e09e8
[2021-05-16 23:05:09,258] {docker.py:276} INFO - 21/05/17 02:05:09 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-16 23:05:09,259] {docker.py:276} INFO - 21/05/17 02:05:09 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-16 23:05:09,260] {docker.py:276} INFO - 21/05/17 02:05:09 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-16 23:05:09,477] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210517T020259, start_date=20210517T020318, end_date=20210517T020509
[2021-05-16 23:05:09,532] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-16 23:05:09,573] {local_task_job.py:146} INFO - Task exited with return code 0
