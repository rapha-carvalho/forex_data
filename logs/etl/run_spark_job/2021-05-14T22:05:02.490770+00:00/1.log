[2021-05-14 19:05:52,613] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:05:02.490770+00:00 [queued]>
[2021-05-14 19:05:52,618] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T22:05:02.490770+00:00 [queued]>
[2021-05-14 19:05:52,618] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:05:52,618] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 19:05:52,618] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 19:05:52,623] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T22:05:02.490770+00:00
[2021-05-14 19:05:52,626] {standard_task_runner.py:52} INFO - Started process 25423 to run task
[2021-05-14 19:05:52,633] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T22:05:02.490770+00:00', '--job-id', '558', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpr1pk95dz', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpezvg5fqs']
[2021-05-14 19:05:52,635] {standard_task_runner.py:77} INFO - Job 558: Subtask run_spark_job
[2021-05-14 19:05:52,663] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T22:05:02.490770+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-14 19:05:52,685] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T22:05:02.490770+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T22:05:02.490770+00:00
[2021-05-14 19:05:52,689] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 19:05:55,517] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 19:05:55,520] {docker.py:312} INFO - Digest: sha256:3ff139a9dbefaff7945c18f6cbdbc77460bc981cf21cbe1d495f99b227e826c8
[2021-05-14 19:05:55,520] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 19:05:55,524] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 19:05:57,511] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 19:05:58,068] {docker.py:276} INFO - 21/05/14 22:05:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 19:06:00,094] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 19:06:00,108] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 19:06:00,170] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceUtils: ==============================================================
[2021-05-14 19:06:00,171] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 19:06:00,171] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceUtils: ==============================================================
[2021-05-14 19:06:00,172] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkContext: Submitted application: spark.py
[2021-05-14 19:06:00,219] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 19:06:00,234] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 19:06:00,235] {docker.py:276} INFO - 21/05/14 22:06:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 19:06:00,301] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SecurityManager: Changing view acls to: jovyan
21/05/14 22:06:00 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 19:06:00,301] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SecurityManager: Changing view acls groups to: 
21/05/14 22:06:00 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 19:06:00,302] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 19:06:00,636] {docker.py:276} INFO - 21/05/14 22:06:00 INFO Utils: Successfully started service 'sparkDriver' on port 46455.
[2021-05-14 19:06:00,676] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 19:06:00,720] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 19:06:00,771] {docker.py:276} INFO - 21/05/14 22:06:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 19:06:00,772] {docker.py:276} INFO - 21/05/14 22:06:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 19:06:00,780] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 19:06:00,799] {docker.py:276} INFO - 21/05/14 22:06:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-89ad5f5a-fc83-4ab3-8eda-b3b9abb7a0fd
[2021-05-14 19:06:00,826] {docker.py:276} INFO - 21/05/14 22:06:00 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 19:06:00,849] {docker.py:276} INFO - 21/05/14 22:06:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 19:06:01,130] {docker.py:276} INFO - 21/05/14 22:06:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 19:06:01,213] {docker.py:276} INFO - 21/05/14 22:06:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://9bbba8114907:4040
[2021-05-14 19:06:01,479] {docker.py:276} INFO - 21/05/14 22:06:01 INFO Executor: Starting executor ID driver on host 9bbba8114907
[2021-05-14 19:06:01,521] {docker.py:276} INFO - 21/05/14 22:06:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33069.
21/05/14 22:06:01 INFO NettyBlockTransferService: Server created on 9bbba8114907:33069
[2021-05-14 19:06:01,524] {docker.py:276} INFO - 21/05/14 22:06:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 19:06:01,535] {docker.py:276} INFO - 21/05/14 22:06:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9bbba8114907, 33069, None)
[2021-05-14 19:06:01,544] {docker.py:276} INFO - 21/05/14 22:06:01 INFO BlockManagerMasterEndpoint: Registering block manager 9bbba8114907:33069 with 934.4 MiB RAM, BlockManagerId(driver, 9bbba8114907, 33069, None)
[2021-05-14 19:06:01,548] {docker.py:276} INFO - 21/05/14 22:06:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9bbba8114907, 33069, None)
[2021-05-14 19:06:01,549] {docker.py:276} INFO - 21/05/14 22:06:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9bbba8114907, 33069, None)
[2021-05-14 19:06:02,112] {docker.py:276} INFO - 21/05/14 22:06:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 19:06:02,113] {docker.py:276} INFO - 21/05/14 22:06:02 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 19:06:03,128] {docker.py:276} INFO - 21/05/14 22:06:03 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 19:06:03,176] {docker.py:276} INFO - 21/05/14 22:06:03 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/14 22:06:03 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 19:06:09,019] {docker.py:276} INFO - 21/05/14 22:06:09 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620943516_to_1620945316.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620945316_to_1620947116.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620947116_to_1620948916.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620948916_to_1620950716.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620950716_to_1620952516.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620952516_to_1620954316.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620954316_to_1620956116.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620956116_to_1620957916.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620957916_to_1620959716.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620959716_to_1620961516.csv.
[2021-05-14 19:06:09,542] {docker.py:276} INFO - 21/05/14 22:06:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:09,574] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-14 19:06:09,575] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 19:06:09,576] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:06:09,582] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:06:09,592] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:06:09,693] {docker.py:276} INFO - 21/05/14 22:06:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-14 19:06:09,765] {docker.py:276} INFO - 21/05/14 22:06:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-14 19:06:09,774] {docker.py:276} INFO - 21/05/14 22:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 9bbba8114907:33069 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:06:09,782] {docker.py:276} INFO - 21/05/14 22:06:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:06:09,804] {docker.py:276} INFO - 21/05/14 22:06:09 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 19:06:09,806] {docker.py:276} INFO - 21/05/14 22:06:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-14 19:06:09,898] {docker.py:276} INFO - 21/05/14 22:06:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (9bbba8114907, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:09,902] {docker.py:276} INFO - 21/05/14 22:06:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (9bbba8114907, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:09,903] {docker.py:276} INFO - 21/05/14 22:06:09 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (9bbba8114907, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:09,906] {docker.py:276} INFO - 21/05/14 22:06:09 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (9bbba8114907, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:09,926] {docker.py:276} INFO - 21/05/14 22:06:09 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-14 19:06:09,926] {docker.py:276} INFO - 21/05/14 22:06:09 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
21/05/14 22:06:09 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-14 19:06:09,927] {docker.py:276} INFO - 21/05/14 22:06:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 19:06:10,344] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-14 19:06:10,350] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (9bbba8114907, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:10,352] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-14 19:06:10,357] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 454 ms on 9bbba8114907 (executor driver) (1/141)
[2021-05-14 19:06:10,564] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1886 bytes result sent to driver
[2021-05-14 19:06:10,566] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (9bbba8114907, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:10,567] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-14 19:06:10,568] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 218 ms on 9bbba8114907 (executor driver) (2/141)
[2021-05-14 19:06:10,749] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-14 19:06:10,752] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (9bbba8114907, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:10,752] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
21/05/14 22:06:10 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 187 ms on 9bbba8114907 (executor driver) (3/141)
[2021-05-14 19:06:10,824] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
21/05/14 22:06:10 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (9bbba8114907, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:10,828] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 924 ms on 9bbba8114907 (executor driver) (4/141)
21/05/14 22:06:10 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-14 19:06:10,840] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
21/05/14 22:06:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-14 19:06:10,846] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (9bbba8114907, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:06:10 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
21/05/14 22:06:10 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (9bbba8114907, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:06:10 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 944 ms on 9bbba8114907 (executor driver) (5/141)
21/05/14 22:06:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 981 ms on 9bbba8114907 (executor driver) (6/141)
[2021-05-14 19:06:10,857] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-14 19:06:10,936] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-14 19:06:10,936] {docker.py:276} INFO - 21/05/14 22:06:10 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (9bbba8114907, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:10,939] {docker.py:276} INFO - 21/05/14 22:06:10 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
21/05/14 22:06:10 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 188 ms on 9bbba8114907 (executor driver) (7/141)
[2021-05-14 19:06:11,023] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-14 19:06:11,025] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (9bbba8114907, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,026] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-14 19:06:11,027] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 203 ms on 9bbba8114907 (executor driver) (8/141)
[2021-05-14 19:06:11,043] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-14 19:06:11,044] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (9bbba8114907, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,045] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
21/05/14 22:06:11 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 200 ms on 9bbba8114907 (executor driver) (9/141)
[2021-05-14 19:06:11,046] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-14 19:06:11,048] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (9bbba8114907, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,049] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 206 ms on 9bbba8114907 (executor driver) (10/141)
[2021-05-14 19:06:11,050] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-14 19:06:11,121] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-14 19:06:11,123] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (9bbba8114907, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,124] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 189 ms on 9bbba8114907 (executor driver) (11/141)
[2021-05-14 19:06:11,125] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-14 19:06:11,205] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-14 19:06:11,208] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (9bbba8114907, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,209] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 185 ms on 9bbba8114907 (executor driver) (12/141)
[2021-05-14 19:06:11,210] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-14 19:06:11,222] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-14 19:06:11,223] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (9bbba8114907, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,224] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 177 ms on 9bbba8114907 (executor driver) (13/141)
[2021-05-14 19:06:11,225] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-14 19:06:11,236] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-14 19:06:11,237] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (9bbba8114907, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,238] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-14 19:06:11,239] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 195 ms on 9bbba8114907 (executor driver) (14/141)
[2021-05-14 19:06:11,302] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1886 bytes result sent to driver
[2021-05-14 19:06:11,303] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (9bbba8114907, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,303] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-14 19:06:11,304] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 182 ms on 9bbba8114907 (executor driver) (15/141)
[2021-05-14 19:06:11,389] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1886 bytes result sent to driver
[2021-05-14 19:06:11,390] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (9bbba8114907, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,391] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 185 ms on 9bbba8114907 (executor driver) (16/141)
[2021-05-14 19:06:11,392] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-14 19:06:11,397] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1886 bytes result sent to driver
[2021-05-14 19:06:11,399] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (9bbba8114907, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,399] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-14 19:06:11,400] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 178 ms on 9bbba8114907 (executor driver) (17/141)
[2021-05-14 19:06:11,420] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1886 bytes result sent to driver
[2021-05-14 19:06:11,421] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (9bbba8114907, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,422] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 185 ms on 9bbba8114907 (executor driver) (18/141)
[2021-05-14 19:06:11,423] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-14 19:06:11,486] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-14 19:06:11,488] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (9bbba8114907, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,489] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 186 ms on 9bbba8114907 (executor driver) (19/141)
[2021-05-14 19:06:11,489] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-14 19:06:11,571] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1843 bytes result sent to driver
[2021-05-14 19:06:11,573] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
[2021-05-14 19:06:11,575] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (9bbba8114907, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,576] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 186 ms on 9bbba8114907 (executor driver) (20/141)
[2021-05-14 19:06:11,577] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-14 19:06:11,578] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (9bbba8114907, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,579] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 180 ms on 9bbba8114907 (executor driver) (21/141)
[2021-05-14 19:06:11,582] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-14 19:06:11,592] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-14 19:06:11,593] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (9bbba8114907, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,594] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 173 ms on 9bbba8114907 (executor driver) (22/141)
[2021-05-14 19:06:11,595] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-14 19:06:11,743] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-14 19:06:11,745] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (9bbba8114907, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,748] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 260 ms on 9bbba8114907 (executor driver) (23/141)
[2021-05-14 19:06:11,748] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-14 19:06:11,751] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-14 19:06:11,753] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (9bbba8114907, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,754] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-14 19:06:11,755] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 177 ms on 9bbba8114907 (executor driver) (24/141)
[2021-05-14 19:06:11,756] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-14 19:06:11,757] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (9bbba8114907, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,758] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 184 ms on 9bbba8114907 (executor driver) (25/141)
[2021-05-14 19:06:11,759] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-14 19:06:11,767] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-14 19:06:11,769] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (9bbba8114907, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,770] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 177 ms on 9bbba8114907 (executor driver) (26/141)
[2021-05-14 19:06:11,771] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-14 19:06:11,927] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
21/05/14 22:06:11 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
[2021-05-14 19:06:11,929] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (9bbba8114907, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,930] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 185 ms on 9bbba8114907 (executor driver) (27/141)
[2021-05-14 19:06:11,931] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-14 19:06:11,933] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (9bbba8114907, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,934] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 182 ms on 9bbba8114907 (executor driver) (28/141)
[2021-05-14 19:06:11,935] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-14 19:06:11,949] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1886 bytes result sent to driver
[2021-05-14 19:06:11,951] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (9bbba8114907, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,952] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 184 ms on 9bbba8114907 (executor driver) (29/141)
[2021-05-14 19:06:11,954] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-14 19:06:11,955] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1886 bytes result sent to driver
[2021-05-14 19:06:11,956] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (9bbba8114907, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:11,957] {docker.py:276} INFO - 21/05/14 22:06:11 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 200 ms on 9bbba8114907 (executor driver) (30/141)
[2021-05-14 19:06:11,958] {docker.py:276} INFO - 21/05/14 22:06:11 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-14 19:06:12,122] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1886 bytes result sent to driver
21/05/14 22:06:12 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (9bbba8114907, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:06:12 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
21/05/14 22:06:12 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 183 ms on 9bbba8114907 (executor driver) (31/141)
[2021-05-14 19:06:12,134] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-14 19:06:12,135] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (9bbba8114907, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,136] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 186 ms on 9bbba8114907 (executor driver) (32/141)
[2021-05-14 19:06:12,136] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-14 19:06:12,154] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1886 bytes result sent to driver
[2021-05-14 19:06:12,155] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (9bbba8114907, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,156] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 225 ms on 9bbba8114907 (executor driver) (33/141)
[2021-05-14 19:06:12,157] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-14 19:06:12,159] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-14 19:06:12,161] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (9bbba8114907, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,162] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-14 19:06:12,163] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 207 ms on 9bbba8114907 (executor driver) (34/141)
[2021-05-14 19:06:12,308] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-14 19:06:12,310] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-14 19:06:12,310] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (9bbba8114907, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,312] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (9bbba8114907, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,313] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 203 ms on 9bbba8114907 (executor driver) (35/141)
[2021-05-14 19:06:12,314] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 179 ms on 9bbba8114907 (executor driver) (36/141)
[2021-05-14 19:06:12,316] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-14 19:06:12,317] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-14 19:06:12,325] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-14 19:06:12,326] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (9bbba8114907, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,327] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-14 19:06:12,328] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 173 ms on 9bbba8114907 (executor driver) (37/141)
[2021-05-14 19:06:12,339] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-14 19:06:12,340] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (9bbba8114907, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,341] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 181 ms on 9bbba8114907 (executor driver) (38/141)
[2021-05-14 19:06:12,342] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-14 19:06:12,486] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-14 19:06:12,487] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (9bbba8114907, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,488] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 177 ms on 9bbba8114907 (executor driver) (39/141)
[2021-05-14 19:06:12,491] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-14 19:06:12,493] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-14 19:06:12,498] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (9bbba8114907, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,499] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 190 ms on 9bbba8114907 (executor driver) (40/141)
[2021-05-14 19:06:12,500] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-14 19:06:12,517] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-14 19:06:12,517] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (9bbba8114907, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,519] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 178 ms on 9bbba8114907 (executor driver) (41/141)
[2021-05-14 19:06:12,521] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-14 19:06:12,522] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-14 19:06:12,523] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (9bbba8114907, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,523] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-14 19:06:12,524] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 199 ms on 9bbba8114907 (executor driver) (42/141)
[2021-05-14 19:06:12,677] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-14 19:06:12,679] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (9bbba8114907, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,680] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-14 19:06:12,681] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 193 ms on 9bbba8114907 (executor driver) (43/141)
[2021-05-14 19:06:12,694] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1886 bytes result sent to driver
[2021-05-14 19:06:12,694] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (9bbba8114907, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,695] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 179 ms on 9bbba8114907 (executor driver) (44/141)
[2021-05-14 19:06:12,696] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-14 19:06:12,697] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1886 bytes result sent to driver
[2021-05-14 19:06:12,700] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (9bbba8114907, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,700] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 202 ms on 9bbba8114907 (executor driver) (45/141)
[2021-05-14 19:06:12,701] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-14 19:06:12,714] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1886 bytes result sent to driver
[2021-05-14 19:06:12,715] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (9bbba8114907, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,716] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 194 ms on 9bbba8114907 (executor driver) (46/141)
[2021-05-14 19:06:12,716] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-14 19:06:12,865] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1886 bytes result sent to driver
[2021-05-14 19:06:12,867] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (9bbba8114907, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,869] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-14 19:06:12,869] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 191 ms on 9bbba8114907 (executor driver) (47/141)
[2021-05-14 19:06:12,870] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-14 19:06:12,873] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (9bbba8114907, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,874] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-14 19:06:12,875] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 180 ms on 9bbba8114907 (executor driver) (48/141)
[2021-05-14 19:06:12,876] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (9bbba8114907, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,877] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-14 19:06:12,878] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 180 ms on 9bbba8114907 (executor driver) (49/141)
[2021-05-14 19:06:12,879] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-14 19:06:12,888] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-14 19:06:12,890] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (9bbba8114907, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:12,891] {docker.py:276} INFO - 21/05/14 22:06:12 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 176 ms on 9bbba8114907 (executor driver) (50/141)
[2021-05-14 19:06:12,891] {docker.py:276} INFO - 21/05/14 22:06:12 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-14 19:06:13,053] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
21/05/14 22:06:13 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-14 19:06:13,054] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-14 19:06:13,054] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (9bbba8114907, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,055] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-14 19:06:13,056] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (9bbba8114907, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,057] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 184 ms on 9bbba8114907 (executor driver) (51/141)
[2021-05-14 19:06:13,057] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-14 19:06:13,058] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (9bbba8114907, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,058] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 183 ms on 9bbba8114907 (executor driver) (52/141)
[2021-05-14 19:06:13,059] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-14 19:06:13,060] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 194 ms on 9bbba8114907 (executor driver) (53/141)
[2021-05-14 19:06:13,065] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
[2021-05-14 19:06:13,066] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (9bbba8114907, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,066] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-14 19:06:13,067] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 177 ms on 9bbba8114907 (executor driver) (54/141)
[2021-05-14 19:06:13,230] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1843 bytes result sent to driver
[2021-05-14 19:06:13,231] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-14 19:06:13,233] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (9bbba8114907, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,234] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 180 ms on 9bbba8114907 (executor driver) (55/141)
[2021-05-14 19:06:13,236] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-14 19:06:13,237] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-14 19:06:13,238] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (9bbba8114907, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,239] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 184 ms on 9bbba8114907 (executor driver) (56/141)
[2021-05-14 19:06:13,240] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-14 19:06:13,241] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-14 19:06:13,243] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (9bbba8114907, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,244] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 186 ms on 9bbba8114907 (executor driver) (57/141)
[2021-05-14 19:06:13,245] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 180 ms on 9bbba8114907 (executor driver) (58/141)
[2021-05-14 19:06:13,246] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (9bbba8114907, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,251] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-14 19:06:13,252] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-14 19:06:13,419] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1886 bytes result sent to driver
[2021-05-14 19:06:13,420] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (9bbba8114907, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,421] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-14 19:06:13,421] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 184 ms on 9bbba8114907 (executor driver) (59/141)
[2021-05-14 19:06:13,424] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1886 bytes result sent to driver
[2021-05-14 19:06:13,425] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (9bbba8114907, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,426] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 181 ms on 9bbba8114907 (executor driver) (60/141)
21/05/14 22:06:13 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1886 bytes result sent to driver
[2021-05-14 19:06:13,427] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-14 19:06:13,428] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (9bbba8114907, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,428] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-14 19:06:13,429] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 197 ms on 9bbba8114907 (executor driver) (61/141)
[2021-05-14 19:06:13,432] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1886 bytes result sent to driver
[2021-05-14 19:06:13,438] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (9bbba8114907, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,439] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 198 ms on 9bbba8114907 (executor driver) (62/141)
[2021-05-14 19:06:13,440] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-14 19:06:13,598] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-14 19:06:13,600] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (9bbba8114907, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/14 22:06:13 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-14 19:06:13,601] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (9bbba8114907, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,602] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 177 ms on 9bbba8114907 (executor driver) (63/141)
[2021-05-14 19:06:13,603] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-14 19:06:13,603] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 177 ms on 9bbba8114907 (executor driver) (64/141)
[2021-05-14 19:06:13,604] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-14 19:06:13,605] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-14 19:06:13,606] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (9bbba8114907, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,606] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 186 ms on 9bbba8114907 (executor driver) (65/141)
[2021-05-14 19:06:13,607] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-14 19:06:13,611] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-14 19:06:13,613] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (9bbba8114907, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,614] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-14 19:06:13,614] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 176 ms on 9bbba8114907 (executor driver) (66/141)
[2021-05-14 19:06:13,786] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-14 19:06:13,788] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-14 19:06:13,789] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (9bbba8114907, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,791] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 178 ms on 9bbba8114907 (executor driver) (67/141)
21/05/14 22:06:13 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-14 19:06:13,791] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-14 19:06:13,793] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (9bbba8114907, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,794] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-14 19:06:13,795] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 195 ms on 9bbba8114907 (executor driver) (68/141)
[2021-05-14 19:06:13,802] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (9bbba8114907, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,803] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 200 ms on 9bbba8114907 (executor driver) (69/141)
[2021-05-14 19:06:13,803] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-14 19:06:13,832] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-14 19:06:13,834] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (9bbba8114907, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,835] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-14 19:06:13,836] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 230 ms on 9bbba8114907 (executor driver) (70/141)
[2021-05-14 19:06:13,971] {docker.py:276} INFO - 21/05/14 22:06:13 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-14 19:06:13,973] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (9bbba8114907, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,974] {docker.py:276} INFO - 21/05/14 22:06:13 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 178 ms on 9bbba8114907 (executor driver) (71/141)
[2021-05-14 19:06:13,975] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-14 19:06:13,976] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-14 19:06:13,977] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (9bbba8114907, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,978] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 191 ms on 9bbba8114907 (executor driver) (72/141)
[2021-05-14 19:06:13,979] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-14 19:06:13,987] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-14 19:06:13,988] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (9bbba8114907, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:13,990] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 198 ms on 9bbba8114907 (executor driver) (73/141)
[2021-05-14 19:06:13,990] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-14 19:06:14,003] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-14 19:06:14,004] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (9bbba8114907, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,005] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-14 19:06:14,005] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 171 ms on 9bbba8114907 (executor driver) (74/141)
[2021-05-14 19:06:14,152] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1886 bytes result sent to driver
[2021-05-14 19:06:14,154] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (9bbba8114907, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,156] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
21/05/14 22:06:14 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 179 ms on 9bbba8114907 (executor driver) (75/141)
[2021-05-14 19:06:14,162] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1886 bytes result sent to driver
[2021-05-14 19:06:14,163] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1886 bytes result sent to driver
[2021-05-14 19:06:14,164] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (9bbba8114907, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,165] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-14 19:06:14,166] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (9bbba8114907, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,169] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-14 19:06:14,169] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 195 ms on 9bbba8114907 (executor driver) (76/141)
[2021-05-14 19:06:14,170] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 179 ms on 9bbba8114907 (executor driver) (77/141)
[2021-05-14 19:06:14,181] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1886 bytes result sent to driver
[2021-05-14 19:06:14,182] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (9bbba8114907, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,183] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 180 ms on 9bbba8114907 (executor driver) (78/141)
[2021-05-14 19:06:14,184] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-14 19:06:14,339] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-14 19:06:14,339] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (9bbba8114907, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,341] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 177 ms on 9bbba8114907 (executor driver) (79/141)
[2021-05-14 19:06:14,342] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-14 19:06:14,342] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
21/05/14 22:06:14 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-14 19:06:14,343] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (9bbba8114907, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,344] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 179 ms on 9bbba8114907 (executor driver) (80/141)
21/05/14 22:06:14 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-14 19:06:14,345] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (9bbba8114907, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,346] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 193 ms on 9bbba8114907 (executor driver) (81/141)
[2021-05-14 19:06:14,347] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-14 19:06:14,351] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-14 19:06:14,352] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (9bbba8114907, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,353] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 171 ms on 9bbba8114907 (executor driver) (82/141)
[2021-05-14 19:06:14,358] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-14 19:06:14,513] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-14 19:06:14,515] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (9bbba8114907, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,516] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-14 19:06:14,516] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 173 ms on 9bbba8114907 (executor driver) (83/141)
[2021-05-14 19:06:14,528] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-14 19:06:14,530] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (9bbba8114907, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,531] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 186 ms on 9bbba8114907 (executor driver) (84/141)
[2021-05-14 19:06:14,532] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-14 19:06:14,533] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-14 19:06:14,535] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (9bbba8114907, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,535] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 183 ms on 9bbba8114907 (executor driver) (85/141)
[2021-05-14 19:06:14,536] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-14 19:06:14,549] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-14 19:06:14,550] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (9bbba8114907, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,551] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-14 19:06:14,552] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 213 ms on 9bbba8114907 (executor driver) (86/141)
[2021-05-14 19:06:14,685] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-14 19:06:14,689] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (9bbba8114907, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,689] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-14 19:06:14,690] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 175 ms on 9bbba8114907 (executor driver) (87/141)
[2021-05-14 19:06:14,702] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-14 19:06:14,702] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-14 19:06:14,703] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (9bbba8114907, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,704] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 174 ms on 9bbba8114907 (executor driver) (88/141)
[2021-05-14 19:06:14,705] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-14 19:06:14,705] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 171 ms on 9bbba8114907 (executor driver) (89/141)
[2021-05-14 19:06:14,706] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (9bbba8114907, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,708] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-14 19:06:14,724] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-14 19:06:14,734] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (9bbba8114907, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,735] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-14 19:06:14,735] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 185 ms on 9bbba8114907 (executor driver) (90/141)
[2021-05-14 19:06:14,868] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1886 bytes result sent to driver
[2021-05-14 19:06:14,870] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (9bbba8114907, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,871] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 185 ms on 9bbba8114907 (executor driver) (91/141)
[2021-05-14 19:06:14,872] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-14 19:06:14,875] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1886 bytes result sent to driver
[2021-05-14 19:06:14,876] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (9bbba8114907, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,877] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 175 ms on 9bbba8114907 (executor driver) (92/141)
[2021-05-14 19:06:14,878] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-14 19:06:14,879] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1886 bytes result sent to driver
[2021-05-14 19:06:14,880] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (9bbba8114907, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,881] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 174 ms on 9bbba8114907 (executor driver) (93/141)
[2021-05-14 19:06:14,882] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-14 19:06:14,916] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1843 bytes result sent to driver
[2021-05-14 19:06:14,918] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (9bbba8114907, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:14,920] {docker.py:276} INFO - 21/05/14 22:06:14 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-14 19:06:14,921] {docker.py:276} INFO - 21/05/14 22:06:14 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 186 ms on 9bbba8114907 (executor driver) (94/141)
[2021-05-14 19:06:15,052] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1843 bytes result sent to driver
21/05/14 22:06:15 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
[2021-05-14 19:06:15,053] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-14 19:06:15,054] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (9bbba8114907, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,055] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-14 19:06:15,056] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (9bbba8114907, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,058] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
21/05/14 22:06:15 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (9bbba8114907, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,058] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 189 ms on 9bbba8114907 (executor driver) (95/141)
[2021-05-14 19:06:15,059] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 180 ms on 9bbba8114907 (executor driver) (96/141)
[2021-05-14 19:06:15,060] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 184 ms on 9bbba8114907 (executor driver) (97/141)
[2021-05-14 19:06:15,064] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-14 19:06:15,101] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-14 19:06:15,104] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (9bbba8114907, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,105] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 187 ms on 9bbba8114907 (executor driver) (98/141)
[2021-05-14 19:06:15,106] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-14 19:06:15,232] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1843 bytes result sent to driver
[2021-05-14 19:06:15,233] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-14 19:06:15,234] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (9bbba8114907, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,235] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
[2021-05-14 19:06:15,236] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 181 ms on 9bbba8114907 (executor driver) (99/141)
[2021-05-14 19:06:15,237] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-14 19:06:15,238] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (9bbba8114907, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,239] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 186 ms on 9bbba8114907 (executor driver) (100/141)
[2021-05-14 19:06:15,240] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-14 19:06:15,243] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (9bbba8114907, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,244] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 187 ms on 9bbba8114907 (executor driver) (101/141)
[2021-05-14 19:06:15,245] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-14 19:06:15,296] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1843 bytes result sent to driver
[2021-05-14 19:06:15,298] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (9bbba8114907, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,299] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 197 ms on 9bbba8114907 (executor driver) (102/141)
21/05/14 22:06:15 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-14 19:06:15,412] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-14 19:06:15,414] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (9bbba8114907, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,416] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 177 ms on 9bbba8114907 (executor driver) (103/141)
[2021-05-14 19:06:15,417] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-14 19:06:15,418] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1843 bytes result sent to driver
[2021-05-14 19:06:15,419] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (9bbba8114907, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,420] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1843 bytes result sent to driver
[2021-05-14 19:06:15,421] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-14 19:06:15,421] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 177 ms on 9bbba8114907 (executor driver) (104/141)
[2021-05-14 19:06:15,422] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (9bbba8114907, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,424] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-14 19:06:15,425] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 191 ms on 9bbba8114907 (executor driver) (105/141)
[2021-05-14 19:06:15,477] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
[2021-05-14 19:06:15,480] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (9bbba8114907, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,481] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
21/05/14 22:06:15 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 184 ms on 9bbba8114907 (executor driver) (106/141)
[2021-05-14 19:06:15,589] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1886 bytes result sent to driver
[2021-05-14 19:06:15,591] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (9bbba8114907, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,592] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-14 19:06:15,593] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 174 ms on 9bbba8114907 (executor driver) (107/141)
[2021-05-14 19:06:15,596] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1886 bytes result sent to driver
[2021-05-14 19:06:15,597] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (9bbba8114907, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,598] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 186 ms on 9bbba8114907 (executor driver) (108/141)
[2021-05-14 19:06:15,599] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-14 19:06:15,607] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1886 bytes result sent to driver
[2021-05-14 19:06:15,608] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (9bbba8114907, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,609] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-14 19:06:15,609] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 187 ms on 9bbba8114907 (executor driver) (109/141)
[2021-05-14 19:06:15,668] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1886 bytes result sent to driver
[2021-05-14 19:06:15,669] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (9bbba8114907, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,670] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
21/05/14 22:06:15 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 191 ms on 9bbba8114907 (executor driver) (110/141)
[2021-05-14 19:06:15,766] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1843 bytes result sent to driver
[2021-05-14 19:06:15,768] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (9bbba8114907, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,769] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 178 ms on 9bbba8114907 (executor driver) (111/141)
[2021-05-14 19:06:15,770] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-14 19:06:15,776] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1843 bytes result sent to driver
21/05/14 22:06:15 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1843 bytes result sent to driver
[2021-05-14 19:06:15,777] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (9bbba8114907, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,779] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
21/05/14 22:06:15 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (9bbba8114907, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,780] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 172 ms on 9bbba8114907 (executor driver) (112/141)
[2021-05-14 19:06:15,780] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 184 ms on 9bbba8114907 (executor driver) (113/141)
[2021-05-14 19:06:15,781] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-14 19:06:15,845] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-14 19:06:15,846] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (9bbba8114907, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,847] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 180 ms on 9bbba8114907 (executor driver) (114/141)
[2021-05-14 19:06:15,848] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-14 19:06:15,943] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-14 19:06:15,947] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (9bbba8114907, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,948] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 179 ms on 9bbba8114907 (executor driver) (115/141)
[2021-05-14 19:06:15,949] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-14 19:06:15,952] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-14 19:06:15,952] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-14 19:06:15,954] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (9bbba8114907, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,955] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 176 ms on 9bbba8114907 (executor driver) (116/141)
[2021-05-14 19:06:15,955] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-14 19:06:15,956] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 179 ms on 9bbba8114907 (executor driver) (117/141)
[2021-05-14 19:06:15,958] {docker.py:276} INFO - 21/05/14 22:06:15 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (9bbba8114907, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:15,959] {docker.py:276} INFO - 21/05/14 22:06:15 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-14 19:06:16,021] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1843 bytes result sent to driver
[2021-05-14 19:06:16,022] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (9bbba8114907, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,023] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 177 ms on 9bbba8114907 (executor driver) (118/141)
[2021-05-14 19:06:16,023] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-14 19:06:16,125] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1843 bytes result sent to driver
[2021-05-14 19:06:16,128] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (9bbba8114907, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,129] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 184 ms on 9bbba8114907 (executor driver) (119/141)
[2021-05-14 19:06:16,130] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-14 19:06:16,133] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1843 bytes result sent to driver
[2021-05-14 19:06:16,134] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (9bbba8114907, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,135] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 182 ms on 9bbba8114907 (executor driver) (120/141)
[2021-05-14 19:06:16,136] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-14 19:06:16,138] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1843 bytes result sent to driver
[2021-05-14 19:06:16,139] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 182 ms on 9bbba8114907 (executor driver) (121/141)
[2021-05-14 19:06:16,141] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (9bbba8114907, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,142] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-14 19:06:16,200] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1843 bytes result sent to driver
[2021-05-14 19:06:16,203] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (9bbba8114907, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,204] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-14 19:06:16,205] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 182 ms on 9bbba8114907 (executor driver) (122/141)
[2021-05-14 19:06:16,306] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1886 bytes result sent to driver
[2021-05-14 19:06:16,308] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1886 bytes result sent to driver
[2021-05-14 19:06:16,309] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (9bbba8114907, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,310] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 183 ms on 9bbba8114907 (executor driver) (123/141)
[2021-05-14 19:06:16,311] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-14 19:06:16,312] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1886 bytes result sent to driver
[2021-05-14 19:06:16,313] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (9bbba8114907, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,314] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 181 ms on 9bbba8114907 (executor driver) (124/141)
[2021-05-14 19:06:16,315] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-14 19:06:16,316] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 176 ms on 9bbba8114907 (executor driver) (125/141)
[2021-05-14 19:06:16,317] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (9bbba8114907, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,319] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-14 19:06:16,387] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1886 bytes result sent to driver
[2021-05-14 19:06:16,389] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (9bbba8114907, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,391] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
21/05/14 22:06:16 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 189 ms on 9bbba8114907 (executor driver) (126/141)
[2021-05-14 19:06:16,493] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1843 bytes result sent to driver
[2021-05-14 19:06:16,495] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 183 ms on 9bbba8114907 (executor driver) (127/141)
[2021-05-14 19:06:16,496] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1843 bytes result sent to driver
[2021-05-14 19:06:16,497] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-14 19:06:16,498] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (9bbba8114907, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,499] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-14 19:06:16,500] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (9bbba8114907, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,502] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-14 19:06:16,507] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (9bbba8114907, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,508] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-14 19:06:16,509] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 191 ms on 9bbba8114907 (executor driver) (128/141)
[2021-05-14 19:06:16,509] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 201 ms on 9bbba8114907 (executor driver) (129/141)
[2021-05-14 19:06:16,568] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1843 bytes result sent to driver
[2021-05-14 19:06:16,570] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (9bbba8114907, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,571] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 183 ms on 9bbba8114907 (executor driver) (130/141)
[2021-05-14 19:06:16,572] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-14 19:06:16,674] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1843 bytes result sent to driver
[2021-05-14 19:06:16,676] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1843 bytes result sent to driver
[2021-05-14 19:06:16,677] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (9bbba8114907, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,678] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-14 19:06:16,680] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (9bbba8114907, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,680] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 185 ms on 9bbba8114907 (executor driver) (131/141)
[2021-05-14 19:06:16,681] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 182 ms on 9bbba8114907 (executor driver) (132/141)
[2021-05-14 19:06:16,682] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-14 19:06:16,694] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1800 bytes result sent to driver
[2021-05-14 19:06:16,696] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (9bbba8114907, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,697] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 195 ms on 9bbba8114907 (executor driver) (133/141)
[2021-05-14 19:06:16,697] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-14 19:06:16,786] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1843 bytes result sent to driver
[2021-05-14 19:06:16,787] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (9bbba8114907, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,789] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
21/05/14 22:06:16 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 220 ms on 9bbba8114907 (executor driver) (134/141)
[2021-05-14 19:06:16,856] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
[2021-05-14 19:06:16,858] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (9bbba8114907, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,859] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 183 ms on 9bbba8114907 (executor driver) (135/141)
[2021-05-14 19:06:16,859] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-14 19:06:16,860] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1843 bytes result sent to driver
[2021-05-14 19:06:16,862] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (9bbba8114907, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,863] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 183 ms on 9bbba8114907 (executor driver) (136/141)
[2021-05-14 19:06:16,864] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-14 19:06:16,867] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-14 19:06:16,869] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (9bbba8114907, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:16,871] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
21/05/14 22:06:16 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 175 ms on 9bbba8114907 (executor driver) (137/141)
[2021-05-14 19:06:16,963] {docker.py:276} INFO - 21/05/14 22:06:16 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1843 bytes result sent to driver
[2021-05-14 19:06:16,969] {docker.py:276} INFO - 21/05/14 22:06:16 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 182 ms on 9bbba8114907 (executor driver) (138/141)
[2021-05-14 19:06:17,040] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1843 bytes result sent to driver
[2021-05-14 19:06:17,041] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 180 ms on 9bbba8114907 (executor driver) (139/141)
[2021-05-14 19:06:17,054] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1843 bytes result sent to driver
[2021-05-14 19:06:17,055] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 186 ms on 9bbba8114907 (executor driver) (140/141)
[2021-05-14 19:06:17,062] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1843 bytes result sent to driver
[2021-05-14 19:06:17,063] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 206 ms on 9bbba8114907 (executor driver) (141/141)
[2021-05-14 19:06:17,066] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 19:06:17,067] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.453 s
[2021-05-14 19:06:17,073] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 19:06:17,074] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 19:06:17,078] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.543614 s
[2021-05-14 19:06:17,127] {docker.py:276} INFO - 21/05/14 22:06:17 INFO InMemoryFileIndex: It took 8131 ms to list leaf files for 141 paths.
[2021-05-14 19:06:17,265] {docker.py:276} INFO - 21/05/14 22:06:17 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620943516_to_1620945316.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620945316_to_1620947116.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620947116_to_1620948916.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620948916_to_1620950716.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620950716_to_1620952516.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620952516_to_1620954316.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620954316_to_1620956116.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620956116_to_1620957916.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620957916_to_1620959716.csv, s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620959716_to_1620961516.csv.
[2021-05-14 19:06:17,306] {docker.py:276} INFO - 21/05/14 22:06:17 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:17,308] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/14 22:06:17 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:06:17 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:06:17,309] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:06:17,310] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:06:17,326] {docker.py:276} INFO - 21/05/14 22:06:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-14 19:06:17,335] {docker.py:276} INFO - 21/05/14 22:06:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-14 19:06:17,336] {docker.py:276} INFO - 21/05/14 22:06:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 9bbba8114907:33069 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-14 19:06:17,338] {docker.py:276} INFO - 21/05/14 22:06:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:06:17,340] {docker.py:276} INFO - 21/05/14 22:06:17 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 22:06:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-14 19:06:17,344] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (9bbba8114907, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,345] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (9bbba8114907, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,348] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (9bbba8114907, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,349] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (9bbba8114907, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,350] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-14 19:06:17,358] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
21/05/14 22:06:17 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-14 19:06:17,359] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-14 19:06:17,435] {docker.py:276} INFO - 21/05/14 22:06:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 9bbba8114907:33069 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:06:17,525] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-14 19:06:17,526] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-14 19:06:17,527] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (9bbba8114907, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,527] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-14 19:06:17,528] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-14 19:06:17,529] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (9bbba8114907, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,530] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 186 ms on 9bbba8114907 (executor driver) (1/141)
[2021-05-14 19:06:17,531] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-14 19:06:17,533] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (9bbba8114907, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,533] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 190 ms on 9bbba8114907 (executor driver) (2/141)
[2021-05-14 19:06:17,534] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 187 ms on 9bbba8114907 (executor driver) (3/141)
[2021-05-14 19:06:17,534] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-14 19:06:17,537] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-14 19:06:17,538] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (9bbba8114907, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,539] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
21/05/14 22:06:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 191 ms on 9bbba8114907 (executor driver) (4/141)
[2021-05-14 19:06:17,705] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-14 19:06:17,706] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-14 19:06:17,707] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (9bbba8114907, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,709] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-14 19:06:17,709] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 172 ms on 9bbba8114907 (executor driver) (5/141)
[2021-05-14 19:06:17,711] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (9bbba8114907, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,712] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 184 ms on 9bbba8114907 (executor driver) (6/141)
[2021-05-14 19:06:17,713] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-14 19:06:17,715] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-14 19:06:17,715] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-14 19:06:17,716] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (9bbba8114907, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,717] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 192 ms on 9bbba8114907 (executor driver) (7/141)
[2021-05-14 19:06:17,718] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-14 19:06:17,720] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (9bbba8114907, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,721] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 189 ms on 9bbba8114907 (executor driver) (8/141)
[2021-05-14 19:06:17,722] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-14 19:06:17,886] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
[2021-05-14 19:06:17,887] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (9bbba8114907, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,887] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 177 ms on 9bbba8114907 (executor driver) (9/141)
[2021-05-14 19:06:17,888] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-14 19:06:17,889] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-14 19:06:17,890] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (9bbba8114907, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,891] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-14 19:06:17,892] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 172 ms on 9bbba8114907 (executor driver) (10/141)
[2021-05-14 19:06:17,895] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-14 19:06:17,896] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (9bbba8114907, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,897] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
21/05/14 22:06:17 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 191 ms on 9bbba8114907 (executor driver) (11/141)
[2021-05-14 19:06:17,905] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-14 19:06:17,905] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (9bbba8114907, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:17,907] {docker.py:276} INFO - 21/05/14 22:06:17 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 190 ms on 9bbba8114907 (executor driver) (12/141)
[2021-05-14 19:06:17,908] {docker.py:276} INFO - 21/05/14 22:06:17 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-14 19:06:18,061] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1886 bytes result sent to driver
[2021-05-14 19:06:18,062] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (9bbba8114907, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,063] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 168 ms on 9bbba8114907 (executor driver) (13/141)
[2021-05-14 19:06:18,064] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-14 19:06:18,065] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1886 bytes result sent to driver
[2021-05-14 19:06:18,066] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (9bbba8114907, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,067] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 177 ms on 9bbba8114907 (executor driver) (14/141)
[2021-05-14 19:06:18,068] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-14 19:06:18,075] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1886 bytes result sent to driver
[2021-05-14 19:06:18,076] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (9bbba8114907, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,077] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 191 ms on 9bbba8114907 (executor driver) (15/141)
21/05/14 22:06:18 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-14 19:06:18,083] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1886 bytes result sent to driver
[2021-05-14 19:06:18,084] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (9bbba8114907, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,085] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 180 ms on 9bbba8114907 (executor driver) (16/141)
[2021-05-14 19:06:18,088] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-14 19:06:18,231] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1843 bytes result sent to driver
[2021-05-14 19:06:18,233] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (9bbba8114907, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,234] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 168 ms on 9bbba8114907 (executor driver) (17/141)
[2021-05-14 19:06:18,234] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-14 19:06:18,235] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-14 19:06:18,236] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (9bbba8114907, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,239] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 176 ms on 9bbba8114907 (executor driver) (18/141)
[2021-05-14 19:06:18,239] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-14 19:06:18,245] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1843 bytes result sent to driver
[2021-05-14 19:06:18,246] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (9bbba8114907, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,247] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 171 ms on 9bbba8114907 (executor driver) (19/141)
21/05/14 22:06:18 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-14 19:06:18,290] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-14 19:06:18,291] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (9bbba8114907, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,292] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 208 ms on 9bbba8114907 (executor driver) (20/141)
[2021-05-14 19:06:18,292] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-14 19:06:18,408] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-14 19:06:18,409] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-14 19:06:18,410] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (9bbba8114907, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,411] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 174 ms on 9bbba8114907 (executor driver) (21/141)
[2021-05-14 19:06:18,412] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-14 19:06:18,413] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 180 ms on 9bbba8114907 (executor driver) (22/141)
[2021-05-14 19:06:18,414] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-14 19:06:18,414] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (9bbba8114907, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,415] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-14 19:06:18,416] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (9bbba8114907, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,418] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 173 ms on 9bbba8114907 (executor driver) (23/141)
[2021-05-14 19:06:18,419] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-14 19:06:18,459] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-14 19:06:18,461] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (9bbba8114907, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,462] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 172 ms on 9bbba8114907 (executor driver) (24/141)
[2021-05-14 19:06:18,464] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-14 19:06:18,590] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
[2021-05-14 19:06:18,592] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (9bbba8114907, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,595] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
21/05/14 22:06:18 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1843 bytes result sent to driver
21/05/14 22:06:18 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
21/05/14 22:06:18 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 181 ms on 9bbba8114907 (executor driver) (25/141)
[2021-05-14 19:06:18,596] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 186 ms on 9bbba8114907 (executor driver) (26/141)
[2021-05-14 19:06:18,598] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (9bbba8114907, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,599] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (9bbba8114907, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,600] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-14 19:06:18,605] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
21/05/14 22:06:18 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 189 ms on 9bbba8114907 (executor driver) (27/141)
[2021-05-14 19:06:18,628] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1843 bytes result sent to driver
[2021-05-14 19:06:18,629] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (9bbba8114907, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,630] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-14 19:06:18,630] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 170 ms on 9bbba8114907 (executor driver) (28/141)
[2021-05-14 19:06:18,773] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1886 bytes result sent to driver
21/05/14 22:06:18 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1886 bytes result sent to driver
[2021-05-14 19:06:18,774] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1886 bytes result sent to driver
[2021-05-14 19:06:18,775] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (9bbba8114907, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,776] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-14 19:06:18,777] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (9bbba8114907, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,779] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-14 19:06:18,780] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (9bbba8114907, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,780] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 183 ms on 9bbba8114907 (executor driver) (29/141)
[2021-05-14 19:06:18,781] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-14 19:06:18,782] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 183 ms on 9bbba8114907 (executor driver) (30/141)
[2021-05-14 19:06:18,783] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 191 ms on 9bbba8114907 (executor driver) (31/141)
[2021-05-14 19:06:18,805] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1886 bytes result sent to driver
[2021-05-14 19:06:18,806] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (9bbba8114907, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,808] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-14 19:06:18,808] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 178 ms on 9bbba8114907 (executor driver) (32/141)
[2021-05-14 19:06:18,955] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1843 bytes result sent to driver
21/05/14 22:06:18 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1843 bytes result sent to driver
[2021-05-14 19:06:18,956] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-14 19:06:18,958] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (9bbba8114907, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,959] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-14 19:06:18,960] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (9bbba8114907, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,961] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 183 ms on 9bbba8114907 (executor driver) (33/141)
[2021-05-14 19:06:18,962] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 182 ms on 9bbba8114907 (executor driver) (34/141)
[2021-05-14 19:06:18,963] {docker.py:276} INFO - 21/05/14 22:06:18 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-14 19:06:18,964] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (9bbba8114907, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,965] {docker.py:276} INFO - 21/05/14 22:06:18 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 192 ms on 9bbba8114907 (executor driver) (35/141)
21/05/14 22:06:18 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-14 19:06:18,981] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1843 bytes result sent to driver
[2021-05-14 19:06:18,983] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (9bbba8114907, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:18,984] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 179 ms on 9bbba8114907 (executor driver) (36/141)
[2021-05-14 19:06:18,985] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-14 19:06:19,132] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
21/05/14 22:06:19 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1843 bytes result sent to driver
[2021-05-14 19:06:19,134] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (9bbba8114907, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,135] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
21/05/14 22:06:19 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (9bbba8114907, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,137] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 174 ms on 9bbba8114907 (executor driver) (37/141)
[2021-05-14 19:06:19,139] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1843 bytes result sent to driver
[2021-05-14 19:06:19,139] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 181 ms on 9bbba8114907 (executor driver) (38/141)
[2021-05-14 19:06:19,140] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-14 19:06:19,141] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (9bbba8114907, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,143] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 187 ms on 9bbba8114907 (executor driver) (39/141)
21/05/14 22:06:19 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-14 19:06:19,148] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1843 bytes result sent to driver
[2021-05-14 19:06:19,149] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (9bbba8114907, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,150] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
21/05/14 22:06:19 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 168 ms on 9bbba8114907 (executor driver) (40/141)
[2021-05-14 19:06:19,311] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1843 bytes result sent to driver
[2021-05-14 19:06:19,313] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (9bbba8114907, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,314] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 181 ms on 9bbba8114907 (executor driver) (41/141)
[2021-05-14 19:06:19,316] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-14 19:06:19,317] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-14 19:06:19,318] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (9bbba8114907, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,319] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 170 ms on 9bbba8114907 (executor driver) (42/141)
[2021-05-14 19:06:19,320] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-14 19:06:19,321] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1843 bytes result sent to driver
[2021-05-14 19:06:19,323] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (9bbba8114907, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,323] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 188 ms on 9bbba8114907 (executor driver) (43/141)
[2021-05-14 19:06:19,324] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-14 19:06:19,352] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-14 19:06:19,353] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (9bbba8114907, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,354] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 214 ms on 9bbba8114907 (executor driver) (44/141)
21/05/14 22:06:19 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-14 19:06:19,490] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1886 bytes result sent to driver
21/05/14 22:06:19 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1886 bytes result sent to driver
[2021-05-14 19:06:19,491] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1886 bytes result sent to driver
[2021-05-14 19:06:19,492] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (9bbba8114907, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,493] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-14 19:06:19,494] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (9bbba8114907, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,495] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-14 19:06:19,495] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (9bbba8114907, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,496] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 176 ms on 9bbba8114907 (executor driver) (45/141)
[2021-05-14 19:06:19,497] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-14 19:06:19,498] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 174 ms on 9bbba8114907 (executor driver) (46/141)
[2021-05-14 19:06:19,498] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 184 ms on 9bbba8114907 (executor driver) (47/141)
[2021-05-14 19:06:19,540] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1886 bytes result sent to driver
[2021-05-14 19:06:19,541] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (9bbba8114907, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,542] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 189 ms on 9bbba8114907 (executor driver) (48/141)
[2021-05-14 19:06:19,543] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-14 19:06:19,665] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-14 19:06:19,667] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (9bbba8114907, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,667] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 174 ms on 9bbba8114907 (executor driver) (49/141)
[2021-05-14 19:06:19,669] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
21/05/14 22:06:19 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-14 19:06:19,670] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (9bbba8114907, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,674] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 182 ms on 9bbba8114907 (executor driver) (50/141)
[2021-05-14 19:06:19,675] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-14 19:06:19,680] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-14 19:06:19,681] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (9bbba8114907, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,681] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 189 ms on 9bbba8114907 (executor driver) (51/141)
[2021-05-14 19:06:19,682] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-14 19:06:19,716] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-14 19:06:19,717] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (9bbba8114907, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,719] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
21/05/14 22:06:19 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 179 ms on 9bbba8114907 (executor driver) (52/141)
[2021-05-14 19:06:19,836] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1843 bytes result sent to driver
[2021-05-14 19:06:19,837] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (9bbba8114907, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,838] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 173 ms on 9bbba8114907 (executor driver) (53/141)
[2021-05-14 19:06:19,839] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-14 19:06:19,841] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1843 bytes result sent to driver
[2021-05-14 19:06:19,842] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (9bbba8114907, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,843] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 173 ms on 9bbba8114907 (executor driver) (54/141)
21/05/14 22:06:19 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-14 19:06:19,849] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1843 bytes result sent to driver
[2021-05-14 19:06:19,850] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (9bbba8114907, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,851] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-14 19:06:19,851] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 171 ms on 9bbba8114907 (executor driver) (55/141)
[2021-05-14 19:06:19,892] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1843 bytes result sent to driver
[2021-05-14 19:06:19,894] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (9bbba8114907, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:19,895] {docker.py:276} INFO - 21/05/14 22:06:19 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-14 19:06:19,896] {docker.py:276} INFO - 21/05/14 22:06:19 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 177 ms on 9bbba8114907 (executor driver) (56/141)
[2021-05-14 19:06:20,011] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1843 bytes result sent to driver
[2021-05-14 19:06:20,012] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (9bbba8114907, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,013] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 177 ms on 9bbba8114907 (executor driver) (57/141)
[2021-05-14 19:06:20,013] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-14 19:06:20,019] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1843 bytes result sent to driver
[2021-05-14 19:06:20,020] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (9bbba8114907, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,020] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 171 ms on 9bbba8114907 (executor driver) (58/141)
[2021-05-14 19:06:20,044] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1843 bytes result sent to driver
[2021-05-14 19:06:20,046] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (9bbba8114907, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,047] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 205 ms on 9bbba8114907 (executor driver) (59/141)
[2021-05-14 19:06:20,064] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
[2021-05-14 19:06:20,065] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (9bbba8114907, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,066] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-14 19:06:20,067] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 175 ms on 9bbba8114907 (executor driver) (60/141)
[2021-05-14 19:06:20,183] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1886 bytes result sent to driver
[2021-05-14 19:06:20,185] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (9bbba8114907, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,186] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1886 bytes result sent to driver
[2021-05-14 19:06:20,187] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 174 ms on 9bbba8114907 (executor driver) (61/141)
[2021-05-14 19:06:20,188] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-14 19:06:20,189] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (9bbba8114907, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,189] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 169 ms on 9bbba8114907 (executor driver) (62/141)
[2021-05-14 19:06:20,190] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-14 19:06:20,224] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1886 bytes result sent to driver
[2021-05-14 19:06:20,226] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (9bbba8114907, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,227] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 183 ms on 9bbba8114907 (executor driver) (63/141)
[2021-05-14 19:06:20,228] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-14 19:06:20,244] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1886 bytes result sent to driver
[2021-05-14 19:06:20,246] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (9bbba8114907, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,247] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 182 ms on 9bbba8114907 (executor driver) (64/141)
[2021-05-14 19:06:20,247] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-14 19:06:20,359] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1843 bytes result sent to driver
[2021-05-14 19:06:20,360] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1843 bytes result sent to driver
[2021-05-14 19:06:20,361] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (9bbba8114907, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,363] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-14 19:06:20,364] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (9bbba8114907, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,365] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 177 ms on 9bbba8114907 (executor driver) (65/141)
[2021-05-14 19:06:20,366] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 182 ms on 9bbba8114907 (executor driver) (66/141)
[2021-05-14 19:06:20,399] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
[2021-05-14 19:06:20,401] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (9bbba8114907, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,402] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 176 ms on 9bbba8114907 (executor driver) (67/141)
21/05/14 22:06:20 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-14 19:06:20,418] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-14 19:06:20,419] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (9bbba8114907, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,420] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-14 19:06:20,421] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 175 ms on 9bbba8114907 (executor driver) (68/141)
[2021-05-14 19:06:20,535] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-14 19:06:20,536] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-14 19:06:20,537] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (9bbba8114907, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,538] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-14 19:06:20,540] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (9bbba8114907, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,541] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 179 ms on 9bbba8114907 (executor driver) (69/141)
[2021-05-14 19:06:20,541] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-14 19:06:20,542] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 179 ms on 9bbba8114907 (executor driver) (70/141)
[2021-05-14 19:06:20,570] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-14 19:06:20,571] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (9bbba8114907, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,572] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 172 ms on 9bbba8114907 (executor driver) (71/141)
[2021-05-14 19:06:20,573] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
[2021-05-14 19:06:20,593] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-14 19:06:20,596] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (9bbba8114907, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,597] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 178 ms on 9bbba8114907 (executor driver) (72/141)
[2021-05-14 19:06:20,711] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1843 bytes result sent to driver
[2021-05-14 19:06:20,713] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (9bbba8114907, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,715] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1843 bytes result sent to driver
[2021-05-14 19:06:20,716] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 176 ms on 9bbba8114907 (executor driver) (73/141)
[2021-05-14 19:06:20,716] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-14 19:06:20,717] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (9bbba8114907, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,718] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-14 19:06:20,719] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 184 ms on 9bbba8114907 (executor driver) (74/141)
[2021-05-14 19:06:20,741] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1843 bytes result sent to driver
[2021-05-14 19:06:20,743] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (9bbba8114907, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,744] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-14 19:06:20,745] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 174 ms on 9bbba8114907 (executor driver) (75/141)
[2021-05-14 19:06:20,771] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1843 bytes result sent to driver
[2021-05-14 19:06:20,773] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (9bbba8114907, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,775] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 180 ms on 9bbba8114907 (executor driver) (76/141)
[2021-05-14 19:06:20,887] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1886 bytes result sent to driver
[2021-05-14 19:06:20,888] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (9bbba8114907, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,890] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
21/05/14 22:06:20 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 174 ms on 9bbba8114907 (executor driver) (77/141)
[2021-05-14 19:06:20,907] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1886 bytes result sent to driver
[2021-05-14 19:06:20,908] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (9bbba8114907, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,909] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-14 19:06:20,909] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 168 ms on 9bbba8114907 (executor driver) (78/141)
[2021-05-14 19:06:20,955] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1886 bytes result sent to driver
[2021-05-14 19:06:20,957] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (9bbba8114907, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,959] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 185 ms on 9bbba8114907 (executor driver) (79/141)
[2021-05-14 19:06:20,960] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-14 19:06:20,962] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1886 bytes result sent to driver
[2021-05-14 19:06:20,964] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (9bbba8114907, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:20,966] {docker.py:276} INFO - 21/05/14 22:06:20 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-14 19:06:20,966] {docker.py:276} INFO - 21/05/14 22:06:20 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 254 ms on 9bbba8114907 (executor driver) (80/141)
[2021-05-14 19:06:21,059] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-14 19:06:21,060] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (9bbba8114907, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,061] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 173 ms on 9bbba8114907 (executor driver) (81/141)
[2021-05-14 19:06:21,062] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-14 19:06:21,073] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1843 bytes result sent to driver
[2021-05-14 19:06:21,074] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (9bbba8114907, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,074] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-14 19:06:21,075] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 168 ms on 9bbba8114907 (executor driver) (82/141)
[2021-05-14 19:06:21,142] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1843 bytes result sent to driver
[2021-05-14 19:06:21,144] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (9bbba8114907, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,145] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-14 19:06:21,146] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 190 ms on 9bbba8114907 (executor driver) (83/141)
[2021-05-14 19:06:21,150] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1843 bytes result sent to driver
[2021-05-14 19:06:21,151] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (9bbba8114907, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,152] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-14 19:06:21,153] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 189 ms on 9bbba8114907 (executor driver) (84/141)
[2021-05-14 19:06:21,228] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
[2021-05-14 19:06:21,229] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (9bbba8114907, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,231] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
21/05/14 22:06:21 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 171 ms on 9bbba8114907 (executor driver) (85/141)
[2021-05-14 19:06:21,238] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
[2021-05-14 19:06:21,240] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (9bbba8114907, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,241] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 168 ms on 9bbba8114907 (executor driver) (86/141)
[2021-05-14 19:06:21,242] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-14 19:06:21,320] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-14 19:06:21,322] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-14 19:06:21,323] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (9bbba8114907, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,325] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 174 ms on 9bbba8114907 (executor driver) (87/141)
[2021-05-14 19:06:21,326] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-14 19:06:21,327] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (9bbba8114907, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,327] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 184 ms on 9bbba8114907 (executor driver) (88/141)
[2021-05-14 19:06:21,330] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-14 19:06:21,400] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-14 19:06:21,401] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 172 ms on 9bbba8114907 (executor driver) (89/141)
[2021-05-14 19:06:21,403] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (9bbba8114907, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,404] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-14 19:06:21,411] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1843 bytes result sent to driver
[2021-05-14 19:06:21,412] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (9bbba8114907, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,413] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 174 ms on 9bbba8114907 (executor driver) (90/141)
[2021-05-14 19:06:21,414] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-14 19:06:21,498] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1843 bytes result sent to driver
[2021-05-14 19:06:21,500] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1843 bytes result sent to driver
[2021-05-14 19:06:21,502] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (9bbba8114907, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,503] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 176 ms on 9bbba8114907 (executor driver) (91/141)
[2021-05-14 19:06:21,504] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-14 19:06:21,505] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (9bbba8114907, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,507] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 185 ms on 9bbba8114907 (executor driver) (92/141)
[2021-05-14 19:06:21,509] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-14 19:06:21,571] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1886 bytes result sent to driver
[2021-05-14 19:06:21,572] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (9bbba8114907, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,572] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 171 ms on 9bbba8114907 (executor driver) (93/141)
[2021-05-14 19:06:21,573] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-14 19:06:21,586] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1886 bytes result sent to driver
[2021-05-14 19:06:21,587] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (9bbba8114907, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,587] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 176 ms on 9bbba8114907 (executor driver) (94/141)
[2021-05-14 19:06:21,588] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-14 19:06:21,689] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1886 bytes result sent to driver
[2021-05-14 19:06:21,691] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (9bbba8114907, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,693] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 154 ms on 9bbba8114907 (executor driver) (95/141)
[2021-05-14 19:06:21,694] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1886 bytes result sent to driver
[2021-05-14 19:06:21,695] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-14 19:06:21,696] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (9bbba8114907, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,698] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 163 ms on 9bbba8114907 (executor driver) (96/141)
21/05/14 22:06:21 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-14 19:06:21,745] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1843 bytes result sent to driver
[2021-05-14 19:06:21,746] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (9bbba8114907, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,747] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 141 ms on 9bbba8114907 (executor driver) (97/141)
21/05/14 22:06:21 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-14 19:06:21,751] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-14 19:06:21,753] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (9bbba8114907, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,754] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
21/05/14 22:06:21 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 133 ms on 9bbba8114907 (executor driver) (98/141)
[2021-05-14 19:06:21,863] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-14 19:06:21,865] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (9bbba8114907, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,866] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-14 19:06:21,867] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 171 ms on 9bbba8114907 (executor driver) (99/141)
[2021-05-14 19:06:21,873] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1843 bytes result sent to driver
[2021-05-14 19:06:21,875] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (9bbba8114907, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,876] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-14 19:06:21,876] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 186 ms on 9bbba8114907 (executor driver) (100/141)
[2021-05-14 19:06:21,917] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-14 19:06:21,919] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (9bbba8114907, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,919] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 174 ms on 9bbba8114907 (executor driver) (101/141)
[2021-05-14 19:06:21,919] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-14 19:06:21,922] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-14 19:06:21,923] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (9bbba8114907, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:21,924] {docker.py:276} INFO - 21/05/14 22:06:21 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 172 ms on 9bbba8114907 (executor driver) (102/141)
[2021-05-14 19:06:21,925] {docker.py:276} INFO - 21/05/14 22:06:21 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-14 19:06:22,040] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-14 19:06:22,043] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (9bbba8114907, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,044] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 177 ms on 9bbba8114907 (executor driver) (103/141)
[2021-05-14 19:06:22,044] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1843 bytes result sent to driver
[2021-05-14 19:06:22,044] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-14 19:06:22,045] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (9bbba8114907, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,045] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 168 ms on 9bbba8114907 (executor driver) (104/141)
[2021-05-14 19:06:22,045] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-14 19:06:22,084] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1843 bytes result sent to driver
[2021-05-14 19:06:22,085] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (9bbba8114907, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,086] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-14 19:06:22,086] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 168 ms on 9bbba8114907 (executor driver) (105/141)
[2021-05-14 19:06:22,093] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1843 bytes result sent to driver
[2021-05-14 19:06:22,093] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 171 ms on 9bbba8114907 (executor driver) (106/141)
[2021-05-14 19:06:22,094] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (9bbba8114907, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,095] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-14 19:06:22,214] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1843 bytes result sent to driver
[2021-05-14 19:06:22,234] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (9bbba8114907, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,235] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-14 19:06:22,235] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 174 ms on 9bbba8114907 (executor driver) (107/141)
[2021-05-14 19:06:22,235] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1843 bytes result sent to driver
[2021-05-14 19:06:22,236] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (9bbba8114907, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,236] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-14 19:06:22,236] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 181 ms on 9bbba8114907 (executor driver) (108/141)
[2021-05-14 19:06:22,260] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1886 bytes result sent to driver
[2021-05-14 19:06:22,262] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1886 bytes result sent to driver
[2021-05-14 19:06:22,263] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (9bbba8114907, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,264] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-14 19:06:22,265] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (9bbba8114907, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,266] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 171 ms on 9bbba8114907 (executor driver) (109/141)
[2021-05-14 19:06:22,267] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-14 19:06:22,268] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 182 ms on 9bbba8114907 (executor driver) (110/141)
[2021-05-14 19:06:22,405] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1886 bytes result sent to driver
[2021-05-14 19:06:22,406] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1886 bytes result sent to driver
[2021-05-14 19:06:22,408] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (9bbba8114907, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,409] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-14 19:06:22,410] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (9bbba8114907, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,411] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-14 19:06:22,412] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 191 ms on 9bbba8114907 (executor driver) (111/141)
[2021-05-14 19:06:22,412] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 197 ms on 9bbba8114907 (executor driver) (112/141)
[2021-05-14 19:06:22,430] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1843 bytes result sent to driver
[2021-05-14 19:06:22,431] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (9bbba8114907, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,432] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-14 19:06:22,432] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 168 ms on 9bbba8114907 (executor driver) (113/141)
[2021-05-14 19:06:22,433] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-14 19:06:22,434] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (9bbba8114907, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,435] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 174 ms on 9bbba8114907 (executor driver) (114/141)
[2021-05-14 19:06:22,435] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-14 19:06:22,581] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1843 bytes result sent to driver
[2021-05-14 19:06:22,583] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (9bbba8114907, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,585] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 177 ms on 9bbba8114907 (executor driver) (115/141)
[2021-05-14 19:06:22,586] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-14 19:06:22,587] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-14 19:06:22,587] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (9bbba8114907, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,589] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-14 19:06:22,589] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 180 ms on 9bbba8114907 (executor driver) (116/141)
[2021-05-14 19:06:22,596] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-14 19:06:22,597] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (9bbba8114907, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,598] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 168 ms on 9bbba8114907 (executor driver) (117/141)
[2021-05-14 19:06:22,599] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-14 19:06:22,601] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-14 19:06:22,602] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (9bbba8114907, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,603] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 170 ms on 9bbba8114907 (executor driver) (118/141)
21/05/14 22:06:22 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-14 19:06:22,759] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1843 bytes result sent to driver
[2021-05-14 19:06:22,761] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (9bbba8114907, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,762] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 179 ms on 9bbba8114907 (executor driver) (119/141)
[2021-05-14 19:06:22,763] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-14 19:06:22,765] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1843 bytes result sent to driver
21/05/14 22:06:22 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1843 bytes result sent to driver
[2021-05-14 19:06:22,767] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (9bbba8114907, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,768] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-14 19:06:22,768] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (9bbba8114907, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,769] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
21/05/14 22:06:22 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 172 ms on 9bbba8114907 (executor driver) (120/141)
[2021-05-14 19:06:22,770] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 183 ms on 9bbba8114907 (executor driver) (121/141)
[2021-05-14 19:06:22,773] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1843 bytes result sent to driver
[2021-05-14 19:06:22,774] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (9bbba8114907, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,776] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-14 19:06:22,776] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 173 ms on 9bbba8114907 (executor driver) (122/141)
[2021-05-14 19:06:22,934] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
[2021-05-14 19:06:22,936] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (9bbba8114907, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,936] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-14 19:06:22,937] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 177 ms on 9bbba8114907 (executor driver) (123/141)
[2021-05-14 19:06:22,940] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-14 19:06:22,941] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (9bbba8114907, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,942] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 176 ms on 9bbba8114907 (executor driver) (124/141)
21/05/14 22:06:22 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-14 19:06:22,943] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1843 bytes result sent to driver
[2021-05-14 19:06:22,945] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (9bbba8114907, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,946] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 179 ms on 9bbba8114907 (executor driver) (125/141)
[2021-05-14 19:06:22,946] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
21/05/14 22:06:22 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1843 bytes result sent to driver
[2021-05-14 19:06:22,947] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (9bbba8114907, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:22,948] {docker.py:276} INFO - 21/05/14 22:06:22 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 174 ms on 9bbba8114907 (executor driver) (126/141)
[2021-05-14 19:06:22,949] {docker.py:276} INFO - 21/05/14 22:06:22 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-14 19:06:23,107] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1886 bytes result sent to driver
[2021-05-14 19:06:23,108] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (9bbba8114907, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,110] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 174 ms on 9bbba8114907 (executor driver) (127/141)
[2021-05-14 19:06:23,111] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-14 19:06:23,122] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1886 bytes result sent to driver
[2021-05-14 19:06:23,123] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (9bbba8114907, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,125] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 178 ms on 9bbba8114907 (executor driver) (128/141)
[2021-05-14 19:06:23,126] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-14 19:06:23,127] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1886 bytes result sent to driver
[2021-05-14 19:06:23,128] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (9bbba8114907, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,129] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-14 19:06:23,129] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 185 ms on 9bbba8114907 (executor driver) (129/141)
[2021-05-14 19:06:23,130] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1886 bytes result sent to driver
[2021-05-14 19:06:23,131] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (9bbba8114907, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,132] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 191 ms on 9bbba8114907 (executor driver) (130/141)
[2021-05-14 19:06:23,133] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-14 19:06:23,279] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-14 19:06:23,281] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (9bbba8114907, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,282] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 174 ms on 9bbba8114907 (executor driver) (131/141)
[2021-05-14 19:06:23,283] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-14 19:06:23,296] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1843 bytes result sent to driver
[2021-05-14 19:06:23,297] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (9bbba8114907, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,298] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1843 bytes result sent to driver
[2021-05-14 19:06:23,299] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-14 19:06:23,300] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 168 ms on 9bbba8114907 (executor driver) (132/141)
[2021-05-14 19:06:23,300] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (9bbba8114907, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,302] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 174 ms on 9bbba8114907 (executor driver) (133/141)
[2021-05-14 19:06:23,302] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-14 19:06:23,303] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1843 bytes result sent to driver
[2021-05-14 19:06:23,305] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (9bbba8114907, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,305] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-14 19:06:23,306] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 182 ms on 9bbba8114907 (executor driver) (134/141)
[2021-05-14 19:06:23,449] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1843 bytes result sent to driver
[2021-05-14 19:06:23,450] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (9bbba8114907, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,452] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-14 19:06:23,452] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 171 ms on 9bbba8114907 (executor driver) (135/141)
[2021-05-14 19:06:23,468] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1843 bytes result sent to driver
[2021-05-14 19:06:23,469] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (9bbba8114907, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,470] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-14 19:06:23,471] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1843 bytes result sent to driver
[2021-05-14 19:06:23,472] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 171 ms on 9bbba8114907 (executor driver) (136/141)
[2021-05-14 19:06:23,473] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (9bbba8114907, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:23,474] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 170 ms on 9bbba8114907 (executor driver) (137/141)
21/05/14 22:06:23 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-14 19:06:23,477] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1843 bytes result sent to driver
[2021-05-14 19:06:23,478] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 181 ms on 9bbba8114907 (executor driver) (138/141)
[2021-05-14 19:06:23,617] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-14 19:06:23,619] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 169 ms on 9bbba8114907 (executor driver) (139/141)
[2021-05-14 19:06:23,640] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-14 19:06:23,642] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 169 ms on 9bbba8114907 (executor driver) (140/141)
[2021-05-14 19:06:23,646] {docker.py:276} INFO - 21/05/14 22:06:23 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
[2021-05-14 19:06:23,648] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 178 ms on 9bbba8114907 (executor driver) (141/141)
[2021-05-14 19:06:23,648] {docker.py:276} INFO - 21/05/14 22:06:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 19:06:23,649] {docker.py:276} INFO - 21/05/14 22:06:23 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.305 s
[2021-05-14 19:06:23,650] {docker.py:276} INFO - 21/05/14 22:06:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 22:06:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-14 19:06:23,650] {docker.py:276} INFO - 21/05/14 22:06:23 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.351488 s
[2021-05-14 19:06:23,667] {docker.py:276} INFO - 21/05/14 22:06:23 INFO InMemoryFileIndex: It took 6411 ms to list leaf files for 141 paths.
[2021-05-14 19:06:23,813] {docker.py:276} INFO - 21/05/14 22:06:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 9bbba8114907:33069 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 19:06:26,191] {docker.py:276} INFO - 21/05/14 22:06:26 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 19:06:26,196] {docker.py:276} INFO - 21/05/14 22:06:26 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 19:06:26,201] {docker.py:276} INFO - 21/05/14 22:06:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:06:26,672] {docker.py:276} INFO - 21/05/14 22:06:26 INFO CodeGenerator: Code generated in 268.1323 ms
[2021-05-14 19:06:26,684] {docker.py:276} INFO - 21/05/14 22:06:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-14 19:06:26,701] {docker.py:276} INFO - 21/05/14 22:06:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 19:06:26,702] {docker.py:276} INFO - 21/05/14 22:06:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 9bbba8114907:33069 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:06:26,703] {docker.py:276} INFO - 21/05/14 22:06:26 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:26,720] {docker.py:276} INFO - 21/05/14 22:06:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:06:26,823] {docker.py:276} INFO - 21/05/14 22:06:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:26,824] {docker.py:276} INFO - 21/05/14 22:06:26 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/14 22:06:26 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:06:26 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 19:06:26,824] {docker.py:276} INFO - 21/05/14 22:06:26 INFO DAGScheduler: Missing parents: List()
[2021-05-14 19:06:26,825] {docker.py:276} INFO - 21/05/14 22:06:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:06:26,853] {docker.py:276} INFO - 21/05/14 22:06:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 19:06:26,869] {docker.py:276} INFO - 21/05/14 22:06:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 19:06:26,869] {docker.py:276} INFO - 21/05/14 22:06:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 9bbba8114907:33069 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 19:06:26,870] {docker.py:276} INFO - 21/05/14 22:06:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:06:26,871] {docker.py:276} INFO - 21/05/14 22:06:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/14 22:06:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-14 19:06:26,874] {docker.py:276} INFO - 21/05/14 22:06:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (9bbba8114907, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:26,875] {docker.py:276} INFO - 21/05/14 22:06:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-14 19:06:26,970] {docker.py:276} INFO - 21/05/14 22:06:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621026316_to_1621028116.csv, range: 0-111710, partition values: [empty row]
[2021-05-14 19:06:26,999] {docker.py:276} INFO - 21/05/14 22:06:27 INFO CodeGenerator: Code generated in 20.5861 ms
[2021-05-14 19:06:27,408] {docker.py:276} INFO - 21/05/14 22:06:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-14 19:06:27,409] {docker.py:276} INFO - 21/05/14 22:06:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 538 ms on 9bbba8114907 (executor driver) (1/1)
[2021-05-14 19:06:27,410] {docker.py:276} INFO - 21/05/14 22:06:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 19:06:27,410] {docker.py:276} INFO - 21/05/14 22:06:27 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.582 s
[2021-05-14 19:06:27,411] {docker.py:276} INFO - 21/05/14 22:06:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 19:06:27,411] {docker.py:276} INFO - 21/05/14 22:06:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 19:06:27,412] {docker.py:276} INFO - 21/05/14 22:06:27 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.589509 s
[2021-05-14 19:06:27,441] {docker.py:276} INFO - 21/05/14 22:06:27 INFO CodeGenerator: Code generated in 14.1455 ms
[2021-05-14 19:06:27,509] {docker.py:276} INFO - 21/05/14 22:06:27 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 22:06:27 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:06:27,509] {docker.py:276} INFO - 21/05/14 22:06:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 19:06:27,515] {docker.py:276} INFO - 21/05/14 22:06:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-14 19:06:27,540] {docker.py:276} INFO - 21/05/14 22:06:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 9bbba8114907:33069 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 19:06:27,545] {docker.py:276} INFO - 21/05/14 22:06:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 19:06:27,546] {docker.py:276} INFO - 21/05/14 22:06:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 9bbba8114907:33069 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:06:27,548] {docker.py:276} INFO - 21/05/14 22:06:27 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:27,552] {docker.py:276} INFO - 21/05/14 22:06:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:06:27,965] {docker.py:276} INFO - 21/05/14 22:06:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 9bbba8114907:33069 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:06:28,134] {docker.py:276} INFO - 21/05/14 22:06:28 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 22:06:28 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 19:06:28,134] {docker.py:276} INFO - 21/05/14 22:06:28 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 19:06:28,746] {docker.py:276} INFO - 21/05/14 22:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:28,749] {docker.py:276} INFO - 21/05/14 22:06:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:28,749] {docker.py:276} INFO - 21/05/14 22:06:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206287933865312698177981_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206287933865312698177981_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206287933865312698177981_0000}; taskId=attempt_202105142206287933865312698177981_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35b10b5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:28,750] {docker.py:276} INFO - 21/05/14 22:06:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:28,779] {docker.py:276} INFO - 21/05/14 22:06:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:06:28,869] {docker.py:276} INFO - 21/05/14 22:06:28 INFO CodeGenerator: Code generated in 60.115 ms
[2021-05-14 19:06:28,871] {docker.py:276} INFO - 21/05/14 22:06:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 19:06:28,915] {docker.py:276} INFO - 21/05/14 22:06:28 INFO CodeGenerator: Code generated in 36.483 ms
[2021-05-14 19:06:28,918] {docker.py:276} INFO - 21/05/14 22:06:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
[2021-05-14 19:06:28,936] {docker.py:276} INFO - 21/05/14 22:06:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 19:06:28,938] {docker.py:276} INFO - 21/05/14 22:06:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 9bbba8114907:33069 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 19:06:28,940] {docker.py:276} INFO - 21/05/14 22:06:28 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:28,949] {docker.py:276} INFO - 21/05/14 22:06:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 19:06:29,087] {docker.py:276} INFO - 21/05/14 22:06:29 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 19:06:29,091] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 19:06:29,094] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2021-05-14 19:06:29,094] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 22:06:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-14 19:06:29,096] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-14 19:06:29,098] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:06:29,113] {docker.py:276} INFO - 21/05/14 22:06:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-14 19:06:29,123] {docker.py:276} INFO - 21/05/14 22:06:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-14 19:06:29,124] {docker.py:276} INFO - 21/05/14 22:06:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 9bbba8114907:33069 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-14 19:06:29,125] {docker.py:276} INFO - 21/05/14 22:06:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:06:29,127] {docker.py:276} INFO - 21/05/14 22:06:29 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/14 22:06:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-14 19:06:29,129] {docker.py:276} INFO - 21/05/14 22:06:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (9bbba8114907, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:29,130] {docker.py:276} INFO - 21/05/14 22:06:29 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (9bbba8114907, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:29,131] {docker.py:276} INFO - 21/05/14 22:06:29 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (9bbba8114907, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:29,132] {docker.py:276} INFO - 21/05/14 22:06:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (9bbba8114907, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:29,133] {docker.py:276} INFO - 21/05/14 22:06:29 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-14 19:06:29,134] {docker.py:276} INFO - 21/05/14 22:06:29 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-14 19:06:29,134] {docker.py:276} INFO - 21/05/14 22:06:29 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-14 19:06:29,135] {docker.py:276} INFO - 21/05/14 22:06:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-14 19:06:29,250] {docker.py:276} INFO - 21/05/14 22:06:29 INFO CodeGenerator: Code generated in 37.3827 ms
[2021-05-14 19:06:29,252] {docker.py:276} INFO - 21/05/14 22:06:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 9bbba8114907:33069 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 19:06:29,285] {docker.py:276} INFO - 21/05/14 22:06:29 INFO CodeGenerator: Code generated in 13.9175 ms
[2021-05-14 19:06:29,319] {docker.py:276} INFO - 21/05/14 22:06:29 INFO CodeGenerator: Code generated in 25.5168 ms
[2021-05-14 19:06:29,344] {docker.py:276} INFO - 21/05/14 22:06:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620966916_to_1620968716.csv, range: 0-104007, partition values: [empty row]
[2021-05-14 19:06:29,350] {docker.py:276} INFO - 21/05/14 22:06:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621026316_to_1621028116.csv, range: 0-111710, partition values: [empty row]
21/05/14 22:06:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621019116_to_1621020916.csv, range: 0-104261, partition values: [empty row]
[2021-05-14 19:06:29,358] {docker.py:276} INFO - 21/05/14 22:06:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620945316_to_1620947116.csv, range: 0-104192, partition values: [empty row]
[2021-05-14 19:06:30,155] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620993916_to_1620995716.csv, range: 0-103960, partition values: [empty row]
[2021-05-14 19:06:30,211] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620977716_to_1620979516.csv, range: 0-104256, partition values: [empty row]
[2021-05-14 19:06:30,541] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620957916_to_1620959716.csv, range: 0-103948, partition values: [empty row]
[2021-05-14 19:06:30,674] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620948916_to_1620950716.csv, range: 0-104189, partition values: [empty row]
[2021-05-14 19:06:30,704] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621004716_to_1621006516.csv, range: 0-104252, partition values: [empty row]
[2021-05-14 19:06:30,768] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621022716_to_1621024516.csv, range: 0-111675, partition values: [empty row]
[2021-05-14 19:06:30,883] {docker.py:276} INFO - 21/05/14 22:06:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620963316_to_1620965116.csv, range: 0-103945, partition values: [empty row]
[2021-05-14 19:06:31,089] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620983116_to_1620984916.csv, range: 0-104249, partition values: [empty row]
[2021-05-14 19:06:31,181] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620975916_to_1620977716.csv, range: 0-104189, partition values: [empty row]
[2021-05-14 19:06:31,225] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620943516_to_1620945316.csv, range: 0-103943, partition values: [empty row]
[2021-05-14 19:06:31,292] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621024516_to_1621026316.csv, range: 0-111651, partition values: [empty row]
[2021-05-14 19:06:31,441] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620974116_to_1620975916.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 19:06:31,544] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620979516_to_1620981316.csv, range: 0-104185, partition values: [empty row]
[2021-05-14 19:06:31,570] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620965116_to_1620966916.csv, range: 0-103931, partition values: [empty row]
[2021-05-14 19:06:31,644] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620943516_to_1620945316.csv, range: 0-111580, partition values: [empty row]
[2021-05-14 19:06:31,822] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620959716_to_1620961516.csv, range: 0-104245, partition values: [empty row]
[2021-05-14 19:06:31,924] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620988516_to_1620990316.csv, range: 0-103927, partition values: [empty row]
[2021-05-14 19:06:31,926] {docker.py:276} INFO - 21/05/14 22:06:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620981316_to_1620983116.csv, range: 0-104178, partition values: [empty row]
[2021-05-14 19:06:31,996] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620945316_to_1620947116.csv, range: 0-110685, partition values: [empty row]
[2021-05-14 19:06:32,170] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621008316_to_1621010116.csv, range: 0-104245, partition values: [empty row]
[2021-05-14 19:06:32,284] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620983116_to_1620984916.csv, range: 0-103924, partition values: [empty row]
[2021-05-14 19:06:32,304] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620988516_to_1620990316.csv, range: 0-104178, partition values: [empty row]
[2021-05-14 19:06:32,363] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621020916_to_1621022716.csv, range: 0-105267, partition values: [empty row]
[2021-05-14 19:06:32,509] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620970516_to_1620972316.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 19:06:32,641] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621002916_to_1621004716.csv, range: 0-103920, partition values: [empty row]
[2021-05-14 19:06:32,648] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620974116_to_1620975916.csv, range: 0-104171, partition values: [empty row]
[2021-05-14 19:06:32,720] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621026316_to_1621028116.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:06:32,869] {docker.py:276} INFO - 21/05/14 22:06:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621002916_to_1621004716.csv, range: 0-104239, partition values: [empty row]
[2021-05-14 19:06:32,998] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620954316_to_1620956116.csv, range: 0-103914, partition values: [empty row]
[2021-05-14 19:06:33,005] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621001116_to_1621002916.csv, range: 0-104170, partition values: [empty row]
[2021-05-14 19:06:33,083] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621026316_to_1621028116.csv, range: 0-104506, partition values: [empty row]
[2021-05-14 19:06:33,216] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621010116_to_1621011916.csv, range: 0-104239, partition values: [empty row]
[2021-05-14 19:06:33,348] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620956116_to_1620957916.csv, range: 0-103904, partition values: [empty row]
[2021-05-14 19:06:33,359] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621004716_to_1621006516.csv, range: 0-104169, partition values: [empty row]
[2021-05-14 19:06:33,473] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621013716_to_1621015516.csv, range: 0-104388, partition values: [empty row]
[2021-05-14 19:06:33,552] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620992116_to_1620993916.csv, range: 0-104237, partition values: [empty row]
[2021-05-14 19:06:33,714] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620954316_to_1620956116.csv, range: 0-103899, partition values: [empty row]
[2021-05-14 19:06:33,719] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620957916_to_1620959716.csv, range: 0-104160, partition values: [empty row]
[2021-05-14 19:06:33,818] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621024516_to_1621026316.csv, range: 0-104383, partition values: [empty row]
[2021-05-14 19:06:33,888] {docker.py:276} INFO - 21/05/14 22:06:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620970516_to_1620972316.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 19:06:34,065] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621004716_to_1621006516.csv, range: 0-103883, partition values: [empty row]
[2021-05-14 19:06:34,068] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620988516_to_1620990316.csv, range: 0-104159, partition values: [empty row]
[2021-05-14 19:06:34,180] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620984916_to_1620986716.csv, range: 0-104377, partition values: [empty row]
[2021-05-14 19:06:34,242] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621001116_to_1621002916.csv, range: 0-104236, partition values: [empty row]
[2021-05-14 19:06:34,414] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620974116_to_1620975916.csv, range: 0-103882, partition values: [empty row]
[2021-05-14 19:06:34,430] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621006516_to_1621008316.csv, range: 0-104158, partition values: [empty row]
[2021-05-14 19:06:34,521] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621022716_to_1621024516.csv, range: 0-104371, partition values: [empty row]
[2021-05-14 19:06:34,581] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620963316_to_1620965116.csv, range: 0-104234, partition values: [empty row]
[2021-05-14 19:06:34,783] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621020916_to_1621022716.csv, range: 0-104157, partition values: [empty row]
[2021-05-14 19:06:34,858] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620956116_to_1620957916.csv, range: 0-104353, partition values: [empty row]
[2021-05-14 19:06:34,985] {docker.py:276} INFO - 21/05/14 22:06:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620961516_to_1620963316.csv, range: 0-104232, partition values: [empty row]
[2021-05-14 19:06:35,129] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620999316_to_1621001116.csv, range: 0-104154, partition values: [empty row]
[2021-05-14 19:06:35,193] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620961516_to_1620963316.csv, range: 0-104346, partition values: [empty row]
[2021-05-14 19:06:35,334] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620966916_to_1620968716.csv, range: 0-104227, partition values: [empty row]
[2021-05-14 19:06:35,543] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620954316_to_1620956116.csv, range: 0-104331, partition values: [empty row]
[2021-05-14 19:06:35,702] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620981316_to_1620983116.csv, range: 0-104226, partition values: [empty row]
[2021-05-14 19:06:35,769] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621013716_to_1621015516.csv, range: 0-103882, partition values: [empty row]
[2021-05-14 19:06:35,891] {docker.py:276} INFO - 21/05/14 22:06:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620968716_to_1620970516.csv, range: 0-104326, partition values: [empty row]
[2021-05-14 19:06:36,055] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620986716_to_1620988516.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 19:06:36,136] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621011916_to_1621013716.csv, range: 0-103879, partition values: [empty row]
[2021-05-14 19:06:36,235] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621015516_to_1621017316.csv, range: 0-104316, partition values: [empty row]
[2021-05-14 19:06:36,381] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621002916_to_1621004716.csv, range: 0-104154, partition values: [empty row]
[2021-05-14 19:06:36,403] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620992116_to_1620993916.csv, range: 0-104220, partition values: [empty row]
[2021-05-14 19:06:36,499] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620977716_to_1620979516.csv, range: 0-103875, partition values: [empty row]
[2021-05-14 19:06:36,611] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620947116_to_1620948916.csv, range: 0-104315, partition values: [empty row]
[2021-05-14 19:06:36,752] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620956116_to_1620957916.csv, range: 0-104217, partition values: [empty row]
[2021-05-14 19:06:36,875] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620963316_to_1620965116.csv, range: 0-104153, partition values: [empty row]
[2021-05-14 19:06:36,878] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620986716_to_1620988516.csv, range: 0-103865, partition values: [empty row]
[2021-05-14 19:06:36,966] {docker.py:276} INFO - 21/05/14 22:06:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621015516_to_1621017316.csv, range: 0-104303, partition values: [empty row]
[2021-05-14 19:06:37,120] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620999316_to_1621001116.csv, range: 0-104215, partition values: [empty row]
[2021-05-14 19:06:37,238] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621008316_to_1621010116.csv, range: 0-103854, partition values: [empty row]
[2021-05-14 19:06:37,245] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621011916_to_1621013716.csv, range: 0-104150, partition values: [empty row]
[2021-05-14 19:06:37,307] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620993916_to_1620995716.csv, range: 0-104292, partition values: [empty row]
[2021-05-14 19:06:37,503] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620986716_to_1620988516.csv, range: 0-104214, partition values: [empty row]
[2021-05-14 19:06:37,601] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620952516_to_1620954316.csv, range: 0-103843, partition values: [empty row]
[2021-05-14 19:06:37,605] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620990316_to_1620992116.csv, range: 0-104139, partition values: [empty row]
[2021-05-14 19:06:37,655] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620983116_to_1620984916.csv, range: 0-104291, partition values: [empty row]
[2021-05-14 19:06:37,851] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620984916_to_1620986716.csv, range: 0-104211, partition values: [empty row]
[2021-05-14 19:06:37,953] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620975916_to_1620977716.csv, range: 0-104135, partition values: [empty row]
[2021-05-14 19:06:37,959] {docker.py:276} INFO - 21/05/14 22:06:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621020916_to_1621022716.csv, range: 0-103838, partition values: [empty row]
[2021-05-14 19:06:38,003] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620968716_to_1620970516.csv, range: 0-104290, partition values: [empty row]
[2021-05-14 19:06:38,212] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620995716_to_1620997516.csv, range: 0-104211, partition values: [empty row]
[2021-05-14 19:06:38,330] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620959716_to_1620961516.csv, range: 0-103831, partition values: [empty row]
[2021-05-14 19:06:38,347] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621011916_to_1621013716.csv, range: 0-104285, partition values: [empty row]
[2021-05-14 19:06:38,350] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621006516_to_1621008316.csv, range: 0-104132, partition values: [empty row]
[2021-05-14 19:06:38,583] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621017316_to_1621019116.csv, range: 0-104209, partition values: [empty row]
[2021-05-14 19:06:38,719] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620965116_to_1620966916.csv, range: 0-104279, partition values: [empty row]
[2021-05-14 19:06:38,747] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621013716_to_1621015516.csv, range: 0-104127, partition values: [empty row]
[2021-05-14 19:06:38,884] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620999316_to_1621001116.csv, range: 0-103824, partition values: [empty row]
[2021-05-14 19:06:38,944] {docker.py:276} INFO - 21/05/14 22:06:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620990316_to_1620992116.csv, range: 0-104208, partition values: [empty row]
[2021-05-14 19:06:39,061] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620972316_to_1620974116.csv, range: 0-104273, partition values: [empty row]
[2021-05-14 19:06:39,116] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1621008316_to_1621010116.csv, range: 0-104125, partition values: [empty row]
[2021-05-14 19:06:39,234] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620961516_to_1620963316.csv, range: 0-103821, partition values: [empty row]
[2021-05-14 19:06:39,411] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620972316_to_1620974116.csv, range: 0-104272, partition values: [empty row]
[2021-05-14 19:06:39,494] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620997516_to_1620999316.csv, range: 0-104124, partition values: [empty row]
[2021-05-14 19:06:39,597] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620992116_to_1620993916.csv, range: 0-103821, partition values: [empty row]
[2021-05-14 19:06:39,694] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620997516_to_1620999316.csv, range: 0-104205, partition values: [empty row]
[2021-05-14 19:06:39,754] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621019116_to_1621020916.csv, range: 0-104271, partition values: [empty row]
[2021-05-14 19:06:39,872] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620943516_to_1620945316.csv, range: 0-104109, partition values: [empty row]
[2021-05-14 19:06:39,966] {docker.py:276} INFO - 21/05/14 22:06:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620968716_to_1620970516.csv, range: 0-103819, partition values: [empty row]
[2021-05-14 19:06:40,041] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620979516_to_1620981316.csv, range: 0-104203, partition values: [empty row]
[2021-05-14 19:06:40,100] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620995716_to_1620997516.csv, range: 0-104269, partition values: [empty row]
[2021-05-14 19:06:40,226] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620950716_to_1620952516.csv, range: 0-104080, partition values: [empty row]
[2021-05-14 19:06:40,313] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620990316_to_1620992116.csv, range: 0-103816, partition values: [empty row]
[2021-05-14 19:06:40,395] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621010116_to_1621011916.csv, range: 0-104200, partition values: [empty row]
[2021-05-14 19:06:40,439] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620959716_to_1620961516.csv, range: 0-104268, partition values: [empty row]
[2021-05-14 19:06:40,602] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620981316_to_1620983116.csv, range: 0-104072, partition values: [empty row]
[2021-05-14 19:06:40,746] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620952516_to_1620954316.csv, range: 0-104199, partition values: [empty row]
[2021-05-14 19:06:40,813] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620972316_to_1620974116.csv, range: 0-103805, partition values: [empty row]
[2021-05-14 19:06:40,818] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620957916_to_1620959716.csv, range: 0-104264, partition values: [empty row]
[2021-05-14 19:06:40,961] {docker.py:276} INFO - 21/05/14 22:06:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620965116_to_1620966916.csv, range: 0-104053, partition values: [empty row]
[2021-05-14 19:06:41,107] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620977716_to_1620979516.csv, range: 0-104198, partition values: [empty row]
[2021-05-14 19:06:41,157] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621017316_to_1621019116.csv, range: 0-104264, partition values: [empty row]
[2021-05-14 19:06:41,157] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620975916_to_1620977716.csv, range: 0-103793, partition values: [empty row]
[2021-05-14 19:06:41,367] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620966916_to_1620968716.csv, range: 0-104049, partition values: [empty row]
[2021-05-14 19:06:41,466] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620952516_to_1620954316.csv, range: 0-104193, partition values: [empty row]
[2021-05-14 19:06:41,496] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620993916_to_1620995716.csv, range: 0-104261, partition values: [empty row]
[2021-05-14 19:06:41,503] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621017316_to_1621019116.csv, range: 0-103793, partition values: [empty row]
[2021-05-14 19:06:41,729] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620979516_to_1620981316.csv, range: 0-104038, partition values: [empty row]
[2021-05-14 19:06:41,837] {docker.py:276} INFO - 21/05/14 22:06:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620950716_to_1620952516.csv, range: 0-103793, partition values: [empty row]
[2021-05-14 19:06:42,078] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620948916_to_1620950716.csv, range: 0-104025, partition values: [empty row]
[2021-05-14 19:06:42,169] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621024516_to_1621026316.csv, range: 0-103781, partition values: [empty row]
[2021-05-14 19:06:42,288] {docker.py:276} INFO - 21/05/14 22:06:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2722 bytes result sent to driver
[2021-05-14 19:06:42,289] {docker.py:276} INFO - 21/05/14 22:06:42 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (9bbba8114907, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:42,290] {docker.py:276} INFO - 21/05/14 22:06:42 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2722 bytes result sent to driver
[2021-05-14 19:06:42,293] {docker.py:276} INFO - 21/05/14 22:06:42 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 13179 ms on 9bbba8114907 (executor driver) (1/5)
[2021-05-14 19:06:42,294] {docker.py:276} INFO - 21/05/14 22:06:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 13180 ms on 9bbba8114907 (executor driver) (2/5)
[2021-05-14 19:06:42,295] {docker.py:276} INFO - 21/05/14 22:06:42 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-14 19:06:42,307] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620997516_to_1620999316.csv, range: 0-103769, partition values: [empty row]
[2021-05-14 19:06:42,429] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620970516_to_1620972316.csv, range: 0-104022, partition values: [empty row]
[2021-05-14 19:06:42,653] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620995716_to_1620997516.csv, range: 0-103762, partition values: [empty row]
[2021-05-14 19:06:42,708] {docker.py:276} INFO - 21/05/14 22:06:42 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2679 bytes result sent to driver
[2021-05-14 19:06:42,709] {docker.py:276} INFO - 21/05/14 22:06:42 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 13593 ms on 9bbba8114907 (executor driver) (3/5)
[2021-05-14 19:06:42,786] {docker.py:276} INFO - 21/05/14 22:06:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621010116_to_1621011916.csv, range: 0-104010, partition values: [empty row]
[2021-05-14 19:06:43,004] {docker.py:276} INFO - 21/05/14 22:06:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621006516_to_1621008316.csv, range: 0-103752, partition values: [empty row]
[2021-05-14 19:06:43,331] {docker.py:276} INFO - 21/05/14 22:06:43 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-14 19:06:43,332] {docker.py:276} INFO - 21/05/14 22:06:43 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 14218 ms on 9bbba8114907 (executor driver) (4/5)
[2021-05-14 19:06:43,344] {docker.py:276} INFO - 21/05/14 22:06:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620950716_to_1620952516.csv, range: 0-103750, partition values: [empty row]
[2021-05-14 19:06:43,775] {docker.py:276} INFO - 21/05/14 22:06:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621001116_to_1621002916.csv, range: 0-103747, partition values: [empty row]
[2021-05-14 19:06:44,156] {docker.py:276} INFO - 21/05/14 22:06:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1620984916_to_1620986716.csv, range: 0-103691, partition values: [empty row]
[2021-05-14 19:06:44,518] {docker.py:276} INFO - 21/05/14 22:06:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620948916_to_1620950716.csv, range: 0-103661, partition values: [empty row]
[2021-05-14 19:06:44,864] {docker.py:276} INFO - 21/05/14 22:06:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621019116_to_1621020916.csv, range: 0-103646, partition values: [empty row]
[2021-05-14 19:06:45,227] {docker.py:276} INFO - 21/05/14 22:06:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14_19_05_16/from_1621015516_to_1621017316.csv, range: 0-103613, partition values: [empty row]
[2021-05-14 19:06:45,570] {docker.py:276} INFO - 21/05/14 22:06:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620945316_to_1620947116.csv, range: 0-103599, partition values: [empty row]
[2021-05-14 19:06:45,921] {docker.py:276} INFO - 21/05/14 22:06:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14_19_05_16/from_1620947116_to_1620948916.csv, range: 0-103576, partition values: [empty row]
[2021-05-14 19:06:46,274] {docker.py:276} INFO - 21/05/14 22:06:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1621022716_to_1621024516.csv, range: 0-103489, partition values: [empty row]
[2021-05-14 19:06:46,618] {docker.py:276} INFO - 21/05/14 22:06:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14_19_05_16/from_1620947116_to_1620948916.csv, range: 0-103388, partition values: [empty row]
[2021-05-14 19:06:47,093] {docker.py:276} INFO - 21/05/14 22:06:47 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-14 19:06:47,094] {docker.py:276} INFO - 21/05/14 22:06:47 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4811 ms on 9bbba8114907 (executor driver) (5/5)
21/05/14 22:06:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-14 19:06:47,095] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 18.011 s
[2021-05-14 19:06:47,096] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 19:06:47,098] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: running: Set()
[2021-05-14 19:06:47,099] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-14 19:06:47,100] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: failed: Set()
[2021-05-14 19:06:47,104] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 19:06:47,160] {docker.py:276} INFO - 21/05/14 22:06:47 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.1 KiB, free 934.0 MiB)
[2021-05-14 19:06:47,162] {docker.py:276} INFO - 21/05/14 22:06:47 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 933.9 MiB)
[2021-05-14 19:06:47,163] {docker.py:276} INFO - 21/05/14 22:06:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 9bbba8114907:33069 (size: 74.1 KiB, free: 934.3 MiB)
[2021-05-14 19:06:47,165] {docker.py:276} INFO - 21/05/14 22:06:47 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-14 19:06:47,167] {docker.py:276} INFO - 21/05/14 22:06:47 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 22:06:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-14 19:06:47,175] {docker.py:276} INFO - 21/05/14 22:06:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (9bbba8114907, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:47,176] {docker.py:276} INFO - 21/05/14 22:06:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (9bbba8114907, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:47,177] {docker.py:276} INFO - 21/05/14 22:06:47 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (9bbba8114907, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:47,178] {docker.py:276} INFO - 21/05/14 22:06:47 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (9bbba8114907, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:47,179] {docker.py:276} INFO - 21/05/14 22:06:47 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
[2021-05-14 19:06:47,184] {docker.py:276} INFO - 21/05/14 22:06:47 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-14 19:06:47,185] {docker.py:276} INFO - 21/05/14 22:06:47 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-14 19:06:47,196] {docker.py:276} INFO - 21/05/14 22:06:47 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-14 19:06:47,264] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:47,265] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:47,265] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:47,267] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:47,268] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-14 19:06:47,269] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-14 19:06:47,269] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2021-05-14 19:06:47,269] {docker.py:276} INFO - 21/05/14 22:06:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms
[2021-05-14 19:06:47,284] {docker.py:276} INFO - 21/05/14 22:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:47,286] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629985411858212966272_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629985411858212966272_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629985411858212966272_0004}; taskId=attempt_20210514220629985411858212966272_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c6efb41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:47,287] {docker.py:276} INFO - 21/05/14 22:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:47,287] {docker.py:276} INFO - 21/05/14 22:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:47,288] {docker.py:276} INFO - 21/05/14 22:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:06:47,289] {docker.py:276} INFO - 21/05/14 22:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:47,289] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:47,290] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291744924665629629650_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291744924665629629650_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291744924665629629650_0004}; taskId=attempt_202105142206291744924665629629650_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28d14d32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:47,290] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Starting: Task committer attempt_20210514220629985411858212966272_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629985411858212966272_0004_m_000001_289
[2021-05-14 19:06:47,291] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297962913967431333442_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297962913967431333442_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297962913967431333442_0004}; taskId=attempt_202105142206297962913967431333442_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f4d08db}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:47,291] {docker.py:276} INFO - 21/05/14 22:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:47,293] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:47,293] {docker.py:276} INFO - 21/05/14 22:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:47,293] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206297962913967431333442_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297962913967431333442_0004_m_000002_290
[2021-05-14 19:06:47,294] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206291744924665629629650_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291744924665629629650_0004_m_000003_291
[2021-05-14 19:06:47,294] {docker.py:276} INFO - 21/05/14 22:06:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295752968364249458008_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295752968364249458008_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295752968364249458008_0004}; taskId=attempt_202105142206295752968364249458008_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b75ddac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:47,295] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206295752968364249458008_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295752968364249458008_0004_m_000000_288
[2021-05-14 19:06:47,309] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Task committer attempt_20210514220629985411858212966272_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629985411858212966272_0004_m_000001_289 : duration 0:00.018s
[2021-05-14 19:06:47,326] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Task committer attempt_202105142206291744924665629629650_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291744924665629629650_0004_m_000003_291 : duration 0:00.033s
[2021-05-14 19:06:47,327] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Task committer attempt_202105142206295752968364249458008_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295752968364249458008_0004_m_000000_288 : duration 0:00.033s
[2021-05-14 19:06:47,334] {docker.py:276} INFO - 21/05/14 22:06:47 INFO StagingCommitter: Task committer attempt_202105142206297962913967431333442_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297962913967431333442_0004_m_000002_290 : duration 0:00.042s
[2021-05-14 19:06:49,309] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_20210514220629985411858212966272_0004_m_000001_289: needsTaskCommit() Task attempt_20210514220629985411858212966272_0004_m_000001_289
[2021-05-14 19:06:49,310] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_20210514220629985411858212966272_0004_m_000001_289: needsTaskCommit() Task attempt_20210514220629985411858212966272_0004_m_000001_289: duration 0:00.000s
[2021-05-14 19:06:49,311] {docker.py:276} INFO - 21/05/14 22:06:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629985411858212966272_0004_m_000001_289
[2021-05-14 19:06:49,315] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206291744924665629629650_0004_m_000003_291: needsTaskCommit() Task attempt_202105142206291744924665629629650_0004_m_000003_291
[2021-05-14 19:06:49,315] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206291744924665629629650_0004_m_000003_291: needsTaskCommit() Task attempt_202105142206291744924665629629650_0004_m_000003_291: duration 0:00.001s
[2021-05-14 19:06:49,317] {docker.py:276} INFO - 21/05/14 22:06:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291744924665629629650_0004_m_000003_291
[2021-05-14 19:06:49,319] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4630 bytes result sent to driver
[2021-05-14 19:06:49,320] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-14 19:06:49,320] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (9bbba8114907, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:49,322] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-14 19:06:49,322] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (9bbba8114907, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:49,323] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 2149 ms on 9bbba8114907 (executor driver) (1/200)
[2021-05-14 19:06:49,324] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 2149 ms on 9bbba8114907 (executor driver) (2/200)
[2021-05-14 19:06:49,324] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-14 19:06:49,328] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206295752968364249458008_0004_m_000000_288: needsTaskCommit() Task attempt_202105142206295752968364249458008_0004_m_000000_288
[2021-05-14 19:06:49,329] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206295752968364249458008_0004_m_000000_288: needsTaskCommit() Task attempt_202105142206295752968364249458008_0004_m_000000_288: duration 0:00.001s
[2021-05-14 19:06:49,329] {docker.py:276} INFO - 21/05/14 22:06:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295752968364249458008_0004_m_000000_288
[2021-05-14 19:06:49,331] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
[2021-05-14 19:06:49,332] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (9bbba8114907, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:49,333] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2162 ms on 9bbba8114907 (executor driver) (3/200)
[2021-05-14 19:06:49,334] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-14 19:06:49,340] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:49,340] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:49,341] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:49,343] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:49,343] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:49,344] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:49,344] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293474341483668979302_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293474341483668979302_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293474341483668979302_0004}; taskId=attempt_202105142206293474341483668979302_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44bf27c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:49,345] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206293474341483668979302_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293474341483668979302_0004_m_000005_293
[2021-05-14 19:06:49,346] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:06:49,347] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:49,347] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:06:49,347] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:49,348] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:49,348] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298427509638484795210_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298427509638484795210_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298427509638484795210_0004}; taskId=attempt_202105142206298427509638484795210_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ef1cce2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:49,348] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206298427509638484795210_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298427509638484795210_0004_m_000006_294
[2021-05-14 19:06:49,351] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:49,352] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291217502048192430169_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291217502048192430169_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291217502048192430169_0004}; taskId=attempt_202105142206291217502048192430169_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1798b640}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:49,353] {docker.py:276} INFO - 21/05/14 22:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206291217502048192430169_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291217502048192430169_0004_m_000004_292
[2021-05-14 19:06:49,361] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206293474341483668979302_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293474341483668979302_0004_m_000005_293 : duration 0:00.013s
[2021-05-14 19:06:49,361] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206298427509638484795210_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298427509638484795210_0004_m_000006_294 : duration 0:00.014s
[2021-05-14 19:06:49,378] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206291217502048192430169_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291217502048192430169_0004_m_000004_292 : duration 0:00.027s
[2021-05-14 19:06:49,408] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206297962913967431333442_0004_m_000002_290: needsTaskCommit() Task attempt_202105142206297962913967431333442_0004_m_000002_290
[2021-05-14 19:06:49,409] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206297962913967431333442_0004_m_000002_290: needsTaskCommit() Task attempt_202105142206297962913967431333442_0004_m_000002_290: duration 0:00.001s
21/05/14 22:06:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297962913967431333442_0004_m_000002_290
[2021-05-14 19:06:49,410] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-14 19:06:49,411] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (9bbba8114907, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:49,412] {docker.py:276} INFO - 21/05/14 22:06:49 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-14 19:06:49,413] {docker.py:276} INFO - 21/05/14 22:06:49 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 2238 ms on 9bbba8114907 (executor driver) (4/200)
[2021-05-14 19:06:49,422] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:49,422] {docker.py:276} INFO - 21/05/14 22:06:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:49,424] {docker.py:276} INFO - 21/05/14 22:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:49,424] {docker.py:276} INFO - 21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291382591317371289056_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291382591317371289056_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291382591317371289056_0004}; taskId=attempt_202105142206291382591317371289056_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e38de7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:49,425] {docker.py:276} INFO - 21/05/14 22:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:06:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206291382591317371289056_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291382591317371289056_0004_m_000007_295
[2021-05-14 19:06:49,429] {docker.py:276} INFO - 21/05/14 22:06:49 INFO StagingCommitter: Task committer attempt_202105142206291382591317371289056_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291382591317371289056_0004_m_000007_295 : duration 0:00.005s
[2021-05-14 19:06:51,954] {docker.py:276} INFO - 21/05/14 22:06:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206298427509638484795210_0004_m_000006_294: needsTaskCommit() Task attempt_202105142206298427509638484795210_0004_m_000006_294
[2021-05-14 19:06:51,955] {docker.py:276} INFO - 21/05/14 22:06:51 INFO StagingCommitter: Task committer attempt_202105142206298427509638484795210_0004_m_000006_294: needsTaskCommit() Task attempt_202105142206298427509638484795210_0004_m_000006_294: duration 0:00.002s
[2021-05-14 19:06:51,956] {docker.py:276} INFO - 21/05/14 22:06:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298427509638484795210_0004_m_000006_294
[2021-05-14 19:06:51,959] {docker.py:276} INFO - 21/05/14 22:06:51 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4587 bytes result sent to driver
[2021-05-14 19:06:51,961] {docker.py:276} INFO - 21/05/14 22:06:51 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (9bbba8114907, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:51,962] {docker.py:276} INFO - 21/05/14 22:06:51 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-14 19:06:51,963] {docker.py:276} INFO - 21/05/14 22:06:51 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 2601 ms on 9bbba8114907 (executor driver) (5/200)
[2021-05-14 19:06:51,975] {docker.py:276} INFO - 21/05/14 22:06:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:51,975] {docker.py:276} INFO - 21/05/14 22:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:51,978] {docker.py:276} INFO - 21/05/14 22:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:06:51,979] {docker.py:276} INFO - 21/05/14 22:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:51,979] {docker.py:276} INFO - 21/05/14 22:06:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:51,980] {docker.py:276} INFO - 21/05/14 22:06:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294447567241647007873_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294447567241647007873_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294447567241647007873_0004}; taskId=attempt_202105142206294447567241647007873_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@da65159}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:51,980] {docker.py:276} INFO - 21/05/14 22:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:51,980] {docker.py:276} INFO - 21/05/14 22:06:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206294447567241647007873_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294447567241647007873_0004_m_000008_296
[2021-05-14 19:06:51,984] {docker.py:276} INFO - 21/05/14 22:06:51 INFO StagingCommitter: Task committer attempt_202105142206294447567241647007873_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294447567241647007873_0004_m_000008_296 : duration 0:00.004s
[2021-05-14 19:06:52,067] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206291217502048192430169_0004_m_000004_292: needsTaskCommit() Task attempt_202105142206291217502048192430169_0004_m_000004_292
[2021-05-14 19:06:52,067] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_202105142206291217502048192430169_0004_m_000004_292: needsTaskCommit() Task attempt_202105142206291217502048192430169_0004_m_000004_292: duration 0:00.001s
21/05/14 22:06:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291217502048192430169_0004_m_000004_292
[2021-05-14 19:06:52,069] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4587 bytes result sent to driver
[2021-05-14 19:06:52,069] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (9bbba8114907, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:52,070] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-14 19:06:52,072] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 2721 ms on 9bbba8114907 (executor driver) (6/200)
[2021-05-14 19:06:52,081] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206291382591317371289056_0004_m_000007_295: needsTaskCommit() Task attempt_202105142206291382591317371289056_0004_m_000007_295
[2021-05-14 19:06:52,082] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_202105142206291382591317371289056_0004_m_000007_295: needsTaskCommit() Task attempt_202105142206291382591317371289056_0004_m_000007_295: duration 0:00.001s
21/05/14 22:06:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291382591317371289056_0004_m_000007_295
[2021-05-14 19:06:52,083] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-14 19:06:52,084] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (9bbba8114907, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:52,085] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-14 19:06:52,086] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2643 ms on 9bbba8114907 (executor driver) (7/200)
[2021-05-14 19:06:52,088] {docker.py:276} INFO - 21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-14 19:06:52,090] {docker.py:276} INFO - 21/05/14 22:06:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:52,091] {docker.py:276} INFO - 21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297132851478703289942_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297132851478703289942_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297132851478703289942_0004}; taskId=attempt_202105142206297132851478703289942_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@584eac9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206297132851478703289942_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297132851478703289942_0004_m_000009_297
[2021-05-14 19:06:52,095] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_202105142206297132851478703289942_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297132851478703289942_0004_m_000009_297 : duration 0:00.005s
[2021-05-14 19:06:52,098] {docker.py:276} INFO - 21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:06:52,101] {docker.py:276} INFO - 21/05/14 22:06:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:52,101] {docker.py:276} INFO - 21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:52,102] {docker.py:276} INFO - 21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629683958069307036842_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629683958069307036842_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629683958069307036842_0004}; taskId=attempt_20210514220629683958069307036842_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@230f5cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:52,102] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_20210514220629683958069307036842_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629683958069307036842_0004_m_000010_298
[2021-05-14 19:06:52,107] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_20210514220629683958069307036842_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629683958069307036842_0004_m_000010_298 : duration 0:00.005s
[2021-05-14 19:06:52,210] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206293474341483668979302_0004_m_000005_293: needsTaskCommit() Task attempt_202105142206293474341483668979302_0004_m_000005_293
[2021-05-14 19:06:52,210] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_202105142206293474341483668979302_0004_m_000005_293: needsTaskCommit() Task attempt_202105142206293474341483668979302_0004_m_000005_293: duration 0:00.001s
[2021-05-14 19:06:52,211] {docker.py:276} INFO - 21/05/14 22:06:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293474341483668979302_0004_m_000005_293
[2021-05-14 19:06:52,211] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4587 bytes result sent to driver
[2021-05-14 19:06:52,212] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (9bbba8114907, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:52,213] {docker.py:276} INFO - 21/05/14 22:06:52 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 2861 ms on 9bbba8114907 (executor driver) (8/200)
[2021-05-14 19:06:52,214] {docker.py:276} INFO - 21/05/14 22:06:52 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-14 19:06:52,223] {docker.py:276} INFO - 21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:52,223] {docker.py:276} INFO - 21/05/14 22:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:52,225] {docker.py:276} INFO - 21/05/14 22:06:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293987256181511230965_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293987256181511230965_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293987256181511230965_0004}; taskId=attempt_202105142206293987256181511230965_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@729c272d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:52,225] {docker.py:276} INFO - 21/05/14 22:06:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206293987256181511230965_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293987256181511230965_0004_m_000011_299
[2021-05-14 19:06:52,229] {docker.py:276} INFO - 21/05/14 22:06:52 INFO StagingCommitter: Task committer attempt_202105142206293987256181511230965_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293987256181511230965_0004_m_000011_299 : duration 0:00.004s
[2021-05-14 19:06:54,703] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206297132851478703289942_0004_m_000009_297: needsTaskCommit() Task attempt_202105142206297132851478703289942_0004_m_000009_297
[2021-05-14 19:06:54,704] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_202105142206297132851478703289942_0004_m_000009_297: needsTaskCommit() Task attempt_202105142206297132851478703289942_0004_m_000009_297: duration 0:00.003s
[2021-05-14 19:06:54,704] {docker.py:276} INFO - 21/05/14 22:06:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297132851478703289942_0004_m_000009_297
[2021-05-14 19:06:54,706] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-14 19:06:54,708] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (9bbba8114907, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:54,709] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-14 19:06:54,710] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 2643 ms on 9bbba8114907 (executor driver) (9/200)
[2021-05-14 19:06:54,720] {docker.py:276} INFO - 21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:54,723] {docker.py:276} INFO - 21/05/14 22:06:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:54,724] {docker.py:276} INFO - 21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299133209320229231028_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299133209320229231028_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299133209320229231028_0004}; taskId=attempt_202105142206299133209320229231028_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@397754f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:54,724] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206299133209320229231028_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299133209320229231028_0004_m_000012_300
[2021-05-14 19:06:54,729] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_202105142206299133209320229231028_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299133209320229231028_0004_m_000012_300 : duration 0:00.006s
[2021-05-14 19:06:54,772] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_20210514220629683958069307036842_0004_m_000010_298: needsTaskCommit() Task attempt_20210514220629683958069307036842_0004_m_000010_298
[2021-05-14 19:06:54,772] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_20210514220629683958069307036842_0004_m_000010_298: needsTaskCommit() Task attempt_20210514220629683958069307036842_0004_m_000010_298: duration 0:00.001s
21/05/14 22:06:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629683958069307036842_0004_m_000010_298
[2021-05-14 19:06:54,773] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-14 19:06:54,774] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (9bbba8114907, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:54,775] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-14 19:06:54,776] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2694 ms on 9bbba8114907 (executor driver) (10/200)
[2021-05-14 19:06:54,783] {docker.py:276} INFO - 21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:54,785] {docker.py:276} INFO - 21/05/14 22:06:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297183612057988408276_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297183612057988408276_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297183612057988408276_0004}; taskId=attempt_202105142206297183612057988408276_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6612eca4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:54,786] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206297183612057988408276_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297183612057988408276_0004_m_000013_301
[2021-05-14 19:06:54,790] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_202105142206297183612057988408276_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297183612057988408276_0004_m_000013_301 : duration 0:00.005s
[2021-05-14 19:06:54,831] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206294447567241647007873_0004_m_000008_296: needsTaskCommit() Task attempt_202105142206294447567241647007873_0004_m_000008_296
[2021-05-14 19:06:54,832] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_202105142206294447567241647007873_0004_m_000008_296: needsTaskCommit() Task attempt_202105142206294447567241647007873_0004_m_000008_296: duration 0:00.001s
[2021-05-14 19:06:54,832] {docker.py:276} INFO - 21/05/14 22:06:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294447567241647007873_0004_m_000008_296
[2021-05-14 19:06:54,833] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-14 19:06:54,834] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (9bbba8114907, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:54,835] {docker.py:276} INFO - 21/05/14 22:06:54 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-14 19:06:54,835] {docker.py:276} INFO - 21/05/14 22:06:54 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2879 ms on 9bbba8114907 (executor driver) (11/200)
[2021-05-14 19:06:54,847] {docker.py:276} INFO - 21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:54,847] {docker.py:276} INFO - 21/05/14 22:06:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:54,851] {docker.py:276} INFO - 21/05/14 22:06:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:54,851] {docker.py:276} INFO - 21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:54,851] {docker.py:276} INFO - 21/05/14 22:06:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293125205973497023904_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293125205973497023904_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293125205973497023904_0004}; taskId=attempt_202105142206293125205973497023904_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bc5270}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:54,852] {docker.py:276} INFO - 21/05/14 22:06:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:54,852] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206293125205973497023904_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293125205973497023904_0004_m_000014_302
[2021-05-14 19:06:54,856] {docker.py:276} INFO - 21/05/14 22:06:54 INFO StagingCommitter: Task committer attempt_202105142206293125205973497023904_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293125205973497023904_0004_m_000014_302 : duration 0:00.004s
[2021-05-14 19:06:55,091] {docker.py:276} INFO - 21/05/14 22:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206293987256181511230965_0004_m_000011_299: needsTaskCommit() Task attempt_202105142206293987256181511230965_0004_m_000011_299
[2021-05-14 19:06:55,093] {docker.py:276} INFO - 21/05/14 22:06:55 INFO StagingCommitter: Task committer attempt_202105142206293987256181511230965_0004_m_000011_299: needsTaskCommit() Task attempt_202105142206293987256181511230965_0004_m_000011_299: duration 0:00.002s
[2021-05-14 19:06:55,093] {docker.py:276} INFO - 21/05/14 22:06:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293987256181511230965_0004_m_000011_299
[2021-05-14 19:06:55,095] {docker.py:276} INFO - 21/05/14 22:06:55 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-14 19:06:55,096] {docker.py:276} INFO - 21/05/14 22:06:55 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (9bbba8114907, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:55,098] {docker.py:276} INFO - 21/05/14 22:06:55 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
21/05/14 22:06:55 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2888 ms on 9bbba8114907 (executor driver) (12/200)
[2021-05-14 19:06:55,119] {docker.py:276} INFO - 21/05/14 22:06:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:55,119] {docker.py:276} INFO - 21/05/14 22:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:06:55,122] {docker.py:276} INFO - 21/05/14 22:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:55,122] {docker.py:276} INFO - 21/05/14 22:06:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:55,122] {docker.py:276} INFO - 21/05/14 22:06:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298859297129508602436_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298859297129508602436_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298859297129508602436_0004}; taskId=attempt_202105142206298859297129508602436_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6de89400}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:55,123] {docker.py:276} INFO - 21/05/14 22:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:55,123] {docker.py:276} INFO - 21/05/14 22:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206298859297129508602436_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298859297129508602436_0004_m_000015_303
[2021-05-14 19:06:55,127] {docker.py:276} INFO - 21/05/14 22:06:55 INFO StagingCommitter: Task committer attempt_202105142206298859297129508602436_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298859297129508602436_0004_m_000015_303 : duration 0:00.004s
[2021-05-14 19:06:57,339] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206297183612057988408276_0004_m_000013_301: needsTaskCommit() Task attempt_202105142206297183612057988408276_0004_m_000013_301
[2021-05-14 19:06:57,341] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206297183612057988408276_0004_m_000013_301: needsTaskCommit() Task attempt_202105142206297183612057988408276_0004_m_000013_301: duration 0:00.005s
21/05/14 22:06:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297183612057988408276_0004_m_000013_301
[2021-05-14 19:06:57,341] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4587 bytes result sent to driver
[2021-05-14 19:06:57,343] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (9bbba8114907, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:57,345] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2571 ms on 9bbba8114907 (executor driver) (13/200)
[2021-05-14 19:06:57,346] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-14 19:06:57,353] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206299133209320229231028_0004_m_000012_300: needsTaskCommit() Task attempt_202105142206299133209320229231028_0004_m_000012_300
[2021-05-14 19:06:57,354] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206299133209320229231028_0004_m_000012_300: needsTaskCommit() Task attempt_202105142206299133209320229231028_0004_m_000012_300: duration 0:00.001s
[2021-05-14 19:06:57,354] {docker.py:276} INFO - 21/05/14 22:06:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299133209320229231028_0004_m_000012_300
[2021-05-14 19:06:57,355] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4587 bytes result sent to driver
[2021-05-14 19:06:57,356] {docker.py:276} INFO - 21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:57,357] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (9bbba8114907, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:57,358] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2652 ms on 9bbba8114907 (executor driver) (14/200)
[2021-05-14 19:06:57,359] {docker.py:276} INFO - 21/05/14 22:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:06:57,359] {docker.py:276} INFO - 21/05/14 22:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:57,360] {docker.py:276} INFO - 21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296188154780752266107_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296188154780752266107_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296188154780752266107_0004}; taskId=attempt_202105142206296188154780752266107_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10c46e7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206296188154780752266107_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296188154780752266107_0004_m_000016_304
[2021-05-14 19:06:57,360] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
[2021-05-14 19:06:57,363] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206296188154780752266107_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296188154780752266107_0004_m_000016_304 : duration 0:00.004s
[2021-05-14 19:06:57,372] {docker.py:276} INFO - 21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:57,375] {docker.py:276} INFO - 21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:06:57,384] {docker.py:276} INFO - 21/05/14 22:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:57,384] {docker.py:276} INFO - 21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051422062971505069000772150_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_2021051422062971505069000772150_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051422062971505069000772150_0004}; taskId=attempt_2021051422062971505069000772150_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@305c2cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:57,385] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_2021051422062971505069000772150_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_2021051422062971505069000772150_0004_m_000017_305
[2021-05-14 19:06:57,389] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_2021051422062971505069000772150_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_2021051422062971505069000772150_0004_m_000017_305 : duration 0:00.005s
[2021-05-14 19:06:57,549] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206293125205973497023904_0004_m_000014_302: needsTaskCommit() Task attempt_202105142206293125205973497023904_0004_m_000014_302
[2021-05-14 19:06:57,551] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206293125205973497023904_0004_m_000014_302: needsTaskCommit() Task attempt_202105142206293125205973497023904_0004_m_000014_302: duration 0:00.003s
[2021-05-14 19:06:57,551] {docker.py:276} INFO - 21/05/14 22:06:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293125205973497023904_0004_m_000014_302
[2021-05-14 19:06:57,553] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4587 bytes result sent to driver
[2021-05-14 19:06:57,554] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (9bbba8114907, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:57,555] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-14 19:06:57,556] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 2721 ms on 9bbba8114907 (executor driver) (15/200)
[2021-05-14 19:06:57,571] {docker.py:276} INFO - 21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:57,573] {docker.py:276} INFO - 21/05/14 22:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:57,574] {docker.py:276} INFO - 21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297282607143983012057_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297282607143983012057_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297282607143983012057_0004}; taskId=attempt_202105142206297282607143983012057_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@130099e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:57,574] {docker.py:276} INFO - 21/05/14 22:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:57,574] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206297282607143983012057_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297282607143983012057_0004_m_000018_306
[2021-05-14 19:06:57,578] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206297282607143983012057_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297282607143983012057_0004_m_000018_306 : duration 0:00.004s
[2021-05-14 19:06:57,704] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206298859297129508602436_0004_m_000015_303: needsTaskCommit() Task attempt_202105142206298859297129508602436_0004_m_000015_303
[2021-05-14 19:06:57,705] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206298859297129508602436_0004_m_000015_303: needsTaskCommit() Task attempt_202105142206298859297129508602436_0004_m_000015_303: duration 0:00.002s
[2021-05-14 19:06:57,706] {docker.py:276} INFO - 21/05/14 22:06:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298859297129508602436_0004_m_000015_303
[2021-05-14 19:06:57,708] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4587 bytes result sent to driver
[2021-05-14 19:06:57,709] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (9bbba8114907, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:57,710] {docker.py:276} INFO - 21/05/14 22:06:57 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-14 19:06:57,710] {docker.py:276} INFO - 21/05/14 22:06:57 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2615 ms on 9bbba8114907 (executor driver) (16/200)
[2021-05-14 19:06:57,721] {docker.py:276} INFO - 21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:57,723] {docker.py:276} INFO - 21/05/14 22:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:06:57,724] {docker.py:276} INFO - 21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:06:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291884938663670443464_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291884938663670443464_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291884938663670443464_0004}; taskId=attempt_202105142206291884938663670443464_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@91e3174}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:57,724] {docker.py:276} INFO - 21/05/14 22:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:57,724] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206291884938663670443464_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291884938663670443464_0004_m_000019_307
[2021-05-14 19:06:57,728] {docker.py:276} INFO - 21/05/14 22:06:57 INFO StagingCommitter: Task committer attempt_202105142206291884938663670443464_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291884938663670443464_0004_m_000019_307 : duration 0:00.004s
[2021-05-14 19:06:59,972] {docker.py:276} INFO - 21/05/14 22:06:59 INFO StagingCommitter: Starting: Task committer attempt_2021051422062971505069000772150_0004_m_000017_305: needsTaskCommit() Task attempt_2021051422062971505069000772150_0004_m_000017_305
21/05/14 22:06:59 INFO StagingCommitter: Task committer attempt_2021051422062971505069000772150_0004_m_000017_305: needsTaskCommit() Task attempt_2021051422062971505069000772150_0004_m_000017_305: duration 0:00.001s
21/05/14 22:06:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051422062971505069000772150_0004_m_000017_305
[2021-05-14 19:06:59,973] {docker.py:276} INFO - 21/05/14 22:06:59 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-14 19:06:59,973] {docker.py:276} INFO - 21/05/14 22:06:59 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (9bbba8114907, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:06:59,975] {docker.py:276} INFO - 21/05/14 22:06:59 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
21/05/14 22:06:59 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2617 ms on 9bbba8114907 (executor driver) (17/200)
[2021-05-14 19:06:59,983] {docker.py:276} INFO - 21/05/14 22:06:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:06:59,983] {docker.py:276} INFO - 21/05/14 22:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:06:59,986] {docker.py:276} INFO - 21/05/14 22:06:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:06:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:06:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:59,986] {docker.py:276} INFO - 21/05/14 22:06:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292926318771697972752_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292926318771697972752_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292926318771697972752_0004}; taskId=attempt_202105142206292926318771697972752_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21a5bc86}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:06:59,987] {docker.py:276} INFO - 21/05/14 22:06:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:06:59,987] {docker.py:276} INFO - 21/05/14 22:06:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206292926318771697972752_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292926318771697972752_0004_m_000020_308
[2021-05-14 19:06:59,990] {docker.py:276} INFO - 21/05/14 22:06:59 INFO StagingCommitter: Task committer attempt_202105142206292926318771697972752_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292926318771697972752_0004_m_000020_308 : duration 0:00.004s
[2021-05-14 19:07:00,098] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206296188154780752266107_0004_m_000016_304: needsTaskCommit() Task attempt_202105142206296188154780752266107_0004_m_000016_304
[2021-05-14 19:07:00,098] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_202105142206296188154780752266107_0004_m_000016_304: needsTaskCommit() Task attempt_202105142206296188154780752266107_0004_m_000016_304: duration 0:00.003s
21/05/14 22:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296188154780752266107_0004_m_000016_304
[2021-05-14 19:07:00,100] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-14 19:07:00,101] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (9bbba8114907, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:00,102] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-14 19:07:00,103] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2761 ms on 9bbba8114907 (executor driver) (18/200)
[2021-05-14 19:07:00,112] {docker.py:276} INFO - 21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Getting 5 (40.3 KiB) non-empty blocks including 5 (40.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:00,113] {docker.py:276} INFO - 21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:00,115] {docker.py:276} INFO - 21/05/14 22:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:00,115] {docker.py:276} INFO - 21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629190114721592167500_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629190114721592167500_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629190114721592167500_0004}; taskId=attempt_20210514220629190114721592167500_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72a51510}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:00,116] {docker.py:276} INFO - 21/05/14 22:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_20210514220629190114721592167500_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629190114721592167500_0004_m_000021_309
[2021-05-14 19:07:00,120] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_20210514220629190114721592167500_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629190114721592167500_0004_m_000021_309 : duration 0:00.003s
[2021-05-14 19:07:00,139] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206297282607143983012057_0004_m_000018_306: needsTaskCommit() Task attempt_202105142206297282607143983012057_0004_m_000018_306
[2021-05-14 19:07:00,140] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_202105142206297282607143983012057_0004_m_000018_306: needsTaskCommit() Task attempt_202105142206297282607143983012057_0004_m_000018_306: duration 0:00.001s
21/05/14 22:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297282607143983012057_0004_m_000018_306
[2021-05-14 19:07:00,141] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-14 19:07:00,142] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (9bbba8114907, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:00,143] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2591 ms on 9bbba8114907 (executor driver) (19/200)
[2021-05-14 19:07:00,144] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-14 19:07:00,151] {docker.py:276} INFO - 21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:00,154] {docker.py:276} INFO - 21/05/14 22:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:00,154] {docker.py:276} INFO - 21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296689643569688561926_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296689643569688561926_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296689643569688561926_0004}; taskId=attempt_202105142206296689643569688561926_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40d72d12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:00,154] {docker.py:276} INFO - 21/05/14 22:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206296689643569688561926_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296689643569688561926_0004_m_000022_310
[2021-05-14 19:07:00,158] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_202105142206296689643569688561926_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296689643569688561926_0004_m_000022_310 : duration 0:00.004s
[2021-05-14 19:07:00,291] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206291884938663670443464_0004_m_000019_307: needsTaskCommit() Task attempt_202105142206291884938663670443464_0004_m_000019_307
[2021-05-14 19:07:00,292] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_202105142206291884938663670443464_0004_m_000019_307: needsTaskCommit() Task attempt_202105142206291884938663670443464_0004_m_000019_307: duration 0:00.001s
[2021-05-14 19:07:00,292] {docker.py:276} INFO - 21/05/14 22:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291884938663670443464_0004_m_000019_307
[2021-05-14 19:07:00,293] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-14 19:07:00,293] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (9bbba8114907, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:00,294] {docker.py:276} INFO - 21/05/14 22:07:00 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-14 19:07:00,295] {docker.py:276} INFO - 21/05/14 22:07:00 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2587 ms on 9bbba8114907 (executor driver) (20/200)
[2021-05-14 19:07:00,303] {docker.py:276} INFO - 21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:00,303] {docker.py:276} INFO - 21/05/14 22:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:00,306] {docker.py:276} INFO - 21/05/14 22:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298778516774226896782_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298778516774226896782_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298778516774226896782_0004}; taskId=attempt_202105142206298778516774226896782_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51421a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206298778516774226896782_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298778516774226896782_0004_m_000023_311
[2021-05-14 19:07:00,311] {docker.py:276} INFO - 21/05/14 22:07:00 INFO StagingCommitter: Task committer attempt_202105142206298778516774226896782_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298778516774226896782_0004_m_000023_311 : duration 0:00.005s
[2021-05-14 19:07:02,683] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206296689643569688561926_0004_m_000022_310: needsTaskCommit() Task attempt_202105142206296689643569688561926_0004_m_000022_310
[2021-05-14 19:07:02,684] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206296689643569688561926_0004_m_000022_310: needsTaskCommit() Task attempt_202105142206296689643569688561926_0004_m_000022_310: duration 0:00.001s
21/05/14 22:07:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296689643569688561926_0004_m_000022_310
[2021-05-14 19:07:02,685] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4587 bytes result sent to driver
[2021-05-14 19:07:02,686] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (9bbba8114907, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:02,687] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2546 ms on 9bbba8114907 (executor driver) (21/200)
[2021-05-14 19:07:02,688] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-14 19:07:02,697] {docker.py:276} INFO - 21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:02,699] {docker.py:276} INFO - 21/05/14 22:07:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629242140488162886680_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629242140488162886680_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629242140488162886680_0004}; taskId=attempt_20210514220629242140488162886680_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48e736ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_20210514220629242140488162886680_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629242140488162886680_0004_m_000024_312
[2021-05-14 19:07:02,700] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206292926318771697972752_0004_m_000020_308: needsTaskCommit() Task attempt_202105142206292926318771697972752_0004_m_000020_308
[2021-05-14 19:07:02,701] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206292926318771697972752_0004_m_000020_308: needsTaskCommit() Task attempt_202105142206292926318771697972752_0004_m_000020_308: duration 0:00.001s
21/05/14 22:07:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292926318771697972752_0004_m_000020_308
[2021-05-14 19:07:02,701] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4587 bytes result sent to driver
[2021-05-14 19:07:02,702] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (9bbba8114907, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:02,703] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2733 ms on 9bbba8114907 (executor driver) (22/200)
[2021-05-14 19:07:02,704] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-14 19:07:02,717] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_20210514220629242140488162886680_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629242140488162886680_0004_m_000024_312 : duration 0:00.018s
[2021-05-14 19:07:02,718] {docker.py:276} INFO - 21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:02,718] {docker.py:276} INFO - 21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:02,720] {docker.py:276} INFO - 21/05/14 22:07:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:02,720] {docker.py:276} INFO - 21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291575020099673226742_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291575020099673226742_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291575020099673226742_0004}; taskId=attempt_202105142206291575020099673226742_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7884ee05}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:02,721] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206291575020099673226742_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291575020099673226742_0004_m_000025_313
[2021-05-14 19:07:02,724] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206291575020099673226742_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291575020099673226742_0004_m_000025_313 : duration 0:00.004s
[2021-05-14 19:07:02,868] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_20210514220629190114721592167500_0004_m_000021_309: needsTaskCommit() Task attempt_20210514220629190114721592167500_0004_m_000021_309
[2021-05-14 19:07:02,869] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_20210514220629190114721592167500_0004_m_000021_309: needsTaskCommit() Task attempt_20210514220629190114721592167500_0004_m_000021_309: duration 0:00.002s
21/05/14 22:07:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629190114721592167500_0004_m_000021_309
[2021-05-14 19:07:02,869] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4587 bytes result sent to driver
[2021-05-14 19:07:02,871] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (9bbba8114907, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:02,872] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206298778516774226896782_0004_m_000023_311: needsTaskCommit() Task attempt_202105142206298778516774226896782_0004_m_000023_311
[2021-05-14 19:07:02,873] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206298778516774226896782_0004_m_000023_311: needsTaskCommit() Task attempt_202105142206298778516774226896782_0004_m_000023_311: duration 0:00.001s
21/05/14 22:07:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298778516774226896782_0004_m_000023_311
[2021-05-14 19:07:02,874] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 2773 ms on 9bbba8114907 (executor driver) (23/200)
[2021-05-14 19:07:02,874] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4587 bytes result sent to driver
[2021-05-14 19:07:02,875] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-14 19:07:02,876] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (9bbba8114907, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:02,877] {docker.py:276} INFO - 21/05/14 22:07:02 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2585 ms on 9bbba8114907 (executor driver) (24/200)
[2021-05-14 19:07:02,877] {docker.py:276} INFO - 21/05/14 22:07:02 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
[2021-05-14 19:07:02,886] {docker.py:276} INFO - 21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:02,888] {docker.py:276} INFO - 21/05/14 22:07:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:02,888] {docker.py:276} INFO - 21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291173524011536047526_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291173524011536047526_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291173524011536047526_0004}; taskId=attempt_202105142206291173524011536047526_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20cd2d9c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206291173524011536047526_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291173524011536047526_0004_m_000027_315
[2021-05-14 19:07:02,891] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206291173524011536047526_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291173524011536047526_0004_m_000027_315 : duration 0:00.003s
[2021-05-14 19:07:02,895] {docker.py:276} INFO - 21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 19:07:02,898] {docker.py:276} INFO - 21/05/14 22:07:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:02,901] {docker.py:276} INFO - 21/05/14 22:07:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297212246124895324966_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297212246124895324966_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297212246124895324966_0004}; taskId=attempt_202105142206297212246124895324966_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fd44e2e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206297212246124895324966_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297212246124895324966_0004_m_000026_314
[2021-05-14 19:07:02,905] {docker.py:276} INFO - 21/05/14 22:07:02 INFO StagingCommitter: Task committer attempt_202105142206297212246124895324966_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297212246124895324966_0004_m_000026_314 : duration 0:00.003s
[2021-05-14 19:07:05,374] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_20210514220629242140488162886680_0004_m_000024_312: needsTaskCommit() Task attempt_20210514220629242140488162886680_0004_m_000024_312
[2021-05-14 19:07:05,375] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_20210514220629242140488162886680_0004_m_000024_312: needsTaskCommit() Task attempt_20210514220629242140488162886680_0004_m_000024_312: duration 0:00.003s
21/05/14 22:07:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629242140488162886680_0004_m_000024_312
[2021-05-14 19:07:05,378] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-14 19:07:05,379] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (9bbba8114907, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:05,380] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2695 ms on 9bbba8114907 (executor driver) (25/200)
[2021-05-14 19:07:05,381] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-14 19:07:05,391] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:05,391] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:05,393] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:05,394] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,395] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291112528926283703603_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291112528926283703603_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291112528926283703603_0004}; taskId=attempt_202105142206291112528926283703603_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a3ba535}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206291112528926283703603_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291112528926283703603_0004_m_000028_316
[2021-05-14 19:07:05,397] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206291112528926283703603_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291112528926283703603_0004_m_000028_316 : duration 0:00.004s
[2021-05-14 19:07:05,453] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206297212246124895324966_0004_m_000026_314: needsTaskCommit() Task attempt_202105142206297212246124895324966_0004_m_000026_314
[2021-05-14 19:07:05,454] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206297212246124895324966_0004_m_000026_314: needsTaskCommit() Task attempt_202105142206297212246124895324966_0004_m_000026_314: duration 0:00.002s
[2021-05-14 19:07:05,454] {docker.py:276} INFO - 21/05/14 22:07:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297212246124895324966_0004_m_000026_314
[2021-05-14 19:07:05,455] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
[2021-05-14 19:07:05,456] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (9bbba8114907, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:05,457] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2587 ms on 9bbba8114907 (executor driver) (26/200)
[2021-05-14 19:07:05,457] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-14 19:07:05,465] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:05,465] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:05,467] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:05,468] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:05,468] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,468] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298686258117309275184_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686258117309275184_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298686258117309275184_0004}; taskId=attempt_202105142206298686258117309275184_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@deb7fdc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,469] {docker.py:276} INFO - 21/05/14 22:07:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:05,469] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206298686258117309275184_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686258117309275184_0004_m_000029_317
[2021-05-14 19:07:05,471] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206298686258117309275184_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686258117309275184_0004_m_000029_317 : duration 0:00.003s
[2021-05-14 19:07:05,545] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206291575020099673226742_0004_m_000025_313: needsTaskCommit() Task attempt_202105142206291575020099673226742_0004_m_000025_313
[2021-05-14 19:07:05,545] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206291575020099673226742_0004_m_000025_313: needsTaskCommit() Task attempt_202105142206291575020099673226742_0004_m_000025_313: duration 0:00.002s
21/05/14 22:07:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291575020099673226742_0004_m_000025_313
[2021-05-14 19:07:05,546] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-14 19:07:05,547] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (9bbba8114907, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:05,548] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
[2021-05-14 19:07:05,557] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2857 ms on 9bbba8114907 (executor driver) (27/200)
[2021-05-14 19:07:05,565] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:05,566] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:05,568] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,569] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293884181424542843709_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293884181424542843709_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293884181424542843709_0004}; taskId=attempt_202105142206293884181424542843709_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@409d5c00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,569] {docker.py:276} INFO - 21/05/14 22:07:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206293884181424542843709_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293884181424542843709_0004_m_000030_318
[2021-05-14 19:07:05,572] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206293884181424542843709_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293884181424542843709_0004_m_000030_318 : duration 0:00.003s
[2021-05-14 19:07:05,762] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206291173524011536047526_0004_m_000027_315: needsTaskCommit() Task attempt_202105142206291173524011536047526_0004_m_000027_315
[2021-05-14 19:07:05,763] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206291173524011536047526_0004_m_000027_315: needsTaskCommit() Task attempt_202105142206291173524011536047526_0004_m_000027_315: duration 0:00.003s
[2021-05-14 19:07:05,764] {docker.py:276} INFO - 21/05/14 22:07:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291173524011536047526_0004_m_000027_315
[2021-05-14 19:07:05,765] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4587 bytes result sent to driver
[2021-05-14 19:07:05,767] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (9bbba8114907, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:05,768] {docker.py:276} INFO - 21/05/14 22:07:05 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2895 ms on 9bbba8114907 (executor driver) (28/200)
[2021-05-14 19:07:05,769] {docker.py:276} INFO - 21/05/14 22:07:05 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-14 19:07:05,779] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:05,779] {docker.py:276} INFO - 21/05/14 22:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:05,781] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:05,782] {docker.py:276} INFO - 21/05/14 22:07:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:05,782] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,782] {docker.py:276} INFO - 21/05/14 22:07:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291186352190370310314_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291186352190370310314_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291186352190370310314_0004}; taskId=attempt_202105142206291186352190370310314_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2091992b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:05,783] {docker.py:276} INFO - 21/05/14 22:07:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:05,783] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206291186352190370310314_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291186352190370310314_0004_m_000031_319
[2021-05-14 19:07:05,786] {docker.py:276} INFO - 21/05/14 22:07:05 INFO StagingCommitter: Task committer attempt_202105142206291186352190370310314_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291186352190370310314_0004_m_000031_319 : duration 0:00.002s
[2021-05-14 19:07:08,062] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206298686258117309275184_0004_m_000029_317: needsTaskCommit() Task attempt_202105142206298686258117309275184_0004_m_000029_317
[2021-05-14 19:07:08,063] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206298686258117309275184_0004_m_000029_317: needsTaskCommit() Task attempt_202105142206298686258117309275184_0004_m_000029_317: duration 0:00.004s
21/05/14 22:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298686258117309275184_0004_m_000029_317
[2021-05-14 19:07:08,065] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4587 bytes result sent to driver
[2021-05-14 19:07:08,066] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (9bbba8114907, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:08,068] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-14 19:07:08,068] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 2614 ms on 9bbba8114907 (executor driver) (29/200)
[2021-05-14 19:07:08,078] {docker.py:276} INFO - 21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:08,081] {docker.py:276} INFO - 21/05/14 22:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:08,081] {docker.py:276} INFO - 21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:08,082] {docker.py:276} INFO - 21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292399303732607384738_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292399303732607384738_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292399303732607384738_0004}; taskId=attempt_202105142206292399303732607384738_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dbe6a27}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206292399303732607384738_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292399303732607384738_0004_m_000032_320
[2021-05-14 19:07:08,085] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206292399303732607384738_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292399303732607384738_0004_m_000032_320 : duration 0:00.003s
[2021-05-14 19:07:08,151] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206291112528926283703603_0004_m_000028_316: needsTaskCommit() Task attempt_202105142206291112528926283703603_0004_m_000028_316
[2021-05-14 19:07:08,152] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206291112528926283703603_0004_m_000028_316: needsTaskCommit() Task attempt_202105142206291112528926283703603_0004_m_000028_316: duration 0:00.001s
21/05/14 22:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291112528926283703603_0004_m_000028_316
[2021-05-14 19:07:08,153] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
[2021-05-14 19:07:08,153] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (9bbba8114907, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:08,154] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-14 19:07:08,155] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2777 ms on 9bbba8114907 (executor driver) (30/200)
[2021-05-14 19:07:08,162] {docker.py:276} INFO - 21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:08,165] {docker.py:276} INFO - 21/05/14 22:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296971665737551681542_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296971665737551681542_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296971665737551681542_0004}; taskId=attempt_202105142206296971665737551681542_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@213beba6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:08,165] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206296971665737551681542_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296971665737551681542_0004_m_000033_321
[2021-05-14 19:07:08,169] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206296971665737551681542_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296971665737551681542_0004_m_000033_321 : duration 0:00.004s
[2021-05-14 19:07:08,185] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206293884181424542843709_0004_m_000030_318: needsTaskCommit() Task attempt_202105142206293884181424542843709_0004_m_000030_318
[2021-05-14 19:07:08,185] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206293884181424542843709_0004_m_000030_318: needsTaskCommit() Task attempt_202105142206293884181424542843709_0004_m_000030_318: duration 0:00.001s
21/05/14 22:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293884181424542843709_0004_m_000030_318
[2021-05-14 19:07:08,186] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4587 bytes result sent to driver
[2021-05-14 19:07:08,187] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (9bbba8114907, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:08,188] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-14 19:07:08,188] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2643 ms on 9bbba8114907 (executor driver) (31/200)
[2021-05-14 19:07:08,196] {docker.py:276} INFO - 21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:08,198] {docker.py:276} INFO - 21/05/14 22:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292499959842712291559_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292499959842712291559_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292499959842712291559_0004}; taskId=attempt_202105142206292499959842712291559_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27c3f7b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206292499959842712291559_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292499959842712291559_0004_m_000034_322
[2021-05-14 19:07:08,200] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206292499959842712291559_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292499959842712291559_0004_m_000034_322 : duration 0:00.003s
[2021-05-14 19:07:08,384] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206291186352190370310314_0004_m_000031_319: needsTaskCommit() Task attempt_202105142206291186352190370310314_0004_m_000031_319
[2021-05-14 19:07:08,385] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_202105142206291186352190370310314_0004_m_000031_319: needsTaskCommit() Task attempt_202105142206291186352190370310314_0004_m_000031_319: duration 0:00.002s
21/05/14 22:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291186352190370310314_0004_m_000031_319
[2021-05-14 19:07:08,386] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
[2021-05-14 19:07:08,387] {docker.py:276} INFO - 21/05/14 22:07:08 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (9bbba8114907, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:08,388] {docker.py:276} INFO - 21/05/14 22:07:08 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
21/05/14 22:07:08 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2624 ms on 9bbba8114907 (executor driver) (32/200)
[2021-05-14 19:07:08,396] {docker.py:276} INFO - 21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:08,399] {docker.py:276} INFO - 21/05/14 22:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629237513487623383049_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629237513487623383049_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629237513487623383049_0004}; taskId=attempt_20210514220629237513487623383049_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7282c879}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:08,399] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Starting: Task committer attempt_20210514220629237513487623383049_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629237513487623383049_0004_m_000035_323
[2021-05-14 19:07:08,402] {docker.py:276} INFO - 21/05/14 22:07:08 INFO StagingCommitter: Task committer attempt_20210514220629237513487623383049_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629237513487623383049_0004_m_000035_323 : duration 0:00.004s
[2021-05-14 19:07:10,915] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206296971665737551681542_0004_m_000033_321: needsTaskCommit() Task attempt_202105142206296971665737551681542_0004_m_000033_321
[2021-05-14 19:07:10,916] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206296971665737551681542_0004_m_000033_321: needsTaskCommit() Task attempt_202105142206296971665737551681542_0004_m_000033_321: duration 0:00.001s
[2021-05-14 19:07:10,917] {docker.py:276} INFO - 21/05/14 22:07:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296971665737551681542_0004_m_000033_321
[2021-05-14 19:07:10,919] {docker.py:276} INFO - 21/05/14 22:07:10 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4587 bytes result sent to driver
[2021-05-14 19:07:10,921] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (9bbba8114907, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:10,922] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2770 ms on 9bbba8114907 (executor driver) (33/200)
21/05/14 22:07:10 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-14 19:07:10,935] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206292399303732607384738_0004_m_000032_320: needsTaskCommit() Task attempt_202105142206292399303732607384738_0004_m_000032_320
[2021-05-14 19:07:10,936] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206292399303732607384738_0004_m_000032_320: needsTaskCommit() Task attempt_202105142206292399303732607384738_0004_m_000032_320: duration 0:00.000s
21/05/14 22:07:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292399303732607384738_0004_m_000032_320
[2021-05-14 19:07:10,938] {docker.py:276} INFO - 21/05/14 22:07:10 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4587 bytes result sent to driver
[2021-05-14 19:07:10,939] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (9bbba8114907, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:10,939] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2875 ms on 9bbba8114907 (executor driver) (34/200)
[2021-05-14 19:07:10,939] {docker.py:276} INFO - 21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:07:10,940] {docker.py:276} INFO - 21/05/14 22:07:10 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-14 19:07:10,941] {docker.py:276} INFO - 21/05/14 22:07:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292514153924377367698_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292514153924377367698_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292514153924377367698_0004}; taskId=attempt_202105142206292514153924377367698_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a0c278a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:10,942] {docker.py:276} INFO - 21/05/14 22:07:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206292514153924377367698_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292514153924377367698_0004_m_000036_324
[2021-05-14 19:07:10,945] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206292514153924377367698_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292514153924377367698_0004_m_000036_324 : duration 0:00.003s
[2021-05-14 19:07:10,949] {docker.py:276} INFO - 21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:10,952] {docker.py:276} INFO - 21/05/14 22:07:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:10,953] {docker.py:276} INFO - 21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298836286143403133392_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298836286143403133392_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298836286143403133392_0004}; taskId=attempt_202105142206298836286143403133392_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66629195}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206298836286143403133392_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298836286143403133392_0004_m_000037_325
[2021-05-14 19:07:10,954] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206298836286143403133392_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298836286143403133392_0004_m_000037_325 : duration 0:00.003s
[2021-05-14 19:07:10,966] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206292499959842712291559_0004_m_000034_322: needsTaskCommit() Task attempt_202105142206292499959842712291559_0004_m_000034_322
[2021-05-14 19:07:10,966] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206292499959842712291559_0004_m_000034_322: needsTaskCommit() Task attempt_202105142206292499959842712291559_0004_m_000034_322: duration 0:00.000s
21/05/14 22:07:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292499959842712291559_0004_m_000034_322
[2021-05-14 19:07:10,967] {docker.py:276} INFO - 21/05/14 22:07:10 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4587 bytes result sent to driver
[2021-05-14 19:07:10,969] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (9bbba8114907, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:10,969] {docker.py:276} INFO - 21/05/14 22:07:10 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-14 19:07:10,970] {docker.py:276} INFO - 21/05/14 22:07:10 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2785 ms on 9bbba8114907 (executor driver) (35/200)
[2021-05-14 19:07:10,979] {docker.py:276} INFO - 21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:10,980] {docker.py:276} INFO - 21/05/14 22:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:10,982] {docker.py:276} INFO - 21/05/14 22:07:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:10,983] {docker.py:276} INFO - 21/05/14 22:07:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:10,983] {docker.py:276} INFO - 21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:10,983] {docker.py:276} INFO - 21/05/14 22:07:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296945434312144567024_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296945434312144567024_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296945434312144567024_0004}; taskId=attempt_202105142206296945434312144567024_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1eed28dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:10,984] {docker.py:276} INFO - 21/05/14 22:07:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:10,984] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206296945434312144567024_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296945434312144567024_0004_m_000038_326
[2021-05-14 19:07:10,986] {docker.py:276} INFO - 21/05/14 22:07:10 INFO StagingCommitter: Task committer attempt_202105142206296945434312144567024_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296945434312144567024_0004_m_000038_326 : duration 0:00.003s
[2021-05-14 19:07:11,036] {docker.py:276} INFO - 21/05/14 22:07:11 INFO StagingCommitter: Starting: Task committer attempt_20210514220629237513487623383049_0004_m_000035_323: needsTaskCommit() Task attempt_20210514220629237513487623383049_0004_m_000035_323
[2021-05-14 19:07:11,037] {docker.py:276} INFO - 21/05/14 22:07:11 INFO StagingCommitter: Task committer attempt_20210514220629237513487623383049_0004_m_000035_323: needsTaskCommit() Task attempt_20210514220629237513487623383049_0004_m_000035_323: duration 0:00.001s
21/05/14 22:07:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629237513487623383049_0004_m_000035_323
[2021-05-14 19:07:11,038] {docker.py:276} INFO - 21/05/14 22:07:11 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4587 bytes result sent to driver
[2021-05-14 19:07:11,040] {docker.py:276} INFO - 21/05/14 22:07:11 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (9bbba8114907, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:11,040] {docker.py:276} INFO - 21/05/14 22:07:11 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2656 ms on 9bbba8114907 (executor driver) (36/200)
[2021-05-14 19:07:11,041] {docker.py:276} INFO - 21/05/14 22:07:11 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-14 19:07:11,049] {docker.py:276} INFO - 21/05/14 22:07:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:11,052] {docker.py:276} INFO - 21/05/14 22:07:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294188575223242417876_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294188575223242417876_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294188575223242417876_0004}; taskId=attempt_202105142206294188575223242417876_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e794850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:11,052] {docker.py:276} INFO - 21/05/14 22:07:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:11 INFO StagingCommitter: Starting: Task committer attempt_202105142206294188575223242417876_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294188575223242417876_0004_m_000039_327
[2021-05-14 19:07:11,055] {docker.py:276} INFO - 21/05/14 22:07:11 INFO StagingCommitter: Task committer attempt_202105142206294188575223242417876_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294188575223242417876_0004_m_000039_327 : duration 0:00.003s
[2021-05-14 19:07:13,553] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206298836286143403133392_0004_m_000037_325: needsTaskCommit() Task attempt_202105142206298836286143403133392_0004_m_000037_325
21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206298836286143403133392_0004_m_000037_325: needsTaskCommit() Task attempt_202105142206298836286143403133392_0004_m_000037_325: duration 0:00.001s
21/05/14 22:07:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298836286143403133392_0004_m_000037_325
[2021-05-14 19:07:13,555] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206292514153924377367698_0004_m_000036_324: needsTaskCommit() Task attempt_202105142206292514153924377367698_0004_m_000036_324
[2021-05-14 19:07:13,556] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206292514153924377367698_0004_m_000036_324: needsTaskCommit() Task attempt_202105142206292514153924377367698_0004_m_000036_324: duration 0:00.001s
21/05/14 22:07:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292514153924377367698_0004_m_000036_324
[2021-05-14 19:07:13,558] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (9bbba8114907, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:13,558] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-14 19:07:13,559] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-14 19:07:13,559] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (9bbba8114907, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:13,560] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2623 ms on 9bbba8114907 (executor driver) (37/200)
[2021-05-14 19:07:13,560] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2642 ms on 9bbba8114907 (executor driver) (38/200)
[2021-05-14 19:07:13,561] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-14 19:07:13,581] {docker.py:276} INFO - 21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:13,581] {docker.py:276} INFO - 21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:07:13,583] {docker.py:276} INFO - 21/05/14 22:07:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:13,584] {docker.py:276} INFO - 21/05/14 22:07:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:13,585] {docker.py:276} INFO - 21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:13,585] {docker.py:276} INFO - 21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292311484938262700235_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292311484938262700235_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292311484938262700235_0004}; taskId=attempt_202105142206292311484938262700235_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5cf6832c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:13,586] {docker.py:276} INFO - 21/05/14 22:07:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:13,586] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206292311484938262700235_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292311484938262700235_0004_m_000040_328
[2021-05-14 19:07:13,587] {docker.py:276} INFO - 21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:13,587] {docker.py:276} INFO - 21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:13,589] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206292311484938262700235_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292311484938262700235_0004_m_000040_328 : duration 0:00.003s
[2021-05-14 19:07:13,590] {docker.py:276} INFO - 21/05/14 22:07:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:13,590] {docker.py:276} INFO - 21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:13,590] {docker.py:276} INFO - 21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292576570234516500882_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292576570234516500882_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292576570234516500882_0004}; taskId=attempt_202105142206292576570234516500882_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64807240}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:13,591] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206292576570234516500882_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292576570234516500882_0004_m_000041_329
[2021-05-14 19:07:13,593] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206292576570234516500882_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292576570234516500882_0004_m_000041_329 : duration 0:00.004s
[2021-05-14 19:07:13,765] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206294188575223242417876_0004_m_000039_327: needsTaskCommit() Task attempt_202105142206294188575223242417876_0004_m_000039_327
[2021-05-14 19:07:13,766] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206294188575223242417876_0004_m_000039_327: needsTaskCommit() Task attempt_202105142206294188575223242417876_0004_m_000039_327: duration 0:00.001s
21/05/14 22:07:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294188575223242417876_0004_m_000039_327
[2021-05-14 19:07:13,767] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4587 bytes result sent to driver
[2021-05-14 19:07:13,768] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (9bbba8114907, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:13,769] {docker.py:276} INFO - 21/05/14 22:07:13 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-14 19:07:13,770] {docker.py:276} INFO - 21/05/14 22:07:13 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2732 ms on 9bbba8114907 (executor driver) (39/200)
[2021-05-14 19:07:13,779] {docker.py:276} INFO - 21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:13,781] {docker.py:276} INFO - 21/05/14 22:07:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:13,781] {docker.py:276} INFO - 21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297815685795654748407_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297815685795654748407_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297815685795654748407_0004}; taskId=attempt_202105142206297815685795654748407_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a478355}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:13,782] {docker.py:276} INFO - 21/05/14 22:07:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206297815685795654748407_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297815685795654748407_0004_m_000042_330
[2021-05-14 19:07:13,784] {docker.py:276} INFO - 21/05/14 22:07:13 INFO StagingCommitter: Task committer attempt_202105142206297815685795654748407_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297815685795654748407_0004_m_000042_330 : duration 0:00.003s
[2021-05-14 19:07:14,067] {docker.py:276} INFO - 21/05/14 22:07:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206296945434312144567024_0004_m_000038_326: needsTaskCommit() Task attempt_202105142206296945434312144567024_0004_m_000038_326
[2021-05-14 19:07:14,068] {docker.py:276} INFO - 21/05/14 22:07:14 INFO StagingCommitter: Task committer attempt_202105142206296945434312144567024_0004_m_000038_326: needsTaskCommit() Task attempt_202105142206296945434312144567024_0004_m_000038_326: duration 0:00.001s
21/05/14 22:07:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296945434312144567024_0004_m_000038_326
[2021-05-14 19:07:14,068] {docker.py:276} INFO - 21/05/14 22:07:14 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4587 bytes result sent to driver
[2021-05-14 19:07:14,069] {docker.py:276} INFO - 21/05/14 22:07:14 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (9bbba8114907, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:14,070] {docker.py:276} INFO - 21/05/14 22:07:14 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 3103 ms on 9bbba8114907 (executor driver) (40/200)
[2021-05-14 19:07:14,072] {docker.py:276} INFO - 21/05/14 22:07:14 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-14 19:07:14,082] {docker.py:276} INFO - 21/05/14 22:07:14 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:14,085] {docker.py:276} INFO - 21/05/14 22:07:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:14,085] {docker.py:276} INFO - 21/05/14 22:07:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299171367175581599243_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299171367175581599243_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299171367175581599243_0004}; taskId=attempt_202105142206299171367175581599243_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11bc7321}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:14,086] {docker.py:276} INFO - 21/05/14 22:07:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206299171367175581599243_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299171367175581599243_0004_m_000043_331
[2021-05-14 19:07:14,088] {docker.py:276} INFO - 21/05/14 22:07:14 INFO StagingCommitter: Task committer attempt_202105142206299171367175581599243_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299171367175581599243_0004_m_000043_331 : duration 0:00.003s
[2021-05-14 19:07:16,160] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206292311484938262700235_0004_m_000040_328: needsTaskCommit() Task attempt_202105142206292311484938262700235_0004_m_000040_328
[2021-05-14 19:07:16,161] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206292311484938262700235_0004_m_000040_328: needsTaskCommit() Task attempt_202105142206292311484938262700235_0004_m_000040_328: duration 0:00.001s
21/05/14 22:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292311484938262700235_0004_m_000040_328
[2021-05-14 19:07:16,162] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4587 bytes result sent to driver
[2021-05-14 19:07:16,163] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (9bbba8114907, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:16,165] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 2611 ms on 9bbba8114907 (executor driver) (41/200)
[2021-05-14 19:07:16,166] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-14 19:07:16,176] {docker.py:276} INFO - 21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:16,178] {docker.py:276} INFO - 21/05/14 22:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,178] {docker.py:276} INFO - 21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291529466147982099098_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291529466147982099098_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291529466147982099098_0004}; taskId=attempt_202105142206291529466147982099098_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a21b99e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,179] {docker.py:276} INFO - 21/05/14 22:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206291529466147982099098_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291529466147982099098_0004_m_000044_332
[2021-05-14 19:07:16,182] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206291529466147982099098_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291529466147982099098_0004_m_000044_332 : duration 0:00.003s
[2021-05-14 19:07:16,329] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206292576570234516500882_0004_m_000041_329: needsTaskCommit() Task attempt_202105142206292576570234516500882_0004_m_000041_329
[2021-05-14 19:07:16,330] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206292576570234516500882_0004_m_000041_329: needsTaskCommit() Task attempt_202105142206292576570234516500882_0004_m_000041_329: duration 0:00.001s
21/05/14 22:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292576570234516500882_0004_m_000041_329
[2021-05-14 19:07:16,332] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-14 19:07:16,334] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (9bbba8114907, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:16,335] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-14 19:07:16,336] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 2779 ms on 9bbba8114907 (executor driver) (42/200)
[2021-05-14 19:07:16,346] {docker.py:276} INFO - 21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:16,349] {docker.py:276} INFO - 21/05/14 22:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,349] {docker.py:276} INFO - 21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298716526134497074950_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298716526134497074950_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298716526134497074950_0004}; taskId=attempt_202105142206298716526134497074950_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dec7d81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,350] {docker.py:276} INFO - 21/05/14 22:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:16,350] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206298716526134497074950_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298716526134497074950_0004_m_000045_333
[2021-05-14 19:07:16,353] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206298716526134497074950_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298716526134497074950_0004_m_000045_333 : duration 0:00.004s
[2021-05-14 19:07:16,404] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206297815685795654748407_0004_m_000042_330: needsTaskCommit() Task attempt_202105142206297815685795654748407_0004_m_000042_330
[2021-05-14 19:07:16,405] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206297815685795654748407_0004_m_000042_330: needsTaskCommit() Task attempt_202105142206297815685795654748407_0004_m_000042_330: duration 0:00.001s
21/05/14 22:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297815685795654748407_0004_m_000042_330
[2021-05-14 19:07:16,406] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4544 bytes result sent to driver
[2021-05-14 19:07:16,407] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (9bbba8114907, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:16,408] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-14 19:07:16,409] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2643 ms on 9bbba8114907 (executor driver) (43/200)
[2021-05-14 19:07:16,419] {docker.py:276} INFO - 21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:16,421] {docker.py:276} INFO - 21/05/14 22:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,422] {docker.py:276} INFO - 21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293707841173930954212_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293707841173930954212_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293707841173930954212_0004}; taskId=attempt_202105142206293707841173930954212_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49af348}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206293707841173930954212_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293707841173930954212_0004_m_000046_334
[2021-05-14 19:07:16,425] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206293707841173930954212_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293707841173930954212_0004_m_000046_334 : duration 0:00.003s
[2021-05-14 19:07:16,544] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206299171367175581599243_0004_m_000043_331: needsTaskCommit() Task attempt_202105142206299171367175581599243_0004_m_000043_331
[2021-05-14 19:07:16,544] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206299171367175581599243_0004_m_000043_331: needsTaskCommit() Task attempt_202105142206299171367175581599243_0004_m_000043_331: duration 0:00.000s
[2021-05-14 19:07:16,545] {docker.py:276} INFO - 21/05/14 22:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299171367175581599243_0004_m_000043_331
[2021-05-14 19:07:16,547] {docker.py:276} INFO - 21/05/14 22:07:16 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4544 bytes result sent to driver
[2021-05-14 19:07:16,548] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (9bbba8114907, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:16,549] {docker.py:276} INFO - 21/05/14 22:07:16 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2483 ms on 9bbba8114907 (executor driver) (44/200)
21/05/14 22:07:16 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-14 19:07:16,569] {docker.py:276} INFO - 21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:16,571] {docker.py:276} INFO - 21/05/14 22:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:16,572] {docker.py:276} INFO - 21/05/14 22:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296631740240947191173_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296631740240947191173_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296631740240947191173_0004}; taskId=attempt_202105142206296631740240947191173_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b786c86}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:16,572] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206296631740240947191173_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296631740240947191173_0004_m_000047_335
[2021-05-14 19:07:16,575] {docker.py:276} INFO - 21/05/14 22:07:16 INFO StagingCommitter: Task committer attempt_202105142206296631740240947191173_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296631740240947191173_0004_m_000047_335 : duration 0:00.003s
[2021-05-14 19:07:19,015] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206291529466147982099098_0004_m_000044_332: needsTaskCommit() Task attempt_202105142206291529466147982099098_0004_m_000044_332
[2021-05-14 19:07:19,016] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206291529466147982099098_0004_m_000044_332: needsTaskCommit() Task attempt_202105142206291529466147982099098_0004_m_000044_332: duration 0:00.001s
21/05/14 22:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291529466147982099098_0004_m_000044_332
[2021-05-14 19:07:19,019] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4587 bytes result sent to driver
[2021-05-14 19:07:19,021] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (9bbba8114907, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:19,022] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
21/05/14 22:07:19 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2862 ms on 9bbba8114907 (executor driver) (45/200)
[2021-05-14 19:07:19,026] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206293707841173930954212_0004_m_000046_334: needsTaskCommit() Task attempt_202105142206293707841173930954212_0004_m_000046_334
21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206293707841173930954212_0004_m_000046_334: needsTaskCommit() Task attempt_202105142206293707841173930954212_0004_m_000046_334: duration 0:00.000s
[2021-05-14 19:07:19,026] {docker.py:276} INFO - 21/05/14 22:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293707841173930954212_0004_m_000046_334
[2021-05-14 19:07:19,029] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4587 bytes result sent to driver
[2021-05-14 19:07:19,029] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (9bbba8114907, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:19,030] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-14 19:07:19,031] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2627 ms on 9bbba8114907 (executor driver) (46/200)
[2021-05-14 19:07:19,039] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:19,040] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:07:19,040] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:19,040] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:19,042] {docker.py:276} INFO - 21/05/14 22:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:19,043] {docker.py:276} INFO - 21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:19,043] {docker.py:276} INFO - 21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291873921545578740305_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291873921545578740305_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291873921545578740305_0004}; taskId=attempt_202105142206291873921545578740305_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77498b42}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:19,044] {docker.py:276} INFO - 21/05/14 22:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:19,045] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206291873921545578740305_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291873921545578740305_0004_m_000049_337
[2021-05-14 19:07:19,045] {docker.py:276} INFO - 21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:19,046] {docker.py:276} INFO - 21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291551214938256808067_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291551214938256808067_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291551214938256808067_0004}; taskId=attempt_202105142206291551214938256808067_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a2a6f24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:19,046] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206291551214938256808067_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291551214938256808067_0004_m_000048_336
[2021-05-14 19:07:19,046] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206291873921545578740305_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291873921545578740305_0004_m_000049_337 : duration 0:00.003s
[2021-05-14 19:07:19,048] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206291551214938256808067_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291551214938256808067_0004_m_000048_336 : duration 0:00.004s
[2021-05-14 19:07:19,107] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206298716526134497074950_0004_m_000045_333: needsTaskCommit() Task attempt_202105142206298716526134497074950_0004_m_000045_333
21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206298716526134497074950_0004_m_000045_333: needsTaskCommit() Task attempt_202105142206298716526134497074950_0004_m_000045_333: duration 0:00.000s
[2021-05-14 19:07:19,108] {docker.py:276} INFO - 21/05/14 22:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298716526134497074950_0004_m_000045_333
[2021-05-14 19:07:19,109] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4587 bytes result sent to driver
[2021-05-14 19:07:19,111] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (9bbba8114907, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:19,112] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2782 ms on 9bbba8114907 (executor driver) (47/200)
[2021-05-14 19:07:19,113] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-14 19:07:19,123] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:19,125] {docker.py:276} INFO - 21/05/14 22:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:19,126] {docker.py:276} INFO - 21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292030324863256758583_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292030324863256758583_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292030324863256758583_0004}; taskId=attempt_202105142206292030324863256758583_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41c2736b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:19,126] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206292030324863256758583_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292030324863256758583_0004_m_000050_338
[2021-05-14 19:07:19,129] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206292030324863256758583_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292030324863256758583_0004_m_000050_338 : duration 0:00.003s
[2021-05-14 19:07:19,209] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206296631740240947191173_0004_m_000047_335: needsTaskCommit() Task attempt_202105142206296631740240947191173_0004_m_000047_335
[2021-05-14 19:07:19,211] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206296631740240947191173_0004_m_000047_335: needsTaskCommit() Task attempt_202105142206296631740240947191173_0004_m_000047_335: duration 0:00.001s
21/05/14 22:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296631740240947191173_0004_m_000047_335
[2021-05-14 19:07:19,214] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4587 bytes result sent to driver
[2021-05-14 19:07:19,215] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (9bbba8114907, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:19,216] {docker.py:276} INFO - 21/05/14 22:07:19 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-14 19:07:19,217] {docker.py:276} INFO - 21/05/14 22:07:19 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2672 ms on 9bbba8114907 (executor driver) (48/200)
[2021-05-14 19:07:19,226] {docker.py:276} INFO - 21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:19,230] {docker.py:276} INFO - 21/05/14 22:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291855428616627076356_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291855428616627076356_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291855428616627076356_0004}; taskId=attempt_202105142206291855428616627076356_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41865d75}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:19,230] {docker.py:276} INFO - 21/05/14 22:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:19,230] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206291855428616627076356_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291855428616627076356_0004_m_000051_339
[2021-05-14 19:07:19,233] {docker.py:276} INFO - 21/05/14 22:07:19 INFO StagingCommitter: Task committer attempt_202105142206291855428616627076356_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291855428616627076356_0004_m_000051_339 : duration 0:00.003s
[2021-05-14 19:07:21,672] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206292030324863256758583_0004_m_000050_338: needsTaskCommit() Task attempt_202105142206292030324863256758583_0004_m_000050_338
[2021-05-14 19:07:21,673] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206292030324863256758583_0004_m_000050_338: needsTaskCommit() Task attempt_202105142206292030324863256758583_0004_m_000050_338: duration 0:00.001s
21/05/14 22:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292030324863256758583_0004_m_000050_338
[2021-05-14 19:07:21,674] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-14 19:07:21,676] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (9bbba8114907, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:21,677] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
21/05/14 22:07:21 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2548 ms on 9bbba8114907 (executor driver) (49/200)
[2021-05-14 19:07:21,694] {docker.py:276} INFO - 21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:21,696] {docker.py:276} INFO - 21/05/14 22:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297053903079916448604_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297053903079916448604_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297053903079916448604_0004}; taskId=attempt_202105142206297053903079916448604_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55e76d09}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:21,696] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206297053903079916448604_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297053903079916448604_0004_m_000052_340
[2021-05-14 19:07:21,699] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206297053903079916448604_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297053903079916448604_0004_m_000052_340 : duration 0:00.003s
[2021-05-14 19:07:21,778] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206291855428616627076356_0004_m_000051_339: needsTaskCommit() Task attempt_202105142206291855428616627076356_0004_m_000051_339
[2021-05-14 19:07:21,779] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206291855428616627076356_0004_m_000051_339: needsTaskCommit() Task attempt_202105142206291855428616627076356_0004_m_000051_339: duration 0:00.001s
21/05/14 22:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291855428616627076356_0004_m_000051_339
[2021-05-14 19:07:21,781] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4587 bytes result sent to driver
[2021-05-14 19:07:21,782] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (9bbba8114907, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:21,784] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2550 ms on 9bbba8114907 (executor driver) (50/200)
[2021-05-14 19:07:21,784] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-14 19:07:21,787] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206291873921545578740305_0004_m_000049_337: needsTaskCommit() Task attempt_202105142206291873921545578740305_0004_m_000049_337
[2021-05-14 19:07:21,787] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206291873921545578740305_0004_m_000049_337: needsTaskCommit() Task attempt_202105142206291873921545578740305_0004_m_000049_337: duration 0:00.000s
[2021-05-14 19:07:21,788] {docker.py:276} INFO - 21/05/14 22:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291873921545578740305_0004_m_000049_337
[2021-05-14 19:07:21,789] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4587 bytes result sent to driver
[2021-05-14 19:07:21,790] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (9bbba8114907, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:21,790] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2744 ms on 9bbba8114907 (executor driver) (51/200)
[2021-05-14 19:07:21,791] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-14 19:07:21,796] {docker.py:276} INFO - 21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:21,798] {docker.py:276} INFO - 21/05/14 22:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:21,799] {docker.py:276} INFO - 21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:21,800] {docker.py:276} INFO - 21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291238305967654044542_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291238305967654044542_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291238305967654044542_0004}; taskId=attempt_202105142206291238305967654044542_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d0ddb83}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206291238305967654044542_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291238305967654044542_0004_m_000053_341
[2021-05-14 19:07:21,801] {docker.py:276} INFO - 21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:21,802] {docker.py:276} INFO - 21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:21,802] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206291238305967654044542_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291238305967654044542_0004_m_000053_341 : duration 0:00.003s
[2021-05-14 19:07:21,803] {docker.py:276} INFO - 21/05/14 22:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:21,804] {docker.py:276} INFO - 21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298200234154496673850_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298200234154496673850_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298200234154496673850_0004}; taskId=attempt_202105142206298200234154496673850_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15da3d14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:21,804] {docker.py:276} INFO - 21/05/14 22:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:21,804] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206298200234154496673850_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298200234154496673850_0004_m_000054_342
[2021-05-14 19:07:21,807] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206298200234154496673850_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298200234154496673850_0004_m_000054_342 : duration 0:00.003s
[2021-05-14 19:07:21,839] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206291551214938256808067_0004_m_000048_336: needsTaskCommit() Task attempt_202105142206291551214938256808067_0004_m_000048_336
[2021-05-14 19:07:21,840] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206291551214938256808067_0004_m_000048_336: needsTaskCommit() Task attempt_202105142206291551214938256808067_0004_m_000048_336: duration 0:00.000s
[2021-05-14 19:07:21,841] {docker.py:276} INFO - 21/05/14 22:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291551214938256808067_0004_m_000048_336
[2021-05-14 19:07:21,841] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4587 bytes result sent to driver
[2021-05-14 19:07:21,843] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (9bbba8114907, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:21,844] {docker.py:276} INFO - 21/05/14 22:07:21 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 2805 ms on 9bbba8114907 (executor driver) (52/200)
[2021-05-14 19:07:21,845] {docker.py:276} INFO - 21/05/14 22:07:21 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-14 19:07:21,855] {docker.py:276} INFO - 21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:21,857] {docker.py:276} INFO - 21/05/14 22:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:21,858] {docker.py:276} INFO - 21/05/14 22:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:21,858] {docker.py:276} INFO - 21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297226011029056659626_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297226011029056659626_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297226011029056659626_0004}; taskId=attempt_202105142206297226011029056659626_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd8eff2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:21,858] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105142206297226011029056659626_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297226011029056659626_0004_m_000055_343
[2021-05-14 19:07:21,861] {docker.py:276} INFO - 21/05/14 22:07:21 INFO StagingCommitter: Task committer attempt_202105142206297226011029056659626_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297226011029056659626_0004_m_000055_343 : duration 0:00.003s
[2021-05-14 19:07:24,371] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206297053903079916448604_0004_m_000052_340: needsTaskCommit() Task attempt_202105142206297053903079916448604_0004_m_000052_340
[2021-05-14 19:07:24,373] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206297053903079916448604_0004_m_000052_340: needsTaskCommit() Task attempt_202105142206297053903079916448604_0004_m_000052_340: duration 0:00.001s
21/05/14 22:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297053903079916448604_0004_m_000052_340
[2021-05-14 19:07:24,375] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4587 bytes result sent to driver
[2021-05-14 19:07:24,376] {docker.py:276} INFO - 21/05/14 22:07:24 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (9bbba8114907, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:24,377] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
21/05/14 22:07:24 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2704 ms on 9bbba8114907 (executor driver) (53/200)
[2021-05-14 19:07:24,387] {docker.py:276} INFO - 21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:24,389] {docker.py:276} INFO - 21/05/14 22:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299134811885924609055_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299134811885924609055_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299134811885924609055_0004}; taskId=attempt_202105142206299134811885924609055_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33a141c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:24,390] {docker.py:276} INFO - 21/05/14 22:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206299134811885924609055_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299134811885924609055_0004_m_000056_344
[2021-05-14 19:07:24,392] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206299134811885924609055_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299134811885924609055_0004_m_000056_344 : duration 0:00.003s
[2021-05-14 19:07:24,470] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206298200234154496673850_0004_m_000054_342: needsTaskCommit() Task attempt_202105142206298200234154496673850_0004_m_000054_342
21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206298200234154496673850_0004_m_000054_342: needsTaskCommit() Task attempt_202105142206298200234154496673850_0004_m_000054_342: duration 0:00.001s
21/05/14 22:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298200234154496673850_0004_m_000054_342
[2021-05-14 19:07:24,472] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
21/05/14 22:07:24 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (9bbba8114907, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:24,473] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-14 19:07:24,474] {docker.py:276} INFO - 21/05/14 22:07:24 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2686 ms on 9bbba8114907 (executor driver) (54/200)
[2021-05-14 19:07:24,484] {docker.py:276} INFO - 21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:24,486] {docker.py:276} INFO - 21/05/14 22:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294364432636965787640_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294364432636965787640_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294364432636965787640_0004}; taskId=attempt_202105142206294364432636965787640_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e26f62f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:24,487] {docker.py:276} INFO - 21/05/14 22:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206294364432636965787640_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294364432636965787640_0004_m_000057_345
[2021-05-14 19:07:24,489] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206294364432636965787640_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294364432636965787640_0004_m_000057_345 : duration 0:00.003s
[2021-05-14 19:07:24,499] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206297226011029056659626_0004_m_000055_343: needsTaskCommit() Task attempt_202105142206297226011029056659626_0004_m_000055_343
[2021-05-14 19:07:24,499] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206297226011029056659626_0004_m_000055_343: needsTaskCommit() Task attempt_202105142206297226011029056659626_0004_m_000055_343: duration 0:00.000s
21/05/14 22:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297226011029056659626_0004_m_000055_343
[2021-05-14 19:07:24,500] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-14 19:07:24,500] {docker.py:276} INFO - 21/05/14 22:07:24 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (9bbba8114907, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:24,501] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
21/05/14 22:07:24 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2662 ms on 9bbba8114907 (executor driver) (55/200)
[2021-05-14 19:07:24,508] {docker.py:276} INFO - 21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:24,510] {docker.py:276} INFO - 21/05/14 22:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296909384053102121942_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296909384053102121942_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296909384053102121942_0004}; taskId=attempt_202105142206296909384053102121942_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79741275}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:24,511] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206296909384053102121942_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296909384053102121942_0004_m_000058_346
[2021-05-14 19:07:24,513] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206296909384053102121942_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296909384053102121942_0004_m_000058_346 : duration 0:00.003s
[2021-05-14 19:07:24,572] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206291238305967654044542_0004_m_000053_341: needsTaskCommit() Task attempt_202105142206291238305967654044542_0004_m_000053_341
[2021-05-14 19:07:24,573] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_202105142206291238305967654044542_0004_m_000053_341: needsTaskCommit() Task attempt_202105142206291238305967654044542_0004_m_000053_341: duration 0:00.001s
[2021-05-14 19:07:24,573] {docker.py:276} INFO - 21/05/14 22:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291238305967654044542_0004_m_000053_341
[2021-05-14 19:07:24,574] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-14 19:07:24,575] {docker.py:276} INFO - 21/05/14 22:07:24 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (9bbba8114907, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:24,576] {docker.py:276} INFO - 21/05/14 22:07:24 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-14 19:07:24,576] {docker.py:276} INFO - 21/05/14 22:07:24 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2798 ms on 9bbba8114907 (executor driver) (56/200)
[2021-05-14 19:07:24,592] {docker.py:276} INFO - 21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:24,594] {docker.py:276} INFO - 21/05/14 22:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:24,594] {docker.py:276} INFO - 21/05/14 22:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:24,594] {docker.py:276} INFO - 21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:24,595] {docker.py:276} INFO - 21/05/14 22:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629765082589787222334_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629765082589787222334_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629765082589787222334_0004}; taskId=attempt_20210514220629765082589787222334_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24a080}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:24,595] {docker.py:276} INFO - 21/05/14 22:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:24,595] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Starting: Task committer attempt_20210514220629765082589787222334_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629765082589787222334_0004_m_000059_347
[2021-05-14 19:07:24,598] {docker.py:276} INFO - 21/05/14 22:07:24 INFO StagingCommitter: Task committer attempt_20210514220629765082589787222334_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629765082589787222334_0004_m_000059_347 : duration 0:00.003s
[2021-05-14 19:07:27,051] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206294364432636965787640_0004_m_000057_345: needsTaskCommit() Task attempt_202105142206294364432636965787640_0004_m_000057_345
[2021-05-14 19:07:27,053] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206299134811885924609055_0004_m_000056_344: needsTaskCommit() Task attempt_202105142206299134811885924609055_0004_m_000056_344
[2021-05-14 19:07:27,054] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206294364432636965787640_0004_m_000057_345: needsTaskCommit() Task attempt_202105142206294364432636965787640_0004_m_000057_345: duration 0:00.001s
21/05/14 22:07:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294364432636965787640_0004_m_000057_345
21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206299134811885924609055_0004_m_000056_344: needsTaskCommit() Task attempt_202105142206299134811885924609055_0004_m_000056_344: duration 0:00.001s
[2021-05-14 19:07:27,054] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299134811885924609055_0004_m_000056_344
[2021-05-14 19:07:27,056] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4587 bytes result sent to driver
[2021-05-14 19:07:27,056] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (9bbba8114907, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:27,057] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-14 19:07:27,058] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
[2021-05-14 19:07:27,059] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2587 ms on 9bbba8114907 (executor driver) (57/200)
[2021-05-14 19:07:27,060] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (9bbba8114907, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:27,060] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2688 ms on 9bbba8114907 (executor driver) (58/200)
[2021-05-14 19:07:27,061] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-14 19:07:27,069] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:27,069] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:27,071] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:27,071] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,072] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292731480982643229948_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292731480982643229948_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292731480982643229948_0004}; taskId=attempt_202105142206292731480982643229948_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@536deb99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,072] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:27,072] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206292731480982643229948_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292731480982643229948_0004_m_000060_348
[2021-05-14 19:07:27,073] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:27,073] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:27,074] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:27,074] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,074] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298442575452261396170_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298442575452261396170_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298442575452261396170_0004}; taskId=attempt_202105142206298442575452261396170_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@118d4ee2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206298442575452261396170_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298442575452261396170_0004_m_000061_349
[2021-05-14 19:07:27,075] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206292731480982643229948_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292731480982643229948_0004_m_000060_348 : duration 0:00.004s
[2021-05-14 19:07:27,079] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206298442575452261396170_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298442575452261396170_0004_m_000061_349 : duration 0:00.005s
[2021-05-14 19:07:27,121] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206296909384053102121942_0004_m_000058_346: needsTaskCommit() Task attempt_202105142206296909384053102121942_0004_m_000058_346
[2021-05-14 19:07:27,122] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206296909384053102121942_0004_m_000058_346: needsTaskCommit() Task attempt_202105142206296909384053102121942_0004_m_000058_346: duration 0:00.000s
[2021-05-14 19:07:27,122] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296909384053102121942_0004_m_000058_346
[2021-05-14 19:07:27,123] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4587 bytes result sent to driver
[2021-05-14 19:07:27,124] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (9bbba8114907, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:27,124] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-14 19:07:27,125] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2627 ms on 9bbba8114907 (executor driver) (59/200)
[2021-05-14 19:07:27,132] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:27,134] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:27,135] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,135] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292755628108001776842_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292755628108001776842_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292755628108001776842_0004}; taskId=attempt_202105142206292755628108001776842_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@970261e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,135] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206292755628108001776842_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292755628108001776842_0004_m_000062_350
[2021-05-14 19:07:27,138] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206292755628108001776842_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292755628108001776842_0004_m_000062_350 : duration 0:00.003s
[2021-05-14 19:07:27,177] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_20210514220629765082589787222334_0004_m_000059_347: needsTaskCommit() Task attempt_20210514220629765082589787222334_0004_m_000059_347
[2021-05-14 19:07:27,178] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_20210514220629765082589787222334_0004_m_000059_347: needsTaskCommit() Task attempt_20210514220629765082589787222334_0004_m_000059_347: duration 0:00.001s
[2021-05-14 19:07:27,178] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629765082589787222334_0004_m_000059_347
[2021-05-14 19:07:27,180] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4587 bytes result sent to driver
[2021-05-14 19:07:27,181] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (9bbba8114907, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:27,182] {docker.py:276} INFO - 21/05/14 22:07:27 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2609 ms on 9bbba8114907 (executor driver) (60/200)
[2021-05-14 19:07:27,183] {docker.py:276} INFO - 21/05/14 22:07:27 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-14 19:07:27,192] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:27,193] {docker.py:276} INFO - 21/05/14 22:07:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:27,195] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:27,195] {docker.py:276} INFO - 21/05/14 22:07:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:27,196] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,196] {docker.py:276} INFO - 21/05/14 22:07:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295559461718885311297_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295559461718885311297_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295559461718885311297_0004}; taskId=attempt_202105142206295559461718885311297_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bb0aa70}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:27,196] {docker.py:276} INFO - 21/05/14 22:07:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:27,197] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206295559461718885311297_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295559461718885311297_0004_m_000063_351
[2021-05-14 19:07:27,200] {docker.py:276} INFO - 21/05/14 22:07:27 INFO StagingCommitter: Task committer attempt_202105142206295559461718885311297_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295559461718885311297_0004_m_000063_351 : duration 0:00.004s
[2021-05-14 19:07:29,712] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206292755628108001776842_0004_m_000062_350: needsTaskCommit() Task attempt_202105142206292755628108001776842_0004_m_000062_350
[2021-05-14 19:07:29,713] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206292755628108001776842_0004_m_000062_350: needsTaskCommit() Task attempt_202105142206292755628108001776842_0004_m_000062_350: duration 0:00.001s
21/05/14 22:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292755628108001776842_0004_m_000062_350
[2021-05-14 19:07:29,716] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-14 19:07:29,717] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (9bbba8114907, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:29,718] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2597 ms on 9bbba8114907 (executor driver) (61/200)
[2021-05-14 19:07:29,719] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-14 19:07:29,738] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:29,739] {docker.py:276} INFO - 21/05/14 22:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:29,740] {docker.py:276} INFO - 21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291226015942289884511_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291226015942289884511_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291226015942289884511_0004}; taskId=attempt_202105142206291226015942289884511_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72b03bca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:29,740] {docker.py:276} INFO - 21/05/14 22:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:29,740] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206291226015942289884511_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291226015942289884511_0004_m_000064_352
[2021-05-14 19:07:29,743] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206291226015942289884511_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291226015942289884511_0004_m_000064_352 : duration 0:00.003s
[2021-05-14 19:07:29,829] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206292731480982643229948_0004_m_000060_348: needsTaskCommit() Task attempt_202105142206292731480982643229948_0004_m_000060_348
[2021-05-14 19:07:29,830] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206292731480982643229948_0004_m_000060_348: needsTaskCommit() Task attempt_202105142206292731480982643229948_0004_m_000060_348: duration 0:00.001s
21/05/14 22:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292731480982643229948_0004_m_000060_348
[2021-05-14 19:07:29,833] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4587 bytes result sent to driver
[2021-05-14 19:07:29,834] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (9bbba8114907, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:29,835] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 2784 ms on 9bbba8114907 (executor driver) (62/200)
[2021-05-14 19:07:29,836] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-14 19:07:29,844] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206295559461718885311297_0004_m_000063_351: needsTaskCommit() Task attempt_202105142206295559461718885311297_0004_m_000063_351
[2021-05-14 19:07:29,844] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206295559461718885311297_0004_m_000063_351: needsTaskCommit() Task attempt_202105142206295559461718885311297_0004_m_000063_351: duration 0:00.000s
21/05/14 22:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295559461718885311297_0004_m_000063_351
[2021-05-14 19:07:29,846] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4587 bytes result sent to driver
[2021-05-14 19:07:29,847] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (9bbba8114907, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:29,848] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-14 19:07:29,848] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2671 ms on 9bbba8114907 (executor driver) (63/200)
[2021-05-14 19:07:29,849] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:29,850] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:29,852] {docker.py:276} INFO - 21/05/14 22:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:29,852] {docker.py:276} INFO - 21/05/14 22:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:29,852] {docker.py:276} INFO - 21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:29,853] {docker.py:276} INFO - 21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298519468602371852602_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298519468602371852602_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298519468602371852602_0004}; taskId=attempt_202105142206298519468602371852602_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24d61e57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:29,853] {docker.py:276} INFO - 21/05/14 22:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:29,854] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206298519468602371852602_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298519468602371852602_0004_m_000065_353
[2021-05-14 19:07:29,857] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206298519468602371852602_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298519468602371852602_0004_m_000065_353 : duration 0:00.003s
[2021-05-14 19:07:29,857] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:29,859] {docker.py:276} INFO - 21/05/14 22:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:29,859] {docker.py:276} INFO - 21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294576649775302929629_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294576649775302929629_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294576649775302929629_0004}; taskId=attempt_202105142206294576649775302929629_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fd1f925}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:29,860] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206294576649775302929629_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294576649775302929629_0004_m_000066_354
[2021-05-14 19:07:29,862] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206294576649775302929629_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294576649775302929629_0004_m_000066_354 : duration 0:00.003s
[2021-05-14 19:07:29,900] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206298442575452261396170_0004_m_000061_349: needsTaskCommit() Task attempt_202105142206298442575452261396170_0004_m_000061_349
[2021-05-14 19:07:29,901] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206298442575452261396170_0004_m_000061_349: needsTaskCommit() Task attempt_202105142206298442575452261396170_0004_m_000061_349: duration 0:00.001s
21/05/14 22:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298442575452261396170_0004_m_000061_349
[2021-05-14 19:07:29,905] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4587 bytes result sent to driver
[2021-05-14 19:07:29,906] {docker.py:276} INFO - 21/05/14 22:07:29 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (9bbba8114907, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:29,907] {docker.py:276} INFO - 21/05/14 22:07:29 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
21/05/14 22:07:29 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2851 ms on 9bbba8114907 (executor driver) (64/200)
[2021-05-14 19:07:29,917] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:29,917] {docker.py:276} INFO - 21/05/14 22:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:29,919] {docker.py:276} INFO - 21/05/14 22:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:29,920] {docker.py:276} INFO - 21/05/14 22:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295600000405026244455_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295600000405026244455_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295600000405026244455_0004}; taskId=attempt_202105142206295600000405026244455_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ea57005}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:29,920] {docker.py:276} INFO - 21/05/14 22:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:29,920] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105142206295600000405026244455_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295600000405026244455_0004_m_000067_355
[2021-05-14 19:07:29,923] {docker.py:276} INFO - 21/05/14 22:07:29 INFO StagingCommitter: Task committer attempt_202105142206295600000405026244455_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295600000405026244455_0004_m_000067_355 : duration 0:00.003s
[2021-05-14 19:07:32,317] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206291226015942289884511_0004_m_000064_352: needsTaskCommit() Task attempt_202105142206291226015942289884511_0004_m_000064_352
[2021-05-14 19:07:32,318] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206291226015942289884511_0004_m_000064_352: needsTaskCommit() Task attempt_202105142206291226015942289884511_0004_m_000064_352: duration 0:00.001s
21/05/14 22:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291226015942289884511_0004_m_000064_352
[2021-05-14 19:07:32,319] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4587 bytes result sent to driver
[2021-05-14 19:07:32,321] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (9bbba8114907, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:32,322] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2608 ms on 9bbba8114907 (executor driver) (65/200)
21/05/14 22:07:32 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-14 19:07:32,332] {docker.py:276} INFO - 21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:32,334] {docker.py:276} INFO - 21/05/14 22:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293745611077752083202_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293745611077752083202_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293745611077752083202_0004}; taskId=attempt_202105142206293745611077752083202_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@350fb02d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206293745611077752083202_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293745611077752083202_0004_m_000068_356
[2021-05-14 19:07:32,337] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206293745611077752083202_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293745611077752083202_0004_m_000068_356 : duration 0:00.002s
[2021-05-14 19:07:32,338] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206295600000405026244455_0004_m_000067_355: needsTaskCommit() Task attempt_202105142206295600000405026244455_0004_m_000067_355
[2021-05-14 19:07:32,339] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206295600000405026244455_0004_m_000067_355: needsTaskCommit() Task attempt_202105142206295600000405026244455_0004_m_000067_355: duration 0:00.001s
21/05/14 22:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295600000405026244455_0004_m_000067_355
[2021-05-14 19:07:32,340] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
[2021-05-14 19:07:32,341] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (9bbba8114907, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:32,342] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2439 ms on 9bbba8114907 (executor driver) (66/200)
[2021-05-14 19:07:32,342] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-14 19:07:32,350] {docker.py:276} INFO - 21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:32,351] {docker.py:276} INFO - 21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:32,353] {docker.py:276} INFO - 21/05/14 22:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629130194143097809491_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629130194143097809491_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629130194143097809491_0004}; taskId=attempt_20210514220629130194143097809491_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@763fc3b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:32,353] {docker.py:276} INFO - 21/05/14 22:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:32,354] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_20210514220629130194143097809491_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629130194143097809491_0004_m_000069_357
[2021-05-14 19:07:32,356] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_20210514220629130194143097809491_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629130194143097809491_0004_m_000069_357 : duration 0:00.003s
[2021-05-14 19:07:32,419] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206294576649775302929629_0004_m_000066_354: needsTaskCommit() Task attempt_202105142206294576649775302929629_0004_m_000066_354
[2021-05-14 19:07:32,420] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206294576649775302929629_0004_m_000066_354: needsTaskCommit() Task attempt_202105142206294576649775302929629_0004_m_000066_354: duration 0:00.001s
21/05/14 22:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294576649775302929629_0004_m_000066_354
[2021-05-14 19:07:32,422] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-14 19:07:32,423] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (9bbba8114907, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:32,424] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2579 ms on 9bbba8114907 (executor driver) (67/200)
[2021-05-14 19:07:32,424] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-14 19:07:32,434] {docker.py:276} INFO - 21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Getting 5 (40.4 KiB) non-empty blocks including 5 (40.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:32,436] {docker.py:276} INFO - 21/05/14 22:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294335936516790974803_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294335936516790974803_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294335936516790974803_0004}; taskId=attempt_202105142206294335936516790974803_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b69f2e1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:32,436] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206294335936516790974803_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294335936516790974803_0004_m_000070_358
[2021-05-14 19:07:32,439] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206294335936516790974803_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294335936516790974803_0004_m_000070_358 : duration 0:00.003s
[2021-05-14 19:07:32,652] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206298519468602371852602_0004_m_000065_353: needsTaskCommit() Task attempt_202105142206298519468602371852602_0004_m_000065_353
[2021-05-14 19:07:32,653] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206298519468602371852602_0004_m_000065_353: needsTaskCommit() Task attempt_202105142206298519468602371852602_0004_m_000065_353: duration 0:00.001s
21/05/14 22:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298519468602371852602_0004_m_000065_353
[2021-05-14 19:07:32,655] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-14 19:07:32,657] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (9bbba8114907, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:32,658] {docker.py:276} INFO - 21/05/14 22:07:32 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2826 ms on 9bbba8114907 (executor driver) (68/200)
[2021-05-14 19:07:32,658] {docker.py:276} INFO - 21/05/14 22:07:32 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-14 19:07:32,678] {docker.py:276} INFO - 21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:32,679] {docker.py:276} INFO - 21/05/14 22:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297609671971234179575_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297609671971234179575_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297609671971234179575_0004}; taskId=attempt_202105142206297609671971234179575_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c8fa3b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206297609671971234179575_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297609671971234179575_0004_m_000071_359
[2021-05-14 19:07:32,682] {docker.py:276} INFO - 21/05/14 22:07:32 INFO StagingCommitter: Task committer attempt_202105142206297609671971234179575_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297609671971234179575_0004_m_000071_359 : duration 0:00.003s
[2021-05-14 19:07:34,906] {docker.py:276} INFO - 21/05/14 22:07:34 INFO StagingCommitter: Starting: Task committer attempt_202105142206293745611077752083202_0004_m_000068_356: needsTaskCommit() Task attempt_202105142206293745611077752083202_0004_m_000068_356
[2021-05-14 19:07:34,906] {docker.py:276} INFO - 21/05/14 22:07:34 INFO StagingCommitter: Task committer attempt_202105142206293745611077752083202_0004_m_000068_356: needsTaskCommit() Task attempt_202105142206293745611077752083202_0004_m_000068_356: duration 0:00.001s
21/05/14 22:07:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293745611077752083202_0004_m_000068_356
[2021-05-14 19:07:34,907] {docker.py:276} INFO - 21/05/14 22:07:34 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4587 bytes result sent to driver
[2021-05-14 19:07:34,908] {docker.py:276} INFO - 21/05/14 22:07:34 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (9bbba8114907, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:34,909] {docker.py:276} INFO - 21/05/14 22:07:34 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-14 19:07:34,910] {docker.py:276} INFO - 21/05/14 22:07:34 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2592 ms on 9bbba8114907 (executor driver) (69/200)
[2021-05-14 19:07:34,918] {docker.py:276} INFO - 21/05/14 22:07:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:34,919] {docker.py:276} INFO - 21/05/14 22:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296634496092265609312_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296634496092265609312_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296634496092265609312_0004}; taskId=attempt_202105142206296634496092265609312_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d74cafd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:34 INFO StagingCommitter: Starting: Task committer attempt_202105142206296634496092265609312_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296634496092265609312_0004_m_000072_360
[2021-05-14 19:07:34,922] {docker.py:276} INFO - 21/05/14 22:07:34 INFO StagingCommitter: Task committer attempt_202105142206296634496092265609312_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296634496092265609312_0004_m_000072_360 : duration 0:00.003s
[2021-05-14 19:07:35,131] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206294335936516790974803_0004_m_000070_358: needsTaskCommit() Task attempt_202105142206294335936516790974803_0004_m_000070_358
21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_202105142206294335936516790974803_0004_m_000070_358: needsTaskCommit() Task attempt_202105142206294335936516790974803_0004_m_000070_358: duration 0:00.001s
21/05/14 22:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294335936516790974803_0004_m_000070_358
[2021-05-14 19:07:35,133] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4587 bytes result sent to driver
[2021-05-14 19:07:35,135] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (9bbba8114907, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:35,136] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-14 19:07:35,137] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2717 ms on 9bbba8114907 (executor driver) (70/200)
[2021-05-14 19:07:35,146] {docker.py:276} INFO - 21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:35,148] {docker.py:276} INFO - 21/05/14 22:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629847352830125274915_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629847352830125274915_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629847352830125274915_0004}; taskId=attempt_20210514220629847352830125274915_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77ff4bd4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:35,149] {docker.py:276} INFO - 21/05/14 22:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_20210514220629847352830125274915_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629847352830125274915_0004_m_000073_361
[2021-05-14 19:07:35,152] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_20210514220629847352830125274915_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629847352830125274915_0004_m_000073_361 : duration 0:00.003s
[2021-05-14 19:07:35,245] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_20210514220629130194143097809491_0004_m_000069_357: needsTaskCommit() Task attempt_20210514220629130194143097809491_0004_m_000069_357
21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_20210514220629130194143097809491_0004_m_000069_357: needsTaskCommit() Task attempt_20210514220629130194143097809491_0004_m_000069_357: duration 0:00.001s
21/05/14 22:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629130194143097809491_0004_m_000069_357
[2021-05-14 19:07:35,246] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4587 bytes result sent to driver
[2021-05-14 19:07:35,249] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (9bbba8114907, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:35,250] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2912 ms on 9bbba8114907 (executor driver) (71/200)
[2021-05-14 19:07:35,251] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-14 19:07:35,259] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206297609671971234179575_0004_m_000071_359: needsTaskCommit() Task attempt_202105142206297609671971234179575_0004_m_000071_359
21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_202105142206297609671971234179575_0004_m_000071_359: needsTaskCommit() Task attempt_202105142206297609671971234179575_0004_m_000071_359: duration 0:00.000s
21/05/14 22:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297609671971234179575_0004_m_000071_359
[2021-05-14 19:07:35,260] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4587 bytes result sent to driver
[2021-05-14 19:07:35,261] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (9bbba8114907, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:35,262] {docker.py:276} INFO - 21/05/14 22:07:35 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-14 19:07:35,262] {docker.py:276} INFO - 21/05/14 22:07:35 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2609 ms on 9bbba8114907 (executor driver) (72/200)
[2021-05-14 19:07:35,263] {docker.py:276} INFO - 21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:35,264] {docker.py:276} INFO - 21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:35,266] {docker.py:276} INFO - 21/05/14 22:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:35,267] {docker.py:276} INFO - 21/05/14 22:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:35,267] {docker.py:276} INFO - 21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:35,267] {docker.py:276} INFO - 21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293807563758725773177_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293807563758725773177_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293807563758725773177_0004}; taskId=attempt_202105142206293807563758725773177_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65bd3465}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:35,268] {docker.py:276} INFO - 21/05/14 22:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:35,268] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206293807563758725773177_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293807563758725773177_0004_m_000074_362
[2021-05-14 19:07:35,270] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_202105142206293807563758725773177_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293807563758725773177_0004_m_000074_362 : duration 0:00.002s
[2021-05-14 19:07:35,271] {docker.py:276} INFO - 21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:35,275] {docker.py:276} INFO - 21/05/14 22:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297382676917649472850_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297382676917649472850_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297382676917649472850_0004}; taskId=attempt_202105142206297382676917649472850_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@476d1cec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:35,276] {docker.py:276} INFO - 21/05/14 22:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:35,276] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206297382676917649472850_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297382676917649472850_0004_m_000075_363
[2021-05-14 19:07:35,277] {docker.py:276} INFO - 21/05/14 22:07:35 INFO StagingCommitter: Task committer attempt_202105142206297382676917649472850_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297382676917649472850_0004_m_000075_363 : duration 0:00.003s
[2021-05-14 19:07:37,562] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Starting: Task committer attempt_202105142206296634496092265609312_0004_m_000072_360: needsTaskCommit() Task attempt_202105142206296634496092265609312_0004_m_000072_360
[2021-05-14 19:07:37,563] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Task committer attempt_202105142206296634496092265609312_0004_m_000072_360: needsTaskCommit() Task attempt_202105142206296634496092265609312_0004_m_000072_360: duration 0:00.002s
21/05/14 22:07:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296634496092265609312_0004_m_000072_360
[2021-05-14 19:07:37,566] {docker.py:276} INFO - 21/05/14 22:07:37 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4544 bytes result sent to driver
[2021-05-14 19:07:37,567] {docker.py:276} INFO - 21/05/14 22:07:37 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (9bbba8114907, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:37,568] {docker.py:276} INFO - 21/05/14 22:07:37 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 2661 ms on 9bbba8114907 (executor driver) (73/200)
[2021-05-14 19:07:37,568] {docker.py:276} INFO - 21/05/14 22:07:37 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
[2021-05-14 19:07:37,578] {docker.py:276} INFO - 21/05/14 22:07:37 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:37,580] {docker.py:276} INFO - 21/05/14 22:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296995310001673401031_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296995310001673401031_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296995310001673401031_0004}; taskId=attempt_202105142206296995310001673401031_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@714c83ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:37 INFO StagingCommitter: Starting: Task committer attempt_202105142206296995310001673401031_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296995310001673401031_0004_m_000076_364
[2021-05-14 19:07:37,583] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Task committer attempt_202105142206296995310001673401031_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296995310001673401031_0004_m_000076_364 : duration 0:00.003s
[2021-05-14 19:07:37,945] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Starting: Task committer attempt_20210514220629847352830125274915_0004_m_000073_361: needsTaskCommit() Task attempt_20210514220629847352830125274915_0004_m_000073_361
[2021-05-14 19:07:37,946] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Task committer attempt_20210514220629847352830125274915_0004_m_000073_361: needsTaskCommit() Task attempt_20210514220629847352830125274915_0004_m_000073_361: duration 0:00.000s
[2021-05-14 19:07:37,946] {docker.py:276} INFO - 21/05/14 22:07:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629847352830125274915_0004_m_000073_361
[2021-05-14 19:07:37,948] {docker.py:276} INFO - 21/05/14 22:07:37 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4587 bytes result sent to driver
[2021-05-14 19:07:37,950] {docker.py:276} INFO - 21/05/14 22:07:37 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (9bbba8114907, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:37,951] {docker.py:276} INFO - 21/05/14 22:07:37 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2818 ms on 9bbba8114907 (executor driver) (74/200)
[2021-05-14 19:07:37,952] {docker.py:276} INFO - 21/05/14 22:07:37 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
[2021-05-14 19:07:37,961] {docker.py:276} INFO - 21/05/14 22:07:37 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:37,963] {docker.py:276} INFO - 21/05/14 22:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:37,963] {docker.py:276} INFO - 21/05/14 22:07:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294290320155652064073_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294290320155652064073_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294290320155652064073_0004}; taskId=attempt_202105142206294290320155652064073_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3010c653}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:37,964] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Starting: Task committer attempt_202105142206294290320155652064073_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294290320155652064073_0004_m_000077_365
[2021-05-14 19:07:37,967] {docker.py:276} INFO - 21/05/14 22:07:37 INFO StagingCommitter: Task committer attempt_202105142206294290320155652064073_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294290320155652064073_0004_m_000077_365 : duration 0:00.003s
[2021-05-14 19:07:38,061] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206293807563758725773177_0004_m_000074_362: needsTaskCommit() Task attempt_202105142206293807563758725773177_0004_m_000074_362
[2021-05-14 19:07:38,062] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Task committer attempt_202105142206293807563758725773177_0004_m_000074_362: needsTaskCommit() Task attempt_202105142206293807563758725773177_0004_m_000074_362: duration 0:00.002s
21/05/14 22:07:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293807563758725773177_0004_m_000074_362
[2021-05-14 19:07:38,064] {docker.py:276} INFO - 21/05/14 22:07:38 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4587 bytes result sent to driver
[2021-05-14 19:07:38,065] {docker.py:276} INFO - 21/05/14 22:07:38 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (9bbba8114907, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:38,066] {docker.py:276} INFO - 21/05/14 22:07:38 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-14 19:07:38,067] {docker.py:276} INFO - 21/05/14 22:07:38 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 2822 ms on 9bbba8114907 (executor driver) (75/200)
[2021-05-14 19:07:38,069] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206297382676917649472850_0004_m_000075_363: needsTaskCommit() Task attempt_202105142206297382676917649472850_0004_m_000075_363
[2021-05-14 19:07:38,070] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Task committer attempt_202105142206297382676917649472850_0004_m_000075_363: needsTaskCommit() Task attempt_202105142206297382676917649472850_0004_m_000075_363: duration 0:00.000s
21/05/14 22:07:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297382676917649472850_0004_m_000075_363
[2021-05-14 19:07:38,071] {docker.py:276} INFO - 21/05/14 22:07:38 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4587 bytes result sent to driver
[2021-05-14 19:07:38,072] {docker.py:276} INFO - 21/05/14 22:07:38 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (9bbba8114907, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:38,073] {docker.py:276} INFO - 21/05/14 22:07:38 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
21/05/14 22:07:38 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2816 ms on 9bbba8114907 (executor driver) (76/200)
[2021-05-14 19:07:38,082] {docker.py:276} INFO - 21/05/14 22:07:38 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:38,084] {docker.py:276} INFO - 21/05/14 22:07:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:38,084] {docker.py:276} INFO - 21/05/14 22:07:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295111861256397199043_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295111861256397199043_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295111861256397199043_0004}; taskId=attempt_202105142206295111861256397199043_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f0fd153}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:38,085] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206295111861256397199043_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295111861256397199043_0004_m_000079_367
[2021-05-14 19:07:38,085] {docker.py:276} INFO - 21/05/14 22:07:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:38,086] {docker.py:276} INFO - 21/05/14 22:07:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:07:38,088] {docker.py:276} INFO - 21/05/14 22:07:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:38,088] {docker.py:276} INFO - 21/05/14 22:07:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:38,089] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Task committer attempt_202105142206295111861256397199043_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295111861256397199043_0004_m_000079_367 : duration 0:00.004s
[2021-05-14 19:07:38,089] {docker.py:276} INFO - 21/05/14 22:07:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:38,090] {docker.py:276} INFO - 21/05/14 22:07:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297640660339813738623_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297640660339813738623_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297640660339813738623_0004}; taskId=attempt_202105142206297640660339813738623_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4802c477}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:38,090] {docker.py:276} INFO - 21/05/14 22:07:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:38,090] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206297640660339813738623_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297640660339813738623_0004_m_000078_366
[2021-05-14 19:07:38,095] {docker.py:276} INFO - 21/05/14 22:07:38 INFO StagingCommitter: Task committer attempt_202105142206297640660339813738623_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297640660339813738623_0004_m_000078_366 : duration 0:00.005s
[2021-05-14 19:07:40,337] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206296995310001673401031_0004_m_000076_364: needsTaskCommit() Task attempt_202105142206296995310001673401031_0004_m_000076_364
[2021-05-14 19:07:40,338] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206296995310001673401031_0004_m_000076_364: needsTaskCommit() Task attempt_202105142206296995310001673401031_0004_m_000076_364: duration 0:00.001s
21/05/14 22:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296995310001673401031_0004_m_000076_364
[2021-05-14 19:07:40,341] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-14 19:07:40,343] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (9bbba8114907, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:40,344] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-14 19:07:40,345] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2782 ms on 9bbba8114907 (executor driver) (77/200)
[2021-05-14 19:07:40,355] {docker.py:276} INFO - 21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:40,357] {docker.py:276} INFO - 21/05/14 22:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298905895494796518031_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298905895494796518031_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298905895494796518031_0004}; taskId=attempt_202105142206298905895494796518031_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66b707bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:40,358] {docker.py:276} INFO - 21/05/14 22:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206298905895494796518031_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298905895494796518031_0004_m_000080_368
[2021-05-14 19:07:40,361] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206298905895494796518031_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298905895494796518031_0004_m_000080_368 : duration 0:00.003s
[2021-05-14 19:07:40,665] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206294290320155652064073_0004_m_000077_365: needsTaskCommit() Task attempt_202105142206294290320155652064073_0004_m_000077_365
[2021-05-14 19:07:40,667] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206294290320155652064073_0004_m_000077_365: needsTaskCommit() Task attempt_202105142206294290320155652064073_0004_m_000077_365: duration 0:00.001s
21/05/14 22:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294290320155652064073_0004_m_000077_365
[2021-05-14 19:07:40,671] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
21/05/14 22:07:40 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (9bbba8114907, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:40,672] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2726 ms on 9bbba8114907 (executor driver) (78/200)
[2021-05-14 19:07:40,673] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-14 19:07:40,682] {docker.py:276} INFO - 21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:40,684] {docker.py:276} INFO - 21/05/14 22:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:40,685] {docker.py:276} INFO - 21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298542451318156052769_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298542451318156052769_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298542451318156052769_0004}; taskId=attempt_202105142206298542451318156052769_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49a760f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:40,685] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206298542451318156052769_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298542451318156052769_0004_m_000081_369
[2021-05-14 19:07:40,688] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206298542451318156052769_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298542451318156052769_0004_m_000081_369 : duration 0:00.004s
[2021-05-14 19:07:40,768] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206297640660339813738623_0004_m_000078_366: needsTaskCommit() Task attempt_202105142206297640660339813738623_0004_m_000078_366
[2021-05-14 19:07:40,770] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206297640660339813738623_0004_m_000078_366: needsTaskCommit() Task attempt_202105142206297640660339813738623_0004_m_000078_366: duration 0:00.001s
21/05/14 22:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297640660339813738623_0004_m_000078_366
[2021-05-14 19:07:40,772] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4544 bytes result sent to driver
[2021-05-14 19:07:40,774] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (9bbba8114907, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:40,775] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
21/05/14 22:07:40 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2713 ms on 9bbba8114907 (executor driver) (79/200)
[2021-05-14 19:07:40,784] {docker.py:276} INFO - 21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:40,786] {docker.py:276} INFO - 21/05/14 22:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:40,787] {docker.py:276} INFO - 21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:40,787] {docker.py:276} INFO - 21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296212234297798163618_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296212234297798163618_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296212234297798163618_0004}; taskId=attempt_202105142206296212234297798163618_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b4dea14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:40,788] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206296212234297798163618_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296212234297798163618_0004_m_000082_370
[2021-05-14 19:07:40,790] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206296212234297798163618_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296212234297798163618_0004_m_000082_370 : duration 0:00.003s
[2021-05-14 19:07:40,842] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206295111861256397199043_0004_m_000079_367: needsTaskCommit() Task attempt_202105142206295111861256397199043_0004_m_000079_367
[2021-05-14 19:07:40,843] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_202105142206295111861256397199043_0004_m_000079_367: needsTaskCommit() Task attempt_202105142206295111861256397199043_0004_m_000079_367: duration 0:00.002s
21/05/14 22:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295111861256397199043_0004_m_000079_367
[2021-05-14 19:07:40,846] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4544 bytes result sent to driver
[2021-05-14 19:07:40,847] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (9bbba8114907, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:40,849] {docker.py:276} INFO - 21/05/14 22:07:40 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-14 19:07:40,850] {docker.py:276} INFO - 21/05/14 22:07:40 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2780 ms on 9bbba8114907 (executor driver) (80/200)
[2021-05-14 19:07:40,859] {docker.py:276} INFO - 21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:40,860] {docker.py:276} INFO - 21/05/14 22:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:40,861] {docker.py:276} INFO - 21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:40,869] {docker.py:276} INFO - 21/05/14 22:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629383659740957090087_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629383659740957090087_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629383659740957090087_0004}; taskId=attempt_20210514220629383659740957090087_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@485369d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:40,870] {docker.py:276} INFO - 21/05/14 22:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:40 INFO StagingCommitter: Starting: Task committer attempt_20210514220629383659740957090087_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629383659740957090087_0004_m_000083_371
[2021-05-14 19:07:40,873] {docker.py:276} INFO - 21/05/14 22:07:40 INFO StagingCommitter: Task committer attempt_20210514220629383659740957090087_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629383659740957090087_0004_m_000083_371 : duration 0:00.004s
[2021-05-14 19:07:42,990] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206298905895494796518031_0004_m_000080_368: needsTaskCommit() Task attempt_202105142206298905895494796518031_0004_m_000080_368
[2021-05-14 19:07:42,991] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206298905895494796518031_0004_m_000080_368: needsTaskCommit() Task attempt_202105142206298905895494796518031_0004_m_000080_368: duration 0:00.001s
21/05/14 22:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298905895494796518031_0004_m_000080_368
[2021-05-14 19:07:42,993] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4587 bytes result sent to driver
[2021-05-14 19:07:42,994] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (9bbba8114907, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:42,995] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2655 ms on 9bbba8114907 (executor driver) (81/200)
[2021-05-14 19:07:42,996] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-14 19:07:43,005] {docker.py:276} INFO - 21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:43,007] {docker.py:276} INFO - 21/05/14 22:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293522454379082330025_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293522454379082330025_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293522454379082330025_0004}; taskId=attempt_202105142206293522454379082330025_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@439248be}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206293522454379082330025_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293522454379082330025_0004_m_000084_372
[2021-05-14 19:07:43,010] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206293522454379082330025_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293522454379082330025_0004_m_000084_372 : duration 0:00.003s
[2021-05-14 19:07:43,427] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_20210514220629383659740957090087_0004_m_000083_371: needsTaskCommit() Task attempt_20210514220629383659740957090087_0004_m_000083_371
[2021-05-14 19:07:43,429] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_20210514220629383659740957090087_0004_m_000083_371: needsTaskCommit() Task attempt_20210514220629383659740957090087_0004_m_000083_371: duration 0:00.002s
21/05/14 22:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629383659740957090087_0004_m_000083_371
[2021-05-14 19:07:43,431] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206298542451318156052769_0004_m_000081_369: needsTaskCommit() Task attempt_202105142206298542451318156052769_0004_m_000081_369
[2021-05-14 19:07:43,432] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206298542451318156052769_0004_m_000081_369: needsTaskCommit() Task attempt_202105142206298542451318156052769_0004_m_000081_369: duration 0:00.000s
[2021-05-14 19:07:43,432] {docker.py:276} INFO - 21/05/14 22:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298542451318156052769_0004_m_000081_369
[2021-05-14 19:07:43,433] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4587 bytes result sent to driver
[2021-05-14 19:07:43,433] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4587 bytes result sent to driver
[2021-05-14 19:07:43,434] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (9bbba8114907, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:43,435] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-14 19:07:43,436] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (9bbba8114907, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:43,437] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2593 ms on 9bbba8114907 (executor driver) (82/200)
[2021-05-14 19:07:43,437] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-14 19:07:43,438] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2771 ms on 9bbba8114907 (executor driver) (83/200)
[2021-05-14 19:07:43,448] {docker.py:276} INFO - 21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:43,448] {docker.py:276} INFO - 21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:43,449] {docker.py:276} INFO - 21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:43,451] {docker.py:276} INFO - 21/05/14 22:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:43,452] {docker.py:276} INFO - 21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:43,452] {docker.py:276} INFO - 21/05/14 22:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:43,453] {docker.py:276} INFO - 21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298686210910840975342_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686210910840975342_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298686210910840975342_0004}; taskId=attempt_202105142206298686210910840975342_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@102ab47a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206298686210910840975342_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686210910840975342_0004_m_000085_373 
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293530877210705619510_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293530877210705619510_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293530877210705619510_0004}; taskId=attempt_202105142206293530877210705619510_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67557a21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:43,453] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206293530877210705619510_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293530877210705619510_0004_m_000086_374
[2021-05-14 19:07:43,455] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206298686210910840975342_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298686210910840975342_0004_m_000085_373 : duration 0:00.004s
[2021-05-14 19:07:43,456] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206293530877210705619510_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293530877210705619510_0004_m_000086_374 : duration 0:00.004s
[2021-05-14 19:07:43,628] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206296212234297798163618_0004_m_000082_370: needsTaskCommit() Task attempt_202105142206296212234297798163618_0004_m_000082_370
[2021-05-14 19:07:43,628] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206296212234297798163618_0004_m_000082_370: needsTaskCommit() Task attempt_202105142206296212234297798163618_0004_m_000082_370: duration 0:00.001s
21/05/14 22:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296212234297798163618_0004_m_000082_370
[2021-05-14 19:07:43,630] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4587 bytes result sent to driver
[2021-05-14 19:07:43,632] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (9bbba8114907, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:43,633] {docker.py:276} INFO - 21/05/14 22:07:43 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2862 ms on 9bbba8114907 (executor driver) (84/200)
[2021-05-14 19:07:43,634] {docker.py:276} INFO - 21/05/14 22:07:43 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-14 19:07:43,644] {docker.py:276} INFO - 21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:43,646] {docker.py:276} INFO - 21/05/14 22:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294203502836587750376_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294203502836587750376_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294203502836587750376_0004}; taskId=attempt_202105142206294203502836587750376_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ba95e8c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:43,647] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206294203502836587750376_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294203502836587750376_0004_m_000087_375
[2021-05-14 19:07:43,650] {docker.py:276} INFO - 21/05/14 22:07:43 INFO StagingCommitter: Task committer attempt_202105142206294203502836587750376_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294203502836587750376_0004_m_000087_375 : duration 0:00.003s
[2021-05-14 19:07:45,595] {docker.py:276} INFO - 21/05/14 22:07:45 INFO StagingCommitter: Starting: Task committer attempt_202105142206293522454379082330025_0004_m_000084_372: needsTaskCommit() Task attempt_202105142206293522454379082330025_0004_m_000084_372
21/05/14 22:07:45 INFO StagingCommitter: Task committer attempt_202105142206293522454379082330025_0004_m_000084_372: needsTaskCommit() Task attempt_202105142206293522454379082330025_0004_m_000084_372: duration 0:00.000s
21/05/14 22:07:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293522454379082330025_0004_m_000084_372
[2021-05-14 19:07:45,597] {docker.py:276} INFO - 21/05/14 22:07:45 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-14 19:07:45,598] {docker.py:276} INFO - 21/05/14 22:07:45 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (9bbba8114907, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:45,600] {docker.py:276} INFO - 21/05/14 22:07:45 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
21/05/14 22:07:45 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 2608 ms on 9bbba8114907 (executor driver) (85/200)
[2021-05-14 19:07:45,609] {docker.py:276} INFO - 21/05/14 22:07:45 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:45,611] {docker.py:276} INFO - 21/05/14 22:07:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296977656496101698130_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296977656496101698130_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296977656496101698130_0004}; taskId=attempt_202105142206296977656496101698130_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2145d06c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:45,612] {docker.py:276} INFO - 21/05/14 22:07:45 INFO StagingCommitter: Starting: Task committer attempt_202105142206296977656496101698130_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296977656496101698130_0004_m_000088_376
[2021-05-14 19:07:45,615] {docker.py:276} INFO - 21/05/14 22:07:45 INFO StagingCommitter: Task committer attempt_202105142206296977656496101698130_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296977656496101698130_0004_m_000088_376 : duration 0:00.003s
[2021-05-14 19:07:46,138] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206293530877210705619510_0004_m_000086_374: needsTaskCommit() Task attempt_202105142206293530877210705619510_0004_m_000086_374
[2021-05-14 19:07:46,139] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_202105142206293530877210705619510_0004_m_000086_374: needsTaskCommit() Task attempt_202105142206293530877210705619510_0004_m_000086_374: duration 0:00.001s
21/05/14 22:07:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293530877210705619510_0004_m_000086_374
[2021-05-14 19:07:46,139] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4587 bytes result sent to driver
[2021-05-14 19:07:46,141] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (9bbba8114907, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:46,142] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-14 19:07:46,143] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2710 ms on 9bbba8114907 (executor driver) (86/200)
[2021-05-14 19:07:46,151] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206298686210910840975342_0004_m_000085_373: needsTaskCommit() Task attempt_202105142206298686210910840975342_0004_m_000085_373
[2021-05-14 19:07:46,151] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_202105142206298686210910840975342_0004_m_000085_373: needsTaskCommit() Task attempt_202105142206298686210910840975342_0004_m_000085_373: duration 0:00.000s
21/05/14 22:07:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298686210910840975342_0004_m_000085_373
[2021-05-14 19:07:46,152] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4587 bytes result sent to driver
[2021-05-14 19:07:46,153] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (9bbba8114907, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:46,154] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-14 19:07:46,154] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2724 ms on 9bbba8114907 (executor driver) (87/200)
[2021-05-14 19:07:46,155] {docker.py:276} INFO - 21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:46,156] {docker.py:276} INFO - 21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-14 19:07:46,158] {docker.py:276} INFO - 21/05/14 22:07:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:46,159] {docker.py:276} INFO - 21/05/14 22:07:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:46,159] {docker.py:276} INFO - 21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:46,159] {docker.py:276} INFO - 21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293672964752849919899_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293672964752849919899_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293672964752849919899_0004}; taskId=attempt_202105142206293672964752849919899_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6854c1c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:46,160] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206293672964752849919899_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293672964752849919899_0004_m_000089_377
[2021-05-14 19:07:46,162] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_202105142206293672964752849919899_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293672964752849919899_0004_m_000089_377 : duration 0:00.002s
[2021-05-14 19:07:46,163] {docker.py:276} INFO - 21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:46,164] {docker.py:276} INFO - 21/05/14 22:07:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:46,165] {docker.py:276} INFO - 21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629202617395420059367_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629202617395420059367_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629202617395420059367_0004}; taskId=attempt_20210514220629202617395420059367_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a33c31e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_20210514220629202617395420059367_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629202617395420059367_0004_m_000090_378
[2021-05-14 19:07:46,168] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_20210514220629202617395420059367_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629202617395420059367_0004_m_000090_378 : duration 0:00.003s
[2021-05-14 19:07:46,371] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206294203502836587750376_0004_m_000087_375: needsTaskCommit() Task attempt_202105142206294203502836587750376_0004_m_000087_375
[2021-05-14 19:07:46,373] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_202105142206294203502836587750376_0004_m_000087_375: needsTaskCommit() Task attempt_202105142206294203502836587750376_0004_m_000087_375: duration 0:00.002s
21/05/14 22:07:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294203502836587750376_0004_m_000087_375
[2021-05-14 19:07:46,375] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4587 bytes result sent to driver
[2021-05-14 19:07:46,376] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (9bbba8114907, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:46,377] {docker.py:276} INFO - 21/05/14 22:07:46 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2749 ms on 9bbba8114907 (executor driver) (88/200)
[2021-05-14 19:07:46,379] {docker.py:276} INFO - 21/05/14 22:07:46 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-14 19:07:46,389] {docker.py:276} INFO - 21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:46,390] {docker.py:276} INFO - 21/05/14 22:07:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:46,391] {docker.py:276} INFO - 21/05/14 22:07:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296115181666763050436_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296115181666763050436_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296115181666763050436_0004}; taskId=attempt_202105142206296115181666763050436_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@472502ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206296115181666763050436_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296115181666763050436_0004_m_000091_379
[2021-05-14 19:07:46,393] {docker.py:276} INFO - 21/05/14 22:07:46 INFO StagingCommitter: Task committer attempt_202105142206296115181666763050436_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296115181666763050436_0004_m_000091_379 : duration 0:00.003s
[2021-05-14 19:07:48,393] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105142206296977656496101698130_0004_m_000088_376: needsTaskCommit() Task attempt_202105142206296977656496101698130_0004_m_000088_376
[2021-05-14 19:07:48,395] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_202105142206296977656496101698130_0004_m_000088_376: needsTaskCommit() Task attempt_202105142206296977656496101698130_0004_m_000088_376: duration 0:00.001s
[2021-05-14 19:07:48,396] {docker.py:276} INFO - 21/05/14 22:07:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296977656496101698130_0004_m_000088_376
[2021-05-14 19:07:48,397] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-14 19:07:48,401] {docker.py:276} INFO - 21/05/14 22:07:48 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (9bbba8114907, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:48,402] {docker.py:276} INFO - 21/05/14 22:07:48 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2808 ms on 9bbba8114907 (executor driver) (89/200)
[2021-05-14 19:07:48,404] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-14 19:07:48,412] {docker.py:276} INFO - 21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:48,413] {docker.py:276} INFO - 21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:48,414] {docker.py:276} INFO - 21/05/14 22:07:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:48,415] {docker.py:276} INFO - 21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:48,415] {docker.py:276} INFO - 21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298554303997088467610_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298554303997088467610_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298554303997088467610_0004}; taskId=attempt_202105142206298554303997088467610_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62b2f731}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:48,416] {docker.py:276} INFO - 21/05/14 22:07:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:48,416] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105142206298554303997088467610_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298554303997088467610_0004_m_000092_380
[2021-05-14 19:07:48,418] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_202105142206298554303997088467610_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298554303997088467610_0004_m_000092_380 : duration 0:00.004s
[2021-05-14 19:07:48,797] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_20210514220629202617395420059367_0004_m_000090_378: needsTaskCommit() Task attempt_20210514220629202617395420059367_0004_m_000090_378
21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_20210514220629202617395420059367_0004_m_000090_378: needsTaskCommit() Task attempt_20210514220629202617395420059367_0004_m_000090_378: duration 0:00.001s
21/05/14 22:07:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629202617395420059367_0004_m_000090_378
[2021-05-14 19:07:48,799] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4544 bytes result sent to driver
[2021-05-14 19:07:48,801] {docker.py:276} INFO - 21/05/14 22:07:48 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (9bbba8114907, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:48,802] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
21/05/14 22:07:48 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2652 ms on 9bbba8114907 (executor driver) (90/200)
[2021-05-14 19:07:48,812] {docker.py:276} INFO - 21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:48,814] {docker.py:276} INFO - 21/05/14 22:07:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:48,815] {docker.py:276} INFO - 21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629188838118098004423_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629188838118098004423_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629188838118098004423_0004}; taskId=attempt_20210514220629188838118098004423_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e3db8ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_20210514220629188838118098004423_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629188838118098004423_0004_m_000093_381
[2021-05-14 19:07:48,818] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_20210514220629188838118098004423_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629188838118098004423_0004_m_000093_381 : duration 0:00.003s
[2021-05-14 19:07:48,819] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105142206293672964752849919899_0004_m_000089_377: needsTaskCommit() Task attempt_202105142206293672964752849919899_0004_m_000089_377
[2021-05-14 19:07:48,820] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_202105142206293672964752849919899_0004_m_000089_377: needsTaskCommit() Task attempt_202105142206293672964752849919899_0004_m_000089_377: duration 0:00.001s
21/05/14 22:07:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293672964752849919899_0004_m_000089_377
[2021-05-14 19:07:48,821] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4544 bytes result sent to driver
[2021-05-14 19:07:48,821] {docker.py:276} INFO - 21/05/14 22:07:48 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (9bbba8114907, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:48,822] {docker.py:276} INFO - 21/05/14 22:07:48 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-14 19:07:48,823] {docker.py:276} INFO - 21/05/14 22:07:48 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 2686 ms on 9bbba8114907 (executor driver) (91/200)
[2021-05-14 19:07:48,831] {docker.py:276} INFO - 21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:48,832] {docker.py:276} INFO - 21/05/14 22:07:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292048684962513151749_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292048684962513151749_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292048684962513151749_0004}; taskId=attempt_202105142206292048684962513151749_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@273012ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105142206292048684962513151749_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292048684962513151749_0004_m_000094_382
[2021-05-14 19:07:48,835] {docker.py:276} INFO - 21/05/14 22:07:48 INFO StagingCommitter: Task committer attempt_202105142206292048684962513151749_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292048684962513151749_0004_m_000094_382 : duration 0:00.003s
[2021-05-14 19:07:48,995] {docker.py:276} INFO - 21/05/14 22:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206296115181666763050436_0004_m_000091_379: needsTaskCommit() Task attempt_202105142206296115181666763050436_0004_m_000091_379
[2021-05-14 19:07:48,995] {docker.py:276} INFO - 21/05/14 22:07:49 INFO StagingCommitter: Task committer attempt_202105142206296115181666763050436_0004_m_000091_379: needsTaskCommit() Task attempt_202105142206296115181666763050436_0004_m_000091_379: duration 0:00.001s
21/05/14 22:07:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296115181666763050436_0004_m_000091_379
[2021-05-14 19:07:48,997] {docker.py:276} INFO - 21/05/14 22:07:49 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4544 bytes result sent to driver
[2021-05-14 19:07:48,998] {docker.py:276} INFO - 21/05/14 22:07:49 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (9bbba8114907, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:49,000] {docker.py:276} INFO - 21/05/14 22:07:49 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
21/05/14 22:07:49 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2627 ms on 9bbba8114907 (executor driver) (92/200)
[2021-05-14 19:07:49,016] {docker.py:276} INFO - 21/05/14 22:07:49 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:49,018] {docker.py:276} INFO - 21/05/14 22:07:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:49,018] {docker.py:276} INFO - 21/05/14 22:07:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295091630284227656386_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295091630284227656386_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295091630284227656386_0004}; taskId=attempt_202105142206295091630284227656386_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b59c29a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206295091630284227656386_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295091630284227656386_0004_m_000095_383
[2021-05-14 19:07:49,020] {docker.py:276} INFO - 21/05/14 22:07:49 INFO StagingCommitter: Task committer attempt_202105142206295091630284227656386_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295091630284227656386_0004_m_000095_383 : duration 0:00.002s
[2021-05-14 19:07:51,085] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206298554303997088467610_0004_m_000092_380: needsTaskCommit() Task attempt_202105142206298554303997088467610_0004_m_000092_380
[2021-05-14 19:07:51,086] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206298554303997088467610_0004_m_000092_380: needsTaskCommit() Task attempt_202105142206298554303997088467610_0004_m_000092_380: duration 0:00.001s
21/05/14 22:07:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298554303997088467610_0004_m_000092_380
[2021-05-14 19:07:51,088] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4587 bytes result sent to driver
[2021-05-14 19:07:51,089] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (9bbba8114907, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:51,090] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2694 ms on 9bbba8114907 (executor driver) (93/200)
[2021-05-14 19:07:51,090] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-14 19:07:51,102] {docker.py:276} INFO - 21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:51,104] {docker.py:276} INFO - 21/05/14 22:07:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298496739896685743956_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298496739896685743956_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298496739896685743956_0004}; taskId=attempt_202105142206298496739896685743956_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2354ef3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:51,104] {docker.py:276} INFO - 21/05/14 22:07:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206298496739896685743956_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298496739896685743956_0004_m_000096_384
[2021-05-14 19:07:51,107] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206298496739896685743956_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298496739896685743956_0004_m_000096_384 : duration 0:00.003s
[2021-05-14 19:07:51,431] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_20210514220629188838118098004423_0004_m_000093_381: needsTaskCommit() Task attempt_20210514220629188838118098004423_0004_m_000093_381
[2021-05-14 19:07:51,431] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_20210514220629188838118098004423_0004_m_000093_381: needsTaskCommit() Task attempt_20210514220629188838118098004423_0004_m_000093_381: duration 0:00.000s
21/05/14 22:07:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629188838118098004423_0004_m_000093_381
[2021-05-14 19:07:51,432] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4587 bytes result sent to driver
[2021-05-14 19:07:51,433] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (9bbba8114907, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:51,434] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2637 ms on 9bbba8114907 (executor driver) (94/200)
[2021-05-14 19:07:51,434] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
[2021-05-14 19:07:51,442] {docker.py:276} INFO - 21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:51,444] {docker.py:276} INFO - 21/05/14 22:07:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:51,445] {docker.py:276} INFO - 21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629601041004401756124_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629601041004401756124_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629601041004401756124_0004}; taskId=attempt_20210514220629601041004401756124_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b5c4d1e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_20210514220629601041004401756124_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629601041004401756124_0004_m_000097_385
[2021-05-14 19:07:51,447] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_20210514220629601041004401756124_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629601041004401756124_0004_m_000097_385 : duration 0:00.003s
[2021-05-14 19:07:51,482] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206292048684962513151749_0004_m_000094_382: needsTaskCommit() Task attempt_202105142206292048684962513151749_0004_m_000094_382
[2021-05-14 19:07:51,483] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206292048684962513151749_0004_m_000094_382: needsTaskCommit() Task attempt_202105142206292048684962513151749_0004_m_000094_382: duration 0:00.001s
21/05/14 22:07:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292048684962513151749_0004_m_000094_382
[2021-05-14 19:07:51,484] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4587 bytes result sent to driver
[2021-05-14 19:07:51,486] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (9bbba8114907, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:51,487] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-14 19:07:51,487] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2668 ms on 9bbba8114907 (executor driver) (95/200)
[2021-05-14 19:07:51,498] {docker.py:276} INFO - 21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:51,500] {docker.py:276} INFO - 21/05/14 22:07:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:51,500] {docker.py:276} INFO - 21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297386069399068214607_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297386069399068214607_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297386069399068214607_0004}; taskId=attempt_202105142206297386069399068214607_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75ace004}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:51,501] {docker.py:276} INFO - 21/05/14 22:07:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206297386069399068214607_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297386069399068214607_0004_m_000098_386
[2021-05-14 19:07:51,504] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206297386069399068214607_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297386069399068214607_0004_m_000098_386 : duration 0:00.003s
[2021-05-14 19:07:51,808] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206295091630284227656386_0004_m_000095_383: needsTaskCommit() Task attempt_202105142206295091630284227656386_0004_m_000095_383
21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206295091630284227656386_0004_m_000095_383: needsTaskCommit() Task attempt_202105142206295091630284227656386_0004_m_000095_383: duration 0:00.000s
21/05/14 22:07:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295091630284227656386_0004_m_000095_383
[2021-05-14 19:07:51,811] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4587 bytes result sent to driver
[2021-05-14 19:07:51,812] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (9bbba8114907, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:51,813] {docker.py:276} INFO - 21/05/14 22:07:51 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-14 19:07:51,814] {docker.py:276} INFO - 21/05/14 22:07:51 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2788 ms on 9bbba8114907 (executor driver) (96/200)
[2021-05-14 19:07:51,823] {docker.py:276} INFO - 21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:51,826] {docker.py:276} INFO - 21/05/14 22:07:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296138882446742181964_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296138882446742181964_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296138882446742181964_0004}; taskId=attempt_202105142206296138882446742181964_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@739924e6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:51,826] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206296138882446742181964_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296138882446742181964_0004_m_000099_387
[2021-05-14 19:07:51,829] {docker.py:276} INFO - 21/05/14 22:07:51 INFO StagingCommitter: Task committer attempt_202105142206296138882446742181964_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296138882446742181964_0004_m_000099_387 : duration 0:00.003s
[2021-05-14 19:07:53,786] {docker.py:276} INFO - 21/05/14 22:07:53 INFO StagingCommitter: Starting: Task committer attempt_202105142206298496739896685743956_0004_m_000096_384: needsTaskCommit() Task attempt_202105142206298496739896685743956_0004_m_000096_384
21/05/14 22:07:53 INFO StagingCommitter: Task committer attempt_202105142206298496739896685743956_0004_m_000096_384: needsTaskCommit() Task attempt_202105142206298496739896685743956_0004_m_000096_384: duration 0:00.001s
[2021-05-14 19:07:53,787] {docker.py:276} INFO - 21/05/14 22:07:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298496739896685743956_0004_m_000096_384
[2021-05-14 19:07:53,789] {docker.py:276} INFO - 21/05/14 22:07:53 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
[2021-05-14 19:07:53,790] {docker.py:276} INFO - 21/05/14 22:07:53 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (9bbba8114907, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:53,792] {docker.py:276} INFO - 21/05/14 22:07:53 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2675 ms on 9bbba8114907 (executor driver) (97/200)
21/05/14 22:07:53 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-14 19:07:53,806] {docker.py:276} INFO - 21/05/14 22:07:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:53,808] {docker.py:276} INFO - 21/05/14 22:07:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298332933231556914537_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298332933231556914537_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298332933231556914537_0004}; taskId=attempt_202105142206298332933231556914537_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ac69929}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:53,809] {docker.py:276} INFO - 21/05/14 22:07:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:53 INFO StagingCommitter: Starting: Task committer attempt_202105142206298332933231556914537_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298332933231556914537_0004_m_000100_388
[2021-05-14 19:07:53,811] {docker.py:276} INFO - 21/05/14 22:07:53 INFO StagingCommitter: Task committer attempt_202105142206298332933231556914537_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298332933231556914537_0004_m_000100_388 : duration 0:00.003s
[2021-05-14 19:07:53,957] {docker.py:276} INFO - 21/05/14 22:07:53 INFO StagingCommitter: Starting: Task committer attempt_20210514220629601041004401756124_0004_m_000097_385: needsTaskCommit() Task attempt_20210514220629601041004401756124_0004_m_000097_385
[2021-05-14 19:07:53,957] {docker.py:276} INFO - 21/05/14 22:07:53 INFO StagingCommitter: Task committer attempt_20210514220629601041004401756124_0004_m_000097_385: needsTaskCommit() Task attempt_20210514220629601041004401756124_0004_m_000097_385: duration 0:00.000s
21/05/14 22:07:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629601041004401756124_0004_m_000097_385
[2021-05-14 19:07:53,958] {docker.py:276} INFO - 21/05/14 22:07:53 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4544 bytes result sent to driver
[2021-05-14 19:07:53,959] {docker.py:276} INFO - 21/05/14 22:07:53 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (9bbba8114907, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:53,959] {docker.py:276} INFO - 21/05/14 22:07:53 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-14 19:07:53,960] {docker.py:276} INFO - 21/05/14 22:07:53 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2499 ms on 9bbba8114907 (executor driver) (98/200)
[2021-05-14 19:07:53,996] {docker.py:276} INFO - 21/05/14 22:07:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:53,996] {docker.py:276} INFO - 21/05/14 22:07:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:53,999] {docker.py:276} INFO - 21/05/14 22:07:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:53,999] {docker.py:276} INFO - 21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294190955760419625616_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294190955760419625616_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294190955760419625616_0004}; taskId=attempt_202105142206294190955760419625616_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7cdcd411}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206294190955760419625616_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294190955760419625616_0004_m_000101_389
[2021-05-14 19:07:54,001] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Task committer attempt_202105142206294190955760419625616_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294190955760419625616_0004_m_000101_389 : duration 0:00.002s
[2021-05-14 19:07:54,323] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206297386069399068214607_0004_m_000098_386: needsTaskCommit() Task attempt_202105142206297386069399068214607_0004_m_000098_386
[2021-05-14 19:07:54,324] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Task committer attempt_202105142206297386069399068214607_0004_m_000098_386: needsTaskCommit() Task attempt_202105142206297386069399068214607_0004_m_000098_386: duration 0:00.001s
[2021-05-14 19:07:54,325] {docker.py:276} INFO - 21/05/14 22:07:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297386069399068214607_0004_m_000098_386
[2021-05-14 19:07:54,326] {docker.py:276} INFO - 21/05/14 22:07:54 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4587 bytes result sent to driver
[2021-05-14 19:07:54,328] {docker.py:276} INFO - 21/05/14 22:07:54 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (9bbba8114907, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:54,329] {docker.py:276} INFO - 21/05/14 22:07:54 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2816 ms on 9bbba8114907 (executor driver) (99/200)
[2021-05-14 19:07:54,329] {docker.py:276} INFO - 21/05/14 22:07:54 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-14 19:07:54,337] {docker.py:276} INFO - 21/05/14 22:07:54 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:54,338] {docker.py:276} INFO - 21/05/14 22:07:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:54,340] {docker.py:276} INFO - 21/05/14 22:07:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:07:54,340] {docker.py:276} INFO - 21/05/14 22:07:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:54,340] {docker.py:276} INFO - 21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:54,341] {docker.py:276} INFO - 21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297531380751101395503_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297531380751101395503_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297531380751101395503_0004}; taskId=attempt_202105142206297531380751101395503_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@547fe41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:54,341] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206297531380751101395503_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297531380751101395503_0004_m_000102_390
[2021-05-14 19:07:54,343] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Task committer attempt_202105142206297531380751101395503_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297531380751101395503_0004_m_000102_390 : duration 0:00.002s
[2021-05-14 19:07:54,526] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206296138882446742181964_0004_m_000099_387: needsTaskCommit() Task attempt_202105142206296138882446742181964_0004_m_000099_387
[2021-05-14 19:07:54,527] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Task committer attempt_202105142206296138882446742181964_0004_m_000099_387: needsTaskCommit() Task attempt_202105142206296138882446742181964_0004_m_000099_387: duration 0:00.000s
21/05/14 22:07:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296138882446742181964_0004_m_000099_387
[2021-05-14 19:07:54,528] {docker.py:276} INFO - 21/05/14 22:07:54 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-14 19:07:54,529] {docker.py:276} INFO - 21/05/14 22:07:54 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (9bbba8114907, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:54,529] {docker.py:276} INFO - 21/05/14 22:07:54 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2722 ms on 9bbba8114907 (executor driver) (100/200)
[2021-05-14 19:07:54,530] {docker.py:276} INFO - 21/05/14 22:07:54 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-14 19:07:54,538] {docker.py:276} INFO - 21/05/14 22:07:54 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:54,538] {docker.py:276} INFO - 21/05/14 22:07:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:54,540] {docker.py:276} INFO - 21/05/14 22:07:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:54,540] {docker.py:276} INFO - 21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297390424894268080960_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297390424894268080960_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297390424894268080960_0004}; taskId=attempt_202105142206297390424894268080960_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a76d2f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:54,541] {docker.py:276} INFO - 21/05/14 22:07:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206297390424894268080960_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297390424894268080960_0004_m_000103_391
[2021-05-14 19:07:54,544] {docker.py:276} INFO - 21/05/14 22:07:54 INFO StagingCommitter: Task committer attempt_202105142206297390424894268080960_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297390424894268080960_0004_m_000103_391 : duration 0:00.003s
[2021-05-14 19:07:56,638] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206294190955760419625616_0004_m_000101_389: needsTaskCommit() Task attempt_202105142206294190955760419625616_0004_m_000101_389
21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206294190955760419625616_0004_m_000101_389: needsTaskCommit() Task attempt_202105142206294190955760419625616_0004_m_000101_389: duration 0:00.000s
21/05/14 22:07:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294190955760419625616_0004_m_000101_389
[2021-05-14 19:07:56,639] {docker.py:276} INFO - 21/05/14 22:07:56 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4587 bytes result sent to driver
[2021-05-14 19:07:56,641] {docker.py:276} INFO - 21/05/14 22:07:56 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (9bbba8114907, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:07:56 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
21/05/14 22:07:56 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2686 ms on 9bbba8114907 (executor driver) (101/200)
[2021-05-14 19:07:56,643] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206298332933231556914537_0004_m_000100_388: needsTaskCommit() Task attempt_202105142206298332933231556914537_0004_m_000100_388
21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206298332933231556914537_0004_m_000100_388: needsTaskCommit() Task attempt_202105142206298332933231556914537_0004_m_000100_388: duration 0:00.000s
21/05/14 22:07:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298332933231556914537_0004_m_000100_388
[2021-05-14 19:07:56,645] {docker.py:276} INFO - 21/05/14 22:07:56 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4587 bytes result sent to driver
21/05/14 22:07:56 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (9bbba8114907, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:07:56 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 2859 ms on 9bbba8114907 (executor driver) (102/200)
[2021-05-14 19:07:56,646] {docker.py:276} INFO - 21/05/14 22:07:56 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-14 19:07:56,650] {docker.py:276} INFO - 21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:56,652] {docker.py:276} INFO - 21/05/14 22:07:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:56,652] {docker.py:276} INFO - 21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296368386042035318334_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296368386042035318334_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296368386042035318334_0004}; taskId=attempt_202105142206296368386042035318334_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bff540e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206296368386042035318334_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296368386042035318334_0004_m_000104_392
[2021-05-14 19:07:56,653] {docker.py:276} INFO - 21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:07:56,654] {docker.py:276} INFO - 21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:56,655] {docker.py:276} INFO - 21/05/14 22:07:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292423414854545562570_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292423414854545562570_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292423414854545562570_0004}; taskId=attempt_202105142206292423414854545562570_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e1b09c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:56,656] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206292423414854545562570_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292423414854545562570_0004_m_000105_393
[2021-05-14 19:07:56,656] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206296368386042035318334_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296368386042035318334_0004_m_000104_392 : duration 0:00.003s
[2021-05-14 19:07:56,659] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206292423414854545562570_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292423414854545562570_0004_m_000105_393 : duration 0:00.003s
[2021-05-14 19:07:56,810] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206297531380751101395503_0004_m_000102_390: needsTaskCommit() Task attempt_202105142206297531380751101395503_0004_m_000102_390
[2021-05-14 19:07:56,810] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206297531380751101395503_0004_m_000102_390: needsTaskCommit() Task attempt_202105142206297531380751101395503_0004_m_000102_390: duration 0:00.001s
21/05/14 22:07:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297531380751101395503_0004_m_000102_390
[2021-05-14 19:07:56,812] {docker.py:276} INFO - 21/05/14 22:07:56 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4544 bytes result sent to driver
[2021-05-14 19:07:56,813] {docker.py:276} INFO - 21/05/14 22:07:56 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (9bbba8114907, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:56,814] {docker.py:276} INFO - 21/05/14 22:07:56 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-14 19:07:56,815] {docker.py:276} INFO - 21/05/14 22:07:56 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2490 ms on 9bbba8114907 (executor driver) (103/200)
[2021-05-14 19:07:56,822] {docker.py:276} INFO - 21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:56,825] {docker.py:276} INFO - 21/05/14 22:07:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292556831023456101906_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292556831023456101906_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292556831023456101906_0004}; taskId=attempt_202105142206292556831023456101906_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f07e728}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:56,826] {docker.py:276} INFO - 21/05/14 22:07:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:56 INFO StagingCommitter: Starting: Task committer attempt_202105142206292556831023456101906_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292556831023456101906_0004_m_000106_394
[2021-05-14 19:07:56,830] {docker.py:276} INFO - 21/05/14 22:07:56 INFO StagingCommitter: Task committer attempt_202105142206292556831023456101906_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292556831023456101906_0004_m_000106_394 : duration 0:00.004s
[2021-05-14 19:07:57,285] {docker.py:276} INFO - 21/05/14 22:07:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206297390424894268080960_0004_m_000103_391: needsTaskCommit() Task attempt_202105142206297390424894268080960_0004_m_000103_391
[2021-05-14 19:07:57,285] {docker.py:276} INFO - 21/05/14 22:07:57 INFO StagingCommitter: Task committer attempt_202105142206297390424894268080960_0004_m_000103_391: needsTaskCommit() Task attempt_202105142206297390424894268080960_0004_m_000103_391: duration 0:00.000s
21/05/14 22:07:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297390424894268080960_0004_m_000103_391
[2021-05-14 19:07:57,286] {docker.py:276} INFO - 21/05/14 22:07:57 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4544 bytes result sent to driver
[2021-05-14 19:07:57,287] {docker.py:276} INFO - 21/05/14 22:07:57 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (9bbba8114907, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:57,288] {docker.py:276} INFO - 21/05/14 22:07:57 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2761 ms on 9bbba8114907 (executor driver) (104/200)
21/05/14 22:07:57 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-14 19:07:57,304] {docker.py:276} INFO - 21/05/14 22:07:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:57,306] {docker.py:276} INFO - 21/05/14 22:07:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294590777648091832352_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294590777648091832352_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294590777648091832352_0004}; taskId=attempt_202105142206294590777648091832352_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40ef0ba4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206294590777648091832352_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294590777648091832352_0004_m_000107_395
[2021-05-14 19:07:57,309] {docker.py:276} INFO - 21/05/14 22:07:57 INFO StagingCommitter: Task committer attempt_202105142206294590777648091832352_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294590777648091832352_0004_m_000107_395 : duration 0:00.003s
[2021-05-14 19:07:59,219] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206296368386042035318334_0004_m_000104_392: needsTaskCommit() Task attempt_202105142206296368386042035318334_0004_m_000104_392
[2021-05-14 19:07:59,219] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206296368386042035318334_0004_m_000104_392: needsTaskCommit() Task attempt_202105142206296368386042035318334_0004_m_000104_392: duration 0:00.001s
[2021-05-14 19:07:59,220] {docker.py:276} INFO - 21/05/14 22:07:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296368386042035318334_0004_m_000104_392
[2021-05-14 19:07:59,222] {docker.py:276} INFO - 21/05/14 22:07:59 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4587 bytes result sent to driver
[2021-05-14 19:07:59,223] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (9bbba8114907, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:59,225] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 2587 ms on 9bbba8114907 (executor driver) (105/200)
21/05/14 22:07:59 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-14 19:07:59,235] {docker.py:276} INFO - 21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:59,237] {docker.py:276} INFO - 21/05/14 22:07:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:59,237] {docker.py:276} INFO - 21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291682298456093290600_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291682298456093290600_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291682298456093290600_0004}; taskId=attempt_202105142206291682298456093290600_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3df34570}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206291682298456093290600_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291682298456093290600_0004_m_000108_396
[2021-05-14 19:07:59,241] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206291682298456093290600_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291682298456093290600_0004_m_000108_396 : duration 0:00.003s
[2021-05-14 19:07:59,242] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206292423414854545562570_0004_m_000105_393: needsTaskCommit() Task attempt_202105142206292423414854545562570_0004_m_000105_393
[2021-05-14 19:07:59,243] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206292423414854545562570_0004_m_000105_393: needsTaskCommit() Task attempt_202105142206292423414854545562570_0004_m_000105_393: duration 0:00.001s
21/05/14 22:07:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292423414854545562570_0004_m_000105_393
[2021-05-14 19:07:59,244] {docker.py:276} INFO - 21/05/14 22:07:59 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4587 bytes result sent to driver
[2021-05-14 19:07:59,245] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (9bbba8114907, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:59,246] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 2605 ms on 9bbba8114907 (executor driver) (106/200)
[2021-05-14 19:07:59,247] {docker.py:276} INFO - 21/05/14 22:07:59 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-14 19:07:59,254] {docker.py:276} INFO - 21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.2 KiB) non-empty blocks including 5 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:59,256] {docker.py:276} INFO - 21/05/14 22:07:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:07:59,256] {docker.py:276} INFO - 21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296399698061384637751_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296399698061384637751_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296399698061384637751_0004}; taskId=attempt_202105142206296399698061384637751_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43da079c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206296399698061384637751_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296399698061384637751_0004_m_000109_397
[2021-05-14 19:07:59,260] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206296399698061384637751_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296399698061384637751_0004_m_000109_397 : duration 0:00.004s
[2021-05-14 19:07:59,325] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206292556831023456101906_0004_m_000106_394: needsTaskCommit() Task attempt_202105142206292556831023456101906_0004_m_000106_394
21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206292556831023456101906_0004_m_000106_394: needsTaskCommit() Task attempt_202105142206292556831023456101906_0004_m_000106_394: duration 0:00.000s
21/05/14 22:07:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292556831023456101906_0004_m_000106_394
[2021-05-14 19:07:59,327] {docker.py:276} INFO - 21/05/14 22:07:59 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4587 bytes result sent to driver
[2021-05-14 19:07:59,329] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (9bbba8114907, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:07:59,330] {docker.py:276} INFO - 21/05/14 22:07:59 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-14 19:07:59,331] {docker.py:276} INFO - 21/05/14 22:07:59 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 2521 ms on 9bbba8114907 (executor driver) (107/200)
[2021-05-14 19:07:59,340] {docker.py:276} INFO - 21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:07:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:07:59,342] {docker.py:276} INFO - 21/05/14 22:07:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:07:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:07:59,343] {docker.py:276} INFO - 21/05/14 22:07:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298207655977784651242_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298207655977784651242_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298207655977784651242_0004}; taskId=attempt_202105142206298207655977784651242_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21f7ce88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:07:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:07:59,343] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Starting: Task committer attempt_202105142206298207655977784651242_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298207655977784651242_0004_m_000110_398
[2021-05-14 19:07:59,346] {docker.py:276} INFO - 21/05/14 22:07:59 INFO StagingCommitter: Task committer attempt_202105142206298207655977784651242_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298207655977784651242_0004_m_000110_398 : duration 0:00.003s
[2021-05-14 19:08:00,078] {docker.py:276} INFO - 21/05/14 22:08:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206294590777648091832352_0004_m_000107_395: needsTaskCommit() Task attempt_202105142206294590777648091832352_0004_m_000107_395
[2021-05-14 19:08:00,079] {docker.py:276} INFO - 21/05/14 22:08:00 INFO StagingCommitter: Task committer attempt_202105142206294590777648091832352_0004_m_000107_395: needsTaskCommit() Task attempt_202105142206294590777648091832352_0004_m_000107_395: duration 0:00.000s
21/05/14 22:08:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294590777648091832352_0004_m_000107_395
[2021-05-14 19:08:00,080] {docker.py:276} INFO - 21/05/14 22:08:00 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4587 bytes result sent to driver
[2021-05-14 19:08:00,081] {docker.py:276} INFO - 21/05/14 22:08:00 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (9bbba8114907, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:00,082] {docker.py:276} INFO - 21/05/14 22:08:00 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 2799 ms on 9bbba8114907 (executor driver) (108/200)
[2021-05-14 19:08:00,083] {docker.py:276} INFO - 21/05/14 22:08:00 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-14 19:08:00,092] {docker.py:276} INFO - 21/05/14 22:08:00 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:00,094] {docker.py:276} INFO - 21/05/14 22:08:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296032324506402761873_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296032324506402761873_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296032324506402761873_0004}; taskId=attempt_202105142206296032324506402761873_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35876614}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:00,094] {docker.py:276} INFO - 21/05/14 22:08:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206296032324506402761873_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296032324506402761873_0004_m_000111_399
[2021-05-14 19:08:00,098] {docker.py:276} INFO - 21/05/14 22:08:00 INFO StagingCommitter: Task committer attempt_202105142206296032324506402761873_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296032324506402761873_0004_m_000111_399 : duration 0:00.003s
[2021-05-14 19:08:01,896] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Starting: Task committer attempt_202105142206291682298456093290600_0004_m_000108_396: needsTaskCommit() Task attempt_202105142206291682298456093290600_0004_m_000108_396
21/05/14 22:08:01 INFO StagingCommitter: Task committer attempt_202105142206291682298456093290600_0004_m_000108_396: needsTaskCommit() Task attempt_202105142206291682298456093290600_0004_m_000108_396: duration 0:00.001s
21/05/14 22:08:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291682298456093290600_0004_m_000108_396
[2021-05-14 19:08:01,898] {docker.py:276} INFO - 21/05/14 22:08:01 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4544 bytes result sent to driver
[2021-05-14 19:08:01,899] {docker.py:276} INFO - 21/05/14 22:08:01 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (9bbba8114907, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:01,900] {docker.py:276} INFO - 21/05/14 22:08:01 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 2681 ms on 9bbba8114907 (executor driver) (109/200)
21/05/14 22:08:01 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-14 19:08:01,909] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Starting: Task committer attempt_202105142206296399698061384637751_0004_m_000109_397: needsTaskCommit() Task attempt_202105142206296399698061384637751_0004_m_000109_397
[2021-05-14 19:08:01,910] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Task committer attempt_202105142206296399698061384637751_0004_m_000109_397: needsTaskCommit() Task attempt_202105142206296399698061384637751_0004_m_000109_397: duration 0:00.001s
21/05/14 22:08:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296399698061384637751_0004_m_000109_397
[2021-05-14 19:08:01,910] {docker.py:276} INFO - 21/05/14 22:08:01 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4544 bytes result sent to driver
[2021-05-14 19:08:01,911] {docker.py:276} INFO - 21/05/14 22:08:01 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (9bbba8114907, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:01,911] {docker.py:276} INFO - 21/05/14 22:08:01 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 2669 ms on 9bbba8114907 (executor driver) (110/200)
[2021-05-14 19:08:01,912] {docker.py:276} INFO - 21/05/14 22:08:01 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-14 19:08:01,913] {docker.py:276} INFO - 21/05/14 22:08:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:01,915] {docker.py:276} INFO - 21/05/14 22:08:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296957655844860207112_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296957655844860207112_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296957655844860207112_0004}; taskId=attempt_202105142206296957655844860207112_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@543689b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:01,915] {docker.py:276} INFO - 21/05/14 22:08:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:01,916] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Starting: Task committer attempt_202105142206296957655844860207112_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296957655844860207112_0004_m_000112_400
[2021-05-14 19:08:01,918] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Task committer attempt_202105142206296957655844860207112_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296957655844860207112_0004_m_000112_400 : duration 0:00.003s
[2021-05-14 19:08:01,930] {docker.py:276} INFO - 21/05/14 22:08:01 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:01,933] {docker.py:276} INFO - 21/05/14 22:08:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:01,933] {docker.py:276} INFO - 21/05/14 22:08:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:01,933] {docker.py:276} INFO - 21/05/14 22:08:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292665571244719070238_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292665571244719070238_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292665571244719070238_0004}; taskId=attempt_202105142206292665571244719070238_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f8f1b77}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:01,934] {docker.py:276} INFO - 21/05/14 22:08:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:01,934] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Starting: Task committer attempt_202105142206292665571244719070238_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292665571244719070238_0004_m_000113_401
[2021-05-14 19:08:01,937] {docker.py:276} INFO - 21/05/14 22:08:01 INFO StagingCommitter: Task committer attempt_202105142206292665571244719070238_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292665571244719070238_0004_m_000113_401 : duration 0:00.003s
[2021-05-14 19:08:02,074] {docker.py:276} INFO - 21/05/14 22:08:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206298207655977784651242_0004_m_000110_398: needsTaskCommit() Task attempt_202105142206298207655977784651242_0004_m_000110_398
[2021-05-14 19:08:02,076] {docker.py:276} INFO - 21/05/14 22:08:02 INFO StagingCommitter: Task committer attempt_202105142206298207655977784651242_0004_m_000110_398: needsTaskCommit() Task attempt_202105142206298207655977784651242_0004_m_000110_398: duration 0:00.002s
21/05/14 22:08:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298207655977784651242_0004_m_000110_398
[2021-05-14 19:08:02,078] {docker.py:276} INFO - 21/05/14 22:08:02 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4587 bytes result sent to driver
[2021-05-14 19:08:02,079] {docker.py:276} INFO - 21/05/14 22:08:02 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (9bbba8114907, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:02,080] {docker.py:276} INFO - 21/05/14 22:08:02 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2754 ms on 9bbba8114907 (executor driver) (111/200)
[2021-05-14 19:08:02,081] {docker.py:276} INFO - 21/05/14 22:08:02 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-14 19:08:02,091] {docker.py:276} INFO - 21/05/14 22:08:02 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:02,093] {docker.py:276} INFO - 21/05/14 22:08:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:02,094] {docker.py:276} INFO - 21/05/14 22:08:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297804417049437415510_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297804417049437415510_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297804417049437415510_0004}; taskId=attempt_202105142206297804417049437415510_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6cdc62f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:02,094] {docker.py:276} INFO - 21/05/14 22:08:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206297804417049437415510_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297804417049437415510_0004_m_000114_402
[2021-05-14 19:08:02,098] {docker.py:276} INFO - 21/05/14 22:08:02 INFO StagingCommitter: Task committer attempt_202105142206297804417049437415510_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297804417049437415510_0004_m_000114_402 : duration 0:00.004s
[2021-05-14 19:08:02,994] {docker.py:276} INFO - 21/05/14 22:08:03 INFO StagingCommitter: Starting: Task committer attempt_202105142206296032324506402761873_0004_m_000111_399: needsTaskCommit() Task attempt_202105142206296032324506402761873_0004_m_000111_399
21/05/14 22:08:03 INFO StagingCommitter: Task committer attempt_202105142206296032324506402761873_0004_m_000111_399: needsTaskCommit() Task attempt_202105142206296032324506402761873_0004_m_000111_399: duration 0:00.001s
[2021-05-14 19:08:02,995] {docker.py:276} INFO - 21/05/14 22:08:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296032324506402761873_0004_m_000111_399
[2021-05-14 19:08:02,996] {docker.py:276} INFO - 21/05/14 22:08:03 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4587 bytes result sent to driver
[2021-05-14 19:08:02,997] {docker.py:276} INFO - 21/05/14 22:08:03 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (9bbba8114907, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:02,998] {docker.py:276} INFO - 21/05/14 22:08:03 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2920 ms on 9bbba8114907 (executor driver) (112/200)
[2021-05-14 19:08:02,999] {docker.py:276} INFO - 21/05/14 22:08:03 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-14 19:08:03,007] {docker.py:276} INFO - 21/05/14 22:08:03 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:03,009] {docker.py:276} INFO - 21/05/14 22:08:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299024860128920035617_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299024860128920035617_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299024860128920035617_0004}; taskId=attempt_202105142206299024860128920035617_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cb073f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:03,010] {docker.py:276} INFO - 21/05/14 22:08:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:03 INFO StagingCommitter: Starting: Task committer attempt_202105142206299024860128920035617_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299024860128920035617_0004_m_000115_403
[2021-05-14 19:08:03,012] {docker.py:276} INFO - 21/05/14 22:08:03 INFO StagingCommitter: Task committer attempt_202105142206299024860128920035617_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299024860128920035617_0004_m_000115_403 : duration 0:00.003s
[2021-05-14 19:08:04,547] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206292665571244719070238_0004_m_000113_401: needsTaskCommit() Task attempt_202105142206292665571244719070238_0004_m_000113_401
[2021-05-14 19:08:04,548] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206292665571244719070238_0004_m_000113_401: needsTaskCommit() Task attempt_202105142206292665571244719070238_0004_m_000113_401: duration 0:00.001s
21/05/14 22:08:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292665571244719070238_0004_m_000113_401
[2021-05-14 19:08:04,550] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4587 bytes result sent to driver
[2021-05-14 19:08:04,552] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (9bbba8114907, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:04,553] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-14 19:08:04,553] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 2644 ms on 9bbba8114907 (executor driver) (113/200)
[2021-05-14 19:08:04,556] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206296957655844860207112_0004_m_000112_400: needsTaskCommit() Task attempt_202105142206296957655844860207112_0004_m_000112_400
[2021-05-14 19:08:04,557] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206296957655844860207112_0004_m_000112_400: needsTaskCommit() Task attempt_202105142206296957655844860207112_0004_m_000112_400: duration 0:00.000s
21/05/14 22:08:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296957655844860207112_0004_m_000112_400
[2021-05-14 19:08:04,557] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4587 bytes result sent to driver
[2021-05-14 19:08:04,559] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (9bbba8114907, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:04,559] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 2663 ms on 9bbba8114907 (executor driver) (114/200)
[2021-05-14 19:08:04,560] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-14 19:08:04,564] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:04,564] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:04,566] {docker.py:276} INFO - 21/05/14 22:08:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:04,566] {docker.py:276} INFO - 21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:04,566] {docker.py:276} INFO - 21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292044896118111572245_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292044896118111572245_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292044896118111572245_0004}; taskId=attempt_202105142206292044896118111572245_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64f56c09}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:04,567] {docker.py:276} INFO - 21/05/14 22:08:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:04,567] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206292044896118111572245_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292044896118111572245_0004_m_000116_404
[2021-05-14 19:08:04,569] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206292044896118111572245_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292044896118111572245_0004_m_000116_404 : duration 0:00.002s
[2021-05-14 19:08:04,569] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:04,570] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:04,571] {docker.py:276} INFO - 21/05/14 22:08:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:04,572] {docker.py:276} INFO - 21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:04,572] {docker.py:276} INFO - 21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297539988160621403187_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297539988160621403187_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297539988160621403187_0004}; taskId=attempt_202105142206297539988160621403187_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@112d0c49}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:04,573] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206297539988160621403187_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297539988160621403187_0004_m_000117_405
[2021-05-14 19:08:04,575] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206297539988160621403187_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297539988160621403187_0004_m_000117_405 : duration 0:00.003s
[2021-05-14 19:08:04,737] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206297804417049437415510_0004_m_000114_402: needsTaskCommit() Task attempt_202105142206297804417049437415510_0004_m_000114_402
[2021-05-14 19:08:04,738] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206297804417049437415510_0004_m_000114_402: needsTaskCommit() Task attempt_202105142206297804417049437415510_0004_m_000114_402: duration 0:00.000s
21/05/14 22:08:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297804417049437415510_0004_m_000114_402
[2021-05-14 19:08:04,739] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4544 bytes result sent to driver
[2021-05-14 19:08:04,740] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (9bbba8114907, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:04,740] {docker.py:276} INFO - 21/05/14 22:08:04 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-14 19:08:04,741] {docker.py:276} INFO - 21/05/14 22:08:04 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2666 ms on 9bbba8114907 (executor driver) (115/200)
[2021-05-14 19:08:04,750] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:04,751] {docker.py:276} INFO - 21/05/14 22:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:04,753] {docker.py:276} INFO - 21/05/14 22:08:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296867063300293295821_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296867063300293295821_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296867063300293295821_0004}; taskId=attempt_202105142206296867063300293295821_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19626526}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:04,753] {docker.py:276} INFO - 21/05/14 22:08:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:04 INFO StagingCommitter: Starting: Task committer attempt_202105142206296867063300293295821_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296867063300293295821_0004_m_000118_406
[2021-05-14 19:08:04,756] {docker.py:276} INFO - 21/05/14 22:08:04 INFO StagingCommitter: Task committer attempt_202105142206296867063300293295821_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296867063300293295821_0004_m_000118_406 : duration 0:00.003s
[2021-05-14 19:08:05,954] {docker.py:276} INFO - 21/05/14 22:08:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206299024860128920035617_0004_m_000115_403: needsTaskCommit() Task attempt_202105142206299024860128920035617_0004_m_000115_403
[2021-05-14 19:08:05,955] {docker.py:276} INFO - 21/05/14 22:08:05 INFO StagingCommitter: Task committer attempt_202105142206299024860128920035617_0004_m_000115_403: needsTaskCommit() Task attempt_202105142206299024860128920035617_0004_m_000115_403: duration 0:00.000s
21/05/14 22:08:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299024860128920035617_0004_m_000115_403
[2021-05-14 19:08:05,956] {docker.py:276} INFO - 21/05/14 22:08:05 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4544 bytes result sent to driver
[2021-05-14 19:08:05,958] {docker.py:276} INFO - 21/05/14 22:08:05 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (9bbba8114907, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:05,959] {docker.py:276} INFO - 21/05/14 22:08:05 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-14 19:08:05,960] {docker.py:276} INFO - 21/05/14 22:08:05 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2965 ms on 9bbba8114907 (executor driver) (116/200)
[2021-05-14 19:08:05,982] {docker.py:276} INFO - 21/05/14 22:08:05 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:05,984] {docker.py:276} INFO - 21/05/14 22:08:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292164230651376908024_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292164230651376908024_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292164230651376908024_0004}; taskId=attempt_202105142206292164230651376908024_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e36b26d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206292164230651376908024_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292164230651376908024_0004_m_000119_407
[2021-05-14 19:08:05,986] {docker.py:276} INFO - 21/05/14 22:08:06 INFO StagingCommitter: Task committer attempt_202105142206292164230651376908024_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292164230651376908024_0004_m_000119_407 : duration 0:00.003s
[2021-05-14 19:08:07,693] {docker.py:276} INFO - 21/05/14 22:08:07 INFO StagingCommitter: Starting: Task committer attempt_202105142206297539988160621403187_0004_m_000117_405: needsTaskCommit() Task attempt_202105142206297539988160621403187_0004_m_000117_405
[2021-05-14 19:08:07,693] {docker.py:276} INFO - 21/05/14 22:08:07 INFO StagingCommitter: Task committer attempt_202105142206297539988160621403187_0004_m_000117_405: needsTaskCommit() Task attempt_202105142206297539988160621403187_0004_m_000117_405: duration 0:00.000s
21/05/14 22:08:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297539988160621403187_0004_m_000117_405
[2021-05-14 19:08:07,694] {docker.py:276} INFO - 21/05/14 22:08:07 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4587 bytes result sent to driver
[2021-05-14 19:08:07,696] {docker.py:276} INFO - 21/05/14 22:08:07 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (9bbba8114907, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:07,697] {docker.py:276} INFO - 21/05/14 22:08:07 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 3143 ms on 9bbba8114907 (executor driver) (117/200)
[2021-05-14 19:08:07,698] {docker.py:276} INFO - 21/05/14 22:08:07 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-14 19:08:07,708] {docker.py:276} INFO - 21/05/14 22:08:07 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:07,710] {docker.py:276} INFO - 21/05/14 22:08:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629287485586194237867_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629287485586194237867_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629287485586194237867_0004}; taskId=attempt_20210514220629287485586194237867_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a7ea884}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:07,710] {docker.py:276} INFO - 21/05/14 22:08:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:07 INFO StagingCommitter: Starting: Task committer attempt_20210514220629287485586194237867_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629287485586194237867_0004_m_000120_408
[2021-05-14 19:08:07,714] {docker.py:276} INFO - 21/05/14 22:08:07 INFO StagingCommitter: Task committer attempt_20210514220629287485586194237867_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629287485586194237867_0004_m_000120_408 : duration 0:00.003s
[2021-05-14 19:08:08,509] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206292044896118111572245_0004_m_000116_404: needsTaskCommit() Task attempt_202105142206292044896118111572245_0004_m_000116_404
[2021-05-14 19:08:08,510] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Task committer attempt_202105142206292044896118111572245_0004_m_000116_404: needsTaskCommit() Task attempt_202105142206292044896118111572245_0004_m_000116_404: duration 0:00.001s
21/05/14 22:08:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292044896118111572245_0004_m_000116_404
[2021-05-14 19:08:08,513] {docker.py:276} INFO - 21/05/14 22:08:08 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4587 bytes result sent to driver
[2021-05-14 19:08:08,515] {docker.py:276} INFO - 21/05/14 22:08:08 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (9bbba8114907, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:08,516] {docker.py:276} INFO - 21/05/14 22:08:08 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 3970 ms on 9bbba8114907 (executor driver) (118/200)
[2021-05-14 19:08:08,517] {docker.py:276} INFO - 21/05/14 22:08:08 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
[2021-05-14 19:08:08,533] {docker.py:276} INFO - 21/05/14 22:08:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:08,533] {docker.py:276} INFO - 21/05/14 22:08:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:08,535] {docker.py:276} INFO - 21/05/14 22:08:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:08,535] {docker.py:276} INFO - 21/05/14 22:08:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:08,535] {docker.py:276} INFO - 21/05/14 22:08:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292577672286385516302_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292577672286385516302_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292577672286385516302_0004}; taskId=attempt_202105142206292577672286385516302_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@683909db}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:08,536] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206292577672286385516302_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292577672286385516302_0004_m_000121_409
[2021-05-14 19:08:08,538] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Task committer attempt_202105142206292577672286385516302_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292577672286385516302_0004_m_000121_409 : duration 0:00.003s
[2021-05-14 19:08:08,694] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206296867063300293295821_0004_m_000118_406: needsTaskCommit() Task attempt_202105142206296867063300293295821_0004_m_000118_406
[2021-05-14 19:08:08,695] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Task committer attempt_202105142206296867063300293295821_0004_m_000118_406: needsTaskCommit() Task attempt_202105142206296867063300293295821_0004_m_000118_406: duration 0:00.002s
21/05/14 22:08:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296867063300293295821_0004_m_000118_406
[2021-05-14 19:08:08,698] {docker.py:276} INFO - 21/05/14 22:08:08 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4587 bytes result sent to driver
[2021-05-14 19:08:08,700] {docker.py:276} INFO - 21/05/14 22:08:08 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (9bbba8114907, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:08,701] {docker.py:276} INFO - 21/05/14 22:08:08 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 3966 ms on 9bbba8114907 (executor driver) (119/200)
[2021-05-14 19:08:08,702] {docker.py:276} INFO - 21/05/14 22:08:08 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-14 19:08:08,712] {docker.py:276} INFO - 21/05/14 22:08:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:08,712] {docker.py:276} INFO - 21/05/14 22:08:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:08,714] {docker.py:276} INFO - 21/05/14 22:08:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296722013545602987824_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296722013545602987824_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296722013545602987824_0004}; taskId=attempt_202105142206296722013545602987824_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@518ba51e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:08 INFO StagingCommitter: Starting: Task committer attempt_202105142206296722013545602987824_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296722013545602987824_0004_m_000122_410
[2021-05-14 19:08:08,717] {docker.py:276} INFO - 21/05/14 22:08:08 INFO StagingCommitter: Task committer attempt_202105142206296722013545602987824_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296722013545602987824_0004_m_000122_410 : duration 0:00.003s
[2021-05-14 19:08:10,295] {docker.py:276} INFO - 21/05/14 22:08:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206292164230651376908024_0004_m_000119_407: needsTaskCommit() Task attempt_202105142206292164230651376908024_0004_m_000119_407
[2021-05-14 19:08:10,296] {docker.py:276} INFO - 21/05/14 22:08:10 INFO StagingCommitter: Task committer attempt_202105142206292164230651376908024_0004_m_000119_407: needsTaskCommit() Task attempt_202105142206292164230651376908024_0004_m_000119_407: duration 0:00.001s
[2021-05-14 19:08:10,296] {docker.py:276} INFO - 21/05/14 22:08:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292164230651376908024_0004_m_000119_407
[2021-05-14 19:08:10,298] {docker.py:276} INFO - 21/05/14 22:08:10 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4587 bytes result sent to driver
[2021-05-14 19:08:10,299] {docker.py:276} INFO - 21/05/14 22:08:10 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (9bbba8114907, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:10,299] {docker.py:276} INFO - 21/05/14 22:08:10 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 4347 ms on 9bbba8114907 (executor driver) (120/200)
[2021-05-14 19:08:10,300] {docker.py:276} INFO - 21/05/14 22:08:10 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
[2021-05-14 19:08:10,309] {docker.py:276} INFO - 21/05/14 22:08:10 INFO ShuffleBlockFetcherIterator: Getting 5 (44.5 KiB) non-empty blocks including 5 (44.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:10,311] {docker.py:276} INFO - 21/05/14 22:08:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294195921707287984237_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294195921707287984237_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294195921707287984237_0004}; taskId=attempt_202105142206294195921707287984237_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61a66f2a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:10 INFO StagingCommitter: Starting: Task committer attempt_202105142206294195921707287984237_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294195921707287984237_0004_m_000123_411
[2021-05-14 19:08:10,315] {docker.py:276} INFO - 21/05/14 22:08:10 INFO StagingCommitter: Task committer attempt_202105142206294195921707287984237_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294195921707287984237_0004_m_000123_411 : duration 0:00.003s
[2021-05-14 19:08:11,298] {docker.py:276} INFO - 21/05/14 22:08:11 INFO StagingCommitter: Starting: Task committer attempt_202105142206296722013545602987824_0004_m_000122_410: needsTaskCommit() Task attempt_202105142206296722013545602987824_0004_m_000122_410
[2021-05-14 19:08:11,299] {docker.py:276} INFO - 21/05/14 22:08:11 INFO StagingCommitter: Task committer attempt_202105142206296722013545602987824_0004_m_000122_410: needsTaskCommit() Task attempt_202105142206296722013545602987824_0004_m_000122_410: duration 0:00.000s
[2021-05-14 19:08:11,299] {docker.py:276} INFO - 21/05/14 22:08:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296722013545602987824_0004_m_000122_410
[2021-05-14 19:08:11,300] {docker.py:276} INFO - 21/05/14 22:08:11 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-14 19:08:11,302] {docker.py:276} INFO - 21/05/14 22:08:11 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (9bbba8114907, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:11,302] {docker.py:276} INFO - 21/05/14 22:08:11 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2607 ms on 9bbba8114907 (executor driver) (121/200)
[2021-05-14 19:08:11,303] {docker.py:276} INFO - 21/05/14 22:08:11 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-14 19:08:11,311] {docker.py:276} INFO - 21/05/14 22:08:11 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:11,313] {docker.py:276} INFO - 21/05/14 22:08:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:11,313] {docker.py:276} INFO - 21/05/14 22:08:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297136723125769423383_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136723125769423383_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297136723125769423383_0004}; taskId=attempt_202105142206297136723125769423383_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42ad5d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:11,314] {docker.py:276} INFO - 21/05/14 22:08:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:11,314] {docker.py:276} INFO - 21/05/14 22:08:11 INFO StagingCommitter: Starting: Task committer attempt_202105142206297136723125769423383_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136723125769423383_0004_m_000124_412
[2021-05-14 19:08:11,317] {docker.py:276} INFO - 21/05/14 22:08:11 INFO StagingCommitter: Task committer attempt_202105142206297136723125769423383_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136723125769423383_0004_m_000124_412 : duration 0:00.003s
[2021-05-14 19:08:12,181] {docker.py:276} INFO - 21/05/14 22:08:12 INFO StagingCommitter: Starting: Task committer attempt_20210514220629287485586194237867_0004_m_000120_408: needsTaskCommit() Task attempt_20210514220629287485586194237867_0004_m_000120_408
[2021-05-14 19:08:12,183] {docker.py:276} INFO - 21/05/14 22:08:12 INFO StagingCommitter: Task committer attempt_20210514220629287485586194237867_0004_m_000120_408: needsTaskCommit() Task attempt_20210514220629287485586194237867_0004_m_000120_408: duration 0:00.001s
[2021-05-14 19:08:12,184] {docker.py:276} INFO - 21/05/14 22:08:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629287485586194237867_0004_m_000120_408
[2021-05-14 19:08:12,187] {docker.py:276} INFO - 21/05/14 22:08:12 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4544 bytes result sent to driver
[2021-05-14 19:08:12,188] {docker.py:276} INFO - 21/05/14 22:08:12 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (9bbba8114907, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:12,190] {docker.py:276} INFO - 21/05/14 22:08:12 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-14 19:08:12,191] {docker.py:276} INFO - 21/05/14 22:08:12 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 4500 ms on 9bbba8114907 (executor driver) (122/200)
[2021-05-14 19:08:12,207] {docker.py:276} INFO - 21/05/14 22:08:12 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:12,207] {docker.py:276} INFO - 21/05/14 22:08:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:12,209] {docker.py:276} INFO - 21/05/14 22:08:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292024079662513658402_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292024079662513658402_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292024079662513658402_0004}; taskId=attempt_202105142206292024079662513658402_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2293076}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:12 INFO StagingCommitter: Starting: Task committer attempt_202105142206292024079662513658402_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292024079662513658402_0004_m_000125_413
[2021-05-14 19:08:12,212] {docker.py:276} INFO - 21/05/14 22:08:12 INFO StagingCommitter: Task committer attempt_202105142206292024079662513658402_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292024079662513658402_0004_m_000125_413 : duration 0:00.003s
[2021-05-14 19:08:13,429] {docker.py:276} INFO - 21/05/14 22:08:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206292577672286385516302_0004_m_000121_409: needsTaskCommit() Task attempt_202105142206292577672286385516302_0004_m_000121_409
[2021-05-14 19:08:13,430] {docker.py:276} INFO - 21/05/14 22:08:13 INFO StagingCommitter: Task committer attempt_202105142206292577672286385516302_0004_m_000121_409: needsTaskCommit() Task attempt_202105142206292577672286385516302_0004_m_000121_409: duration 0:00.001s
21/05/14 22:08:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292577672286385516302_0004_m_000121_409
[2021-05-14 19:08:13,431] {docker.py:276} INFO - 21/05/14 22:08:13 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4587 bytes result sent to driver
[2021-05-14 19:08:13,433] {docker.py:276} INFO - 21/05/14 22:08:13 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (9bbba8114907, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:13,434] {docker.py:276} INFO - 21/05/14 22:08:13 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 4925 ms on 9bbba8114907 (executor driver) (123/200)
21/05/14 22:08:13 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
[2021-05-14 19:08:13,444] {docker.py:276} INFO - 21/05/14 22:08:13 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:13,446] {docker.py:276} INFO - 21/05/14 22:08:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294045544936774132148_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294045544936774132148_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294045544936774132148_0004}; taskId=attempt_202105142206294045544936774132148_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f90bb7f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:13,447] {docker.py:276} INFO - 21/05/14 22:08:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:13,447] {docker.py:276} INFO - 21/05/14 22:08:13 INFO StagingCommitter: Starting: Task committer attempt_202105142206294045544936774132148_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294045544936774132148_0004_m_000126_414
[2021-05-14 19:08:13,450] {docker.py:276} INFO - 21/05/14 22:08:13 INFO StagingCommitter: Task committer attempt_202105142206294045544936774132148_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294045544936774132148_0004_m_000126_414 : duration 0:00.004s
[2021-05-14 19:08:14,312] {docker.py:276} INFO - 21/05/14 22:08:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206297136723125769423383_0004_m_000124_412: needsTaskCommit() Task attempt_202105142206297136723125769423383_0004_m_000124_412
21/05/14 22:08:14 INFO StagingCommitter: Task committer attempt_202105142206297136723125769423383_0004_m_000124_412: needsTaskCommit() Task attempt_202105142206297136723125769423383_0004_m_000124_412: duration 0:00.000s
21/05/14 22:08:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297136723125769423383_0004_m_000124_412
[2021-05-14 19:08:14,313] {docker.py:276} INFO - 21/05/14 22:08:14 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4587 bytes result sent to driver
[2021-05-14 19:08:14,314] {docker.py:276} INFO - 21/05/14 22:08:14 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (9bbba8114907, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:14,315] {docker.py:276} INFO - 21/05/14 22:08:14 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 3016 ms on 9bbba8114907 (executor driver) (124/200)
[2021-05-14 19:08:14,316] {docker.py:276} INFO - 21/05/14 22:08:14 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-14 19:08:14,325] {docker.py:276} INFO - 21/05/14 22:08:14 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:14,328] {docker.py:276} INFO - 21/05/14 22:08:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298883194681930433804_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298883194681930433804_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298883194681930433804_0004}; taskId=attempt_202105142206298883194681930433804_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bcfa13c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206298883194681930433804_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298883194681930433804_0004_m_000127_415
[2021-05-14 19:08:14,331] {docker.py:276} INFO - 21/05/14 22:08:14 INFO StagingCommitter: Task committer attempt_202105142206298883194681930433804_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298883194681930433804_0004_m_000127_415 : duration 0:00.003s
[2021-05-14 19:08:14,469] {docker.py:276} INFO - 21/05/14 22:08:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206294195921707287984237_0004_m_000123_411: needsTaskCommit() Task attempt_202105142206294195921707287984237_0004_m_000123_411
[2021-05-14 19:08:14,470] {docker.py:276} INFO - 21/05/14 22:08:14 INFO StagingCommitter: Task committer attempt_202105142206294195921707287984237_0004_m_000123_411: needsTaskCommit() Task attempt_202105142206294195921707287984237_0004_m_000123_411: duration 0:00.001s
21/05/14 22:08:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294195921707287984237_0004_m_000123_411
[2021-05-14 19:08:14,472] {docker.py:276} INFO - 21/05/14 22:08:14 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4587 bytes result sent to driver
[2021-05-14 19:08:14,473] {docker.py:276} INFO - 21/05/14 22:08:14 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (9bbba8114907, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:14,474] {docker.py:276} INFO - 21/05/14 22:08:14 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
[2021-05-14 19:08:14,474] {docker.py:276} INFO - 21/05/14 22:08:14 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 4181 ms on 9bbba8114907 (executor driver) (125/200)
[2021-05-14 19:08:14,483] {docker.py:276} INFO - 21/05/14 22:08:14 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:14,485] {docker.py:276} INFO - 21/05/14 22:08:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294674767173358097214_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294674767173358097214_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294674767173358097214_0004}; taskId=attempt_202105142206294674767173358097214_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c0d2a94}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:14,486] {docker.py:276} INFO - 21/05/14 22:08:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:14 INFO StagingCommitter: Starting: Task committer attempt_202105142206294674767173358097214_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294674767173358097214_0004_m_000128_416
[2021-05-14 19:08:14,489] {docker.py:276} INFO - 21/05/14 22:08:14 INFO StagingCommitter: Task committer attempt_202105142206294674767173358097214_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294674767173358097214_0004_m_000128_416 : duration 0:00.003s
[2021-05-14 19:08:15,850] {docker.py:276} INFO - 21/05/14 22:08:15 INFO StagingCommitter: Starting: Task committer attempt_202105142206292024079662513658402_0004_m_000125_413: needsTaskCommit() Task attempt_202105142206292024079662513658402_0004_m_000125_413
[2021-05-14 19:08:15,851] {docker.py:276} INFO - 21/05/14 22:08:15 INFO StagingCommitter: Task committer attempt_202105142206292024079662513658402_0004_m_000125_413: needsTaskCommit() Task attempt_202105142206292024079662513658402_0004_m_000125_413: duration 0:00.001s
21/05/14 22:08:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292024079662513658402_0004_m_000125_413
[2021-05-14 19:08:15,851] {docker.py:276} INFO - 21/05/14 22:08:15 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4587 bytes result sent to driver
[2021-05-14 19:08:15,853] {docker.py:276} INFO - 21/05/14 22:08:15 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (9bbba8114907, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:08:15 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
21/05/14 22:08:15 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 3669 ms on 9bbba8114907 (executor driver) (126/200)
[2021-05-14 19:08:15,860] {docker.py:276} INFO - 21/05/14 22:08:15 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:15,860] {docker.py:276} INFO - 21/05/14 22:08:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:15,862] {docker.py:276} INFO - 21/05/14 22:08:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:15,863] {docker.py:276} INFO - 21/05/14 22:08:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291201854125119461291_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291201854125119461291_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291201854125119461291_0004}; taskId=attempt_202105142206291201854125119461291_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1950a738}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:15,865] {docker.py:276} INFO - 21/05/14 22:08:15 INFO StagingCommitter: Starting: Task committer attempt_202105142206291201854125119461291_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291201854125119461291_0004_m_000129_417
[2021-05-14 19:08:15,872] {docker.py:276} INFO - 21/05/14 22:08:15 INFO StagingCommitter: Task committer attempt_202105142206291201854125119461291_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291201854125119461291_0004_m_000129_417 : duration 0:00.006s
[2021-05-14 19:08:16,859] {docker.py:276} INFO - 21/05/14 22:08:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206298883194681930433804_0004_m_000127_415: needsTaskCommit() Task attempt_202105142206298883194681930433804_0004_m_000127_415
21/05/14 22:08:16 INFO StagingCommitter: Task committer attempt_202105142206298883194681930433804_0004_m_000127_415: needsTaskCommit() Task attempt_202105142206298883194681930433804_0004_m_000127_415: duration 0:00.001s
21/05/14 22:08:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298883194681930433804_0004_m_000127_415
[2021-05-14 19:08:16,860] {docker.py:276} INFO - 21/05/14 22:08:16 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4544 bytes result sent to driver
[2021-05-14 19:08:16,862] {docker.py:276} INFO - 21/05/14 22:08:16 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (9bbba8114907, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:16,863] {docker.py:276} INFO - 21/05/14 22:08:16 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2550 ms on 9bbba8114907 (executor driver) (127/200)
21/05/14 22:08:16 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-14 19:08:16,873] {docker.py:276} INFO - 21/05/14 22:08:16 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:16,874] {docker.py:276} INFO - 21/05/14 22:08:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:16,875] {docker.py:276} INFO - 21/05/14 22:08:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293712158257065347197_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293712158257065347197_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293712158257065347197_0004}; taskId=attempt_202105142206293712158257065347197_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@455c1fb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:16 INFO StagingCommitter: Starting: Task committer attempt_202105142206293712158257065347197_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293712158257065347197_0004_m_000130_418
[2021-05-14 19:08:16,878] {docker.py:276} INFO - 21/05/14 22:08:16 INFO StagingCommitter: Task committer attempt_202105142206293712158257065347197_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293712158257065347197_0004_m_000130_418 : duration 0:00.003s
[2021-05-14 19:08:17,334] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Starting: Task committer attempt_202105142206294045544936774132148_0004_m_000126_414: needsTaskCommit() Task attempt_202105142206294045544936774132148_0004_m_000126_414
21/05/14 22:08:17 INFO StagingCommitter: Task committer attempt_202105142206294045544936774132148_0004_m_000126_414: needsTaskCommit() Task attempt_202105142206294045544936774132148_0004_m_000126_414: duration 0:00.000s
[2021-05-14 19:08:17,335] {docker.py:276} INFO - 21/05/14 22:08:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294045544936774132148_0004_m_000126_414
[2021-05-14 19:08:17,335] {docker.py:276} INFO - 21/05/14 22:08:17 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4544 bytes result sent to driver
[2021-05-14 19:08:17,336] {docker.py:276} INFO - 21/05/14 22:08:17 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (9bbba8114907, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:17,337] {docker.py:276} INFO - 21/05/14 22:08:17 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 3909 ms on 9bbba8114907 (executor driver) (128/200)
[2021-05-14 19:08:17,337] {docker.py:276} INFO - 21/05/14 22:08:17 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-14 19:08:17,357] {docker.py:276} INFO - 21/05/14 22:08:17 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:17,359] {docker.py:276} INFO - 21/05/14 22:08:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:17,359] {docker.py:276} INFO - 21/05/14 22:08:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295343946546779491048_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295343946546779491048_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295343946546779491048_0004}; taskId=attempt_202105142206295343946546779491048_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5efa49eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:17,360] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Starting: Task committer attempt_202105142206295343946546779491048_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295343946546779491048_0004_m_000131_419
[2021-05-14 19:08:17,362] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Task committer attempt_202105142206295343946546779491048_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295343946546779491048_0004_m_000131_419 : duration 0:00.003s
[2021-05-14 19:08:17,608] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Starting: Task committer attempt_202105142206294674767173358097214_0004_m_000128_416: needsTaskCommit() Task attempt_202105142206294674767173358097214_0004_m_000128_416
[2021-05-14 19:08:17,609] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Task committer attempt_202105142206294674767173358097214_0004_m_000128_416: needsTaskCommit() Task attempt_202105142206294674767173358097214_0004_m_000128_416: duration 0:00.001s
21/05/14 22:08:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294674767173358097214_0004_m_000128_416
[2021-05-14 19:08:17,611] {docker.py:276} INFO - 21/05/14 22:08:17 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-14 19:08:17,611] {docker.py:276} INFO - 21/05/14 22:08:17 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (9bbba8114907, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:17,612] {docker.py:276} INFO - 21/05/14 22:08:17 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-14 19:08:17,613] {docker.py:276} INFO - 21/05/14 22:08:17 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 3144 ms on 9bbba8114907 (executor driver) (129/200)
[2021-05-14 19:08:17,621] {docker.py:276} INFO - 21/05/14 22:08:17 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:17,622] {docker.py:276} INFO - 21/05/14 22:08:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:17,623] {docker.py:276} INFO - 21/05/14 22:08:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293035344148665435244_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293035344148665435244_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293035344148665435244_0004}; taskId=attempt_202105142206293035344148665435244_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@726acfad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:17 INFO StagingCommitter: Starting: Task committer attempt_202105142206293035344148665435244_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293035344148665435244_0004_m_000132_420
[2021-05-14 19:08:17,626] {docker.py:276} INFO - 21/05/14 22:08:17 INFO StagingCommitter: Task committer attempt_202105142206293035344148665435244_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293035344148665435244_0004_m_000132_420 : duration 0:00.003s
[2021-05-14 19:08:18,426] {docker.py:276} INFO - 21/05/14 22:08:18 INFO StagingCommitter: Starting: Task committer attempt_202105142206291201854125119461291_0004_m_000129_417: needsTaskCommit() Task attempt_202105142206291201854125119461291_0004_m_000129_417
[2021-05-14 19:08:18,426] {docker.py:276} INFO - 21/05/14 22:08:18 INFO StagingCommitter: Task committer attempt_202105142206291201854125119461291_0004_m_000129_417: needsTaskCommit() Task attempt_202105142206291201854125119461291_0004_m_000129_417: duration 0:00.001s
21/05/14 22:08:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291201854125119461291_0004_m_000129_417
[2021-05-14 19:08:18,429] {docker.py:276} INFO - 21/05/14 22:08:18 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4587 bytes result sent to driver
[2021-05-14 19:08:18,430] {docker.py:276} INFO - 21/05/14 22:08:18 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (9bbba8114907, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:18,431] {docker.py:276} INFO - 21/05/14 22:08:18 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2582 ms on 9bbba8114907 (executor driver) (130/200)
[2021-05-14 19:08:18,432] {docker.py:276} INFO - 21/05/14 22:08:18 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-14 19:08:18,440] {docker.py:276} INFO - 21/05/14 22:08:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:18,442] {docker.py:276} INFO - 21/05/14 22:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:18,442] {docker.py:276} INFO - 21/05/14 22:08:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:18,443] {docker.py:276} INFO - 21/05/14 22:08:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295711817925368184984_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295711817925368184984_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295711817925368184984_0004}; taskId=attempt_202105142206295711817925368184984_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47535bb8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:18 INFO StagingCommitter: Starting: Task committer attempt_202105142206295711817925368184984_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295711817925368184984_0004_m_000133_421
[2021-05-14 19:08:18,446] {docker.py:276} INFO - 21/05/14 22:08:18 INFO StagingCommitter: Task committer attempt_202105142206295711817925368184984_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295711817925368184984_0004_m_000133_421 : duration 0:00.003s
[2021-05-14 19:08:19,496] {docker.py:276} INFO - 21/05/14 22:08:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206293712158257065347197_0004_m_000130_418: needsTaskCommit() Task attempt_202105142206293712158257065347197_0004_m_000130_418
[2021-05-14 19:08:19,497] {docker.py:276} INFO - 21/05/14 22:08:19 INFO StagingCommitter: Task committer attempt_202105142206293712158257065347197_0004_m_000130_418: needsTaskCommit() Task attempt_202105142206293712158257065347197_0004_m_000130_418: duration 0:00.001s
21/05/14 22:08:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293712158257065347197_0004_m_000130_418
[2021-05-14 19:08:19,498] {docker.py:276} INFO - 21/05/14 22:08:19 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4587 bytes result sent to driver
[2021-05-14 19:08:19,499] {docker.py:276} INFO - 21/05/14 22:08:19 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (9bbba8114907, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:19,500] {docker.py:276} INFO - 21/05/14 22:08:19 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 2642 ms on 9bbba8114907 (executor driver) (131/200)
[2021-05-14 19:08:19,501] {docker.py:276} INFO - 21/05/14 22:08:19 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-14 19:08:19,511] {docker.py:276} INFO - 21/05/14 22:08:19 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:19,513] {docker.py:276} INFO - 21/05/14 22:08:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296277944759904316341_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296277944759904316341_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296277944759904316341_0004}; taskId=attempt_202105142206296277944759904316341_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@225b0244}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:19 INFO StagingCommitter: Starting: Task committer attempt_202105142206296277944759904316341_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296277944759904316341_0004_m_000134_422
[2021-05-14 19:08:19,516] {docker.py:276} INFO - 21/05/14 22:08:19 INFO StagingCommitter: Task committer attempt_202105142206296277944759904316341_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296277944759904316341_0004_m_000134_422 : duration 0:00.004s
[2021-05-14 19:08:20,064] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206295343946546779491048_0004_m_000131_419: needsTaskCommit() Task attempt_202105142206295343946546779491048_0004_m_000131_419
21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206295343946546779491048_0004_m_000131_419: needsTaskCommit() Task attempt_202105142206295343946546779491048_0004_m_000131_419: duration 0:00.001s
21/05/14 22:08:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295343946546779491048_0004_m_000131_419
[2021-05-14 19:08:20,065] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4587 bytes result sent to driver
[2021-05-14 19:08:20,066] {docker.py:276} INFO - 21/05/14 22:08:20 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (9bbba8114907, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:20,067] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
21/05/14 22:08:20 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 2734 ms on 9bbba8114907 (executor driver) (132/200)
[2021-05-14 19:08:20,078] {docker.py:276} INFO - 21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:20,092] {docker.py:276} INFO - 21/05/14 22:08:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293840950791333714375_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293840950791333714375_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293840950791333714375_0004}; taskId=attempt_202105142206293840950791333714375_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12bcd174}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:20,093] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206293840950791333714375_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293840950791333714375_0004_m_000135_423
[2021-05-14 19:08:20,093] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206293840950791333714375_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293840950791333714375_0004_m_000135_423 : duration 0:00.003s
[2021-05-14 19:08:20,246] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206295711817925368184984_0004_m_000133_421: needsTaskCommit() Task attempt_202105142206295711817925368184984_0004_m_000133_421
[2021-05-14 19:08:20,247] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206295711817925368184984_0004_m_000133_421: needsTaskCommit() Task attempt_202105142206295711817925368184984_0004_m_000133_421: duration 0:00.001s
[2021-05-14 19:08:20,247] {docker.py:276} INFO - 21/05/14 22:08:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295711817925368184984_0004_m_000133_421
[2021-05-14 19:08:20,249] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-14 19:08:20,250] {docker.py:276} INFO - 21/05/14 22:08:20 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (9bbba8114907, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:20,251] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
21/05/14 22:08:20 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 1823 ms on 9bbba8114907 (executor driver) (133/200)
[2021-05-14 19:08:20,260] {docker.py:276} INFO - 21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:20,262] {docker.py:276} INFO - 21/05/14 22:08:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291418251502897596701_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291418251502897596701_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291418251502897596701_0004}; taskId=attempt_202105142206291418251502897596701_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6adb863e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:20,262] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206291418251502897596701_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291418251502897596701_0004_m_000136_424
[2021-05-14 19:08:20,265] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206291418251502897596701_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291418251502897596701_0004_m_000136_424 : duration 0:00.003s
[2021-05-14 19:08:20,394] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206293035344148665435244_0004_m_000132_420: needsTaskCommit() Task attempt_202105142206293035344148665435244_0004_m_000132_420
[2021-05-14 19:08:20,394] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206293035344148665435244_0004_m_000132_420: needsTaskCommit() Task attempt_202105142206293035344148665435244_0004_m_000132_420: duration 0:00.000s
21/05/14 22:08:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293035344148665435244_0004_m_000132_420
[2021-05-14 19:08:20,395] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4544 bytes result sent to driver
[2021-05-14 19:08:20,397] {docker.py:276} INFO - 21/05/14 22:08:20 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (9bbba8114907, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:20,398] {docker.py:276} INFO - 21/05/14 22:08:20 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
[2021-05-14 19:08:20,398] {docker.py:276} INFO - 21/05/14 22:08:20 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 2789 ms on 9bbba8114907 (executor driver) (134/200)
[2021-05-14 19:08:20,406] {docker.py:276} INFO - 21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:20,407] {docker.py:276} INFO - 21/05/14 22:08:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298831756631284512410_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298831756631284512410_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298831756631284512410_0004}; taskId=attempt_202105142206298831756631284512410_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73918ae0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:20,408] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Starting: Task committer attempt_202105142206298831756631284512410_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298831756631284512410_0004_m_000137_425
[2021-05-14 19:08:20,410] {docker.py:276} INFO - 21/05/14 22:08:20 INFO StagingCommitter: Task committer attempt_202105142206298831756631284512410_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298831756631284512410_0004_m_000137_425 : duration 0:00.003s
[2021-05-14 19:08:22,210] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Starting: Task committer attempt_202105142206296277944759904316341_0004_m_000134_422: needsTaskCommit() Task attempt_202105142206296277944759904316341_0004_m_000134_422
[2021-05-14 19:08:22,211] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Task committer attempt_202105142206296277944759904316341_0004_m_000134_422: needsTaskCommit() Task attempt_202105142206296277944759904316341_0004_m_000134_422: duration 0:00.001s
21/05/14 22:08:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296277944759904316341_0004_m_000134_422
[2021-05-14 19:08:22,213] {docker.py:276} INFO - 21/05/14 22:08:22 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4587 bytes result sent to driver
[2021-05-14 19:08:22,214] {docker.py:276} INFO - 21/05/14 22:08:22 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (9bbba8114907, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:22,215] {docker.py:276} INFO - 21/05/14 22:08:22 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-14 19:08:22,216] {docker.py:276} INFO - 21/05/14 22:08:22 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 2686 ms on 9bbba8114907 (executor driver) (135/200)
[2021-05-14 19:08:22,225] {docker.py:276} INFO - 21/05/14 22:08:22 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:22,227] {docker.py:276} INFO - 21/05/14 22:08:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295835134115572230679_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295835134115572230679_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295835134115572230679_0004}; taskId=attempt_202105142206295835134115572230679_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19d2f9d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:22 INFO StagingCommitter: Starting: Task committer attempt_202105142206295835134115572230679_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295835134115572230679_0004_m_000138_426
[2021-05-14 19:08:22,230] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Task committer attempt_202105142206295835134115572230679_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295835134115572230679_0004_m_000138_426 : duration 0:00.003s
[2021-05-14 19:08:22,836] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Starting: Task committer attempt_202105142206291418251502897596701_0004_m_000136_424: needsTaskCommit() Task attempt_202105142206291418251502897596701_0004_m_000136_424
[2021-05-14 19:08:22,837] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Task committer attempt_202105142206291418251502897596701_0004_m_000136_424: needsTaskCommit() Task attempt_202105142206291418251502897596701_0004_m_000136_424: duration 0:00.000s
21/05/14 22:08:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291418251502897596701_0004_m_000136_424
[2021-05-14 19:08:22,838] {docker.py:276} INFO - 21/05/14 22:08:22 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4587 bytes result sent to driver
[2021-05-14 19:08:22,838] {docker.py:276} INFO - 21/05/14 22:08:22 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (9bbba8114907, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:22,839] {docker.py:276} INFO - 21/05/14 22:08:22 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-14 19:08:22,840] {docker.py:276} INFO - 21/05/14 22:08:22 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 2560 ms on 9bbba8114907 (executor driver) (136/200)
[2021-05-14 19:08:22,848] {docker.py:276} INFO - 21/05/14 22:08:22 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:22,850] {docker.py:276} INFO - 21/05/14 22:08:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299207202265911394637_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299207202265911394637_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299207202265911394637_0004}; taskId=attempt_202105142206299207202265911394637_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c3f0575}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:22,850] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Starting: Task committer attempt_202105142206299207202265911394637_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299207202265911394637_0004_m_000139_427
[2021-05-14 19:08:22,853] {docker.py:276} INFO - 21/05/14 22:08:22 INFO StagingCommitter: Task committer attempt_202105142206299207202265911394637_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299207202265911394637_0004_m_000139_427 : duration 0:00.004s
[2021-05-14 19:08:23,141] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105142206298831756631284512410_0004_m_000137_425: needsTaskCommit() Task attempt_202105142206298831756631284512410_0004_m_000137_425
[2021-05-14 19:08:23,142] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Task committer attempt_202105142206298831756631284512410_0004_m_000137_425: needsTaskCommit() Task attempt_202105142206298831756631284512410_0004_m_000137_425: duration 0:00.002s
21/05/14 22:08:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298831756631284512410_0004_m_000137_425
[2021-05-14 19:08:23,144] {docker.py:276} INFO - 21/05/14 22:08:23 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4587 bytes result sent to driver
[2021-05-14 19:08:23,145] {docker.py:276} INFO - 21/05/14 22:08:23 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (9bbba8114907, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:23,146] {docker.py:276} INFO - 21/05/14 22:08:23 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-14 19:08:23,147] {docker.py:276} INFO - 21/05/14 22:08:23 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 2720 ms on 9bbba8114907 (executor driver) (137/200)
[2021-05-14 19:08:23,156] {docker.py:276} INFO - 21/05/14 22:08:23 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:23,158] {docker.py:276} INFO - 21/05/14 22:08:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292791992775886006681_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292791992775886006681_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292791992775886006681_0004}; taskId=attempt_202105142206292791992775886006681_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ce53864}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:23,158] {docker.py:276} INFO - 21/05/14 22:08:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105142206292791992775886006681_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292791992775886006681_0004_m_000140_428
[2021-05-14 19:08:23,161] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Task committer attempt_202105142206292791992775886006681_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292791992775886006681_0004_m_000140_428 : duration 0:00.003s
[2021-05-14 19:08:23,417] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105142206293840950791333714375_0004_m_000135_423: needsTaskCommit() Task attempt_202105142206293840950791333714375_0004_m_000135_423
[2021-05-14 19:08:23,418] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Task committer attempt_202105142206293840950791333714375_0004_m_000135_423: needsTaskCommit() Task attempt_202105142206293840950791333714375_0004_m_000135_423: duration 0:00.001s
21/05/14 22:08:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293840950791333714375_0004_m_000135_423
[2021-05-14 19:08:23,420] {docker.py:276} INFO - 21/05/14 22:08:23 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4587 bytes result sent to driver
[2021-05-14 19:08:23,421] {docker.py:276} INFO - 21/05/14 22:08:23 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (9bbba8114907, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:23,422] {docker.py:276} INFO - 21/05/14 22:08:23 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
21/05/14 22:08:23 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 3327 ms on 9bbba8114907 (executor driver) (138/200)
[2021-05-14 19:08:23,432] {docker.py:276} INFO - 21/05/14 22:08:23 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:23,434] {docker.py:276} INFO - 21/05/14 22:08:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294561764143104278257_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294561764143104278257_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294561764143104278257_0004}; taskId=attempt_202105142206294561764143104278257_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c1ebe0c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:23,434] {docker.py:276} INFO - 21/05/14 22:08:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:23,435] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Starting: Task committer attempt_202105142206294561764143104278257_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294561764143104278257_0004_m_000141_429
[2021-05-14 19:08:23,438] {docker.py:276} INFO - 21/05/14 22:08:23 INFO StagingCommitter: Task committer attempt_202105142206294561764143104278257_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294561764143104278257_0004_m_000141_429 : duration 0:00.003s
[2021-05-14 19:08:24,829] {docker.py:276} INFO - 21/05/14 22:08:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206295835134115572230679_0004_m_000138_426: needsTaskCommit() Task attempt_202105142206295835134115572230679_0004_m_000138_426
[2021-05-14 19:08:24,830] {docker.py:276} INFO - 21/05/14 22:08:24 INFO StagingCommitter: Task committer attempt_202105142206295835134115572230679_0004_m_000138_426: needsTaskCommit() Task attempt_202105142206295835134115572230679_0004_m_000138_426: duration 0:00.001s
21/05/14 22:08:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295835134115572230679_0004_m_000138_426
[2021-05-14 19:08:24,831] {docker.py:276} INFO - 21/05/14 22:08:24 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4544 bytes result sent to driver
[2021-05-14 19:08:24,833] {docker.py:276} INFO - 21/05/14 22:08:24 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (9bbba8114907, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:24,834] {docker.py:276} INFO - 21/05/14 22:08:24 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 2623 ms on 9bbba8114907 (executor driver) (139/200)
21/05/14 22:08:24 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-14 19:08:24,843] {docker.py:276} INFO - 21/05/14 22:08:24 INFO ShuffleBlockFetcherIterator: Getting 5 (45.8 KiB) non-empty blocks including 5 (45.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:24,844] {docker.py:276} INFO - 21/05/14 22:08:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:24,845] {docker.py:276} INFO - 21/05/14 22:08:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:24,846] {docker.py:276} INFO - 21/05/14 22:08:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293744303045619307989_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293744303045619307989_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293744303045619307989_0004}; taskId=attempt_202105142206293744303045619307989_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@377fc1b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:24 INFO StagingCommitter: Starting: Task committer attempt_202105142206293744303045619307989_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293744303045619307989_0004_m_000142_430
[2021-05-14 19:08:24,849] {docker.py:276} INFO - 21/05/14 22:08:24 INFO StagingCommitter: Task committer attempt_202105142206293744303045619307989_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293744303045619307989_0004_m_000142_430 : duration 0:00.002s
[2021-05-14 19:08:25,490] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_202105142206299207202265911394637_0004_m_000139_427: needsTaskCommit() Task attempt_202105142206299207202265911394637_0004_m_000139_427
21/05/14 22:08:25 INFO StagingCommitter: Task committer attempt_202105142206299207202265911394637_0004_m_000139_427: needsTaskCommit() Task attempt_202105142206299207202265911394637_0004_m_000139_427: duration 0:00.001s
21/05/14 22:08:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299207202265911394637_0004_m_000139_427
[2021-05-14 19:08:25,492] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4544 bytes result sent to driver
[2021-05-14 19:08:25,492] {docker.py:276} INFO - 21/05/14 22:08:25 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (9bbba8114907, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:25,493] {docker.py:276} INFO - 21/05/14 22:08:25 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 2659 ms on 9bbba8114907 (executor driver) (140/200)
[2021-05-14 19:08:25,494] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-14 19:08:25,511] {docker.py:276} INFO - 21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:25,513] {docker.py:276} INFO - 21/05/14 22:08:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295952626336149633294_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295952626336149633294_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295952626336149633294_0004}; taskId=attempt_202105142206295952626336149633294_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21949c8c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_202105142206295952626336149633294_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295952626336149633294_0004_m_000143_431
[2021-05-14 19:08:25,515] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Task committer attempt_202105142206295952626336149633294_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295952626336149633294_0004_m_000143_431 : duration 0:00.002s
[2021-05-14 19:08:25,735] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_202105142206292791992775886006681_0004_m_000140_428: needsTaskCommit() Task attempt_202105142206292791992775886006681_0004_m_000140_428
[2021-05-14 19:08:25,737] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Task committer attempt_202105142206292791992775886006681_0004_m_000140_428: needsTaskCommit() Task attempt_202105142206292791992775886006681_0004_m_000140_428: duration 0:00.001s
21/05/14 22:08:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292791992775886006681_0004_m_000140_428
[2021-05-14 19:08:25,738] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4587 bytes result sent to driver
[2021-05-14 19:08:25,739] {docker.py:276} INFO - 21/05/14 22:08:25 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (9bbba8114907, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:25,741] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
21/05/14 22:08:25 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 2599 ms on 9bbba8114907 (executor driver) (141/200)
[2021-05-14 19:08:25,750] {docker.py:276} INFO - 21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:25,751] {docker.py:276} INFO - 21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:25,752] {docker.py:276} INFO - 21/05/14 22:08:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:25,752] {docker.py:276} INFO - 21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:25,753] {docker.py:276} INFO - 21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629279480691407302151_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629279480691407302151_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629279480691407302151_0004}; taskId=attempt_20210514220629279480691407302151_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e3ebc39}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:25,753] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_20210514220629279480691407302151_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629279480691407302151_0004_m_000144_432
[2021-05-14 19:08:25,755] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Task committer attempt_20210514220629279480691407302151_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629279480691407302151_0004_m_000144_432 : duration 0:00.002s
[2021-05-14 19:08:25,974] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_202105142206294561764143104278257_0004_m_000141_429: needsTaskCommit() Task attempt_202105142206294561764143104278257_0004_m_000141_429
[2021-05-14 19:08:25,976] {docker.py:276} INFO - 21/05/14 22:08:25 INFO StagingCommitter: Task committer attempt_202105142206294561764143104278257_0004_m_000141_429: needsTaskCommit() Task attempt_202105142206294561764143104278257_0004_m_000141_429: duration 0:00.001s
[2021-05-14 19:08:25,977] {docker.py:276} INFO - 21/05/14 22:08:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294561764143104278257_0004_m_000141_429
[2021-05-14 19:08:25,978] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4587 bytes result sent to driver
[2021-05-14 19:08:25,981] {docker.py:276} INFO - 21/05/14 22:08:25 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (9bbba8114907, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:25,983] {docker.py:276} INFO - 21/05/14 22:08:25 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2563 ms on 9bbba8114907 (executor driver) (142/200)
[2021-05-14 19:08:25,983] {docker.py:276} INFO - 21/05/14 22:08:25 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-14 19:08:25,992] {docker.py:276} INFO - 21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:25,993] {docker.py:276} INFO - 21/05/14 22:08:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295338056815538931280_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295338056815538931280_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295338056815538931280_0004}; taskId=attempt_202105142206295338056815538931280_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20afddc2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:25,994] {docker.py:276} INFO - 21/05/14 22:08:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:25 INFO StagingCommitter: Starting: Task committer attempt_202105142206295338056815538931280_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295338056815538931280_0004_m_000145_433
[2021-05-14 19:08:25,997] {docker.py:276} INFO - 21/05/14 22:08:26 INFO StagingCommitter: Task committer attempt_202105142206295338056815538931280_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295338056815538931280_0004_m_000145_433 : duration 0:00.003s
[2021-05-14 19:08:27,646] {docker.py:276} INFO - 21/05/14 22:08:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206293744303045619307989_0004_m_000142_430: needsTaskCommit() Task attempt_202105142206293744303045619307989_0004_m_000142_430
[2021-05-14 19:08:27,647] {docker.py:276} INFO - 21/05/14 22:08:27 INFO StagingCommitter: Task committer attempt_202105142206293744303045619307989_0004_m_000142_430: needsTaskCommit() Task attempt_202105142206293744303045619307989_0004_m_000142_430: duration 0:00.001s
[2021-05-14 19:08:27,648] {docker.py:276} INFO - 21/05/14 22:08:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293744303045619307989_0004_m_000142_430
[2021-05-14 19:08:27,652] {docker.py:276} INFO - 21/05/14 22:08:27 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4587 bytes result sent to driver
[2021-05-14 19:08:27,653] {docker.py:276} INFO - 21/05/14 22:08:27 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (9bbba8114907, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:27,655] {docker.py:276} INFO - 21/05/14 22:08:27 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
21/05/14 22:08:27 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2824 ms on 9bbba8114907 (executor driver) (143/200)
[2021-05-14 19:08:27,666] {docker.py:276} INFO - 21/05/14 22:08:27 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:27,666] {docker.py:276} INFO - 21/05/14 22:08:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:27,668] {docker.py:276} INFO - 21/05/14 22:08:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:27,668] {docker.py:276} INFO - 21/05/14 22:08:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297458595893117702001_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297458595893117702001_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297458595893117702001_0004}; taskId=attempt_202105142206297458595893117702001_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@291adcb6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:27 INFO StagingCommitter: Starting: Task committer attempt_202105142206297458595893117702001_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297458595893117702001_0004_m_000146_434
[2021-05-14 19:08:27,670] {docker.py:276} INFO - 21/05/14 22:08:27 INFO StagingCommitter: Task committer attempt_202105142206297458595893117702001_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297458595893117702001_0004_m_000146_434 : duration 0:00.002s
[2021-05-14 19:08:28,062] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_202105142206295952626336149633294_0004_m_000143_431: needsTaskCommit() Task attempt_202105142206295952626336149633294_0004_m_000143_431
[2021-05-14 19:08:28,063] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_202105142206295952626336149633294_0004_m_000143_431: needsTaskCommit() Task attempt_202105142206295952626336149633294_0004_m_000143_431: duration 0:00.001s
21/05/14 22:08:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295952626336149633294_0004_m_000143_431
[2021-05-14 19:08:28,064] {docker.py:276} INFO - 21/05/14 22:08:28 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4587 bytes result sent to driver
[2021-05-14 19:08:28,065] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (9bbba8114907, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:28,066] {docker.py:276} INFO - 21/05/14 22:08:28 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-14 19:08:28,067] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2579 ms on 9bbba8114907 (executor driver) (144/200)
[2021-05-14 19:08:28,076] {docker.py:276} INFO - 21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 19:08:28,077] {docker.py:276} INFO - 21/05/14 22:08:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:28,078] {docker.py:276} INFO - 21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297004771797269206639_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297004771797269206639_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297004771797269206639_0004}; taskId=attempt_202105142206297004771797269206639_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d3ee5f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_202105142206297004771797269206639_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297004771797269206639_0004_m_000147_435
[2021-05-14 19:08:28,080] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_202105142206297004771797269206639_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297004771797269206639_0004_m_000147_435 : duration 0:00.003s
[2021-05-14 19:08:28,394] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_20210514220629279480691407302151_0004_m_000144_432: needsTaskCommit() Task attempt_20210514220629279480691407302151_0004_m_000144_432
[2021-05-14 19:08:28,395] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_20210514220629279480691407302151_0004_m_000144_432: needsTaskCommit() Task attempt_20210514220629279480691407302151_0004_m_000144_432: duration 0:00.000s
21/05/14 22:08:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629279480691407302151_0004_m_000144_432
[2021-05-14 19:08:28,397] {docker.py:276} INFO - 21/05/14 22:08:28 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4544 bytes result sent to driver
[2021-05-14 19:08:28,398] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (9bbba8114907, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:28,399] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 2663 ms on 9bbba8114907 (executor driver) (145/200)
21/05/14 22:08:28 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-14 19:08:28,409] {docker.py:276} INFO - 21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:28,411] {docker.py:276} INFO - 21/05/14 22:08:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:28,411] {docker.py:276} INFO - 21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295184123109224253922_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295184123109224253922_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295184123109224253922_0004}; taskId=attempt_202105142206295184123109224253922_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c0f3c98}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_202105142206295184123109224253922_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295184123109224253922_0004_m_000148_436
[2021-05-14 19:08:28,414] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_202105142206295184123109224253922_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295184123109224253922_0004_m_000148_436 : duration 0:00.003s
[2021-05-14 19:08:28,539] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_202105142206295338056815538931280_0004_m_000145_433: needsTaskCommit() Task attempt_202105142206295338056815538931280_0004_m_000145_433
[2021-05-14 19:08:28,539] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_202105142206295338056815538931280_0004_m_000145_433: needsTaskCommit() Task attempt_202105142206295338056815538931280_0004_m_000145_433: duration 0:00.000s
21/05/14 22:08:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295338056815538931280_0004_m_000145_433
[2021-05-14 19:08:28,541] {docker.py:276} INFO - 21/05/14 22:08:28 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4544 bytes result sent to driver
[2021-05-14 19:08:28,543] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (9bbba8114907, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:28,544] {docker.py:276} INFO - 21/05/14 22:08:28 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 2567 ms on 9bbba8114907 (executor driver) (146/200)
[2021-05-14 19:08:28,545] {docker.py:276} INFO - 21/05/14 22:08:28 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-14 19:08:28,554] {docker.py:276} INFO - 21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:28,556] {docker.py:276} INFO - 21/05/14 22:08:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294411054804957918519_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294411054804957918519_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294411054804957918519_0004}; taskId=attempt_202105142206294411054804957918519_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58fddce1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:28,556] {docker.py:276} INFO - 21/05/14 22:08:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:28 INFO StagingCommitter: Starting: Task committer attempt_202105142206294411054804957918519_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294411054804957918519_0004_m_000149_437
[2021-05-14 19:08:28,558] {docker.py:276} INFO - 21/05/14 22:08:28 INFO StagingCommitter: Task committer attempt_202105142206294411054804957918519_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294411054804957918519_0004_m_000149_437 : duration 0:00.002s
[2021-05-14 19:08:30,206] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105142206297458595893117702001_0004_m_000146_434: needsTaskCommit() Task attempt_202105142206297458595893117702001_0004_m_000146_434
[2021-05-14 19:08:30,207] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Task committer attempt_202105142206297458595893117702001_0004_m_000146_434: needsTaskCommit() Task attempt_202105142206297458595893117702001_0004_m_000146_434: duration 0:00.001s
21/05/14 22:08:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297458595893117702001_0004_m_000146_434
[2021-05-14 19:08:30,210] {docker.py:276} INFO - 21/05/14 22:08:30 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4587 bytes result sent to driver
21/05/14 22:08:30 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (9bbba8114907, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:30,211] {docker.py:276} INFO - 21/05/14 22:08:30 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 2561 ms on 9bbba8114907 (executor driver) (147/200)
[2021-05-14 19:08:30,211] {docker.py:276} INFO - 21/05/14 22:08:30 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-14 19:08:30,221] {docker.py:276} INFO - 21/05/14 22:08:30 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:30,222] {docker.py:276} INFO - 21/05/14 22:08:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293024099224197051054_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293024099224197051054_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293024099224197051054_0004}; taskId=attempt_202105142206293024099224197051054_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@524ebbf1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:30,223] {docker.py:276} INFO - 21/05/14 22:08:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105142206293024099224197051054_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293024099224197051054_0004_m_000150_438
[2021-05-14 19:08:30,225] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Task committer attempt_202105142206293024099224197051054_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293024099224197051054_0004_m_000150_438 : duration 0:00.002s
[2021-05-14 19:08:30,836] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105142206297004771797269206639_0004_m_000147_435: needsTaskCommit() Task attempt_202105142206297004771797269206639_0004_m_000147_435
[2021-05-14 19:08:30,837] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Task committer attempt_202105142206297004771797269206639_0004_m_000147_435: needsTaskCommit() Task attempt_202105142206297004771797269206639_0004_m_000147_435: duration 0:00.001s
21/05/14 22:08:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297004771797269206639_0004_m_000147_435
[2021-05-14 19:08:30,840] {docker.py:276} INFO - 21/05/14 22:08:30 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4587 bytes result sent to driver
[2021-05-14 19:08:30,841] {docker.py:276} INFO - 21/05/14 22:08:30 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (9bbba8114907, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:30,841] {docker.py:276} INFO - 21/05/14 22:08:30 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-14 19:08:30,843] {docker.py:276} INFO - 21/05/14 22:08:30 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2781 ms on 9bbba8114907 (executor driver) (148/200)
[2021-05-14 19:08:30,852] {docker.py:276} INFO - 21/05/14 22:08:30 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:30,854] {docker.py:276} INFO - 21/05/14 22:08:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296918195202058479562_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296918195202058479562_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296918195202058479562_0004}; taskId=attempt_202105142206296918195202058479562_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58010de5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:30,854] {docker.py:276} INFO - 21/05/14 22:08:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:30 INFO StagingCommitter: Starting: Task committer attempt_202105142206296918195202058479562_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296918195202058479562_0004_m_000151_439
[2021-05-14 19:08:30,857] {docker.py:276} INFO - 21/05/14 22:08:30 INFO StagingCommitter: Task committer attempt_202105142206296918195202058479562_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296918195202058479562_0004_m_000151_439 : duration 0:00.003s
[2021-05-14 19:08:30,992] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Starting: Task committer attempt_202105142206295184123109224253922_0004_m_000148_436: needsTaskCommit() Task attempt_202105142206295184123109224253922_0004_m_000148_436
[2021-05-14 19:08:30,993] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Task committer attempt_202105142206295184123109224253922_0004_m_000148_436: needsTaskCommit() Task attempt_202105142206295184123109224253922_0004_m_000148_436: duration 0:00.001s
21/05/14 22:08:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295184123109224253922_0004_m_000148_436
[2021-05-14 19:08:30,994] {docker.py:276} INFO - 21/05/14 22:08:31 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4587 bytes result sent to driver
[2021-05-14 19:08:30,995] {docker.py:276} INFO - 21/05/14 22:08:31 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (9bbba8114907, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:30,996] {docker.py:276} INFO - 21/05/14 22:08:31 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 2601 ms on 9bbba8114907 (executor driver) (149/200)
[2021-05-14 19:08:30,997] {docker.py:276} INFO - 21/05/14 22:08:31 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-14 19:08:31,005] {docker.py:276} INFO - 21/05/14 22:08:31 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:31,007] {docker.py:276} INFO - 21/05/14 22:08:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298901580992719195091_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298901580992719195091_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298901580992719195091_0004}; taskId=attempt_202105142206298901580992719195091_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@78d459eb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:31 INFO StagingCommitter: Starting: Task committer attempt_202105142206298901580992719195091_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298901580992719195091_0004_m_000152_440
[2021-05-14 19:08:31,009] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Task committer attempt_202105142206298901580992719195091_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298901580992719195091_0004_m_000152_440 : duration 0:00.002s
[2021-05-14 19:08:31,142] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Starting: Task committer attempt_202105142206294411054804957918519_0004_m_000149_437: needsTaskCommit() Task attempt_202105142206294411054804957918519_0004_m_000149_437
[2021-05-14 19:08:31,144] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Task committer attempt_202105142206294411054804957918519_0004_m_000149_437: needsTaskCommit() Task attempt_202105142206294411054804957918519_0004_m_000149_437: duration 0:00.001s
21/05/14 22:08:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294411054804957918519_0004_m_000149_437
[2021-05-14 19:08:31,146] {docker.py:276} INFO - 21/05/14 22:08:31 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4587 bytes result sent to driver
[2021-05-14 19:08:31,147] {docker.py:276} INFO - 21/05/14 22:08:31 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (9bbba8114907, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:31,148] {docker.py:276} INFO - 21/05/14 22:08:31 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-14 19:08:31,149] {docker.py:276} INFO - 21/05/14 22:08:31 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 2608 ms on 9bbba8114907 (executor driver) (150/200)
[2021-05-14 19:08:31,157] {docker.py:276} INFO - 21/05/14 22:08:31 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:31,158] {docker.py:276} INFO - 21/05/14 22:08:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294542008953600436298_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294542008953600436298_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294542008953600436298_0004}; taskId=attempt_202105142206294542008953600436298_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a5fdb8a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:31 INFO StagingCommitter: Starting: Task committer attempt_202105142206294542008953600436298_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294542008953600436298_0004_m_000153_441
[2021-05-14 19:08:31,161] {docker.py:276} INFO - 21/05/14 22:08:31 INFO StagingCommitter: Task committer attempt_202105142206294542008953600436298_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294542008953600436298_0004_m_000153_441 : duration 0:00.003s
[2021-05-14 19:08:32,799] {docker.py:276} INFO - 21/05/14 22:08:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206293024099224197051054_0004_m_000150_438: needsTaskCommit() Task attempt_202105142206293024099224197051054_0004_m_000150_438
[2021-05-14 19:08:32,800] {docker.py:276} INFO - 21/05/14 22:08:32 INFO StagingCommitter: Task committer attempt_202105142206293024099224197051054_0004_m_000150_438: needsTaskCommit() Task attempt_202105142206293024099224197051054_0004_m_000150_438: duration 0:00.001s
21/05/14 22:08:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293024099224197051054_0004_m_000150_438
[2021-05-14 19:08:32,801] {docker.py:276} INFO - 21/05/14 22:08:32 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4544 bytes result sent to driver
[2021-05-14 19:08:32,802] {docker.py:276} INFO - 21/05/14 22:08:32 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (9bbba8114907, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:32,803] {docker.py:276} INFO - 21/05/14 22:08:32 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2596 ms on 9bbba8114907 (executor driver) (151/200)
[2021-05-14 19:08:32,804] {docker.py:276} INFO - 21/05/14 22:08:32 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-14 19:08:32,813] {docker.py:276} INFO - 21/05/14 22:08:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:32,815] {docker.py:276} INFO - 21/05/14 22:08:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:32,815] {docker.py:276} INFO - 21/05/14 22:08:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294954405550264641573_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294954405550264641573_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294954405550264641573_0004}; taskId=attempt_202105142206294954405550264641573_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e1c787e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:32 INFO StagingCommitter: Starting: Task committer attempt_202105142206294954405550264641573_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294954405550264641573_0004_m_000154_442
[2021-05-14 19:08:32,818] {docker.py:276} INFO - 21/05/14 22:08:32 INFO StagingCommitter: Task committer attempt_202105142206294954405550264641573_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294954405550264641573_0004_m_000154_442 : duration 0:00.003s
[2021-05-14 19:08:33,489] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206296918195202058479562_0004_m_000151_439: needsTaskCommit() Task attempt_202105142206296918195202058479562_0004_m_000151_439
21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206296918195202058479562_0004_m_000151_439: needsTaskCommit() Task attempt_202105142206296918195202058479562_0004_m_000151_439: duration 0:00.000s
[2021-05-14 19:08:33,490] {docker.py:276} INFO - 21/05/14 22:08:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296918195202058479562_0004_m_000151_439
[2021-05-14 19:08:33,491] {docker.py:276} INFO - 21/05/14 22:08:33 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4544 bytes result sent to driver
[2021-05-14 19:08:33,492] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (9bbba8114907, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:33,493] {docker.py:276} INFO - 21/05/14 22:08:33 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-14 19:08:33,494] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 2656 ms on 9bbba8114907 (executor driver) (152/200)
[2021-05-14 19:08:33,512] {docker.py:276} INFO - 21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:33,514] {docker.py:276} INFO - 21/05/14 22:08:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:33,514] {docker.py:276} INFO - 21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298685635659002331397_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298685635659002331397_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298685635659002331397_0004}; taskId=attempt_202105142206298685635659002331397_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ef42f84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206298685635659002331397_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298685635659002331397_0004_m_000155_443
[2021-05-14 19:08:33,517] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206298685635659002331397_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298685635659002331397_0004_m_000155_443 : duration 0:00.002s
[2021-05-14 19:08:33,659] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206298901580992719195091_0004_m_000152_440: needsTaskCommit() Task attempt_202105142206298901580992719195091_0004_m_000152_440
[2021-05-14 19:08:33,660] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206298901580992719195091_0004_m_000152_440: needsTaskCommit() Task attempt_202105142206298901580992719195091_0004_m_000152_440: duration 0:00.001s
21/05/14 22:08:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298901580992719195091_0004_m_000152_440
[2021-05-14 19:08:33,661] {docker.py:276} INFO - 21/05/14 22:08:33 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4587 bytes result sent to driver
[2021-05-14 19:08:33,662] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (9bbba8114907, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:33,664] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2672 ms on 9bbba8114907 (executor driver) (153/200)
21/05/14 22:08:33 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-14 19:08:33,674] {docker.py:276} INFO - 21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:33,675] {docker.py:276} INFO - 21/05/14 22:08:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297207579501205679832_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297207579501205679832_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297207579501205679832_0004}; taskId=attempt_202105142206297207579501205679832_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35e1a2a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:33,676] {docker.py:276} INFO - 21/05/14 22:08:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:33,676] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206297207579501205679832_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297207579501205679832_0004_m_000156_444
[2021-05-14 19:08:33,678] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206297207579501205679832_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297207579501205679832_0004_m_000156_444 : duration 0:00.002s
[2021-05-14 19:08:33,721] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206294542008953600436298_0004_m_000153_441: needsTaskCommit() Task attempt_202105142206294542008953600436298_0004_m_000153_441
[2021-05-14 19:08:33,722] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206294542008953600436298_0004_m_000153_441: needsTaskCommit() Task attempt_202105142206294542008953600436298_0004_m_000153_441: duration 0:00.000s
21/05/14 22:08:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294542008953600436298_0004_m_000153_441
[2021-05-14 19:08:33,723] {docker.py:276} INFO - 21/05/14 22:08:33 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4587 bytes result sent to driver
[2021-05-14 19:08:33,724] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (9bbba8114907, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:33,725] {docker.py:276} INFO - 21/05/14 22:08:33 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2582 ms on 9bbba8114907 (executor driver) (154/200)
[2021-05-14 19:08:33,726] {docker.py:276} INFO - 21/05/14 22:08:33 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-14 19:08:33,735] {docker.py:276} INFO - 21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:33,737] {docker.py:276} INFO - 21/05/14 22:08:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293517923754977929593_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293517923754977929593_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293517923754977929593_0004}; taskId=attempt_202105142206293517923754977929593_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33e0e60f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:33 INFO StagingCommitter: Starting: Task committer attempt_202105142206293517923754977929593_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293517923754977929593_0004_m_000157_445
[2021-05-14 19:08:33,739] {docker.py:276} INFO - 21/05/14 22:08:33 INFO StagingCommitter: Task committer attempt_202105142206293517923754977929593_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293517923754977929593_0004_m_000157_445 : duration 0:00.002s
[2021-05-14 19:08:35,435] {docker.py:276} INFO - 21/05/14 22:08:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206294954405550264641573_0004_m_000154_442: needsTaskCommit() Task attempt_202105142206294954405550264641573_0004_m_000154_442
[2021-05-14 19:08:35,436] {docker.py:276} INFO - 21/05/14 22:08:35 INFO StagingCommitter: Task committer attempt_202105142206294954405550264641573_0004_m_000154_442: needsTaskCommit() Task attempt_202105142206294954405550264641573_0004_m_000154_442: duration 0:00.001s
21/05/14 22:08:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294954405550264641573_0004_m_000154_442
[2021-05-14 19:08:35,438] {docker.py:276} INFO - 21/05/14 22:08:35 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4587 bytes result sent to driver
[2021-05-14 19:08:35,440] {docker.py:276} INFO - 21/05/14 22:08:35 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (9bbba8114907, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:35,440] {docker.py:276} INFO - 21/05/14 22:08:35 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 2641 ms on 9bbba8114907 (executor driver) (155/200)
[2021-05-14 19:08:35,441] {docker.py:276} INFO - 21/05/14 22:08:35 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-14 19:08:35,450] {docker.py:276} INFO - 21/05/14 22:08:35 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:35,452] {docker.py:276} INFO - 21/05/14 22:08:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299136894483041268503_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299136894483041268503_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299136894483041268503_0004}; taskId=attempt_202105142206299136894483041268503_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4120a8fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:35,452] {docker.py:276} INFO - 21/05/14 22:08:35 INFO StagingCommitter: Starting: Task committer attempt_202105142206299136894483041268503_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299136894483041268503_0004_m_000158_446
[2021-05-14 19:08:35,455] {docker.py:276} INFO - 21/05/14 22:08:35 INFO StagingCommitter: Task committer attempt_202105142206299136894483041268503_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299136894483041268503_0004_m_000158_446 : duration 0:00.003s
[2021-05-14 19:08:36,252] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_202105142206298685635659002331397_0004_m_000155_443: needsTaskCommit() Task attempt_202105142206298685635659002331397_0004_m_000155_443
[2021-05-14 19:08:36,253] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_202105142206298685635659002331397_0004_m_000155_443: needsTaskCommit() Task attempt_202105142206298685635659002331397_0004_m_000155_443: duration 0:00.001s
[2021-05-14 19:08:36,254] {docker.py:276} INFO - 21/05/14 22:08:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298685635659002331397_0004_m_000155_443
[2021-05-14 19:08:36,254] {docker.py:276} INFO - 21/05/14 22:08:36 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4587 bytes result sent to driver
[2021-05-14 19:08:36,256] {docker.py:276} INFO - 21/05/14 22:08:36 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (9bbba8114907, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:36,257] {docker.py:276} INFO - 21/05/14 22:08:36 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2768 ms on 9bbba8114907 (executor driver) (156/200)
21/05/14 22:08:36 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-14 19:08:36,267] {docker.py:276} INFO - 21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:36,268] {docker.py:276} INFO - 21/05/14 22:08:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297616892653255862892_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297616892653255862892_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297616892653255862892_0004}; taskId=attempt_202105142206297616892653255862892_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fb85777}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_202105142206297616892653255862892_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297616892653255862892_0004_m_000159_447
[2021-05-14 19:08:36,271] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_202105142206297616892653255862892_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297616892653255862892_0004_m_000159_447 : duration 0:00.003s
[2021-05-14 19:08:36,290] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_202105142206297207579501205679832_0004_m_000156_444: needsTaskCommit() Task attempt_202105142206297207579501205679832_0004_m_000156_444
[2021-05-14 19:08:36,291] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_202105142206297207579501205679832_0004_m_000156_444: needsTaskCommit() Task attempt_202105142206297207579501205679832_0004_m_000156_444: duration 0:00.000s
21/05/14 22:08:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297207579501205679832_0004_m_000156_444
[2021-05-14 19:08:36,292] {docker.py:276} INFO - 21/05/14 22:08:36 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4544 bytes result sent to driver
[2021-05-14 19:08:36,292] {docker.py:276} INFO - 21/05/14 22:08:36 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (9bbba8114907, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:36,293] {docker.py:276} INFO - 21/05/14 22:08:36 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
21/05/14 22:08:36 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 2635 ms on 9bbba8114907 (executor driver) (157/200)
[2021-05-14 19:08:36,300] {docker.py:276} INFO - 21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Getting 5 (45.0 KiB) non-empty blocks including 5 (45.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:36,302] {docker.py:276} INFO - 21/05/14 22:08:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629321837419336028185_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629321837419336028185_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629321837419336028185_0004}; taskId=attempt_20210514220629321837419336028185_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79238d93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_20210514220629321837419336028185_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629321837419336028185_0004_m_000160_448
[2021-05-14 19:08:36,304] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_20210514220629321837419336028185_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629321837419336028185_0004_m_000160_448 : duration 0:00.002s
[2021-05-14 19:08:36,412] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_202105142206293517923754977929593_0004_m_000157_445: needsTaskCommit() Task attempt_202105142206293517923754977929593_0004_m_000157_445
[2021-05-14 19:08:36,413] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_202105142206293517923754977929593_0004_m_000157_445: needsTaskCommit() Task attempt_202105142206293517923754977929593_0004_m_000157_445: duration 0:00.001s
21/05/14 22:08:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293517923754977929593_0004_m_000157_445
[2021-05-14 19:08:36,414] {docker.py:276} INFO - 21/05/14 22:08:36 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4544 bytes result sent to driver
[2021-05-14 19:08:36,415] {docker.py:276} INFO - 21/05/14 22:08:36 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (9bbba8114907, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:36,416] {docker.py:276} INFO - 21/05/14 22:08:36 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2696 ms on 9bbba8114907 (executor driver) (158/200)
[2021-05-14 19:08:36,417] {docker.py:276} INFO - 21/05/14 22:08:36 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-14 19:08:36,426] {docker.py:276} INFO - 21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:36,427] {docker.py:276} INFO - 21/05/14 22:08:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:36,429] {docker.py:276} INFO - 21/05/14 22:08:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:36,429] {docker.py:276} INFO - 21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:36,429] {docker.py:276} INFO - 21/05/14 22:08:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295508117149485988968_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295508117149485988968_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295508117149485988968_0004}; taskId=attempt_202105142206295508117149485988968_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e62eb97}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:36,430] {docker.py:276} INFO - 21/05/14 22:08:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:36,430] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Starting: Task committer attempt_202105142206295508117149485988968_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295508117149485988968_0004_m_000161_449
[2021-05-14 19:08:36,432] {docker.py:276} INFO - 21/05/14 22:08:36 INFO StagingCommitter: Task committer attempt_202105142206295508117149485988968_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295508117149485988968_0004_m_000161_449 : duration 0:00.003s
[2021-05-14 19:08:38,212] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206299136894483041268503_0004_m_000158_446: needsTaskCommit() Task attempt_202105142206299136894483041268503_0004_m_000158_446
[2021-05-14 19:08:38,213] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_202105142206299136894483041268503_0004_m_000158_446: needsTaskCommit() Task attempt_202105142206299136894483041268503_0004_m_000158_446: duration 0:00.001s
21/05/14 22:08:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299136894483041268503_0004_m_000158_446
[2021-05-14 19:08:38,214] {docker.py:276} INFO - 21/05/14 22:08:38 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4587 bytes result sent to driver
[2021-05-14 19:08:38,216] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (9bbba8114907, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:38,217] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 2781 ms on 9bbba8114907 (executor driver) (159/200)
21/05/14 22:08:38 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-14 19:08:38,227] {docker.py:276} INFO - 21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:38,228] {docker.py:276} INFO - 21/05/14 22:08:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291243870703962385575_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291243870703962385575_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291243870703962385575_0004}; taskId=attempt_202105142206291243870703962385575_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51899f4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206291243870703962385575_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291243870703962385575_0004_m_000162_450
[2021-05-14 19:08:38,231] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_202105142206291243870703962385575_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291243870703962385575_0004_m_000162_450 : duration 0:00.003s
[2021-05-14 19:08:38,911] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_20210514220629321837419336028185_0004_m_000160_448: needsTaskCommit() Task attempt_20210514220629321837419336028185_0004_m_000160_448
[2021-05-14 19:08:38,912] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_20210514220629321837419336028185_0004_m_000160_448: needsTaskCommit() Task attempt_20210514220629321837419336028185_0004_m_000160_448: duration 0:00.001s
21/05/14 22:08:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629321837419336028185_0004_m_000160_448
[2021-05-14 19:08:38,913] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206297616892653255862892_0004_m_000159_447: needsTaskCommit() Task attempt_202105142206297616892653255862892_0004_m_000159_447
[2021-05-14 19:08:38,914] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_202105142206297616892653255862892_0004_m_000159_447: needsTaskCommit() Task attempt_202105142206297616892653255862892_0004_m_000159_447: duration 0:00.001s
[2021-05-14 19:08:38,915] {docker.py:276} INFO - 21/05/14 22:08:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297616892653255862892_0004_m_000159_447
[2021-05-14 19:08:38,916] {docker.py:276} INFO - 21/05/14 22:08:38 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4587 bytes result sent to driver
[2021-05-14 19:08:38,917] {docker.py:276} INFO - 21/05/14 22:08:38 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4587 bytes result sent to driver
[2021-05-14 19:08:38,918] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (9bbba8114907, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:38,919] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2629 ms on 9bbba8114907 (executor driver) (160/200)
21/05/14 22:08:38 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-14 19:08:38,920] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (9bbba8114907, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:38,921] {docker.py:276} INFO - 21/05/14 22:08:38 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2668 ms on 9bbba8114907 (executor driver) (161/200)
[2021-05-14 19:08:38,922] {docker.py:276} INFO - 21/05/14 22:08:38 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-14 19:08:38,929] {docker.py:276} INFO - 21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:38,930] {docker.py:276} INFO - 21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:38,931] {docker.py:276} INFO - 21/05/14 22:08:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:38,931] {docker.py:276} INFO - 21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295242258155098382470_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295242258155098382470_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295242258155098382470_0004}; taskId=attempt_202105142206295242258155098382470_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d7342d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206295242258155098382470_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295242258155098382470_0004_m_000163_451
[2021-05-14 19:08:38,932] {docker.py:276} INFO - 21/05/14 22:08:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:38,933] {docker.py:276} INFO - 21/05/14 22:08:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296028168359853227327_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296028168359853227327_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296028168359853227327_0004}; taskId=attempt_202105142206296028168359853227327_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@305672a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:38,933] {docker.py:276} INFO - 21/05/14 22:08:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:38,933] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Starting: Task committer attempt_202105142206296028168359853227327_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296028168359853227327_0004_m_000164_452
[2021-05-14 19:08:38,935] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_202105142206295242258155098382470_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295242258155098382470_0004_m_000163_451 : duration 0:00.004s
[2021-05-14 19:08:38,937] {docker.py:276} INFO - 21/05/14 22:08:38 INFO StagingCommitter: Task committer attempt_202105142206296028168359853227327_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296028168359853227327_0004_m_000164_452 : duration 0:00.005s
[2021-05-14 19:08:39,266] {docker.py:276} INFO - 21/05/14 22:08:39 INFO StagingCommitter: Starting: Task committer attempt_202105142206295508117149485988968_0004_m_000161_449: needsTaskCommit() Task attempt_202105142206295508117149485988968_0004_m_000161_449
21/05/14 22:08:39 INFO StagingCommitter: Task committer attempt_202105142206295508117149485988968_0004_m_000161_449: needsTaskCommit() Task attempt_202105142206295508117149485988968_0004_m_000161_449: duration 0:00.001s
21/05/14 22:08:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295508117149485988968_0004_m_000161_449
[2021-05-14 19:08:39,268] {docker.py:276} INFO - 21/05/14 22:08:39 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4587 bytes result sent to driver
[2021-05-14 19:08:39,269] {docker.py:276} INFO - 21/05/14 22:08:39 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (9bbba8114907, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:39,270] {docker.py:276} INFO - 21/05/14 22:08:39 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 2859 ms on 9bbba8114907 (executor driver) (162/200)
21/05/14 22:08:39 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
[2021-05-14 19:08:39,279] {docker.py:276} INFO - 21/05/14 22:08:39 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:39,281] {docker.py:276} INFO - 21/05/14 22:08:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292986335285836776327_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292986335285836776327_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292986335285836776327_0004}; taskId=attempt_202105142206292986335285836776327_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5713562e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:39 INFO StagingCommitter: Starting: Task committer attempt_202105142206292986335285836776327_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292986335285836776327_0004_m_000165_453
[2021-05-14 19:08:39,284] {docker.py:276} INFO - 21/05/14 22:08:39 INFO StagingCommitter: Task committer attempt_202105142206292986335285836776327_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292986335285836776327_0004_m_000165_453 : duration 0:00.003s
[2021-05-14 19:08:40,798] {docker.py:276} INFO - 21/05/14 22:08:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206291243870703962385575_0004_m_000162_450: needsTaskCommit() Task attempt_202105142206291243870703962385575_0004_m_000162_450
[2021-05-14 19:08:40,799] {docker.py:276} INFO - 21/05/14 22:08:40 INFO StagingCommitter: Task committer attempt_202105142206291243870703962385575_0004_m_000162_450: needsTaskCommit() Task attempt_202105142206291243870703962385575_0004_m_000162_450: duration 0:00.000s
21/05/14 22:08:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291243870703962385575_0004_m_000162_450
[2021-05-14 19:08:40,800] {docker.py:276} INFO - 21/05/14 22:08:40 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4544 bytes result sent to driver
[2021-05-14 19:08:40,801] {docker.py:276} INFO - 21/05/14 22:08:40 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (9bbba8114907, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:40,803] {docker.py:276} INFO - 21/05/14 22:08:40 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
21/05/14 22:08:40 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 2591 ms on 9bbba8114907 (executor driver) (163/200)
[2021-05-14 19:08:40,813] {docker.py:276} INFO - 21/05/14 22:08:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:40,814] {docker.py:276} INFO - 21/05/14 22:08:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298146637647893578111_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298146637647893578111_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298146637647893578111_0004}; taskId=attempt_202105142206298146637647893578111_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9ac36af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:40 INFO StagingCommitter: Starting: Task committer attempt_202105142206298146637647893578111_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298146637647893578111_0004_m_000166_454
[2021-05-14 19:08:40,817] {docker.py:276} INFO - 21/05/14 22:08:40 INFO StagingCommitter: Task committer attempt_202105142206298146637647893578111_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298146637647893578111_0004_m_000166_454 : duration 0:00.003s
[2021-05-14 19:08:41,488] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Starting: Task committer attempt_202105142206295242258155098382470_0004_m_000163_451: needsTaskCommit() Task attempt_202105142206295242258155098382470_0004_m_000163_451
[2021-05-14 19:08:41,489] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Task committer attempt_202105142206295242258155098382470_0004_m_000163_451: needsTaskCommit() Task attempt_202105142206295242258155098382470_0004_m_000163_451: duration 0:00.001s
[2021-05-14 19:08:41,489] {docker.py:276} INFO - 21/05/14 22:08:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295242258155098382470_0004_m_000163_451
[2021-05-14 19:08:41,490] {docker.py:276} INFO - 21/05/14 22:08:41 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4544 bytes result sent to driver
[2021-05-14 19:08:41,491] {docker.py:276} INFO - 21/05/14 22:08:41 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (9bbba8114907, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:41,492] {docker.py:276} INFO - 21/05/14 22:08:41 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 2578 ms on 9bbba8114907 (executor driver) (164/200)
[2021-05-14 19:08:41,493] {docker.py:276} INFO - 21/05/14 22:08:41 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-14 19:08:41,501] {docker.py:276} INFO - 21/05/14 22:08:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:41,502] {docker.py:276} INFO - 21/05/14 22:08:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206298005267483638150835_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298005267483638150835_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206298005267483638150835_0004}; taskId=attempt_202105142206298005267483638150835_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dfe955f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:41 INFO StagingCommitter: Starting: Task committer attempt_202105142206298005267483638150835_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298005267483638150835_0004_m_000167_455
[2021-05-14 19:08:41,512] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Task committer attempt_202105142206298005267483638150835_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206298005267483638150835_0004_m_000167_455 : duration 0:00.009s
[2021-05-14 19:08:41,559] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Starting: Task committer attempt_202105142206296028168359853227327_0004_m_000164_452: needsTaskCommit() Task attempt_202105142206296028168359853227327_0004_m_000164_452
[2021-05-14 19:08:41,559] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Task committer attempt_202105142206296028168359853227327_0004_m_000164_452: needsTaskCommit() Task attempt_202105142206296028168359853227327_0004_m_000164_452: duration 0:00.001s
21/05/14 22:08:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296028168359853227327_0004_m_000164_452
[2021-05-14 19:08:41,561] {docker.py:276} INFO - 21/05/14 22:08:41 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4587 bytes result sent to driver
[2021-05-14 19:08:41,562] {docker.py:276} INFO - 21/05/14 22:08:41 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (9bbba8114907, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:41,563] {docker.py:276} INFO - 21/05/14 22:08:41 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-14 19:08:41,563] {docker.py:276} INFO - 21/05/14 22:08:41 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2646 ms on 9bbba8114907 (executor driver) (165/200)
[2021-05-14 19:08:41,574] {docker.py:276} INFO - 21/05/14 22:08:41 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:41,574] {docker.py:276} INFO - 21/05/14 22:08:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:41,576] {docker.py:276} INFO - 21/05/14 22:08:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:41,576] {docker.py:276} INFO - 21/05/14 22:08:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:41,576] {docker.py:276} INFO - 21/05/14 22:08:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293041749923779888081_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293041749923779888081_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293041749923779888081_0004}; taskId=attempt_202105142206293041749923779888081_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fcd0785}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:41,577] {docker.py:276} INFO - 21/05/14 22:08:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:41,577] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Starting: Task committer attempt_202105142206293041749923779888081_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293041749923779888081_0004_m_000168_456
[2021-05-14 19:08:41,579] {docker.py:276} INFO - 21/05/14 22:08:41 INFO StagingCommitter: Task committer attempt_202105142206293041749923779888081_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293041749923779888081_0004_m_000168_456 : duration 0:00.002s
[2021-05-14 19:08:42,064] {docker.py:276} INFO - 21/05/14 22:08:42 INFO StagingCommitter: Starting: Task committer attempt_202105142206292986335285836776327_0004_m_000165_453: needsTaskCommit() Task attempt_202105142206292986335285836776327_0004_m_000165_453
[2021-05-14 19:08:42,065] {docker.py:276} INFO - 21/05/14 22:08:42 INFO StagingCommitter: Task committer attempt_202105142206292986335285836776327_0004_m_000165_453: needsTaskCommit() Task attempt_202105142206292986335285836776327_0004_m_000165_453: duration 0:00.000s
21/05/14 22:08:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292986335285836776327_0004_m_000165_453
[2021-05-14 19:08:42,066] {docker.py:276} INFO - 21/05/14 22:08:42 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4587 bytes result sent to driver
[2021-05-14 19:08:42,068] {docker.py:276} INFO - 21/05/14 22:08:42 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (9bbba8114907, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:42,069] {docker.py:276} INFO - 21/05/14 22:08:42 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2802 ms on 9bbba8114907 (executor driver) (166/200)
[2021-05-14 19:08:42,070] {docker.py:276} INFO - 21/05/14 22:08:42 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-14 19:08:42,080] {docker.py:276} INFO - 21/05/14 22:08:42 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:42,082] {docker.py:276} INFO - 21/05/14 22:08:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:42,083] {docker.py:276} INFO - 21/05/14 22:08:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:42,083] {docker.py:276} INFO - 21/05/14 22:08:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629980534554848418421_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629980534554848418421_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629980534554848418421_0004}; taskId=attempt_20210514220629980534554848418421_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d8022bf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:42,084] {docker.py:276} INFO - 21/05/14 22:08:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:42,084] {docker.py:276} INFO - 21/05/14 22:08:42 INFO StagingCommitter: Starting: Task committer attempt_20210514220629980534554848418421_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629980534554848418421_0004_m_000169_457
[2021-05-14 19:08:42,086] {docker.py:276} INFO - 21/05/14 22:08:42 INFO StagingCommitter: Task committer attempt_20210514220629980534554848418421_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629980534554848418421_0004_m_000169_457 : duration 0:00.004s
[2021-05-14 19:08:43,391] {docker.py:276} INFO - 21/05/14 22:08:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206298146637647893578111_0004_m_000166_454: needsTaskCommit() Task attempt_202105142206298146637647893578111_0004_m_000166_454
[2021-05-14 19:08:43,392] {docker.py:276} INFO - 21/05/14 22:08:43 INFO StagingCommitter: Task committer attempt_202105142206298146637647893578111_0004_m_000166_454: needsTaskCommit() Task attempt_202105142206298146637647893578111_0004_m_000166_454: duration 0:00.001s
21/05/14 22:08:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298146637647893578111_0004_m_000166_454
[2021-05-14 19:08:43,394] {docker.py:276} INFO - 21/05/14 22:08:43 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4587 bytes result sent to driver
[2021-05-14 19:08:43,395] {docker.py:276} INFO - 21/05/14 22:08:43 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (9bbba8114907, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:43,396] {docker.py:276} INFO - 21/05/14 22:08:43 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2598 ms on 9bbba8114907 (executor driver) (167/200)
21/05/14 22:08:43 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-14 19:08:43,407] {docker.py:276} INFO - 21/05/14 22:08:43 INFO ShuffleBlockFetcherIterator: Getting 5 (46.3 KiB) non-empty blocks including 5 (46.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:43,407] {docker.py:276} INFO - 21/05/14 22:08:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:43,409] {docker.py:276} INFO - 21/05/14 22:08:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:43,409] {docker.py:276} INFO - 21/05/14 22:08:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295191796392581128627_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295191796392581128627_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295191796392581128627_0004}; taskId=attempt_202105142206295191796392581128627_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35cb56f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:43,410] {docker.py:276} INFO - 21/05/14 22:08:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:43,410] {docker.py:276} INFO - 21/05/14 22:08:43 INFO StagingCommitter: Starting: Task committer attempt_202105142206295191796392581128627_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295191796392581128627_0004_m_000170_458
[2021-05-14 19:08:43,412] {docker.py:276} INFO - 21/05/14 22:08:43 INFO StagingCommitter: Task committer attempt_202105142206295191796392581128627_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295191796392581128627_0004_m_000170_458 : duration 0:00.003s
[2021-05-14 19:08:44,193] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105142206298005267483638150835_0004_m_000167_455: needsTaskCommit() Task attempt_202105142206298005267483638150835_0004_m_000167_455
[2021-05-14 19:08:44,194] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_202105142206298005267483638150835_0004_m_000167_455: needsTaskCommit() Task attempt_202105142206298005267483638150835_0004_m_000167_455: duration 0:00.001s
[2021-05-14 19:08:44,195] {docker.py:276} INFO - 21/05/14 22:08:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206298005267483638150835_0004_m_000167_455
[2021-05-14 19:08:44,196] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4587 bytes result sent to driver
[2021-05-14 19:08:44,197] {docker.py:276} INFO - 21/05/14 22:08:44 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (9bbba8114907, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:44,199] {docker.py:276} INFO - 21/05/14 22:08:44 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2710 ms on 9bbba8114907 (executor driver) (168/200)
[2021-05-14 19:08:44,199] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
[2021-05-14 19:08:44,209] {docker.py:276} INFO - 21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:44,210] {docker.py:276} INFO - 21/05/14 22:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291016021212631361869_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291016021212631361869_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291016021212631361869_0004}; taskId=attempt_202105142206291016021212631361869_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3eb79274}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:44,211] {docker.py:276} INFO - 21/05/14 22:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105142206291016021212631361869_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291016021212631361869_0004_m_000171_459
[2021-05-14 19:08:44,214] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_202105142206291016021212631361869_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291016021212631361869_0004_m_000171_459 : duration 0:00.003s
[2021-05-14 19:08:44,358] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105142206293041749923779888081_0004_m_000168_456: needsTaskCommit() Task attempt_202105142206293041749923779888081_0004_m_000168_456
[2021-05-14 19:08:44,360] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_202105142206293041749923779888081_0004_m_000168_456: needsTaskCommit() Task attempt_202105142206293041749923779888081_0004_m_000168_456: duration 0:00.001s
21/05/14 22:08:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293041749923779888081_0004_m_000168_456
[2021-05-14 19:08:44,362] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4544 bytes result sent to driver
[2021-05-14 19:08:44,363] {docker.py:276} INFO - 21/05/14 22:08:44 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (9bbba8114907, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:44,364] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-14 19:08:44,364] {docker.py:276} INFO - 21/05/14 22:08:44 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2805 ms on 9bbba8114907 (executor driver) (169/200)
[2021-05-14 19:08:44,374] {docker.py:276} INFO - 21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:44,375] {docker.py:276} INFO - 21/05/14 22:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294584324283603621514_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294584324283603621514_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294584324283603621514_0004}; taskId=attempt_202105142206294584324283603621514_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@322d5ee9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:44,376] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105142206294584324283603621514_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294584324283603621514_0004_m_000172_460
[2021-05-14 19:08:44,379] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_202105142206294584324283603621514_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294584324283603621514_0004_m_000172_460 : duration 0:00.003s
[2021-05-14 19:08:44,828] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_20210514220629980534554848418421_0004_m_000169_457: needsTaskCommit() Task attempt_20210514220629980534554848418421_0004_m_000169_457
[2021-05-14 19:08:44,828] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_20210514220629980534554848418421_0004_m_000169_457: needsTaskCommit() Task attempt_20210514220629980534554848418421_0004_m_000169_457: duration 0:00.000s
21/05/14 22:08:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629980534554848418421_0004_m_000169_457
[2021-05-14 19:08:44,830] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4544 bytes result sent to driver
[2021-05-14 19:08:44,831] {docker.py:276} INFO - 21/05/14 22:08:44 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (9bbba8114907, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:44,832] {docker.py:276} INFO - 21/05/14 22:08:44 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
21/05/14 22:08:44 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2768 ms on 9bbba8114907 (executor driver) (170/200)
[2021-05-14 19:08:44,842] {docker.py:276} INFO - 21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:44,844] {docker.py:276} INFO - 21/05/14 22:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:44,844] {docker.py:276} INFO - 21/05/14 22:08:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293419557355445535505_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293419557355445535505_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293419557355445535505_0004}; taskId=attempt_202105142206293419557355445535505_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33b18c7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:44 INFO StagingCommitter: Starting: Task committer attempt_202105142206293419557355445535505_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293419557355445535505_0004_m_000173_461
[2021-05-14 19:08:44,847] {docker.py:276} INFO - 21/05/14 22:08:44 INFO StagingCommitter: Task committer attempt_202105142206293419557355445535505_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293419557355445535505_0004_m_000173_461 : duration 0:00.002s
[2021-05-14 19:08:46,133] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206295191796392581128627_0004_m_000170_458: needsTaskCommit() Task attempt_202105142206295191796392581128627_0004_m_000170_458
[2021-05-14 19:08:46,134] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Task committer attempt_202105142206295191796392581128627_0004_m_000170_458: needsTaskCommit() Task attempt_202105142206295191796392581128627_0004_m_000170_458: duration 0:00.000s
21/05/14 22:08:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295191796392581128627_0004_m_000170_458
[2021-05-14 19:08:46,135] {docker.py:276} INFO - 21/05/14 22:08:46 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4587 bytes result sent to driver
[2021-05-14 19:08:46,136] {docker.py:276} INFO - 21/05/14 22:08:46 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (9bbba8114907, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:46,137] {docker.py:276} INFO - 21/05/14 22:08:46 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-14 19:08:46,137] {docker.py:276} INFO - 21/05/14 22:08:46 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 2746 ms on 9bbba8114907 (executor driver) (171/200)
[2021-05-14 19:08:46,145] {docker.py:276} INFO - 21/05/14 22:08:46 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:46,146] {docker.py:276} INFO - 21/05/14 22:08:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:46,147] {docker.py:276} INFO - 21/05/14 22:08:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294534568138262270912_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294534568138262270912_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294534568138262270912_0004}; taskId=attempt_202105142206294534568138262270912_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@338c646}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206294534568138262270912_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294534568138262270912_0004_m_000174_462
[2021-05-14 19:08:46,149] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Task committer attempt_202105142206294534568138262270912_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294534568138262270912_0004_m_000174_462 : duration 0:00.003s
[2021-05-14 19:08:46,846] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206291016021212631361869_0004_m_000171_459: needsTaskCommit() Task attempt_202105142206291016021212631361869_0004_m_000171_459
[2021-05-14 19:08:46,847] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Task committer attempt_202105142206291016021212631361869_0004_m_000171_459: needsTaskCommit() Task attempt_202105142206291016021212631361869_0004_m_000171_459: duration 0:00.001s
21/05/14 22:08:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291016021212631361869_0004_m_000171_459
[2021-05-14 19:08:46,848] {docker.py:276} INFO - 21/05/14 22:08:46 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4587 bytes result sent to driver
[2021-05-14 19:08:46,849] {docker.py:276} INFO - 21/05/14 22:08:46 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (9bbba8114907, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:46,851] {docker.py:276} INFO - 21/05/14 22:08:46 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2657 ms on 9bbba8114907 (executor driver) (172/200)
21/05/14 22:08:46 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-14 19:08:46,860] {docker.py:276} INFO - 21/05/14 22:08:46 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:46,861] {docker.py:276} INFO - 21/05/14 22:08:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:46,862] {docker.py:276} INFO - 21/05/14 22:08:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206295871117801827266591_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295871117801827266591_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206295871117801827266591_0004}; taskId=attempt_202105142206295871117801827266591_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d5a045f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:46,862] {docker.py:276} INFO - 21/05/14 22:08:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:46 INFO StagingCommitter: Starting: Task committer attempt_202105142206295871117801827266591_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295871117801827266591_0004_m_000175_463
[2021-05-14 19:08:46,865] {docker.py:276} INFO - 21/05/14 22:08:46 INFO StagingCommitter: Task committer attempt_202105142206295871117801827266591_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206295871117801827266591_0004_m_000175_463 : duration 0:00.003s
[2021-05-14 19:08:46,978] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206294584324283603621514_0004_m_000172_460: needsTaskCommit() Task attempt_202105142206294584324283603621514_0004_m_000172_460
[2021-05-14 19:08:46,979] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Task committer attempt_202105142206294584324283603621514_0004_m_000172_460: needsTaskCommit() Task attempt_202105142206294584324283603621514_0004_m_000172_460: duration 0:00.000s
[2021-05-14 19:08:46,980] {docker.py:276} INFO - 21/05/14 22:08:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294584324283603621514_0004_m_000172_460
[2021-05-14 19:08:46,981] {docker.py:276} INFO - 21/05/14 22:08:47 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4587 bytes result sent to driver
[2021-05-14 19:08:46,982] {docker.py:276} INFO - 21/05/14 22:08:47 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (9bbba8114907, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:46,983] {docker.py:276} INFO - 21/05/14 22:08:47 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-14 19:08:46,984] {docker.py:276} INFO - 21/05/14 22:08:47 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 2624 ms on 9bbba8114907 (executor driver) (173/200)
[2021-05-14 19:08:46,993] {docker.py:276} INFO - 21/05/14 22:08:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:46,994] {docker.py:276} INFO - 21/05/14 22:08:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297142808825950530259_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297142808825950530259_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297142808825950530259_0004}; taskId=attempt_202105142206297142808825950530259_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@331ba1c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:46,995] {docker.py:276} INFO - 21/05/14 22:08:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206297142808825950530259_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297142808825950530259_0004_m_000176_464
[2021-05-14 19:08:46,998] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Task committer attempt_202105142206297142808825950530259_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297142808825950530259_0004_m_000176_464 : duration 0:00.003s
[2021-05-14 19:08:47,490] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206293419557355445535505_0004_m_000173_461: needsTaskCommit() Task attempt_202105142206293419557355445535505_0004_m_000173_461
[2021-05-14 19:08:47,491] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Task committer attempt_202105142206293419557355445535505_0004_m_000173_461: needsTaskCommit() Task attempt_202105142206293419557355445535505_0004_m_000173_461: duration 0:00.000s
[2021-05-14 19:08:47,492] {docker.py:276} INFO - 21/05/14 22:08:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293419557355445535505_0004_m_000173_461
[2021-05-14 19:08:47,492] {docker.py:276} INFO - 21/05/14 22:08:47 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4587 bytes result sent to driver
[2021-05-14 19:08:47,493] {docker.py:276} INFO - 21/05/14 22:08:47 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (9bbba8114907, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:47,493] {docker.py:276} INFO - 21/05/14 22:08:47 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
21/05/14 22:08:47 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2666 ms on 9bbba8114907 (executor driver) (174/200)
[2021-05-14 19:08:47,503] {docker.py:276} INFO - 21/05/14 22:08:47 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:47,505] {docker.py:276} INFO - 21/05/14 22:08:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:47,506] {docker.py:276} INFO - 21/05/14 22:08:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291670973373787279796_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291670973373787279796_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291670973373787279796_0004}; taskId=attempt_202105142206291670973373787279796_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f23ecc4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:47 INFO StagingCommitter: Starting: Task committer attempt_202105142206291670973373787279796_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291670973373787279796_0004_m_000177_465
[2021-05-14 19:08:47,509] {docker.py:276} INFO - 21/05/14 22:08:47 INFO StagingCommitter: Task committer attempt_202105142206291670973373787279796_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291670973373787279796_0004_m_000177_465 : duration 0:00.003s
[2021-05-14 19:08:48,996] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206294534568138262270912_0004_m_000174_462: needsTaskCommit() Task attempt_202105142206294534568138262270912_0004_m_000174_462
[2021-05-14 19:08:48,997] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206294534568138262270912_0004_m_000174_462: needsTaskCommit() Task attempt_202105142206294534568138262270912_0004_m_000174_462: duration 0:00.001s
21/05/14 22:08:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294534568138262270912_0004_m_000174_462
[2021-05-14 19:08:48,998] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4544 bytes result sent to driver
[2021-05-14 19:08:49,000] {docker.py:276} INFO - 21/05/14 22:08:49 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (9bbba8114907, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:08:49 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2868 ms on 9bbba8114907 (executor driver) (175/200)
[2021-05-14 19:08:49,001] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-14 19:08:49,008] {docker.py:276} INFO - 21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:49,010] {docker.py:276} INFO - 21/05/14 22:08:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297136691430331130481_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136691430331130481_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297136691430331130481_0004}; taskId=attempt_202105142206297136691430331130481_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66bd1c7a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206297136691430331130481_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136691430331130481_0004_m_000178_466
[2021-05-14 19:08:49,013] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206297136691430331130481_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297136691430331130481_0004_m_000178_466 : duration 0:00.002s
[2021-05-14 19:08:49,739] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206295871117801827266591_0004_m_000175_463: needsTaskCommit() Task attempt_202105142206295871117801827266591_0004_m_000175_463
[2021-05-14 19:08:49,740] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206295871117801827266591_0004_m_000175_463: needsTaskCommit() Task attempt_202105142206295871117801827266591_0004_m_000175_463: duration 0:00.001s
21/05/14 22:08:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206295871117801827266591_0004_m_000175_463
[2021-05-14 19:08:49,743] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4544 bytes result sent to driver
[2021-05-14 19:08:49,743] {docker.py:276} INFO - 21/05/14 22:08:49 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (9bbba8114907, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:49,744] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
21/05/14 22:08:49 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2898 ms on 9bbba8114907 (executor driver) (176/200)
[2021-05-14 19:08:49,762] {docker.py:276} INFO - 21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:49,763] {docker.py:276} INFO - 21/05/14 22:08:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291002097455697123308_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291002097455697123308_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291002097455697123308_0004}; taskId=attempt_202105142206291002097455697123308_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7668d8a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:49,763] {docker.py:276} INFO - 21/05/14 22:08:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206291002097455697123308_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291002097455697123308_0004_m_000179_467
[2021-05-14 19:08:49,766] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206291002097455697123308_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291002097455697123308_0004_m_000179_467 : duration 0:00.003s
[2021-05-14 19:08:49,791] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206297142808825950530259_0004_m_000176_464: needsTaskCommit() Task attempt_202105142206297142808825950530259_0004_m_000176_464
[2021-05-14 19:08:49,792] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206297142808825950530259_0004_m_000176_464: needsTaskCommit() Task attempt_202105142206297142808825950530259_0004_m_000176_464: duration 0:00.001s
21/05/14 22:08:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297142808825950530259_0004_m_000176_464
[2021-05-14 19:08:49,794] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4587 bytes result sent to driver
[2021-05-14 19:08:49,795] {docker.py:276} INFO - 21/05/14 22:08:49 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (9bbba8114907, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:49,797] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
21/05/14 22:08:49 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2817 ms on 9bbba8114907 (executor driver) (177/200)
[2021-05-14 19:08:49,806] {docker.py:276} INFO - 21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:49,808] {docker.py:276} INFO - 21/05/14 22:08:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297929972765892570032_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297929972765892570032_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297929972765892570032_0004}; taskId=attempt_202105142206297929972765892570032_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d0d62ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206297929972765892570032_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297929972765892570032_0004_m_000180_468
[2021-05-14 19:08:49,811] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206297929972765892570032_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297929972765892570032_0004_m_000180_468 : duration 0:00.003s
[2021-05-14 19:08:49,966] {docker.py:276} INFO - 21/05/14 22:08:49 INFO StagingCommitter: Starting: Task committer attempt_202105142206291670973373787279796_0004_m_000177_465: needsTaskCommit() Task attempt_202105142206291670973373787279796_0004_m_000177_465
21/05/14 22:08:49 INFO StagingCommitter: Task committer attempt_202105142206291670973373787279796_0004_m_000177_465: needsTaskCommit() Task attempt_202105142206291670973373787279796_0004_m_000177_465: duration 0:00.000s
21/05/14 22:08:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291670973373787279796_0004_m_000177_465
[2021-05-14 19:08:49,969] {docker.py:276} INFO - 21/05/14 22:08:49 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4587 bytes result sent to driver
[2021-05-14 19:08:49,970] {docker.py:276} INFO - 21/05/14 22:08:50 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (9bbba8114907, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:49,971] {docker.py:276} INFO - 21/05/14 22:08:50 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
[2021-05-14 19:08:49,972] {docker.py:276} INFO - 21/05/14 22:08:50 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2482 ms on 9bbba8114907 (executor driver) (178/200)
[2021-05-14 19:08:49,983] {docker.py:276} INFO - 21/05/14 22:08:50 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:49,984] {docker.py:276} INFO - 21/05/14 22:08:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297986421534805520128_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297986421534805520128_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297986421534805520128_0004}; taskId=attempt_202105142206297986421534805520128_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f9884b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:49,984] {docker.py:276} INFO - 21/05/14 22:08:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:50 INFO StagingCommitter: Starting: Task committer attempt_202105142206297986421534805520128_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297986421534805520128_0004_m_000181_469
[2021-05-14 19:08:49,987] {docker.py:276} INFO - 21/05/14 22:08:50 INFO StagingCommitter: Task committer attempt_202105142206297986421534805520128_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297986421534805520128_0004_m_000181_469 : duration 0:00.002s
[2021-05-14 19:08:51,918] {docker.py:276} INFO - 21/05/14 22:08:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206297136691430331130481_0004_m_000178_466: needsTaskCommit() Task attempt_202105142206297136691430331130481_0004_m_000178_466
21/05/14 22:08:51 INFO StagingCommitter: Task committer attempt_202105142206297136691430331130481_0004_m_000178_466: needsTaskCommit() Task attempt_202105142206297136691430331130481_0004_m_000178_466: duration 0:00.001s
21/05/14 22:08:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297136691430331130481_0004_m_000178_466
[2021-05-14 19:08:51,920] {docker.py:276} INFO - 21/05/14 22:08:51 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4587 bytes result sent to driver
[2021-05-14 19:08:51,921] {docker.py:276} INFO - 21/05/14 22:08:51 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (9bbba8114907, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:51,922] {docker.py:276} INFO - 21/05/14 22:08:51 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
21/05/14 22:08:51 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 2892 ms on 9bbba8114907 (executor driver) (179/200)
[2021-05-14 19:08:51,931] {docker.py:276} INFO - 21/05/14 22:08:51 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:51,932] {docker.py:276} INFO - 21/05/14 22:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296163046277614564878_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296163046277614564878_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296163046277614564878_0004}; taskId=attempt_202105142206296163046277614564878_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d1c1952}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:51,933] {docker.py:276} INFO - 21/05/14 22:08:51 INFO StagingCommitter: Starting: Task committer attempt_202105142206296163046277614564878_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296163046277614564878_0004_m_000182_470
[2021-05-14 19:08:51,936] {docker.py:276} INFO - 21/05/14 22:08:51 INFO StagingCommitter: Task committer attempt_202105142206296163046277614564878_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296163046277614564878_0004_m_000182_470 : duration 0:00.003s
[2021-05-14 19:08:52,394] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206297929972765892570032_0004_m_000180_468: needsTaskCommit() Task attempt_202105142206297929972765892570032_0004_m_000180_468
[2021-05-14 19:08:52,395] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206297929972765892570032_0004_m_000180_468: needsTaskCommit() Task attempt_202105142206297929972765892570032_0004_m_000180_468: duration 0:00.000s
21/05/14 22:08:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297929972765892570032_0004_m_000180_468
[2021-05-14 19:08:52,396] {docker.py:276} INFO - 21/05/14 22:08:52 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4544 bytes result sent to driver
[2021-05-14 19:08:52,397] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (9bbba8114907, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:52,398] {docker.py:276} INFO - 21/05/14 22:08:52 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-14 19:08:52,398] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2572 ms on 9bbba8114907 (executor driver) (180/200)
[2021-05-14 19:08:52,407] {docker.py:276} INFO - 21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:52,409] {docker.py:276} INFO - 21/05/14 22:08:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292095584622536681491_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292095584622536681491_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292095584622536681491_0004}; taskId=attempt_202105142206292095584622536681491_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@410b32ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206292095584622536681491_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292095584622536681491_0004_m_000183_471
[2021-05-14 19:08:52,412] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206292095584622536681491_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292095584622536681491_0004_m_000183_471 : duration 0:00.003s
[2021-05-14 19:08:52,440] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206291002097455697123308_0004_m_000179_467: needsTaskCommit() Task attempt_202105142206291002097455697123308_0004_m_000179_467
[2021-05-14 19:08:52,440] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206291002097455697123308_0004_m_000179_467: needsTaskCommit() Task attempt_202105142206291002097455697123308_0004_m_000179_467: duration 0:00.000s
[2021-05-14 19:08:52,440] {docker.py:276} INFO - 21/05/14 22:08:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291002097455697123308_0004_m_000179_467
[2021-05-14 19:08:52,441] {docker.py:276} INFO - 21/05/14 22:08:52 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4587 bytes result sent to driver
[2021-05-14 19:08:52,442] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (9bbba8114907, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:52,443] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2670 ms on 9bbba8114907 (executor driver) (181/200)
21/05/14 22:08:52 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-14 19:08:52,450] {docker.py:276} INFO - 21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:52,450] {docker.py:276} INFO - 21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:52,452] {docker.py:276} INFO - 21/05/14 22:08:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:52,453] {docker.py:276} INFO - 21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293801633182771319785_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293801633182771319785_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293801633182771319785_0004}; taskId=attempt_202105142206293801633182771319785_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7098e05c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206293801633182771319785_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293801633182771319785_0004_m_000184_472
[2021-05-14 19:08:52,455] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206293801633182771319785_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293801633182771319785_0004_m_000184_472 : duration 0:00.002s
[2021-05-14 19:08:52,574] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206297986421534805520128_0004_m_000181_469: needsTaskCommit() Task attempt_202105142206297986421534805520128_0004_m_000181_469
[2021-05-14 19:08:52,574] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206297986421534805520128_0004_m_000181_469: needsTaskCommit() Task attempt_202105142206297986421534805520128_0004_m_000181_469: duration 0:00.001s
21/05/14 22:08:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297986421534805520128_0004_m_000181_469
[2021-05-14 19:08:52,576] {docker.py:276} INFO - 21/05/14 22:08:52 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-14 19:08:52,577] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (9bbba8114907, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 22:08:52 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-14 19:08:52,579] {docker.py:276} INFO - 21/05/14 22:08:52 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2579 ms on 9bbba8114907 (executor driver) (182/200)
[2021-05-14 19:08:52,588] {docker.py:276} INFO - 21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:52,589] {docker.py:276} INFO - 21/05/14 22:08:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206291400333246369558043_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291400333246369558043_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206291400333246369558043_0004}; taskId=attempt_202105142206291400333246369558043_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6df364d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:52 INFO StagingCommitter: Starting: Task committer attempt_202105142206291400333246369558043_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291400333246369558043_0004_m_000185_473
[2021-05-14 19:08:52,593] {docker.py:276} INFO - 21/05/14 22:08:52 INFO StagingCommitter: Task committer attempt_202105142206291400333246369558043_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206291400333246369558043_0004_m_000185_473 : duration 0:00.003s
[2021-05-14 19:08:54,695] {docker.py:276} INFO - 21/05/14 22:08:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206296163046277614564878_0004_m_000182_470: needsTaskCommit() Task attempt_202105142206296163046277614564878_0004_m_000182_470
21/05/14 22:08:54 INFO StagingCommitter: Task committer attempt_202105142206296163046277614564878_0004_m_000182_470: needsTaskCommit() Task attempt_202105142206296163046277614564878_0004_m_000182_470: duration 0:00.000s
21/05/14 22:08:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296163046277614564878_0004_m_000182_470
21/05/14 22:08:54 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4587 bytes result sent to driver
[2021-05-14 19:08:54,696] {docker.py:276} INFO - 21/05/14 22:08:54 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (9bbba8114907, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:54,696] {docker.py:276} INFO - 21/05/14 22:08:54 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2779 ms on 9bbba8114907 (executor driver) (183/200)
[2021-05-14 19:08:54,697] {docker.py:276} INFO - 21/05/14 22:08:54 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
[2021-05-14 19:08:54,705] {docker.py:276} INFO - 21/05/14 22:08:54 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:54,705] {docker.py:276} INFO - 21/05/14 22:08:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:54,707] {docker.py:276} INFO - 21/05/14 22:08:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 19:08:54,708] {docker.py:276} INFO - 21/05/14 22:08:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:54,708] {docker.py:276} INFO - 21/05/14 22:08:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:54,709] {docker.py:276} INFO - 21/05/14 22:08:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293686858874709744423_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293686858874709744423_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293686858874709744423_0004}; taskId=attempt_202105142206293686858874709744423_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6adcd558}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:54,709] {docker.py:276} INFO - 21/05/14 22:08:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:54,709] {docker.py:276} INFO - 21/05/14 22:08:54 INFO StagingCommitter: Starting: Task committer attempt_202105142206293686858874709744423_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293686858874709744423_0004_m_000186_474
[2021-05-14 19:08:54,712] {docker.py:276} INFO - 21/05/14 22:08:54 INFO StagingCommitter: Task committer attempt_202105142206293686858874709744423_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293686858874709744423_0004_m_000186_474 : duration 0:00.003s
[2021-05-14 19:08:55,045] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206292095584622536681491_0004_m_000183_471: needsTaskCommit() Task attempt_202105142206292095584622536681491_0004_m_000183_471
[2021-05-14 19:08:55,046] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206292095584622536681491_0004_m_000183_471: needsTaskCommit() Task attempt_202105142206292095584622536681491_0004_m_000183_471: duration 0:00.001s
21/05/14 22:08:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292095584622536681491_0004_m_000183_471
[2021-05-14 19:08:55,048] {docker.py:276} INFO - 21/05/14 22:08:55 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4587 bytes result sent to driver
[2021-05-14 19:08:55,049] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (9bbba8114907, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:55,050] {docker.py:276} INFO - 21/05/14 22:08:55 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
[2021-05-14 19:08:55,051] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 2657 ms on 9bbba8114907 (executor driver) (184/200)
[2021-05-14 19:08:55,060] {docker.py:276} INFO - 21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:55,062] {docker.py:276} INFO - 21/05/14 22:08:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:55,062] {docker.py:276} INFO - 21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296478548960041332304_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296478548960041332304_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296478548960041332304_0004}; taskId=attempt_202105142206296478548960041332304_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12dc5627}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206296478548960041332304_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296478548960041332304_0004_m_000187_475
[2021-05-14 19:08:55,064] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206293801633182771319785_0004_m_000184_472: needsTaskCommit() Task attempt_202105142206293801633182771319785_0004_m_000184_472
[2021-05-14 19:08:55,065] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206293801633182771319785_0004_m_000184_472: needsTaskCommit() Task attempt_202105142206293801633182771319785_0004_m_000184_472: duration 0:00.000s
21/05/14 22:08:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293801633182771319785_0004_m_000184_472
[2021-05-14 19:08:55,065] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206296478548960041332304_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296478548960041332304_0004_m_000187_475 : duration 0:00.004s
21/05/14 22:08:55 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4587 bytes result sent to driver
[2021-05-14 19:08:55,066] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (9bbba8114907, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:55,067] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 2628 ms on 9bbba8114907 (executor driver) (185/200)
[2021-05-14 19:08:55,067] {docker.py:276} INFO - 21/05/14 22:08:55 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-14 19:08:55,075] {docker.py:276} INFO - 21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:55,076] {docker.py:276} INFO - 21/05/14 22:08:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:55,077] {docker.py:276} INFO - 21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206299202990304544237208_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299202990304544237208_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206299202990304544237208_0004}; taskId=attempt_202105142206299202990304544237208_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6faf5745}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206299202990304544237208_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299202990304544237208_0004_m_000188_476
[2021-05-14 19:08:55,080] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206299202990304544237208_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206299202990304544237208_0004_m_000188_476 : duration 0:00.003s
[2021-05-14 19:08:55,190] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206291400333246369558043_0004_m_000185_473: needsTaskCommit() Task attempt_202105142206291400333246369558043_0004_m_000185_473
[2021-05-14 19:08:55,191] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206291400333246369558043_0004_m_000185_473: needsTaskCommit() Task attempt_202105142206291400333246369558043_0004_m_000185_473: duration 0:00.000s
21/05/14 22:08:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206291400333246369558043_0004_m_000185_473
[2021-05-14 19:08:55,194] {docker.py:276} INFO - 21/05/14 22:08:55 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4630 bytes result sent to driver
[2021-05-14 19:08:55,195] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (9bbba8114907, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:55,196] {docker.py:276} INFO - 21/05/14 22:08:55 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 2623 ms on 9bbba8114907 (executor driver) (186/200)
[2021-05-14 19:08:55,197] {docker.py:276} INFO - 21/05/14 22:08:55 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-14 19:08:55,206] {docker.py:276} INFO - 21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:55,208] {docker.py:276} INFO - 21/05/14 22:08:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294552492399485398218_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294552492399485398218_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294552492399485398218_0004}; taskId=attempt_202105142206294552492399485398218_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7729592}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:55,209] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Starting: Task committer attempt_202105142206294552492399485398218_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294552492399485398218_0004_m_000189_477
[2021-05-14 19:08:55,211] {docker.py:276} INFO - 21/05/14 22:08:55 INFO StagingCommitter: Task committer attempt_202105142206294552492399485398218_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294552492399485398218_0004_m_000189_477 : duration 0:00.003s
[2021-05-14 19:08:57,452] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206293686858874709744423_0004_m_000186_474: needsTaskCommit() Task attempt_202105142206293686858874709744423_0004_m_000186_474
[2021-05-14 19:08:57,453] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206293686858874709744423_0004_m_000186_474: needsTaskCommit() Task attempt_202105142206293686858874709744423_0004_m_000186_474: duration 0:00.001s
21/05/14 22:08:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293686858874709744423_0004_m_000186_474
[2021-05-14 19:08:57,454] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4544 bytes result sent to driver
[2021-05-14 19:08:57,455] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (9bbba8114907, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:57,457] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-14 19:08:57,458] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 2764 ms on 9bbba8114907 (executor driver) (187/200)
[2021-05-14 19:08:57,467] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:57,472] {docker.py:276} INFO - 21/05/14 22:08:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:57,472] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:57,472] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292088555684699616914_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292088555684699616914_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292088555684699616914_0004}; taskId=attempt_202105142206292088555684699616914_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66d4593e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:57,473] {docker.py:276} INFO - 21/05/14 22:08:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:57,473] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206292088555684699616914_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292088555684699616914_0004_m_000190_478
[2021-05-14 19:08:57,476] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206292088555684699616914_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292088555684699616914_0004_m_000190_478 : duration 0:00.003s
[2021-05-14 19:08:57,615] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206296478548960041332304_0004_m_000187_475: needsTaskCommit() Task attempt_202105142206296478548960041332304_0004_m_000187_475
[2021-05-14 19:08:57,616] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206296478548960041332304_0004_m_000187_475: needsTaskCommit() Task attempt_202105142206296478548960041332304_0004_m_000187_475: duration 0:00.001s
21/05/14 22:08:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296478548960041332304_0004_m_000187_475
[2021-05-14 19:08:57,617] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4544 bytes result sent to driver
[2021-05-14 19:08:57,619] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (9bbba8114907, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:57,619] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 2573 ms on 9bbba8114907 (executor driver) (188/200)
[2021-05-14 19:08:57,620] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-14 19:08:57,630] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:57,630] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:57,632] {docker.py:276} INFO - 21/05/14 22:08:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:57,632] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:57,633] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292713145030756744478_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292713145030756744478_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292713145030756744478_0004}; taskId=attempt_202105142206292713145030756744478_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59311a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:57,633] {docker.py:276} INFO - 21/05/14 22:08:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:57,633] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206292713145030756744478_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292713145030756744478_0004_m_000191_479
[2021-05-14 19:08:57,636] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206292713145030756744478_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292713145030756744478_0004_m_000191_479 : duration 0:00.003s
[2021-05-14 19:08:57,651] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206299202990304544237208_0004_m_000188_476: needsTaskCommit() Task attempt_202105142206299202990304544237208_0004_m_000188_476
[2021-05-14 19:08:57,652] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206299202990304544237208_0004_m_000188_476: needsTaskCommit() Task attempt_202105142206299202990304544237208_0004_m_000188_476: duration 0:00.001s
21/05/14 22:08:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206299202990304544237208_0004_m_000188_476
[2021-05-14 19:08:57,652] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4587 bytes result sent to driver
[2021-05-14 19:08:57,653] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (9bbba8114907, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:57,654] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-14 19:08:57,654] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 2591 ms on 9bbba8114907 (executor driver) (189/200)
[2021-05-14 19:08:57,661] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 19:08:57,661] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:57,666] {docker.py:276} INFO - 21/05/14 22:08:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:08:57,666] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:08:57,666] {docker.py:276} INFO - 21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206294687845686759402245_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294687845686759402245_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206294687845686759402245_0004}; taskId=attempt_202105142206294687845686759402245_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c538eb0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206294687845686759402245_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294687845686759402245_0004_m_000192_480
[2021-05-14 19:08:57,668] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206294687845686759402245_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206294687845686759402245_0004_m_000192_480 : duration 0:00.003s
[2021-05-14 19:08:57,802] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206294552492399485398218_0004_m_000189_477: needsTaskCommit() Task attempt_202105142206294552492399485398218_0004_m_000189_477
[2021-05-14 19:08:57,802] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206294552492399485398218_0004_m_000189_477: needsTaskCommit() Task attempt_202105142206294552492399485398218_0004_m_000189_477: duration 0:00.001s
[2021-05-14 19:08:57,803] {docker.py:276} INFO - 21/05/14 22:08:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294552492399485398218_0004_m_000189_477
[2021-05-14 19:08:57,805] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4587 bytes result sent to driver
[2021-05-14 19:08:57,806] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (9bbba8114907, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:08:57,807] {docker.py:276} INFO - 21/05/14 22:08:57 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 2615 ms on 9bbba8114907 (executor driver) (190/200)
[2021-05-14 19:08:57,808] {docker.py:276} INFO - 21/05/14 22:08:57 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-14 19:08:57,817] {docker.py:276} INFO - 21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:08:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:08:57,819] {docker.py:276} INFO - 21/05/14 22:08:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:08:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:08:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206297145753078555605459_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297145753078555605459_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206297145753078555605459_0004}; taskId=attempt_202105142206297145753078555605459_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14569928}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:08:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:08:57,820] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Starting: Task committer attempt_202105142206297145753078555605459_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297145753078555605459_0004_m_000193_481
[2021-05-14 19:08:57,823] {docker.py:276} INFO - 21/05/14 22:08:57 INFO StagingCommitter: Task committer attempt_202105142206297145753078555605459_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206297145753078555605459_0004_m_000193_481 : duration 0:00.004s
[2021-05-14 19:09:00,191] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206292713145030756744478_0004_m_000191_479: needsTaskCommit() Task attempt_202105142206292713145030756744478_0004_m_000191_479
21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206292713145030756744478_0004_m_000191_479: needsTaskCommit() Task attempt_202105142206292713145030756744478_0004_m_000191_479: duration 0:00.001s
21/05/14 22:09:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292713145030756744478_0004_m_000191_479
[2021-05-14 19:09:00,194] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4587 bytes result sent to driver
[2021-05-14 19:09:00,196] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (9bbba8114907, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:00,197] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2581 ms on 9bbba8114907 (executor driver) (191/200)
[2021-05-14 19:09:00,198] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-14 19:09:00,208] {docker.py:276} INFO - 21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Getting 5 (45.4 KiB) non-empty blocks including 5 (45.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:00,209] {docker.py:276} INFO - 21/05/14 22:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293131370755814725618_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293131370755814725618_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293131370755814725618_0004}; taskId=attempt_202105142206293131370755814725618_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@257fd774}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206293131370755814725618_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293131370755814725618_0004_m_000194_482
[2021-05-14 19:09:00,212] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206293131370755814725618_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293131370755814725618_0004_m_000194_482 : duration 0:00.003s
[2021-05-14 19:09:00,277] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206294687845686759402245_0004_m_000192_480: needsTaskCommit() Task attempt_202105142206294687845686759402245_0004_m_000192_480
[2021-05-14 19:09:00,278] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206294687845686759402245_0004_m_000192_480: needsTaskCommit() Task attempt_202105142206294687845686759402245_0004_m_000192_480: duration 0:00.001s
21/05/14 22:09:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206294687845686759402245_0004_m_000192_480
[2021-05-14 19:09:00,281] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4544 bytes result sent to driver
[2021-05-14 19:09:00,282] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (9bbba8114907, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:00,283] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-14 19:09:00,284] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 2634 ms on 9bbba8114907 (executor driver) (192/200)
[2021-05-14 19:09:00,287] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206292088555684699616914_0004_m_000190_478: needsTaskCommit() Task attempt_202105142206292088555684699616914_0004_m_000190_478
[2021-05-14 19:09:00,288] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206292088555684699616914_0004_m_000190_478: needsTaskCommit() Task attempt_202105142206292088555684699616914_0004_m_000190_478: duration 0:00.000s
21/05/14 22:09:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292088555684699616914_0004_m_000190_478
[2021-05-14 19:09:00,289] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4587 bytes result sent to driver
[2021-05-14 19:09:00,290] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (9bbba8114907, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:00,291] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2839 ms on 9bbba8114907 (executor driver) (193/200)
21/05/14 22:09:00 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-14 19:09:00,297] {docker.py:276} INFO - 21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:00,299] {docker.py:276} INFO - 21/05/14 22:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:00,299] {docker.py:276} INFO - 21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206296345353011986704829_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296345353011986704829_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206296345353011986704829_0004}; taskId=attempt_202105142206296345353011986704829_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5160755f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:00,300] {docker.py:276} INFO - 21/05/14 22:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:09:00,300] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206296345353011986704829_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296345353011986704829_0004_m_000195_483
[2021-05-14 19:09:00,300] {docker.py:276} INFO - 21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:00,302] {docker.py:276} INFO - 21/05/14 22:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206292053750074987456471_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292053750074987456471_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206292053750074987456471_0004}; taskId=attempt_202105142206292053750074987456471_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f83ed53}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:00,302] {docker.py:276} INFO - 21/05/14 22:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:09:00,303] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206292053750074987456471_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292053750074987456471_0004_m_000196_484
[2021-05-14 19:09:00,303] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206296345353011986704829_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206296345353011986704829_0004_m_000195_483 : duration 0:00.005s
[2021-05-14 19:09:00,306] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206292053750074987456471_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206292053750074987456471_0004_m_000196_484 : duration 0:00.003s
[2021-05-14 19:09:00,525] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206297145753078555605459_0004_m_000193_481: needsTaskCommit() Task attempt_202105142206297145753078555605459_0004_m_000193_481
[2021-05-14 19:09:00,526] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206297145753078555605459_0004_m_000193_481: needsTaskCommit() Task attempt_202105142206297145753078555605459_0004_m_000193_481: duration 0:00.001s
[2021-05-14 19:09:00,527] {docker.py:276} INFO - 21/05/14 22:09:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206297145753078555605459_0004_m_000193_481
[2021-05-14 19:09:00,529] {docker.py:276} INFO - 21/05/14 22:09:00 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4544 bytes result sent to driver
[2021-05-14 19:09:00,530] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (9bbba8114907, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:00,531] {docker.py:276} INFO - 21/05/14 22:09:00 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 2728 ms on 9bbba8114907 (executor driver) (194/200)
21/05/14 22:09:00 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-14 19:09:00,541] {docker.py:276} INFO - 21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:00,542] {docker.py:276} INFO - 21/05/14 22:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 22:09:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293940646660055195877_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293940646660055195877_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293940646660055195877_0004}; taskId=attempt_202105142206293940646660055195877_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@635ea1a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 22:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:09:00,543] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Starting: Task committer attempt_202105142206293940646660055195877_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293940646660055195877_0004_m_000197_485
[2021-05-14 19:09:00,546] {docker.py:276} INFO - 21/05/14 22:09:00 INFO StagingCommitter: Task committer attempt_202105142206293940646660055195877_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293940646660055195877_0004_m_000197_485 : duration 0:00.003s
[2021-05-14 19:09:02,821] {docker.py:276} INFO - 21/05/14 22:09:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206293131370755814725618_0004_m_000194_482: needsTaskCommit() Task attempt_202105142206293131370755814725618_0004_m_000194_482
21/05/14 22:09:02 INFO StagingCommitter: Task committer attempt_202105142206293131370755814725618_0004_m_000194_482: needsTaskCommit() Task attempt_202105142206293131370755814725618_0004_m_000194_482: duration 0:00.000s
21/05/14 22:09:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293131370755814725618_0004_m_000194_482
[2021-05-14 19:09:02,823] {docker.py:276} INFO - 21/05/14 22:09:02 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4587 bytes result sent to driver
[2021-05-14 19:09:02,824] {docker.py:276} INFO - 21/05/14 22:09:02 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (9bbba8114907, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:02,826] {docker.py:276} INFO - 21/05/14 22:09:02 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-14 19:09:02,827] {docker.py:276} INFO - 21/05/14 22:09:02 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 2633 ms on 9bbba8114907 (executor driver) (195/200)
[2021-05-14 19:09:02,837] {docker.py:276} INFO - 21/05/14 22:09:02 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:02,839] {docker.py:276} INFO - 21/05/14 22:09:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:09:02,839] {docker.py:276} INFO - 21/05/14 22:09:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:02,840] {docker.py:276} INFO - 21/05/14 22:09:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105142206293728189930502799822_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293728189930502799822_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105142206293728189930502799822_0004}; taskId=attempt_202105142206293728189930502799822_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24e025c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:02,840] {docker.py:276} INFO - 21/05/14 22:09:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:09:02,841] {docker.py:276} INFO - 21/05/14 22:09:02 INFO StagingCommitter: Starting: Task committer attempt_202105142206293728189930502799822_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293728189930502799822_0004_m_000198_486
[2021-05-14 19:09:02,844] {docker.py:276} INFO - 21/05/14 22:09:02 INFO StagingCommitter: Task committer attempt_202105142206293728189930502799822_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_202105142206293728189930502799822_0004_m_000198_486 : duration 0:00.004s
[2021-05-14 19:09:02,989] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Starting: Task committer attempt_202105142206296345353011986704829_0004_m_000195_483: needsTaskCommit() Task attempt_202105142206296345353011986704829_0004_m_000195_483
21/05/14 22:09:03 INFO StagingCommitter: Task committer attempt_202105142206296345353011986704829_0004_m_000195_483: needsTaskCommit() Task attempt_202105142206296345353011986704829_0004_m_000195_483: duration 0:00.000s
21/05/14 22:09:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206296345353011986704829_0004_m_000195_483
[2021-05-14 19:09:02,990] {docker.py:276} INFO - 21/05/14 22:09:03 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4587 bytes result sent to driver
[2021-05-14 19:09:02,993] {docker.py:276} INFO - 21/05/14 22:09:03 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (9bbba8114907, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 19:09:02,993] {docker.py:276} INFO - 21/05/14 22:09:03 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-14 19:09:02,993] {docker.py:276} INFO - 21/05/14 22:09:03 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 2715 ms on 9bbba8114907 (executor driver) (196/200)
[2021-05-14 19:09:03,001] {docker.py:276} INFO - 21/05/14 22:09:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 22:09:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 19:09:03,003] {docker.py:276} INFO - 21/05/14 22:09:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 22:09:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 19:09:03,003] {docker.py:276} INFO - 21/05/14 22:09:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:03,004] {docker.py:276} INFO - 21/05/14 22:09:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514220629645007268069954449_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629645007268069954449_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514220629645007268069954449_0004}; taskId=attempt_20210514220629645007268069954449_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b7135c8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/a5e97d72-9c9a-43b3-aab9-acc87f394849/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:03,004] {docker.py:276} INFO - 21/05/14 22:09:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 19:09:03,004] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Starting: Task committer attempt_20210514220629645007268069954449_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629645007268069954449_0004_m_000199_487
[2021-05-14 19:09:03,007] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Task committer attempt_20210514220629645007268069954449_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/a5e97d72-9c9a-43b3-aab9-acc87f394849/_temporary/0/_temporary/attempt_20210514220629645007268069954449_0004_m_000199_487 : duration 0:00.003s
[2021-05-14 19:09:03,141] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Starting: Task committer attempt_202105142206293940646660055195877_0004_m_000197_485: needsTaskCommit() Task attempt_202105142206293940646660055195877_0004_m_000197_485
[2021-05-14 19:09:03,142] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Task committer attempt_202105142206293940646660055195877_0004_m_000197_485: needsTaskCommit() Task attempt_202105142206293940646660055195877_0004_m_000197_485: duration 0:00.001s
21/05/14 22:09:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293940646660055195877_0004_m_000197_485
[2021-05-14 19:09:03,144] {docker.py:276} INFO - 21/05/14 22:09:03 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4587 bytes result sent to driver
[2021-05-14 19:09:03,146] {docker.py:276} INFO - 21/05/14 22:09:03 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 2619 ms on 9bbba8114907 (executor driver) (197/200)
[2021-05-14 19:09:03,177] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Starting: Task committer attempt_202105142206292053750074987456471_0004_m_000196_484: needsTaskCommit() Task attempt_202105142206292053750074987456471_0004_m_000196_484
[2021-05-14 19:09:03,178] {docker.py:276} INFO - 21/05/14 22:09:03 INFO StagingCommitter: Task committer attempt_202105142206292053750074987456471_0004_m_000196_484: needsTaskCommit() Task attempt_202105142206292053750074987456471_0004_m_000196_484: duration 0:00.000s
21/05/14 22:09:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206292053750074987456471_0004_m_000196_484
[2021-05-14 19:09:03,179] {docker.py:276} INFO - 21/05/14 22:09:03 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4587 bytes result sent to driver
[2021-05-14 19:09:03,181] {docker.py:276} INFO - 21/05/14 22:09:03 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 2894 ms on 9bbba8114907 (executor driver) (198/200)
[2021-05-14 19:09:05,027] {docker.py:276} INFO - 21/05/14 22:09:05 INFO StagingCommitter: Starting: Task committer attempt_202105142206293728189930502799822_0004_m_000198_486: needsTaskCommit() Task attempt_202105142206293728189930502799822_0004_m_000198_486
[2021-05-14 19:09:05,029] {docker.py:276} INFO - 21/05/14 22:09:05 INFO StagingCommitter: Task committer attempt_202105142206293728189930502799822_0004_m_000198_486: needsTaskCommit() Task attempt_202105142206293728189930502799822_0004_m_000198_486: duration 0:00.001s
21/05/14 22:09:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105142206293728189930502799822_0004_m_000198_486
[2021-05-14 19:09:05,030] {docker.py:276} INFO - 21/05/14 22:09:05 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4544 bytes result sent to driver
[2021-05-14 19:09:05,031] {docker.py:276} INFO - 21/05/14 22:09:05 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2209 ms on 9bbba8114907 (executor driver) (199/200)
[2021-05-14 19:09:05,063] {docker.py:276} INFO - 21/05/14 22:09:05 INFO StagingCommitter: Starting: Task committer attempt_20210514220629645007268069954449_0004_m_000199_487: needsTaskCommit() Task attempt_20210514220629645007268069954449_0004_m_000199_487
21/05/14 22:09:05 INFO StagingCommitter: Task committer attempt_20210514220629645007268069954449_0004_m_000199_487: needsTaskCommit() Task attempt_20210514220629645007268069954449_0004_m_000199_487: duration 0:00.001s
21/05/14 22:09:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514220629645007268069954449_0004_m_000199_487
[2021-05-14 19:09:05,065] {docker.py:276} INFO - 21/05/14 22:09:05 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4544 bytes result sent to driver
[2021-05-14 19:09:05,066] {docker.py:276} INFO - 21/05/14 22:09:05 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 2076 ms on 9bbba8114907 (executor driver) (200/200)
21/05/14 22:09:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-14 19:09:05,068] {docker.py:276} INFO - 21/05/14 22:09:05 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 137.907 s
[2021-05-14 19:09:05,069] {docker.py:276} INFO - 21/05/14 22:09:05 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 22:09:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-14 19:09:05,070] {docker.py:276} INFO - 21/05/14 22:09:05 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 156.142532 s
[2021-05-14 19:09:05,072] {docker.py:276} INFO - 21/05/14 22:09:05 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105142206287933865312698177981_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 19:09:05,089] {docker.py:276} INFO - 21/05/14 22:09:05 WARN AbstractS3ACommitter: Task committer attempt_202105142206287933865312698177981_0000_m_000000_0: No pending uploads to commit
[2021-05-14 19:09:05,604] {docker.py:276} INFO - 21/05/14 22:09:05 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 22:09:05 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 19:09:05,785] {docker.py:276} INFO - 21/05/14 22:09:05 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.181s
21/05/14 22:09:05 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.181s
[2021-05-14 19:09:05,786] {docker.py:276} INFO - 21/05/14 22:09:05 INFO AbstractS3ACommitter: Task committer attempt_202105142206287933865312698177981_0000_m_000000_0: commitJob((no job ID)): duration 0:00.714s
[2021-05-14 19:09:06,305] {docker.py:276} INFO - 21/05/14 22:09:06 INFO FileFormatWriter: Write Job a5e97d72-9c9a-43b3-aab9-acc87f394849 committed.
[2021-05-14 19:09:06,315] {docker.py:276} INFO - 21/05/14 22:09:06 INFO FileFormatWriter: Finished processing stats for write job a5e97d72-9c9a-43b3-aab9-acc87f394849.
[2021-05-14 19:09:06,421] {docker.py:276} INFO - 21/05/14 22:09:06 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 19:09:06,435] {docker.py:276} INFO - 21/05/14 22:09:06 INFO SparkUI: Stopped Spark web UI at http://9bbba8114907:4040
[2021-05-14 19:09:06,454] {docker.py:276} INFO - 21/05/14 22:09:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 19:09:06,473] {docker.py:276} INFO - 21/05/14 22:09:06 INFO MemoryStore: MemoryStore cleared
[2021-05-14 19:09:06,473] {docker.py:276} INFO - 21/05/14 22:09:06 INFO BlockManager: BlockManager stopped
[2021-05-14 19:09:06,477] {docker.py:276} INFO - 21/05/14 22:09:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 19:09:06,481] {docker.py:276} INFO - 21/05/14 22:09:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 19:09:06,489] {docker.py:276} INFO - 21/05/14 22:09:06 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 19:09:06,489] {docker.py:276} INFO - 21/05/14 22:09:06 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 19:09:06,490] {docker.py:276} INFO - 21/05/14 22:09:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea89cfd2-a1bf-4ff2-a9cf-8c1ba06d142b
[2021-05-14 19:09:06,493] {docker.py:276} INFO - 21/05/14 22:09:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-04c1f89a-fbf2-476f-96d4-5ca34ef718c3
[2021-05-14 19:09:06,495] {docker.py:276} INFO - 21/05/14 22:09:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-04c1f89a-fbf2-476f-96d4-5ca34ef718c3/pyspark-936503b9-ee27-42b3-ba0e-02e50c369762
[2021-05-14 19:09:06,501] {docker.py:276} INFO - 21/05/14 22:09:06 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 19:09:06,501] {docker.py:276} INFO - 21/05/14 22:09:06 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
21/05/14 22:09:06 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 19:09:06,740] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210514T220502, start_date=20210514T220552, end_date=20210514T220906
[2021-05-14 19:09:06,796] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-14 19:09:06,812] {local_task_job.py:146} INFO - Task exited with return code 0
