[2021-05-15 11:37:21,622] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T14:36:30.255306+00:00 [queued]>
[2021-05-15 11:37:21,627] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T14:36:30.255306+00:00 [queued]>
[2021-05-15 11:37:21,627] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 11:37:21,627] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-15 11:37:21,627] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 11:37:21,633] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T14:36:30.255306+00:00
[2021-05-15 11:37:21,636] {standard_task_runner.py:52} INFO - Started process 28300 to run task
[2021-05-15 11:37:21,642] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T14:36:30.255306+00:00', '--job-id', '707', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpy9imv5kq', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpaic7hn_r']
[2021-05-15 11:37:21,644] {standard_task_runner.py:77} INFO - Job 707: Subtask run_spark_job
[2021-05-15 11:37:21,685] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T14:36:30.255306+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-15 11:37:21,719] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T14:36:30.255306+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-15T14:36:30.255306+00:00
[2021-05-15 11:37:21,722] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-15 11:37:24,584] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-15 11:37:24,586] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-15 11:37:24,587] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-15 11:37:24,591] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-15 11:37:26,828] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-15 11:37:27,399] {docker.py:276} INFO - 21/05/15 14:37:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-15 11:37:29,476] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-15 11:37:29,490] {docker.py:276} INFO - 21/05/15 14:37:29 INFO SparkContext: Running Spark version 3.1.1
[2021-05-15 11:37:29,551] {docker.py:276} INFO - 21/05/15 14:37:29 INFO ResourceUtils: ==============================================================
[2021-05-15 11:37:29,552] {docker.py:276} INFO - 21/05/15 14:37:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-15 11:37:29,552] {docker.py:276} INFO - 21/05/15 14:37:29 INFO ResourceUtils: ==============================================================
[2021-05-15 11:37:29,553] {docker.py:276} INFO - 21/05/15 14:37:29 INFO SparkContext: Submitted application: spark.py
[2021-05-15 11:37:29,588] {docker.py:276} INFO - 21/05/15 14:37:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-15 11:37:29,609] {docker.py:276} INFO - 21/05/15 14:37:29 INFO ResourceProfile: Limiting resource is cpu
21/05/15 14:37:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-15 11:37:29,685] {docker.py:276} INFO - 21/05/15 14:37:29 INFO SecurityManager: Changing view acls to: jovyan
21/05/15 14:37:29 INFO SecurityManager: Changing modify acls to: jovyan
21/05/15 14:37:29 INFO SecurityManager: Changing view acls groups to: 
21/05/15 14:37:29 INFO SecurityManager: Changing modify acls groups to:
[2021-05-15 11:37:29,685] {docker.py:276} INFO - 21/05/15 14:37:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-15 11:37:30,025] {docker.py:276} INFO - 21/05/15 14:37:30 INFO Utils: Successfully started service 'sparkDriver' on port 37009.
[2021-05-15 11:37:30,062] {docker.py:276} INFO - 21/05/15 14:37:30 INFO SparkEnv: Registering MapOutputTracker
[2021-05-15 11:37:30,108] {docker.py:276} INFO - 21/05/15 14:37:30 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-15 11:37:30,140] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-15 11:37:30,141] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-15 11:37:30,148] {docker.py:276} INFO - 21/05/15 14:37:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-15 11:37:30,165] {docker.py:276} INFO - 21/05/15 14:37:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3481421e-e855-41c3-b986-b2bc6309dc1e
[2021-05-15 11:37:30,192] {docker.py:276} INFO - 21/05/15 14:37:30 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-15 11:37:30,232] {docker.py:276} INFO - 21/05/15 14:37:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-15 11:37:30,509] {docker.py:276} INFO - 21/05/15 14:37:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-15 11:37:30,589] {docker.py:276} INFO - 21/05/15 14:37:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bbe2d2545a9d:4040
[2021-05-15 11:37:30,844] {docker.py:276} INFO - 21/05/15 14:37:30 INFO Executor: Starting executor ID driver on host bbe2d2545a9d
[2021-05-15 11:37:30,883] {docker.py:276} INFO - 21/05/15 14:37:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38027.
[2021-05-15 11:37:30,883] {docker.py:276} INFO - 21/05/15 14:37:30 INFO NettyBlockTransferService: Server created on bbe2d2545a9d:38027
[2021-05-15 11:37:30,888] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-15 11:37:30,898] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bbe2d2545a9d, 38027, None)
[2021-05-15 11:37:30,906] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManagerMasterEndpoint: Registering block manager bbe2d2545a9d:38027 with 934.4 MiB RAM, BlockManagerId(driver, bbe2d2545a9d, 38027, None)
[2021-05-15 11:37:30,910] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bbe2d2545a9d, 38027, None)
[2021-05-15 11:37:30,912] {docker.py:276} INFO - 21/05/15 14:37:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bbe2d2545a9d, 38027, None)
[2021-05-15 11:37:31,468] {docker.py:276} INFO - 21/05/15 14:37:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-15 11:37:31,469] {docker.py:276} INFO - 21/05/15 14:37:31 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-15 11:37:32,488] {docker.py:276} INFO - 21/05/15 14:37:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-15 11:37:32,533] {docker.py:276} INFO - 21/05/15 14:37:32 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/15 14:37:32 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-15 11:37:38,357] {docker.py:276} INFO - 21/05/15 14:37:38 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621003005_to_1621004805.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621004805_to_1621006605.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621006605_to_1621008405.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621008405_to_1621010205.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621010205_to_1621012005.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621012005_to_1621013805.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621013805_to_1621015605.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621015605_to_1621017405.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621017405_to_1621019205.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621019205_to_1621021005.csv.
[2021-05-15 11:37:38,888] {docker.py:276} INFO - 21/05/15 14:37:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:38,915] {docker.py:276} INFO - 21/05/15 14:37:38 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-15 11:37:38,916] {docker.py:276} INFO - 21/05/15 14:37:38 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 11:37:38,917] {docker.py:276} INFO - 21/05/15 14:37:38 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 11:37:38,920] {docker.py:276} INFO - 21/05/15 14:37:38 INFO DAGScheduler: Missing parents: List()
[2021-05-15 11:37:38,929] {docker.py:276} INFO - 21/05/15 14:37:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:37:39,060] {docker.py:276} INFO - 21/05/15 14:37:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-15 11:37:39,136] {docker.py:276} INFO - 21/05/15 14:37:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-15 11:37:39,146] {docker.py:276} INFO - 21/05/15 14:37:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on bbe2d2545a9d:38027 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 11:37:39,154] {docker.py:276} INFO - 21/05/15 14:37:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:37:39,186] {docker.py:276} INFO - 21/05/15 14:37:39 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 11:37:39,190] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-15 11:37:39,331] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (bbe2d2545a9d, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:39,337] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (bbe2d2545a9d, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:39,339] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (bbe2d2545a9d, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:39,346] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (bbe2d2545a9d, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:39,372] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-15 11:37:39,372] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-15 11:37:39,373] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-15 11:37:39,373] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-15 11:37:39,822] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1929 bytes result sent to driver
[2021-05-15 11:37:39,828] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (bbe2d2545a9d, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:39,830] {docker.py:276} INFO - 21/05/15 14:37:39 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-15 11:37:39,838] {docker.py:276} INFO - 21/05/15 14:37:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 491 ms on bbe2d2545a9d (executor driver) (1/141)
[2021-05-15 11:37:40,027] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-15 11:37:40,031] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (bbe2d2545a9d, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,032] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-15 11:37:40,033] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 206 ms on bbe2d2545a9d (executor driver) (2/141)
[2021-05-15 11:37:40,230] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-15 11:37:40,232] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (bbe2d2545a9d, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,233] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-15 11:37:40,235] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 205 ms on bbe2d2545a9d (executor driver) (3/141)
[2021-05-15 11:37:40,290] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-15 11:37:40,294] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (bbe2d2545a9d, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,296] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
21/05/15 14:37:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1013 ms on bbe2d2545a9d (executor driver) (4/141)
[2021-05-15 11:37:40,314] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-15 11:37:40,315] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (bbe2d2545a9d, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,317] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 981 ms on bbe2d2545a9d (executor driver) (5/141)
[2021-05-15 11:37:40,317] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-15 11:37:40,325] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-15 11:37:40,330] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (bbe2d2545a9d, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,331] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-15 11:37:40,332] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 995 ms on bbe2d2545a9d (executor driver) (6/141)
[2021-05-15 11:37:40,414] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-15 11:37:40,415] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (bbe2d2545a9d, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,417] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 186 ms on bbe2d2545a9d (executor driver) (7/141)
[2021-05-15 11:37:40,418] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-15 11:37:40,486] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-15 11:37:40,489] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (bbe2d2545a9d, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,490] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-15 11:37:40,491] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 198 ms on bbe2d2545a9d (executor driver) (8/141)
[2021-05-15 11:37:40,512] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1886 bytes result sent to driver
[2021-05-15 11:37:40,514] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (bbe2d2545a9d, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,515] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 200 ms on bbe2d2545a9d (executor driver) (9/141)
[2021-05-15 11:37:40,515] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-15 11:37:40,536] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1886 bytes result sent to driver
[2021-05-15 11:37:40,538] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (bbe2d2545a9d, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,540] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
21/05/15 14:37:40 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 214 ms on bbe2d2545a9d (executor driver) (10/141)
[2021-05-15 11:37:40,599] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1886 bytes result sent to driver
[2021-05-15 11:37:40,600] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (bbe2d2545a9d, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,602] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-15 11:37:40,603] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 188 ms on bbe2d2545a9d (executor driver) (11/141)
[2021-05-15 11:37:40,686] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1886 bytes result sent to driver
[2021-05-15 11:37:40,687] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 199 ms on bbe2d2545a9d (executor driver) (12/141)
[2021-05-15 11:37:40,690] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (bbe2d2545a9d, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,692] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-15 11:37:40,701] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-15 11:37:40,702] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 189 ms on bbe2d2545a9d (executor driver) (13/141)
[2021-05-15 11:37:40,704] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (bbe2d2545a9d, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,705] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-15 11:37:40,722] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-15 11:37:40,724] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (bbe2d2545a9d, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,725] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 188 ms on bbe2d2545a9d (executor driver) (14/141)
[2021-05-15 11:37:40,726] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-15 11:37:40,784] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-15 11:37:40,786] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (bbe2d2545a9d, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,788] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 188 ms on bbe2d2545a9d (executor driver) (15/141)
[2021-05-15 11:37:40,793] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-15 11:37:40,879] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1843 bytes result sent to driver
[2021-05-15 11:37:40,881] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (bbe2d2545a9d, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,882] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-15 11:37:40,883] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 195 ms on bbe2d2545a9d (executor driver) (16/141)
[2021-05-15 11:37:40,884] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-15 11:37:40,885] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (bbe2d2545a9d, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,886] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 183 ms on bbe2d2545a9d (executor driver) (17/141)
[2021-05-15 11:37:40,887] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-15 11:37:40,915] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1886 bytes result sent to driver
[2021-05-15 11:37:40,917] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (bbe2d2545a9d, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,918] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 195 ms on bbe2d2545a9d (executor driver) (18/141)
[2021-05-15 11:37:40,919] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-15 11:37:40,973] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1886 bytes result sent to driver
[2021-05-15 11:37:40,974] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (bbe2d2545a9d, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:40,976] {docker.py:276} INFO - 21/05/15 14:37:40 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 190 ms on bbe2d2545a9d (executor driver) (19/141)
[2021-05-15 11:37:40,976] {docker.py:276} INFO - 21/05/15 14:37:40 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-15 11:37:41,068] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1886 bytes result sent to driver
[2021-05-15 11:37:41,071] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (bbe2d2545a9d, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,072] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 191 ms on bbe2d2545a9d (executor driver) (20/141)
[2021-05-15 11:37:41,073] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-15 11:37:41,085] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1886 bytes result sent to driver
[2021-05-15 11:37:41,086] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (bbe2d2545a9d, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,088] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 204 ms on bbe2d2545a9d (executor driver) (21/141)
[2021-05-15 11:37:41,089] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-15 11:37:41,093] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-15 11:37:41,095] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (bbe2d2545a9d, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,096] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 180 ms on bbe2d2545a9d (executor driver) (22/141)
[2021-05-15 11:37:41,097] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-15 11:37:41,154] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-15 11:37:41,155] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (bbe2d2545a9d, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,157] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-15 11:37:41,157] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 182 ms on bbe2d2545a9d (executor driver) (23/141)
[2021-05-15 11:37:41,248] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-15 11:37:41,249] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (bbe2d2545a9d, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,250] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 180 ms on bbe2d2545a9d (executor driver) (24/141)
[2021-05-15 11:37:41,251] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-15 11:37:41,262] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-15 11:37:41,263] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (bbe2d2545a9d, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,264] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-15 11:37:41,264] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 178 ms on bbe2d2545a9d (executor driver) (25/141)
[2021-05-15 11:37:41,278] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-15 11:37:41,279] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (bbe2d2545a9d, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,280] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 186 ms on bbe2d2545a9d (executor driver) (26/141)
[2021-05-15 11:37:41,282] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-15 11:37:41,348] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1886 bytes result sent to driver
[2021-05-15 11:37:41,349] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (bbe2d2545a9d, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,350] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-15 11:37:41,351] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 195 ms on bbe2d2545a9d (executor driver) (27/141)
[2021-05-15 11:37:41,424] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1886 bytes result sent to driver
[2021-05-15 11:37:41,426] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (bbe2d2545a9d, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,427] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-15 11:37:41,428] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 179 ms on bbe2d2545a9d (executor driver) (28/141)
[2021-05-15 11:37:41,439] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1886 bytes result sent to driver
[2021-05-15 11:37:41,441] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (bbe2d2545a9d, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,442] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 180 ms on bbe2d2545a9d (executor driver) (29/141)
[2021-05-15 11:37:41,442] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-15 11:37:41,473] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1886 bytes result sent to driver
[2021-05-15 11:37:41,474] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (bbe2d2545a9d, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,475] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 196 ms on bbe2d2545a9d (executor driver) (30/141)
[2021-05-15 11:37:41,476] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-15 11:37:41,527] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-15 11:37:41,529] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (bbe2d2545a9d, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,530] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-15 11:37:41,531] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 181 ms on bbe2d2545a9d (executor driver) (31/141)
[2021-05-15 11:37:41,597] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-15 11:37:41,599] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (bbe2d2545a9d, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,600] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 174 ms on bbe2d2545a9d (executor driver) (32/141)
[2021-05-15 11:37:41,602] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-15 11:37:41,622] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-15 11:37:41,639] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (bbe2d2545a9d, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,640] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 183 ms on bbe2d2545a9d (executor driver) (33/141)
[2021-05-15 11:37:41,641] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-15 11:37:41,650] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-15 11:37:41,651] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (bbe2d2545a9d, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,652] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 179 ms on bbe2d2545a9d (executor driver) (34/141)
[2021-05-15 11:37:41,654] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-15 11:37:41,716] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-15 11:37:41,718] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (bbe2d2545a9d, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,719] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 191 ms on bbe2d2545a9d (executor driver) (35/141)
[2021-05-15 11:37:41,720] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-15 11:37:41,770] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-15 11:37:41,771] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (bbe2d2545a9d, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,772] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 175 ms on bbe2d2545a9d (executor driver) (36/141)
[2021-05-15 11:37:41,773] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-15 11:37:41,800] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1886 bytes result sent to driver
[2021-05-15 11:37:41,801] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (bbe2d2545a9d, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,802] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 179 ms on bbe2d2545a9d (executor driver) (37/141)
[2021-05-15 11:37:41,802] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-15 11:37:41,832] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1886 bytes result sent to driver
[2021-05-15 11:37:41,832] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (bbe2d2545a9d, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,834] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 183 ms on bbe2d2545a9d (executor driver) (38/141)
[2021-05-15 11:37:41,835] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-15 11:37:41,894] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1886 bytes result sent to driver
[2021-05-15 11:37:41,896] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (bbe2d2545a9d, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,897] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-15 11:37:41,898] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 180 ms on bbe2d2545a9d (executor driver) (39/141)
[2021-05-15 11:37:41,946] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1886 bytes result sent to driver
[2021-05-15 11:37:41,949] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (bbe2d2545a9d, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,949] {docker.py:276} INFO - 21/05/15 14:37:41 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 179 ms on bbe2d2545a9d (executor driver) (40/141)
[2021-05-15 11:37:41,950] {docker.py:276} INFO - 21/05/15 14:37:41 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-15 11:37:41,980] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-15 11:37:41,981] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (bbe2d2545a9d, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:41,982] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 182 ms on bbe2d2545a9d (executor driver) (41/141)
[2021-05-15 11:37:41,984] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-15 11:37:42,014] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-15 11:37:42,016] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (bbe2d2545a9d, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,017] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 184 ms on bbe2d2545a9d (executor driver) (42/141)
[2021-05-15 11:37:42,018] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-15 11:37:42,074] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-15 11:37:42,075] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (bbe2d2545a9d, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,076] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 180 ms on bbe2d2545a9d (executor driver) (43/141)
[2021-05-15 11:37:42,076] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-15 11:37:42,122] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-15 11:37:42,122] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (bbe2d2545a9d, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,124] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 175 ms on bbe2d2545a9d (executor driver) (44/141)
[2021-05-15 11:37:42,125] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-15 11:37:42,210] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-15 11:37:42,212] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (bbe2d2545a9d, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,214] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-15 11:37:42,214] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 233 ms on bbe2d2545a9d (executor driver) (45/141)
[2021-05-15 11:37:42,233] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1886 bytes result sent to driver
[2021-05-15 11:37:42,234] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (bbe2d2545a9d, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,235] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 220 ms on bbe2d2545a9d (executor driver) (46/141)
[2021-05-15 11:37:42,236] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-15 11:37:42,252] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1886 bytes result sent to driver
[2021-05-15 11:37:42,253] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (bbe2d2545a9d, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,254] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 180 ms on bbe2d2545a9d (executor driver) (47/141)
[2021-05-15 11:37:42,255] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-15 11:37:42,293] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1886 bytes result sent to driver
[2021-05-15 11:37:42,294] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (bbe2d2545a9d, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,295] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 173 ms on bbe2d2545a9d (executor driver) (48/141)
[2021-05-15 11:37:42,296] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-15 11:37:42,400] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1886 bytes result sent to driver
[2021-05-15 11:37:42,402] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (bbe2d2545a9d, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,402] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 192 ms on bbe2d2545a9d (executor driver) (49/141)
[2021-05-15 11:37:42,404] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-15 11:37:42,411] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-15 11:37:42,412] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (bbe2d2545a9d, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,413] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 179 ms on bbe2d2545a9d (executor driver) (50/141)
[2021-05-15 11:37:42,413] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-15 11:37:42,464] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-15 11:37:42,464] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (bbe2d2545a9d, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,465] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 172 ms on bbe2d2545a9d (executor driver) (51/141)
[2021-05-15 11:37:42,466] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-15 11:37:42,470] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-15 11:37:42,472] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (bbe2d2545a9d, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,472] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 220 ms on bbe2d2545a9d (executor driver) (52/141)
[2021-05-15 11:37:42,473] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-15 11:37:42,579] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-15 11:37:42,581] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (bbe2d2545a9d, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,583] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-15 11:37:42,584] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 182 ms on bbe2d2545a9d (executor driver) (53/141)
[2021-05-15 11:37:42,588] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
[2021-05-15 11:37:42,589] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (bbe2d2545a9d, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,590] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 178 ms on bbe2d2545a9d (executor driver) (54/141)
[2021-05-15 11:37:42,590] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-15 11:37:42,647] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1886 bytes result sent to driver
[2021-05-15 11:37:42,649] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (bbe2d2545a9d, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,650] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 179 ms on bbe2d2545a9d (executor driver) (55/141)
[2021-05-15 11:37:42,651] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-15 11:37:42,652] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-15 11:37:42,654] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (bbe2d2545a9d, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,655] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 191 ms on bbe2d2545a9d (executor driver) (56/141)
[2021-05-15 11:37:42,656] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-15 11:37:42,770] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1886 bytes result sent to driver
[2021-05-15 11:37:42,772] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (bbe2d2545a9d, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,773] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 192 ms on bbe2d2545a9d (executor driver) (57/141)
[2021-05-15 11:37:42,774] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-15 11:37:42,780] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1886 bytes result sent to driver
[2021-05-15 11:37:42,781] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (bbe2d2545a9d, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,782] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 194 ms on bbe2d2545a9d (executor driver) (58/141)
[2021-05-15 11:37:42,783] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-15 11:37:42,825] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-15 11:37:42,826] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (bbe2d2545a9d, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,826] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 178 ms on bbe2d2545a9d (executor driver) (59/141)
[2021-05-15 11:37:42,827] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-15 11:37:42,836] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-15 11:37:42,838] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (bbe2d2545a9d, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,838] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 185 ms on bbe2d2545a9d (executor driver) (60/141)
[2021-05-15 11:37:42,839] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-15 11:37:42,954] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-15 11:37:42,957] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (bbe2d2545a9d, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,957] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 186 ms on bbe2d2545a9d (executor driver) (61/141)
[2021-05-15 11:37:42,958] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-15 11:37:42,959] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-15 11:37:42,962] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (bbe2d2545a9d, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,963] {docker.py:276} INFO - 21/05/15 14:37:42 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 181 ms on bbe2d2545a9d (executor driver) (62/141)
[2021-05-15 11:37:42,964] {docker.py:276} INFO - 21/05/15 14:37:42 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-15 11:37:42,997] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-15 11:37:42,999] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (bbe2d2545a9d, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:42,999] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-15 11:37:43,000] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 175 ms on bbe2d2545a9d (executor driver) (63/141)
[2021-05-15 11:37:43,026] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1886 bytes result sent to driver
[2021-05-15 11:37:43,028] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (bbe2d2545a9d, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,028] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 191 ms on bbe2d2545a9d (executor driver) (64/141)
[2021-05-15 11:37:43,029] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-15 11:37:43,139] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1886 bytes result sent to driver
[2021-05-15 11:37:43,141] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (bbe2d2545a9d, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,142] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1886 bytes result sent to driver
[2021-05-15 11:37:43,143] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-15 11:37:43,144] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (bbe2d2545a9d, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,145] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 189 ms on bbe2d2545a9d (executor driver) (65/141)
[2021-05-15 11:37:43,147] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
21/05/15 14:37:43 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 186 ms on bbe2d2545a9d (executor driver) (66/141)
[2021-05-15 11:37:43,189] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1886 bytes result sent to driver
[2021-05-15 11:37:43,191] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (bbe2d2545a9d, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,193] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
21/05/15 14:37:43 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 195 ms on bbe2d2545a9d (executor driver) (67/141)
[2021-05-15 11:37:43,205] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-15 11:37:43,206] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (bbe2d2545a9d, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,207] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-15 11:37:43,208] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 181 ms on bbe2d2545a9d (executor driver) (68/141)
[2021-05-15 11:37:43,325] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-15 11:37:43,327] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (bbe2d2545a9d, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,328] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 188 ms on bbe2d2545a9d (executor driver) (69/141)
[2021-05-15 11:37:43,329] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-15 11:37:43,337] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-15 11:37:43,338] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (bbe2d2545a9d, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,339] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 195 ms on bbe2d2545a9d (executor driver) (70/141)
[2021-05-15 11:37:43,339] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-15 11:37:43,362] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-15 11:37:43,364] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (bbe2d2545a9d, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,364] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 175 ms on bbe2d2545a9d (executor driver) (71/141)
[2021-05-15 11:37:43,365] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-15 11:37:43,402] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-15 11:37:43,404] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (bbe2d2545a9d, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,405] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 199 ms on bbe2d2545a9d (executor driver) (72/141)
[2021-05-15 11:37:43,406] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-15 11:37:43,507] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1886 bytes result sent to driver
[2021-05-15 11:37:43,509] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (bbe2d2545a9d, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,510] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 183 ms on bbe2d2545a9d (executor driver) (73/141)
[2021-05-15 11:37:43,510] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-15 11:37:43,540] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1886 bytes result sent to driver
[2021-05-15 11:37:43,541] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (bbe2d2545a9d, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,543] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-15 11:37:43,543] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 179 ms on bbe2d2545a9d (executor driver) (74/141)
[2021-05-15 11:37:43,546] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1886 bytes result sent to driver
[2021-05-15 11:37:43,547] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (bbe2d2545a9d, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,548] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 212 ms on bbe2d2545a9d (executor driver) (75/141)
[2021-05-15 11:37:43,549] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-15 11:37:43,585] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1886 bytes result sent to driver
[2021-05-15 11:37:43,586] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (bbe2d2545a9d, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,588] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 185 ms on bbe2d2545a9d (executor driver) (76/141)
21/05/15 14:37:43 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-15 11:37:43,689] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-15 11:37:43,691] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (bbe2d2545a9d, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,693] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
21/05/15 14:37:43 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 184 ms on bbe2d2545a9d (executor driver) (77/141)
[2021-05-15 11:37:43,716] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-15 11:37:43,717] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (bbe2d2545a9d, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,718] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 172 ms on bbe2d2545a9d (executor driver) (78/141)
[2021-05-15 11:37:43,719] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-15 11:37:43,721] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-15 11:37:43,722] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (bbe2d2545a9d, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,723] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 182 ms on bbe2d2545a9d (executor driver) (79/141)
[2021-05-15 11:37:43,724] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-15 11:37:43,762] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-15 11:37:43,763] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (bbe2d2545a9d, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,764] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 178 ms on bbe2d2545a9d (executor driver) (80/141)
[2021-05-15 11:37:43,765] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-15 11:37:43,874] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-15 11:37:43,875] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (bbe2d2545a9d, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,876] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-15 11:37:43,877] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 186 ms on bbe2d2545a9d (executor driver) (81/141)
[2021-05-15 11:37:43,893] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1886 bytes result sent to driver
[2021-05-15 11:37:43,894] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (bbe2d2545a9d, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,895] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
21/05/15 14:37:43 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 174 ms on bbe2d2545a9d (executor driver) (82/141)
[2021-05-15 11:37:43,896] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1886 bytes result sent to driver
[2021-05-15 11:37:43,899] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (bbe2d2545a9d, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,900] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 183 ms on bbe2d2545a9d (executor driver) (83/141)
[2021-05-15 11:37:43,901] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-15 11:37:43,946] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1886 bytes result sent to driver
[2021-05-15 11:37:43,948] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (bbe2d2545a9d, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:43,949] {docker.py:276} INFO - 21/05/15 14:37:43 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 186 ms on bbe2d2545a9d (executor driver) (84/141)
[2021-05-15 11:37:43,949] {docker.py:276} INFO - 21/05/15 14:37:43 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-15 11:37:44,068] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1886 bytes result sent to driver
[2021-05-15 11:37:44,071] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (bbe2d2545a9d, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,072] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 198 ms on bbe2d2545a9d (executor driver) (85/141)
21/05/15 14:37:44 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-15 11:37:44,075] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-15 11:37:44,077] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (bbe2d2545a9d, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,078] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-15 11:37:44,078] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 184 ms on bbe2d2545a9d (executor driver) (86/141)
[2021-05-15 11:37:44,079] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1886 bytes result sent to driver
[2021-05-15 11:37:44,081] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (bbe2d2545a9d, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,082] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 183 ms on bbe2d2545a9d (executor driver) (87/141)
[2021-05-15 11:37:44,083] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-15 11:37:44,131] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1843 bytes result sent to driver
[2021-05-15 11:37:44,133] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (bbe2d2545a9d, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,134] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 187 ms on bbe2d2545a9d (executor driver) (88/141)
21/05/15 14:37:44 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-15 11:37:44,253] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-15 11:37:44,256] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (bbe2d2545a9d, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,259] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-15 11:37:44,260] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-15 11:37:44,263] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (bbe2d2545a9d, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,263] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1843 bytes result sent to driver
[2021-05-15 11:37:44,264] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 188 ms on bbe2d2545a9d (executor driver) (89/141)
[2021-05-15 11:37:44,266] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 196 ms on bbe2d2545a9d (executor driver) (90/141)
[2021-05-15 11:37:44,267] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-15 11:37:44,267] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 187 ms on bbe2d2545a9d (executor driver) (91/141)
[2021-05-15 11:37:44,268] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (bbe2d2545a9d, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,270] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-15 11:37:44,312] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1886 bytes result sent to driver
[2021-05-15 11:37:44,313] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (bbe2d2545a9d, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,314] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
[2021-05-15 11:37:44,314] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 182 ms on bbe2d2545a9d (executor driver) (92/141)
[2021-05-15 11:37:44,442] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1886 bytes result sent to driver
[2021-05-15 11:37:44,443] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (bbe2d2545a9d, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,444] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 190 ms on bbe2d2545a9d (executor driver) (93/141)
[2021-05-15 11:37:44,445] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-15 11:37:44,449] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1886 bytes result sent to driver
[2021-05-15 11:37:44,450] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (bbe2d2545a9d, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,451] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 189 ms on bbe2d2545a9d (executor driver) (94/141)
[2021-05-15 11:37:44,451] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-15 11:37:44,454] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1886 bytes result sent to driver
[2021-05-15 11:37:44,455] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (bbe2d2545a9d, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,456] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 189 ms on bbe2d2545a9d (executor driver) (95/141)
[2021-05-15 11:37:44,457] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-15 11:37:44,493] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-15 11:37:44,495] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (bbe2d2545a9d, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,496] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 184 ms on bbe2d2545a9d (executor driver) (96/141)
[2021-05-15 11:37:44,497] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-15 11:37:44,623] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
21/05/15 14:37:44 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-15 11:37:44,625] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (bbe2d2545a9d, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,626] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 182 ms on bbe2d2545a9d (executor driver) (97/141)
[2021-05-15 11:37:44,627] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-15 11:37:44,628] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (bbe2d2545a9d, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,629] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 180 ms on bbe2d2545a9d (executor driver) (98/141)
[2021-05-15 11:37:44,630] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-15 11:37:44,633] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-15 11:37:44,634] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (bbe2d2545a9d, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,636] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-15 11:37:44,636] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 181 ms on bbe2d2545a9d (executor driver) (99/141)
[2021-05-15 11:37:44,673] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1886 bytes result sent to driver
[2021-05-15 11:37:44,674] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (bbe2d2545a9d, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,675] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 182 ms on bbe2d2545a9d (executor driver) (100/141)
[2021-05-15 11:37:44,676] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-15 11:37:44,803] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1886 bytes result sent to driver
[2021-05-15 11:37:44,804] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (bbe2d2545a9d, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,805] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 178 ms on bbe2d2545a9d (executor driver) (101/141)
21/05/15 14:37:44 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-15 11:37:44,808] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1886 bytes result sent to driver
[2021-05-15 11:37:44,809] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (bbe2d2545a9d, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,810] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 187 ms on bbe2d2545a9d (executor driver) (102/141)
[2021-05-15 11:37:44,811] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-15 11:37:44,817] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1886 bytes result sent to driver
[2021-05-15 11:37:44,818] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (bbe2d2545a9d, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,819] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-15 11:37:44,819] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 185 ms on bbe2d2545a9d (executor driver) (103/141)
[2021-05-15 11:37:44,859] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1843 bytes result sent to driver
[2021-05-15 11:37:44,861] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (bbe2d2545a9d, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,861] {docker.py:276} INFO - 21/05/15 14:37:44 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 187 ms on bbe2d2545a9d (executor driver) (104/141)
[2021-05-15 11:37:44,862] {docker.py:276} INFO - 21/05/15 14:37:44 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-15 11:37:44,983] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
21/05/15 14:37:45 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1843 bytes result sent to driver
[2021-05-15 11:37:44,985] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (bbe2d2545a9d, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,987] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-15 11:37:44,988] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (bbe2d2545a9d, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,989] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 180 ms on bbe2d2545a9d (executor driver) (105/141)
[2021-05-15 11:37:44,990] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 187 ms on bbe2d2545a9d (executor driver) (106/141)
[2021-05-15 11:37:44,991] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-15 11:37:44,993] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-15 11:37:44,995] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (bbe2d2545a9d, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:44,996] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 178 ms on bbe2d2545a9d (executor driver) (107/141)
[2021-05-15 11:37:44,997] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-15 11:37:45,039] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1843 bytes result sent to driver
[2021-05-15 11:37:45,040] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (bbe2d2545a9d, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,041] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 181 ms on bbe2d2545a9d (executor driver) (108/141)
[2021-05-15 11:37:45,042] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-15 11:37:45,169] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1843 bytes result sent to driver
[2021-05-15 11:37:45,170] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (bbe2d2545a9d, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,171] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1843 bytes result sent to driver
21/05/15 14:37:45 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 176 ms on bbe2d2545a9d (executor driver) (109/141)
[2021-05-15 11:37:45,171] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-15 11:37:45,172] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 185 ms on bbe2d2545a9d (executor driver) (110/141)
[2021-05-15 11:37:45,173] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-15 11:37:45,174] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (bbe2d2545a9d, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,175] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-15 11:37:45,181] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (bbe2d2545a9d, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,182] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 198 ms on bbe2d2545a9d (executor driver) (111/141)
[2021-05-15 11:37:45,182] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-15 11:37:45,220] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1886 bytes result sent to driver
[2021-05-15 11:37:45,222] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (bbe2d2545a9d, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,223] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-15 11:37:45,224] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 184 ms on bbe2d2545a9d (executor driver) (112/141)
[2021-05-15 11:37:45,359] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1886 bytes result sent to driver
[2021-05-15 11:37:45,361] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1886 bytes result sent to driver
[2021-05-15 11:37:45,362] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1886 bytes result sent to driver
[2021-05-15 11:37:45,363] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (bbe2d2545a9d, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,365] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (bbe2d2545a9d, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,367] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-15 11:37:45,367] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-15 11:37:45,368] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (bbe2d2545a9d, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,370] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-15 11:37:45,371] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 201 ms on bbe2d2545a9d (executor driver) (113/141)
[2021-05-15 11:37:45,372] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 197 ms on bbe2d2545a9d (executor driver) (114/141)
[2021-05-15 11:37:45,372] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 199 ms on bbe2d2545a9d (executor driver) (115/141)
[2021-05-15 11:37:45,399] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-15 11:37:45,400] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (bbe2d2545a9d, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,400] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-15 11:37:45,401] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 179 ms on bbe2d2545a9d (executor driver) (116/141)
[2021-05-15 11:37:45,544] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1843 bytes result sent to driver
[2021-05-15 11:37:45,545] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1843 bytes result sent to driver
[2021-05-15 11:37:45,547] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (bbe2d2545a9d, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,549] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 182 ms on bbe2d2545a9d (executor driver) (117/141)
[2021-05-15 11:37:45,551] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-15 11:37:45,553] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 188 ms on bbe2d2545a9d (executor driver) (118/141)
[2021-05-15 11:37:45,554] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (bbe2d2545a9d, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,555] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-15 11:37:45,556] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-15 11:37:45,557] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (bbe2d2545a9d, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,558] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 196 ms on bbe2d2545a9d (executor driver) (119/141)
[2021-05-15 11:37:45,559] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-15 11:37:45,574] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1886 bytes result sent to driver
[2021-05-15 11:37:45,575] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (bbe2d2545a9d, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,576] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-15 11:37:45,576] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 177 ms on bbe2d2545a9d (executor driver) (120/141)
[2021-05-15 11:37:45,731] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1886 bytes result sent to driver
[2021-05-15 11:37:45,733] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (bbe2d2545a9d, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,734] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1886 bytes result sent to driver
[2021-05-15 11:37:45,735] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 191 ms on bbe2d2545a9d (executor driver) (121/141)
[2021-05-15 11:37:45,737] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-15 11:37:45,739] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (bbe2d2545a9d, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,740] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 183 ms on bbe2d2545a9d (executor driver) (122/141)
[2021-05-15 11:37:45,741] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-15 11:37:45,742] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1886 bytes result sent to driver
[2021-05-15 11:37:45,744] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (bbe2d2545a9d, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,745] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 193 ms on bbe2d2545a9d (executor driver) (123/141)
[2021-05-15 11:37:45,746] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-15 11:37:45,750] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-15 11:37:45,751] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (bbe2d2545a9d, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,753] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 178 ms on bbe2d2545a9d (executor driver) (124/141)
[2021-05-15 11:37:45,753] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-15 11:37:45,913] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-15 11:37:45,916] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (bbe2d2545a9d, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,917] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-15 11:37:45,918] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 185 ms on bbe2d2545a9d (executor driver) (125/141)
[2021-05-15 11:37:45,920] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1843 bytes result sent to driver
[2021-05-15 11:37:45,921] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (bbe2d2545a9d, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,922] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 187 ms on bbe2d2545a9d (executor driver) (126/141)
[2021-05-15 11:37:45,924] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-15 11:37:45,924] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-15 11:37:45,926] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (bbe2d2545a9d, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,927] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
21/05/15 14:37:45 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 183 ms on bbe2d2545a9d (executor driver) (127/141)
[2021-05-15 11:37:45,930] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1843 bytes result sent to driver
[2021-05-15 11:37:45,932] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 181 ms on bbe2d2545a9d (executor driver) (128/141)
[2021-05-15 11:37:45,933] {docker.py:276} INFO - 21/05/15 14:37:45 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (bbe2d2545a9d, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:45,934] {docker.py:276} INFO - 21/05/15 14:37:45 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-15 11:37:46,102] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1886 bytes result sent to driver
21/05/15 14:37:46 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1886 bytes result sent to driver
[2021-05-15 11:37:46,103] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (bbe2d2545a9d, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,104] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-15 11:37:46,105] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1886 bytes result sent to driver
[2021-05-15 11:37:46,106] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (bbe2d2545a9d, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,106] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 186 ms on bbe2d2545a9d (executor driver) (129/141)
[2021-05-15 11:37:46,107] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
21/05/15 14:37:46 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 193 ms on bbe2d2545a9d (executor driver) (130/141)
[2021-05-15 11:37:46,108] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1886 bytes result sent to driver
[2021-05-15 11:37:46,108] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 183 ms on bbe2d2545a9d (executor driver) (131/141)
[2021-05-15 11:37:46,109] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (bbe2d2545a9d, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,111] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 178 ms on bbe2d2545a9d (executor driver) (132/141)
[2021-05-15 11:37:46,111] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-15 11:37:46,112] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (bbe2d2545a9d, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,112] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-15 11:37:46,278] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1843 bytes result sent to driver
[2021-05-15 11:37:46,279] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (bbe2d2545a9d, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,280] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 176 ms on bbe2d2545a9d (executor driver) (133/141)
[2021-05-15 11:37:46,281] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-15 11:37:46,285] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1843 bytes result sent to driver
[2021-05-15 11:37:46,286] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (bbe2d2545a9d, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,287] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 176 ms on bbe2d2545a9d (executor driver) (134/141)
[2021-05-15 11:37:46,289] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-15 11:37:46,291] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
[2021-05-15 11:37:46,291] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1843 bytes result sent to driver
[2021-05-15 11:37:46,293] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (bbe2d2545a9d, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,294] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 185 ms on bbe2d2545a9d (executor driver) (135/141)
[2021-05-15 11:37:46,295] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-15 11:37:46,295] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (bbe2d2545a9d, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,296] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 194 ms on bbe2d2545a9d (executor driver) (136/141)
[2021-05-15 11:37:46,297] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-15 11:37:46,457] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1843 bytes result sent to driver
[2021-05-15 11:37:46,459] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (bbe2d2545a9d, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,460] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 181 ms on bbe2d2545a9d (executor driver) (137/141)
21/05/15 14:37:46 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-15 11:37:46,472] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1886 bytes result sent to driver
[2021-05-15 11:37:46,474] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1886 bytes result sent to driver
[2021-05-15 11:37:46,475] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1886 bytes result sent to driver
[2021-05-15 11:37:46,479] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 187 ms on bbe2d2545a9d (executor driver) (138/141)
[2021-05-15 11:37:46,480] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 184 ms on bbe2d2545a9d (executor driver) (139/141)
[2021-05-15 11:37:46,481] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 196 ms on bbe2d2545a9d (executor driver) (140/141)
[2021-05-15 11:37:46,651] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1886 bytes result sent to driver
[2021-05-15 11:37:46,652] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 194 ms on bbe2d2545a9d (executor driver) (141/141)
[2021-05-15 11:37:46,656] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-15 11:37:46,657] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 7.682 s
[2021-05-15 11:37:46,664] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 11:37:46,665] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-15 11:37:46,670] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 7.789178 s
[2021-05-15 11:37:46,706] {docker.py:276} INFO - 21/05/15 14:37:46 INFO InMemoryFileIndex: It took 8375 ms to list leaf files for 141 paths.
[2021-05-15 11:37:46,821] {docker.py:276} INFO - 21/05/15 14:37:46 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621003005_to_1621004805.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621004805_to_1621006605.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621006605_to_1621008405.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621008405_to_1621010205.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621010205_to_1621012005.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621012005_to_1621013805.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621013805_to_1621015605.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621015605_to_1621017405.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621017405_to_1621019205.csv, s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621019205_to_1621021005.csv.
[2021-05-15 11:37:46,861] {docker.py:276} INFO - 21/05/15 14:37:46 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:46,864] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/15 14:37:46 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/15 14:37:46 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 11:37:46,864] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Missing parents: List()
[2021-05-15 11:37:46,865] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:37:46,879] {docker.py:276} INFO - 21/05/15 14:37:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-15 11:37:46,890] {docker.py:276} INFO - 21/05/15 14:37:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-15 11:37:46,891] {docker.py:276} INFO - 21/05/15 14:37:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on bbe2d2545a9d:38027 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-15 11:37:46,892] {docker.py:276} INFO - 21/05/15 14:37:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:37:46,894] {docker.py:276} INFO - 21/05/15 14:37:46 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 11:37:46,894] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-15 11:37:46,896] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (bbe2d2545a9d, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,897] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (bbe2d2545a9d, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,898] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (bbe2d2545a9d, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,899] {docker.py:276} INFO - 21/05/15 14:37:46 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (bbe2d2545a9d, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:46,900] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-15 11:37:46,900] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-15 11:37:46,901] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-15 11:37:46,901] {docker.py:276} INFO - 21/05/15 14:37:46 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-15 11:37:46,969] {docker.py:276} INFO - 21/05/15 14:37:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on bbe2d2545a9d:38027 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 11:37:47,073] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-15 11:37:47,074] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (bbe2d2545a9d, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 14:37:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
[2021-05-15 11:37:47,075] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 177 ms on bbe2d2545a9d (executor driver) (1/141)
21/05/15 14:37:47 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-15 11:37:47,076] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 180 ms on bbe2d2545a9d (executor driver) (2/141)
[2021-05-15 11:37:47,078] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (bbe2d2545a9d, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,079] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-15 11:37:47,080] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-15 11:37:47,081] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (bbe2d2545a9d, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,082] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-15 11:37:47,082] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 185 ms on bbe2d2545a9d (executor driver) (3/141)
[2021-05-15 11:37:47,096] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-15 11:37:47,097] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (bbe2d2545a9d, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,098] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 202 ms on bbe2d2545a9d (executor driver) (4/141)
[2021-05-15 11:37:47,099] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-15 11:37:47,252] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
21/05/15 14:37:47 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-15 11:37:47,253] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (bbe2d2545a9d, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,254] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
21/05/15 14:37:47 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 177 ms on bbe2d2545a9d (executor driver) (5/141)
[2021-05-15 11:37:47,255] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (bbe2d2545a9d, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,256] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 176 ms on bbe2d2545a9d (executor driver) (6/141)
[2021-05-15 11:37:47,257] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
[2021-05-15 11:37:47,266] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1886 bytes result sent to driver
[2021-05-15 11:37:47,269] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (bbe2d2545a9d, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,270] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 174 ms on bbe2d2545a9d (executor driver) (7/141)
[2021-05-15 11:37:47,271] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-15 11:37:47,296] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1886 bytes result sent to driver
[2021-05-15 11:37:47,297] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (bbe2d2545a9d, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,298] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 224 ms on bbe2d2545a9d (executor driver) (8/141)
21/05/15 14:37:47 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-15 11:37:47,428] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1886 bytes result sent to driver
[2021-05-15 11:37:47,430] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (bbe2d2545a9d, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,431] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 178 ms on bbe2d2545a9d (executor driver) (9/141)
[2021-05-15 11:37:47,431] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-15 11:37:47,443] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-15 11:37:47,444] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (bbe2d2545a9d, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,445] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 177 ms on bbe2d2545a9d (executor driver) (10/141)
[2021-05-15 11:37:47,445] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-15 11:37:47,452] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1886 bytes result sent to driver
[2021-05-15 11:37:47,453] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (bbe2d2545a9d, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,454] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 199 ms on bbe2d2545a9d (executor driver) (11/141)
[2021-05-15 11:37:47,454] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-15 11:37:47,471] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-15 11:37:47,471] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 175 ms on bbe2d2545a9d (executor driver) (12/141)
[2021-05-15 11:37:47,473] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (bbe2d2545a9d, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,474] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-15 11:37:47,602] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1843 bytes result sent to driver
[2021-05-15 11:37:47,604] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (bbe2d2545a9d, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,605] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
21/05/15 14:37:47 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 176 ms on bbe2d2545a9d (executor driver) (13/141)
[2021-05-15 11:37:47,623] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1843 bytes result sent to driver
[2021-05-15 11:37:47,625] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (bbe2d2545a9d, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,627] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 182 ms on bbe2d2545a9d (executor driver) (14/141)
[2021-05-15 11:37:47,628] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-15 11:37:47,629] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1843 bytes result sent to driver
[2021-05-15 11:37:47,631] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (bbe2d2545a9d, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,632] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-15 11:37:47,632] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 179 ms on bbe2d2545a9d (executor driver) (15/141)
[2021-05-15 11:37:47,652] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1886 bytes result sent to driver
[2021-05-15 11:37:47,653] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (bbe2d2545a9d, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,654] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-15 11:37:47,654] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 181 ms on bbe2d2545a9d (executor driver) (16/141)
[2021-05-15 11:37:47,773] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1886 bytes result sent to driver
[2021-05-15 11:37:47,775] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (bbe2d2545a9d, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,776] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 173 ms on bbe2d2545a9d (executor driver) (17/141)
[2021-05-15 11:37:47,777] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-15 11:37:47,810] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1886 bytes result sent to driver
[2021-05-15 11:37:47,812] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (bbe2d2545a9d, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,813] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-15 11:37:47,814] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 189 ms on bbe2d2545a9d (executor driver) (18/141)
[2021-05-15 11:37:47,815] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1886 bytes result sent to driver
[2021-05-15 11:37:47,816] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (bbe2d2545a9d, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,818] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 188 ms on bbe2d2545a9d (executor driver) (19/141)
21/05/15 14:37:47 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-15 11:37:47,822] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-15 11:37:47,824] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (bbe2d2545a9d, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,824] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-15 11:37:47,825] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 172 ms on bbe2d2545a9d (executor driver) (20/141)
[2021-05-15 11:37:47,946] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-15 11:37:47,948] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (bbe2d2545a9d, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,950] {docker.py:276} INFO - 21/05/15 14:37:47 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-15 11:37:47,951] {docker.py:276} INFO - 21/05/15 14:37:47 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 175 ms on bbe2d2545a9d (executor driver) (21/141)
[2021-05-15 11:37:47,991] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-15 11:37:47,992] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (bbe2d2545a9d, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:47,994] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 182 ms on bbe2d2545a9d (executor driver) (22/141)
[2021-05-15 11:37:47,995] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-15 11:37:47,996] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
[2021-05-15 11:37:47,997] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1843 bytes result sent to driver
[2021-05-15 11:37:47,998] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (bbe2d2545a9d, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,000] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 184 ms on bbe2d2545a9d (executor driver) (23/141)
[2021-05-15 11:37:48,000] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-15 11:37:48,001] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 178 ms on bbe2d2545a9d (executor driver) (24/141)
[2021-05-15 11:37:48,003] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (bbe2d2545a9d, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,005] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-15 11:37:48,121] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1929 bytes result sent to driver
[2021-05-15 11:37:48,124] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (bbe2d2545a9d, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,125] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-15 11:37:48,127] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 178 ms on bbe2d2545a9d (executor driver) (25/141)
[2021-05-15 11:37:48,173] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1886 bytes result sent to driver
[2021-05-15 11:37:48,175] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (bbe2d2545a9d, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,176] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 180 ms on bbe2d2545a9d (executor driver) (26/141)
[2021-05-15 11:37:48,177] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-15 11:37:48,179] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1886 bytes result sent to driver
[2021-05-15 11:37:48,181] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (bbe2d2545a9d, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,181] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 190 ms on bbe2d2545a9d (executor driver) (27/141)
[2021-05-15 11:37:48,182] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-15 11:37:48,183] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1886 bytes result sent to driver
[2021-05-15 11:37:48,184] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (bbe2d2545a9d, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,185] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 183 ms on bbe2d2545a9d (executor driver) (28/141)
[2021-05-15 11:37:48,186] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
[2021-05-15 11:37:48,294] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-15 11:37:48,297] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (bbe2d2545a9d, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,299] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 176 ms on bbe2d2545a9d (executor driver) (29/141)
21/05/15 14:37:48 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-15 11:37:48,357] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
[2021-05-15 11:37:48,359] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-15 11:37:48,360] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (bbe2d2545a9d, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,362] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (bbe2d2545a9d, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,363] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 179 ms on bbe2d2545a9d (executor driver) (30/141)
[2021-05-15 11:37:48,364] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 190 ms on bbe2d2545a9d (executor driver) (31/141)
[2021-05-15 11:37:48,365] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-15 11:37:48,366] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-15 11:37:48,385] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-15 11:37:48,386] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (bbe2d2545a9d, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,387] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 207 ms on bbe2d2545a9d (executor driver) (32/141)
[2021-05-15 11:37:48,387] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-15 11:37:48,468] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-15 11:37:48,471] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (bbe2d2545a9d, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,472] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 176 ms on bbe2d2545a9d (executor driver) (33/141)
[2021-05-15 11:37:48,473] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
[2021-05-15 11:37:48,539] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1886 bytes result sent to driver
[2021-05-15 11:37:48,541] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (bbe2d2545a9d, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 14:37:48 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1886 bytes result sent to driver
[2021-05-15 11:37:48,542] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 183 ms on bbe2d2545a9d (executor driver) (34/141)
[2021-05-15 11:37:48,544] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 182 ms on bbe2d2545a9d (executor driver) (35/141)
[2021-05-15 11:37:48,545] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
[2021-05-15 11:37:48,545] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (bbe2d2545a9d, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,549] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-15 11:37:48,563] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1886 bytes result sent to driver
[2021-05-15 11:37:48,564] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (bbe2d2545a9d, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,565] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
21/05/15 14:37:48 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 180 ms on bbe2d2545a9d (executor driver) (36/141)
[2021-05-15 11:37:48,654] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1886 bytes result sent to driver
[2021-05-15 11:37:48,654] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (bbe2d2545a9d, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,655] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 179 ms on bbe2d2545a9d (executor driver) (37/141)
[2021-05-15 11:37:48,655] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-15 11:37:48,721] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1843 bytes result sent to driver
[2021-05-15 11:37:48,722] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (bbe2d2545a9d, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,723] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
[2021-05-15 11:37:48,723] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 178 ms on bbe2d2545a9d (executor driver) (38/141)
[2021-05-15 11:37:48,724] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1886 bytes result sent to driver
[2021-05-15 11:37:48,726] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (bbe2d2545a9d, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,727] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 186 ms on bbe2d2545a9d (executor driver) (39/141)
[2021-05-15 11:37:48,728] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
[2021-05-15 11:37:48,738] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1843 bytes result sent to driver
[2021-05-15 11:37:48,740] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (bbe2d2545a9d, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,740] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-15 11:37:48,741] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 177 ms on bbe2d2545a9d (executor driver) (40/141)
[2021-05-15 11:37:48,846] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1843 bytes result sent to driver
[2021-05-15 11:37:48,847] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (bbe2d2545a9d, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,849] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-15 11:37:48,849] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 202 ms on bbe2d2545a9d (executor driver) (41/141)
[2021-05-15 11:37:48,902] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1843 bytes result sent to driver
[2021-05-15 11:37:48,903] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1843 bytes result sent to driver
[2021-05-15 11:37:48,905] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (bbe2d2545a9d, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,907] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 185 ms on bbe2d2545a9d (executor driver) (42/141)
[2021-05-15 11:37:48,908] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-15 11:37:48,909] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (bbe2d2545a9d, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,910] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 184 ms on bbe2d2545a9d (executor driver) (43/141)
21/05/15 14:37:48 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-15 11:37:48,912] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1843 bytes result sent to driver
[2021-05-15 11:37:48,913] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (bbe2d2545a9d, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:48,921] {docker.py:276} INFO - 21/05/15 14:37:48 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-15 11:37:48,922] {docker.py:276} INFO - 21/05/15 14:37:48 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 182 ms on bbe2d2545a9d (executor driver) (44/141)
[2021-05-15 11:37:49,017] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1886 bytes result sent to driver
[2021-05-15 11:37:49,019] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (bbe2d2545a9d, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,021] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 174 ms on bbe2d2545a9d (executor driver) (45/141)
[2021-05-15 11:37:49,022] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
[2021-05-15 11:37:49,093] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1886 bytes result sent to driver
[2021-05-15 11:37:49,095] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (bbe2d2545a9d, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,097] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
21/05/15 14:37:49 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 188 ms on bbe2d2545a9d (executor driver) (46/141)
[2021-05-15 11:37:49,101] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1843 bytes result sent to driver
[2021-05-15 11:37:49,102] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (bbe2d2545a9d, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,103] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 190 ms on bbe2d2545a9d (executor driver) (47/141)
[2021-05-15 11:37:49,104] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-15 11:37:49,106] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1886 bytes result sent to driver
[2021-05-15 11:37:49,107] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (bbe2d2545a9d, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,107] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 204 ms on bbe2d2545a9d (executor driver) (48/141)
[2021-05-15 11:37:49,108] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
[2021-05-15 11:37:49,189] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-15 11:37:49,190] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (bbe2d2545a9d, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,191] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-15 11:37:49,192] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 173 ms on bbe2d2545a9d (executor driver) (49/141)
[2021-05-15 11:37:49,281] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-15 11:37:49,282] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-15 11:37:49,283] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (bbe2d2545a9d, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,284] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (bbe2d2545a9d, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,286] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-15 11:37:49,287] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 179 ms on bbe2d2545a9d (executor driver) (50/141)
[2021-05-15 11:37:49,287] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-15 11:37:49,289] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 194 ms on bbe2d2545a9d (executor driver) (51/141)
[2021-05-15 11:37:49,290] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-15 11:37:49,290] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 188 ms on bbe2d2545a9d (executor driver) (52/141)
[2021-05-15 11:37:49,291] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (bbe2d2545a9d, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,293] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-15 11:37:49,360] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1886 bytes result sent to driver
[2021-05-15 11:37:49,361] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (bbe2d2545a9d, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,361] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-15 11:37:49,362] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 172 ms on bbe2d2545a9d (executor driver) (53/141)
[2021-05-15 11:37:49,462] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1886 bytes result sent to driver
[2021-05-15 11:37:49,463] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (bbe2d2545a9d, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,465] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 181 ms on bbe2d2545a9d (executor driver) (54/141)
[2021-05-15 11:37:49,466] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-15 11:37:49,475] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1886 bytes result sent to driver
[2021-05-15 11:37:49,476] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (bbe2d2545a9d, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 14:37:49 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1886 bytes result sent to driver
[2021-05-15 11:37:49,478] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-15 11:37:49,478] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 195 ms on bbe2d2545a9d (executor driver) (55/141)
[2021-05-15 11:37:49,479] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (bbe2d2545a9d, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,479] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 189 ms on bbe2d2545a9d (executor driver) (56/141)
[2021-05-15 11:37:49,480] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-15 11:37:49,532] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1843 bytes result sent to driver
[2021-05-15 11:37:49,534] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (bbe2d2545a9d, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,535] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 174 ms on bbe2d2545a9d (executor driver) (57/141)
[2021-05-15 11:37:49,535] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-15 11:37:49,641] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1843 bytes result sent to driver
[2021-05-15 11:37:49,644] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (bbe2d2545a9d, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,644] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 181 ms on bbe2d2545a9d (executor driver) (58/141)
21/05/15 14:37:49 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
[2021-05-15 11:37:49,651] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
[2021-05-15 11:37:49,652] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (bbe2d2545a9d, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,653] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 175 ms on bbe2d2545a9d (executor driver) (59/141)
[2021-05-15 11:37:49,654] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-15 11:37:49,655] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1843 bytes result sent to driver
[2021-05-15 11:37:49,657] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (bbe2d2545a9d, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,658] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 183 ms on bbe2d2545a9d (executor driver) (60/141)
[2021-05-15 11:37:49,659] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-15 11:37:49,710] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-15 11:37:49,711] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (bbe2d2545a9d, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,714] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 181 ms on bbe2d2545a9d (executor driver) (61/141)
[2021-05-15 11:37:49,714] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-15 11:37:49,831] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1886 bytes result sent to driver
[2021-05-15 11:37:49,832] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (bbe2d2545a9d, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,833] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 191 ms on bbe2d2545a9d (executor driver) (62/141)
[2021-05-15 11:37:49,834] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-15 11:37:49,835] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1886 bytes result sent to driver
[2021-05-15 11:37:49,837] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 179 ms on bbe2d2545a9d (executor driver) (63/141)
[2021-05-15 11:37:49,838] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (bbe2d2545a9d, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,840] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
[2021-05-15 11:37:49,865] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1886 bytes result sent to driver
[2021-05-15 11:37:49,866] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (bbe2d2545a9d, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,867] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 181 ms on bbe2d2545a9d (executor driver) (64/141)
[2021-05-15 11:37:49,867] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-15 11:37:49,889] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1886 bytes result sent to driver
[2021-05-15 11:37:49,890] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (bbe2d2545a9d, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:49,891] {docker.py:276} INFO - 21/05/15 14:37:49 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 145 ms on bbe2d2545a9d (executor driver) (65/141)
[2021-05-15 11:37:49,892] {docker.py:276} INFO - 21/05/15 14:37:49 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-15 11:37:50,011] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1843 bytes result sent to driver
[2021-05-15 11:37:50,013] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (bbe2d2545a9d, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,014] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 148 ms on bbe2d2545a9d (executor driver) (66/141)
[2021-05-15 11:37:50,016] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
[2021-05-15 11:37:50,017] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-15 11:37:50,017] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (bbe2d2545a9d, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,018] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-15 11:37:50,023] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 146 ms on bbe2d2545a9d (executor driver) (67/141)
[2021-05-15 11:37:50,040] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-15 11:37:50,040] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (bbe2d2545a9d, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,041] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-15 11:37:50,041] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 175 ms on bbe2d2545a9d (executor driver) (68/141)
[2021-05-15 11:37:50,056] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-15 11:37:50,057] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (bbe2d2545a9d, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,058] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 168 ms on bbe2d2545a9d (executor driver) (69/141)
[2021-05-15 11:37:50,058] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-15 11:37:50,192] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
[2021-05-15 11:37:50,194] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (bbe2d2545a9d, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,195] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-15 11:37:50,196] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 182 ms on bbe2d2545a9d (executor driver) (70/141)
[2021-05-15 11:37:50,197] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (bbe2d2545a9d, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,198] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-15 11:37:50,199] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
21/05/15 14:37:50 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 182 ms on bbe2d2545a9d (executor driver) (71/141)
[2021-05-15 11:37:50,214] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1886 bytes result sent to driver
[2021-05-15 11:37:50,215] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (bbe2d2545a9d, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,215] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 175 ms on bbe2d2545a9d (executor driver) (72/141)
[2021-05-15 11:37:50,216] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-15 11:37:50,223] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1886 bytes result sent to driver
[2021-05-15 11:37:50,224] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (bbe2d2545a9d, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,224] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 168 ms on bbe2d2545a9d (executor driver) (73/141)
[2021-05-15 11:37:50,225] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
[2021-05-15 11:37:50,380] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1886 bytes result sent to driver
[2021-05-15 11:37:50,382] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (bbe2d2545a9d, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,383] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1886 bytes result sent to driver
[2021-05-15 11:37:50,384] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-15 11:37:50,385] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (bbe2d2545a9d, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,386] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 189 ms on bbe2d2545a9d (executor driver) (74/141)
[2021-05-15 11:37:50,387] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 193 ms on bbe2d2545a9d (executor driver) (75/141)
[2021-05-15 11:37:50,387] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-15 11:37:50,389] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1886 bytes result sent to driver
[2021-05-15 11:37:50,390] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (bbe2d2545a9d, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,391] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 177 ms on bbe2d2545a9d (executor driver) (76/141)
[2021-05-15 11:37:50,392] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
[2021-05-15 11:37:50,394] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1843 bytes result sent to driver
[2021-05-15 11:37:50,395] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (bbe2d2545a9d, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,395] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 172 ms on bbe2d2545a9d (executor driver) (77/141)
[2021-05-15 11:37:50,396] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-15 11:37:50,555] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1843 bytes result sent to driver
[2021-05-15 11:37:50,556] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (bbe2d2545a9d, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,557] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 176 ms on bbe2d2545a9d (executor driver) (78/141)
[2021-05-15 11:37:50,558] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-15 11:37:50,565] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
[2021-05-15 11:37:50,567] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (bbe2d2545a9d, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,567] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
21/05/15 14:37:50 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 183 ms on bbe2d2545a9d (executor driver) (79/141)
[2021-05-15 11:37:50,569] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-15 11:37:50,569] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (bbe2d2545a9d, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,570] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
21/05/15 14:37:50 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 176 ms on bbe2d2545a9d (executor driver) (80/141)
[2021-05-15 11:37:50,572] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1843 bytes result sent to driver
[2021-05-15 11:37:50,581] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (bbe2d2545a9d, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,582] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 192 ms on bbe2d2545a9d (executor driver) (81/141)
[2021-05-15 11:37:50,583] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-15 11:37:50,723] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1886 bytes result sent to driver
[2021-05-15 11:37:50,724] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (bbe2d2545a9d, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,725] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 169 ms on bbe2d2545a9d (executor driver) (82/141)
[2021-05-15 11:37:50,726] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-15 11:37:50,749] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1886 bytes result sent to driver
[2021-05-15 11:37:50,750] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (bbe2d2545a9d, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,751] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 185 ms on bbe2d2545a9d (executor driver) (83/141)
[2021-05-15 11:37:50,751] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-15 11:37:50,753] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1886 bytes result sent to driver
[2021-05-15 11:37:50,754] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (bbe2d2545a9d, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,755] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 186 ms on bbe2d2545a9d (executor driver) (84/141)
[2021-05-15 11:37:50,756] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-15 11:37:50,765] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
[2021-05-15 11:37:50,766] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (bbe2d2545a9d, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,766] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-15 11:37:50,767] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 185 ms on bbe2d2545a9d (executor driver) (85/141)
[2021-05-15 11:37:50,901] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
[2021-05-15 11:37:50,903] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (bbe2d2545a9d, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,904] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 180 ms on bbe2d2545a9d (executor driver) (86/141)
[2021-05-15 11:37:50,905] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-15 11:37:50,921] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-15 11:37:50,923] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (bbe2d2545a9d, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,924] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 174 ms on bbe2d2545a9d (executor driver) (87/141)
[2021-05-15 11:37:50,924] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-15 11:37:50,932] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-15 11:37:50,933] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (bbe2d2545a9d, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,934] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 180 ms on bbe2d2545a9d (executor driver) (88/141)
[2021-05-15 11:37:50,934] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-15 11:37:50,942] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-15 11:37:50,942] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (bbe2d2545a9d, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:50,943] {docker.py:276} INFO - 21/05/15 14:37:50 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 178 ms on bbe2d2545a9d (executor driver) (89/141)
[2021-05-15 11:37:50,944] {docker.py:276} INFO - 21/05/15 14:37:50 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
[2021-05-15 11:37:51,078] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1886 bytes result sent to driver
[2021-05-15 11:37:51,080] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (bbe2d2545a9d, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,081] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 179 ms on bbe2d2545a9d (executor driver) (90/141)
[2021-05-15 11:37:51,082] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-15 11:37:51,096] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1886 bytes result sent to driver
[2021-05-15 11:37:51,097] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (bbe2d2545a9d, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,098] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-15 11:37:51,099] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 176 ms on bbe2d2545a9d (executor driver) (91/141)
[2021-05-15 11:37:51,106] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1886 bytes result sent to driver
[2021-05-15 11:37:51,107] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (bbe2d2545a9d, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,107] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 175 ms on bbe2d2545a9d (executor driver) (92/141)
[2021-05-15 11:37:51,108] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-15 11:37:51,122] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1886 bytes result sent to driver
[2021-05-15 11:37:51,124] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (bbe2d2545a9d, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,124] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 182 ms on bbe2d2545a9d (executor driver) (93/141)
[2021-05-15 11:37:51,125] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-15 11:37:51,256] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1843 bytes result sent to driver
[2021-05-15 11:37:51,258] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (bbe2d2545a9d, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,259] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 179 ms on bbe2d2545a9d (executor driver) (94/141)
[2021-05-15 11:37:51,260] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-15 11:37:51,272] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1843 bytes result sent to driver
[2021-05-15 11:37:51,273] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (bbe2d2545a9d, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,274] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
21/05/15 14:37:51 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 178 ms on bbe2d2545a9d (executor driver) (95/141)
[2021-05-15 11:37:51,279] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1843 bytes result sent to driver
[2021-05-15 11:37:51,279] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (bbe2d2545a9d, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,281] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
[2021-05-15 11:37:51,281] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 173 ms on bbe2d2545a9d (executor driver) (96/141)
[2021-05-15 11:37:51,296] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1843 bytes result sent to driver
[2021-05-15 11:37:51,297] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (bbe2d2545a9d, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,299] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 176 ms on bbe2d2545a9d (executor driver) (97/141)
[2021-05-15 11:37:51,300] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-15 11:37:51,434] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-15 11:37:51,436] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (bbe2d2545a9d, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,437] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 180 ms on bbe2d2545a9d (executor driver) (98/141)
[2021-05-15 11:37:51,438] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
[2021-05-15 11:37:51,450] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1886 bytes result sent to driver
[2021-05-15 11:37:51,451] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (bbe2d2545a9d, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,452] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1886 bytes result sent to driver
[2021-05-15 11:37:51,452] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 180 ms on bbe2d2545a9d (executor driver) (99/141)
[2021-05-15 11:37:51,453] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-15 11:37:51,454] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (bbe2d2545a9d, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,454] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-15 11:37:51,455] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 175 ms on bbe2d2545a9d (executor driver) (100/141)
[2021-05-15 11:37:51,617] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1886 bytes result sent to driver
[2021-05-15 11:37:51,618] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
[2021-05-15 11:37:51,619] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (bbe2d2545a9d, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,620] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-15 11:37:51,621] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (bbe2d2545a9d, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,621] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 186 ms on bbe2d2545a9d (executor driver) (101/141)
[2021-05-15 11:37:51,621] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 171 ms on bbe2d2545a9d (executor driver) (102/141)
[2021-05-15 11:37:51,622] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-15 11:37:51,625] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1843 bytes result sent to driver
[2021-05-15 11:37:51,626] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (bbe2d2545a9d, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,627] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 175 ms on bbe2d2545a9d (executor driver) (103/141)
[2021-05-15 11:37:51,628] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-15 11:37:51,792] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1843 bytes result sent to driver
[2021-05-15 11:37:51,794] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (bbe2d2545a9d, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,795] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 169 ms on bbe2d2545a9d (executor driver) (104/141)
[2021-05-15 11:37:51,796] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
[2021-05-15 11:37:51,798] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1843 bytes result sent to driver
[2021-05-15 11:37:51,799] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (bbe2d2545a9d, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,800] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1843 bytes result sent to driver
[2021-05-15 11:37:51,801] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
21/05/15 14:37:51 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (bbe2d2545a9d, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,802] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 183 ms on bbe2d2545a9d (executor driver) (105/141)
[2021-05-15 11:37:51,803] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-15 11:37:51,803] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 183 ms on bbe2d2545a9d (executor driver) (106/141)
[2021-05-15 11:37:51,843] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1886 bytes result sent to driver
[2021-05-15 11:37:51,844] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (bbe2d2545a9d, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,845] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 549 ms on bbe2d2545a9d (executor driver) (107/141)
[2021-05-15 11:37:51,846] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
[2021-05-15 11:37:51,968] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1843 bytes result sent to driver
[2021-05-15 11:37:51,970] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (bbe2d2545a9d, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,971] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-15 11:37:51,972] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 171 ms on bbe2d2545a9d (executor driver) (108/141)
[2021-05-15 11:37:51,981] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1886 bytes result sent to driver
[2021-05-15 11:37:51,982] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (bbe2d2545a9d, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,983] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 190 ms on bbe2d2545a9d (executor driver) (109/141)
[2021-05-15 11:37:51,984] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1886 bytes result sent to driver
[2021-05-15 11:37:51,985] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (bbe2d2545a9d, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:51,986] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
[2021-05-15 11:37:51,986] {docker.py:276} INFO - 21/05/15 14:37:51 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 187 ms on bbe2d2545a9d (executor driver) (110/141)
[2021-05-15 11:37:51,987] {docker.py:276} INFO - 21/05/15 14:37:51 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-15 11:37:52,016] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1886 bytes result sent to driver
[2021-05-15 11:37:52,017] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (bbe2d2545a9d, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,018] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 175 ms on bbe2d2545a9d (executor driver) (111/141)
21/05/15 14:37:52 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-15 11:37:52,152] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1886 bytes result sent to driver
[2021-05-15 11:37:52,153] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (bbe2d2545a9d, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,154] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 184 ms on bbe2d2545a9d (executor driver) (112/141)
[2021-05-15 11:37:52,154] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-15 11:37:52,156] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-15 11:37:52,157] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (bbe2d2545a9d, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,158] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 176 ms on bbe2d2545a9d (executor driver) (113/141)
[2021-05-15 11:37:52,159] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-15 11:37:52,161] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1843 bytes result sent to driver
[2021-05-15 11:37:52,162] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (bbe2d2545a9d, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,162] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-15 11:37:52,163] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 178 ms on bbe2d2545a9d (executor driver) (114/141)
[2021-05-15 11:37:52,194] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1843 bytes result sent to driver
[2021-05-15 11:37:52,195] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (bbe2d2545a9d, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,196] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 180 ms on bbe2d2545a9d (executor driver) (115/141)
[2021-05-15 11:37:52,197] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-15 11:37:52,327] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-15 11:37:52,329] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (bbe2d2545a9d, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,331] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-15 11:37:52,332] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 175 ms on bbe2d2545a9d (executor driver) (116/141)
[2021-05-15 11:37:52,333] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-15 11:37:52,334] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-15 11:37:52,335] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (bbe2d2545a9d, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,336] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 184 ms on bbe2d2545a9d (executor driver) (117/141)
[2021-05-15 11:37:52,337] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-15 11:37:52,338] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (bbe2d2545a9d, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,338] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 177 ms on bbe2d2545a9d (executor driver) (118/141)
[2021-05-15 11:37:52,339] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-15 11:37:52,381] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1886 bytes result sent to driver
[2021-05-15 11:37:52,382] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (bbe2d2545a9d, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,382] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-15 11:37:52,383] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 187 ms on bbe2d2545a9d (executor driver) (119/141)
[2021-05-15 11:37:52,509] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1886 bytes result sent to driver
[2021-05-15 11:37:52,512] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (bbe2d2545a9d, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,513] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1886 bytes result sent to driver
[2021-05-15 11:37:52,513] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 176 ms on bbe2d2545a9d (executor driver) (120/141)
[2021-05-15 11:37:52,515] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (bbe2d2545a9d, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,516] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-15 11:37:52,517] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
[2021-05-15 11:37:52,518] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 182 ms on bbe2d2545a9d (executor driver) (121/141)
[2021-05-15 11:37:52,519] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1886 bytes result sent to driver
[2021-05-15 11:37:52,520] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (bbe2d2545a9d, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,520] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 192 ms on bbe2d2545a9d (executor driver) (122/141)
[2021-05-15 11:37:52,520] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-15 11:37:52,554] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
[2021-05-15 11:37:52,555] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (bbe2d2545a9d, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,556] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-15 11:37:52,557] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 176 ms on bbe2d2545a9d (executor driver) (123/141)
[2021-05-15 11:37:52,689] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1843 bytes result sent to driver
[2021-05-15 11:37:52,690] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (bbe2d2545a9d, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,691] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-15 11:37:52,692] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 172 ms on bbe2d2545a9d (executor driver) (124/141)
[2021-05-15 11:37:52,694] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1843 bytes result sent to driver
[2021-05-15 11:37:52,695] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (bbe2d2545a9d, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,695] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-15 11:37:52,696] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 182 ms on bbe2d2545a9d (executor driver) (125/141)
[2021-05-15 11:37:52,698] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-15 11:37:52,699] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (bbe2d2545a9d, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,701] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 190 ms on bbe2d2545a9d (executor driver) (126/141)
[2021-05-15 11:37:52,702] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-15 11:37:52,759] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1886 bytes result sent to driver
[2021-05-15 11:37:52,760] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (bbe2d2545a9d, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,760] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-15 11:37:52,761] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 207 ms on bbe2d2545a9d (executor driver) (127/141)
[2021-05-15 11:37:52,868] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1886 bytes result sent to driver
[2021-05-15 11:37:52,869] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (bbe2d2545a9d, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,870] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 181 ms on bbe2d2545a9d (executor driver) (128/141)
21/05/15 14:37:52 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-15 11:37:52,877] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1886 bytes result sent to driver
[2021-05-15 11:37:52,878] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (bbe2d2545a9d, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,878] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-15 11:37:52,879] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 180 ms on bbe2d2545a9d (executor driver) (129/141)
[2021-05-15 11:37:52,880] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1886 bytes result sent to driver
[2021-05-15 11:37:52,880] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (bbe2d2545a9d, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,881] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-15 11:37:52,881] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 187 ms on bbe2d2545a9d (executor driver) (130/141)
[2021-05-15 11:37:52,946] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-15 11:37:52,948] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (bbe2d2545a9d, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:52,949] {docker.py:276} INFO - 21/05/15 14:37:52 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 188 ms on bbe2d2545a9d (executor driver) (131/141)
[2021-05-15 11:37:52,949] {docker.py:276} INFO - 21/05/15 14:37:52 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-15 11:37:53,043] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1843 bytes result sent to driver
[2021-05-15 11:37:53,045] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (bbe2d2545a9d, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,046] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1843 bytes result sent to driver
[2021-05-15 11:37:53,048] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-15 11:37:53,049] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (bbe2d2545a9d, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,050] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 170 ms on bbe2d2545a9d (executor driver) (132/141)
[2021-05-15 11:37:53,051] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
21/05/15 14:37:53 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1843 bytes result sent to driver
[2021-05-15 11:37:53,052] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 183 ms on bbe2d2545a9d (executor driver) (133/141)
[2021-05-15 11:37:53,053] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (bbe2d2545a9d, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,053] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 176 ms on bbe2d2545a9d (executor driver) (134/141)
[2021-05-15 11:37:53,054] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-15 11:37:53,125] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1843 bytes result sent to driver
[2021-05-15 11:37:53,126] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (bbe2d2545a9d, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,128] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 181 ms on bbe2d2545a9d (executor driver) (135/141)
[2021-05-15 11:37:53,129] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-15 11:37:53,229] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1843 bytes result sent to driver
[2021-05-15 11:37:53,230] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (bbe2d2545a9d, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,231] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 184 ms on bbe2d2545a9d (executor driver) (136/141)
[2021-05-15 11:37:53,231] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-15 11:37:53,232] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1886 bytes result sent to driver
[2021-05-15 11:37:53,233] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (bbe2d2545a9d, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:53,234] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 182 ms on bbe2d2545a9d (executor driver) (137/141)
[2021-05-15 11:37:53,235] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-15 11:37:53,267] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1886 bytes result sent to driver
[2021-05-15 11:37:53,268] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 224 ms on bbe2d2545a9d (executor driver) (138/141)
[2021-05-15 11:37:53,301] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1886 bytes result sent to driver
[2021-05-15 11:37:53,303] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 178 ms on bbe2d2545a9d (executor driver) (139/141)
[2021-05-15 11:37:53,401] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-15 11:37:53,402] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 168 ms on bbe2d2545a9d (executor driver) (140/141)
[2021-05-15 11:37:53,404] {docker.py:276} INFO - 21/05/15 14:37:53 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1843 bytes result sent to driver
[2021-05-15 11:37:53,405] {docker.py:276} INFO - 21/05/15 14:37:53 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 176 ms on bbe2d2545a9d (executor driver) (141/141)
21/05/15 14:37:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-15 11:37:53,407] {docker.py:276} INFO - 21/05/15 14:37:53 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.508 s
[2021-05-15 11:37:53,408] {docker.py:276} INFO - 21/05/15 14:37:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 14:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-15 11:37:53,408] {docker.py:276} INFO - 21/05/15 14:37:53 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.553767 s
[2021-05-15 11:37:53,425] {docker.py:276} INFO - 21/05/15 14:37:53 INFO InMemoryFileIndex: It took 6613 ms to list leaf files for 141 paths.
[2021-05-15 11:37:53,716] {docker.py:276} INFO - 21/05/15 14:37:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on bbe2d2545a9d:38027 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 11:37:55,962] {docker.py:276} INFO - 21/05/15 14:37:55 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 11:37:55,968] {docker.py:276} INFO - 21/05/15 14:37:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-15 11:37:55,972] {docker.py:276} INFO - 21/05/15 14:37:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 11:37:56,450] {docker.py:276} INFO - 21/05/15 14:37:56 INFO CodeGenerator: Code generated in 265.2254 ms
[2021-05-15 11:37:56,464] {docker.py:276} INFO - 21/05/15 14:37:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-15 11:37:56,478] {docker.py:276} INFO - 21/05/15 14:37:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-15 11:37:56,479] {docker.py:276} INFO - 21/05/15 14:37:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on bbe2d2545a9d:38027 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 11:37:56,481] {docker.py:276} INFO - 21/05/15 14:37:56 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:56,497] {docker.py:276} INFO - 21/05/15 14:37:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:37:56,598] {docker.py:276} INFO - 21/05/15 14:37:56 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:56,601] {docker.py:276} INFO - 21/05/15 14:37:56 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/15 14:37:56 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 11:37:56,601] {docker.py:276} INFO - 21/05/15 14:37:56 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 11:37:56,602] {docker.py:276} INFO - 21/05/15 14:37:56 INFO DAGScheduler: Missing parents: List()
[2021-05-15 11:37:56,602] {docker.py:276} INFO - 21/05/15 14:37:56 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:37:56,626] {docker.py:276} INFO - 21/05/15 14:37:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-15 11:37:56,640] {docker.py:276} INFO - 21/05/15 14:37:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-15 11:37:56,641] {docker.py:276} INFO - 21/05/15 14:37:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on bbe2d2545a9d:38027 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 11:37:56,641] {docker.py:276} INFO - 21/05/15 14:37:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:37:56,642] {docker.py:276} INFO - 21/05/15 14:37:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-05-15 11:37:56,643] {docker.py:276} INFO - 21/05/15 14:37:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-15 11:37:56,646] {docker.py:276} INFO - 21/05/15 14:37:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (bbe2d2545a9d, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:56,648] {docker.py:276} INFO - 21/05/15 14:37:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-15 11:37:56,748] {docker.py:276} INFO - 21/05/15 14:37:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621026405_to_1621028205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:37:56,790] {docker.py:276} INFO - 21/05/15 14:37:56 INFO CodeGenerator: Code generated in 34.6457 ms
[2021-05-15 11:37:57,332] {docker.py:276} INFO - 21/05/15 14:37:57 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-15 11:37:57,333] {docker.py:276} INFO - 21/05/15 14:37:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 690 ms on bbe2d2545a9d (executor driver) (1/1)
[2021-05-15 11:37:57,333] {docker.py:276} INFO - 21/05/15 14:37:57 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-15 11:37:57,334] {docker.py:276} INFO - 21/05/15 14:37:57 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.729 s
[2021-05-15 11:37:57,334] {docker.py:276} INFO - 21/05/15 14:37:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 14:37:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-15 11:37:57,335] {docker.py:276} INFO - 21/05/15 14:37:57 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.736691 s
[2021-05-15 11:37:57,368] {docker.py:276} INFO - 21/05/15 14:37:57 INFO CodeGenerator: Code generated in 14.427 ms
[2021-05-15 11:37:57,437] {docker.py:276} INFO - 21/05/15 14:37:57 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 14:37:57 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 11:37:57,438] {docker.py:276} INFO - 21/05/15 14:37:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 11:37:57,445] {docker.py:276} INFO - 21/05/15 14:37:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-15 11:37:57,483] {docker.py:276} INFO - 21/05/15 14:37:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on bbe2d2545a9d:38027 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 11:37:57,498] {docker.py:276} INFO - 21/05/15 14:37:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-15 11:37:57,499] {docker.py:276} INFO - 21/05/15 14:37:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bbe2d2545a9d:38027 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:37:57,502] {docker.py:276} INFO - 21/05/15 14:37:57 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:57,509] {docker.py:276} INFO - 21/05/15 14:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:37:58,097] {docker.py:276} INFO - 21/05/15 14:37:58 INFO FileSourceStrategy: Pushed Filters: 
21/05/15 14:37:58 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 11:37:58,098] {docker.py:276} INFO - 21/05/15 14:37:58 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-15 11:37:58,711] {docker.py:276} INFO - 21/05/15 14:37:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:37:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:37:58,714] {docker.py:276} INFO - 21/05/15 14:37:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:37:58,715] {docker.py:276} INFO - 21/05/15 14:37:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143758484892628465264288_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143758484892628465264288_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143758484892628465264288_0000}; taskId=attempt_20210515143758484892628465264288_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bcf8b41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:37:58,716] {docker.py:276} INFO - 21/05/15 14:37:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:37:58,755] {docker.py:276} INFO - 21/05/15 14:37:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 11:37:58,851] {docker.py:276} INFO - 21/05/15 14:37:58 INFO CodeGenerator: Code generated in 60.9717 ms
[2021-05-15 11:37:58,854] {docker.py:276} INFO - 21/05/15 14:37:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 11:37:58,916] {docker.py:276} INFO - 21/05/15 14:37:58 INFO CodeGenerator: Code generated in 53.3079 ms
[2021-05-15 11:37:58,919] {docker.py:276} INFO - 21/05/15 14:37:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-15 11:37:58,930] {docker.py:276} INFO - 21/05/15 14:37:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-15 11:37:58,931] {docker.py:276} INFO - 21/05/15 14:37:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on bbe2d2545a9d:38027 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:37:58,933] {docker.py:276} INFO - 21/05/15 14:37:58 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:58,943] {docker.py:276} INFO - 21/05/15 14:37:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 11:37:58,964] {docker.py:276} INFO - 21/05/15 14:37:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on bbe2d2545a9d:38027 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 11:37:59,108] {docker.py:276} INFO - 21/05/15 14:37:59 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 11:37:59,112] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-15 11:37:59,115] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/15 14:37:59 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 11:37:59,115] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-15 11:37:59,117] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-15 11:37:59,118] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:37:59,134] {docker.py:276} INFO - 21/05/15 14:37:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-15 11:37:59,148] {docker.py:276} INFO - 21/05/15 14:37:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-15 11:37:59,149] {docker.py:276} INFO - 21/05/15 14:37:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on bbe2d2545a9d:38027 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 11:37:59,151] {docker.py:276} INFO - 21/05/15 14:37:59 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:37:59,153] {docker.py:276} INFO - 21/05/15 14:37:59 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
[2021-05-15 11:37:59,154] {docker.py:276} INFO - 21/05/15 14:37:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-15 11:37:59,156] {docker.py:276} INFO - 21/05/15 14:37:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (bbe2d2545a9d, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:59,157] {docker.py:276} INFO - 21/05/15 14:37:59 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (bbe2d2545a9d, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:59,158] {docker.py:276} INFO - 21/05/15 14:37:59 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (bbe2d2545a9d, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:59,159] {docker.py:276} INFO - 21/05/15 14:37:59 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (bbe2d2545a9d, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-15 11:37:59,160] {docker.py:276} INFO - 21/05/15 14:37:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
21/05/15 14:37:59 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
21/05/15 14:37:59 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-15 11:37:59,160] {docker.py:276} INFO - 21/05/15 14:37:59 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-15 11:37:59,293] {docker.py:276} INFO - 21/05/15 14:37:59 INFO CodeGenerator: Code generated in 31.5438 ms
[2021-05-15 11:37:59,333] {docker.py:276} INFO - 21/05/15 14:37:59 INFO CodeGenerator: Code generated in 11.7029 ms
[2021-05-15 11:37:59,372] {docker.py:276} INFO - 21/05/15 14:37:59 INFO CodeGenerator: Code generated in 25.2975 ms
[2021-05-15 11:37:59,395] {docker.py:276} INFO - 21/05/15 14:37:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621071405_to_1621073205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:37:59,396] {docker.py:276} INFO - 21/05/15 14:37:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621026405_to_1621028205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:37:59,397] {docker.py:276} INFO - 21/05/15 14:37:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621084005_to_1621085805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:37:59,400] {docker.py:276} INFO - 21/05/15 14:37:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621075005_to_1621076805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:00,229] {docker.py:276} INFO - 21/05/15 14:38:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621085805_to_1621087605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:00,714] {docker.py:276} INFO - 21/05/15 14:38:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621022805_to_1621024605.csv, range: 0-111676, partition values: [empty row]
[2021-05-15 11:38:00,742] {docker.py:276} INFO - 21/05/15 14:38:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621076805_to_1621078605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:00,755] {docker.py:276} INFO - 21/05/15 14:38:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621073205_to_1621075005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:00,769] {docker.py:276} INFO - 21/05/15 14:38:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621028205_to_1621030005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:01,065] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621024605_to_1621026405.csv, range: 0-111651, partition values: [empty row]
[2021-05-15 11:38:01,105] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621078605_to_1621080405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,233] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621075005_to_1621076805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,256] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621030005_to_1621031805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:01,411] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621021005_to_1621022805.csv, range: 0-105628, partition values: [empty row]
[2021-05-15 11:38:01,463] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621080405_to_1621082205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,578] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621076805_to_1621078605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,643] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621031805_to_1621033605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:01,764] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621026405_to_1621028205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,944] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621078605_to_1621080405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:01,952] {docker.py:276} INFO - 21/05/15 14:38:01 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621082205_to_1621084005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,006] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621033605_to_1621035405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:02,114] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621028205_to_1621030005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,296] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621080405_to_1621082205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,337] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621084005_to_1621085805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,387] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621035405_to_1621037205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:02,465] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621030005_to_1621031805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,638] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621082205_to_1621084005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,674] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621085805_to_1621087605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,748] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621037205_to_1621039005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:02,810] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621031805_to_1621033605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:02,984] {docker.py:276} INFO - 21/05/15 14:38:02 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621084005_to_1621085805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,046] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621026405_to_1621028205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,105] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621039005_to_1621040805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:03,155] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621033605_to_1621035405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,328] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621085805_to_1621087605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,390] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621028205_to_1621030005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,468] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621040805_to_1621042605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:03,496] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621035405_to_1621037205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,577] {docker.py:276} INFO - 21/05/15 14:38:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on bbe2d2545a9d:38027 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 11:38:03,670] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621022805_to_1621024605.csv, range: 0-104406, partition values: [empty row]
[2021-05-15 11:38:03,720] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621030005_to_1621031805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:03,821] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621042605_to_1621044405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:03,845] {docker.py:276} INFO - 21/05/15 14:38:03 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621037205_to_1621039005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,012] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621024605_to_1621026405.csv, range: 0-104388, partition values: [empty row]
[2021-05-15 11:38:04,071] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621031805_to_1621033605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,181] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621044405_to_1621046205.csv, range: 0-111710, partition values: [empty row]
21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621039005_to_1621040805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,351] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621013805_to_1621015605.csv, range: 0-104379, partition values: [empty row]
[2021-05-15 11:38:04,409] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621033605_to_1621035405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,528] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621040805_to_1621042605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,535] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621046205_to_1621048005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:04,787] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621035405_to_1621037205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,895] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621042605_to_1621044405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:04,911] {docker.py:276} INFO - 21/05/15 14:38:04 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621048005_to_1621049805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:05,146] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621037205_to_1621039005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:05,241] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621015605_to_1621017405.csv, range: 0-104311, partition values: [empty row]
[2021-05-15 11:38:05,263] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621049805_to_1621051605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:05,513] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621039005_to_1621040805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:05,610] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621051605_to_1621053405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:05,643] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621015605_to_1621017405.csv, range: 0-104307, partition values: [empty row]
[2021-05-15 11:38:05,707] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621044405_to_1621046205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:05,865] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621040805_to_1621042605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:05,978] {docker.py:276} INFO - 21/05/15 14:38:05 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621053405_to_1621055205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:05,989] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621019205_to_1621021005.csv, range: 0-104279, partition values: [empty row]
[2021-05-15 11:38:06,204] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621046205_to_1621048005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:06,208] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621042605_to_1621044405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:06,338] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621012005_to_1621013805.csv, range: 0-104276, partition values: [empty row]
[2021-05-15 11:38:06,345] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621055205_to_1621057005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:06,545] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621044405_to_1621046205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:06,554] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621048005_to_1621049805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:06,678] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621019205_to_1621021005.csv, range: 0-104263, partition values: [empty row]
[2021-05-15 11:38:06,723] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621057005_to_1621058805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:06,890] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621046205_to_1621048005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:06,907] {docker.py:276} INFO - 21/05/15 14:38:06 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621049805_to_1621051605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:07,017] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621017405_to_1621019205.csv, range: 0-104262, partition values: [empty row]
[2021-05-15 11:38:07,090] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621058805_to_1621060605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:07,235] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621048005_to_1621049805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:07,269] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621051605_to_1621053405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:07,367] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621010205_to_1621012005.csv, range: 0-104251, partition values: [empty row]
[2021-05-15 11:38:07,457] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621060605_to_1621062405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:07,576] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621049805_to_1621051605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:07,670] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621053405_to_1621055205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:07,719] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621004805_to_1621006605.csv, range: 0-104248, partition values: [empty row]
[2021-05-15 11:38:07,812] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621062405_to_1621064205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:07,924] {docker.py:276} INFO - 21/05/15 14:38:07 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621051605_to_1621053405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,030] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621055205_to_1621057005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,075] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621008405_to_1621010205.csv, range: 0-104239, partition values: [empty row]
[2021-05-15 11:38:08,183] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621064205_to_1621066005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:08,268] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621053405_to_1621055205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,397] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621057005_to_1621058805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,409] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621003005_to_1621004805.csv, range: 0-104235, partition values: [empty row]
[2021-05-15 11:38:08,537] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621066005_to_1621067805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:08,631] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621055205_to_1621057005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,760] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621058805_to_1621060605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:08,765] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621017405_to_1621019205.csv, range: 0-104219, partition values: [empty row]
[2021-05-15 11:38:08,905] {docker.py:276} INFO - 21/05/15 14:38:08 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621067805_to_1621069605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:09,009] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621057005_to_1621058805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,102] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621060605_to_1621062405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,117] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621010205_to_1621012005.csv, range: 0-104199, partition values: [empty row]
[2021-05-15 11:38:09,266] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621069605_to_1621071405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:09,384] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621058805_to_1621060605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,440] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621062405_to_1621064205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,498] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621006605_to_1621008405.csv, range: 0-104185, partition values: [empty row]
[2021-05-15 11:38:09,622] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621071405_to_1621073205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:09,798] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621060605_to_1621062405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,813] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621064205_to_1621066005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:09,853] {docker.py:276} INFO - 21/05/15 14:38:09 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621004805_to_1621006605.csv, range: 0-104162, partition values: [empty row]
[2021-05-15 11:38:10,005] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621073205_to_1621075005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:10,141] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621062405_to_1621064205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,176] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621066005_to_1621067805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,213] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621012005_to_1621013805.csv, range: 0-104156, partition values: [empty row]
[2021-05-15 11:38:10,356] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621075005_to_1621076805.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:10,485] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621064205_to_1621066005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,515] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621067805_to_1621069605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,568] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621003005_to_1621004805.csv, range: 0-104155, partition values: [empty row]
[2021-05-15 11:38:10,724] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621076805_to_1621078605.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:10,831] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621066005_to_1621067805.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,876] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621069605_to_1621071405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:10,929] {docker.py:276} INFO - 21/05/15 14:38:10 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621006605_to_1621008405.csv, range: 0-104138, partition values: [empty row]
[2021-05-15 11:38:11,099] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621078605_to_1621080405.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:11,176] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621067805_to_1621069605.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:11,278] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621013805_to_1621015605.csv, range: 0-104136, partition values: [empty row]
[2021-05-15 11:38:11,333] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621071405_to_1621073205.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:11,478] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621080405_to_1621082205.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:11,552] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621069605_to_1621071405.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:11,640] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621008405_to_1621010205.csv, range: 0-104113, partition values: [empty row]
[2021-05-15 11:38:11,688] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621073205_to_1621075005.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 11:38:11,850] {docker.py:276} INFO - 21/05/15 14:38:11 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_11_36_45/from_1621082205_to_1621084005.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 11:38:12,000] {docker.py:276} INFO - 21/05/15 14:38:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621021005_to_1621022805.csv, range: 0-104057, partition values: [empty row]
[2021-05-15 11:38:12,535] {docker.py:276} INFO - 21/05/15 14:38:12 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2722 bytes result sent to driver
[2021-05-15 11:38:12,537] {docker.py:276} INFO - 21/05/15 14:38:12 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (bbe2d2545a9d, executor driver, partition 4, PROCESS_LOCAL, 6214 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:12,538] {docker.py:276} INFO - 21/05/15 14:38:12 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-15 11:38:12,543] {docker.py:276} INFO - 21/05/15 14:38:12 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 13401 ms on bbe2d2545a9d (executor driver) (1/5)
[2021-05-15 11:38:12,562] {docker.py:276} INFO - 21/05/15 14:38:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621010205_to_1621012005.csv, range: 0-104023, partition values: [empty row]
[2021-05-15 11:38:12,607] {docker.py:276} INFO - 21/05/15 14:38:12 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-15 11:38:12,608] {docker.py:276} INFO - 21/05/15 14:38:12 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 13467 ms on bbe2d2545a9d (executor driver) (2/5)
[2021-05-15 11:38:12,670] {docker.py:276} INFO - 21/05/15 14:38:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2679 bytes result sent to driver
[2021-05-15 11:38:12,671] {docker.py:276} INFO - 21/05/15 14:38:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 13532 ms on bbe2d2545a9d (executor driver) (3/5)
[2021-05-15 11:38:12,735] {docker.py:276} INFO - 21/05/15 14:38:12 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2679 bytes result sent to driver
[2021-05-15 11:38:12,735] {docker.py:276} INFO - 21/05/15 14:38:12 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 13592 ms on bbe2d2545a9d (executor driver) (4/5)
[2021-05-15 11:38:12,946] {docker.py:276} INFO - 21/05/15 14:38:12 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621003005_to_1621004805.csv, range: 0-103924, partition values: [empty row]
[2021-05-15 11:38:13,317] {docker.py:276} INFO - 21/05/15 14:38:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621024605_to_1621026405.csv, range: 0-103889, partition values: [empty row]
[2021-05-15 11:38:13,830] {docker.py:276} INFO - 21/05/15 14:38:13 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621012005_to_1621013805.csv, range: 0-103883, partition values: [empty row]
[2021-05-15 11:38:14,225] {docker.py:276} INFO - 21/05/15 14:38:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621013805_to_1621015605.csv, range: 0-103880, partition values: [empty row]
[2021-05-15 11:38:14,598] {docker.py:276} INFO - 21/05/15 14:38:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621008405_to_1621010205.csv, range: 0-103877, partition values: [empty row]
[2021-05-15 11:38:14,956] {docker.py:276} INFO - 21/05/15 14:38:14 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621004805_to_1621006605.csv, range: 0-103872, partition values: [empty row]
[2021-05-15 11:38:15,467] {docker.py:276} INFO - 21/05/15 14:38:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621021005_to_1621022805.csv, range: 0-103827, partition values: [empty row]
[2021-05-15 11:38:15,833] {docker.py:276} INFO - 21/05/15 14:38:15 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621017405_to_1621019205.csv, range: 0-103779, partition values: [empty row]
[2021-05-15 11:38:16,338] {docker.py:276} INFO - 21/05/15 14:38:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621006605_to_1621008405.csv, range: 0-103748, partition values: [empty row]
[2021-05-15 11:38:16,843] {docker.py:276} INFO - 21/05/15 14:38:16 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621019205_to_1621021005.csv, range: 0-103655, partition values: [empty row]
[2021-05-15 11:38:17,209] {docker.py:276} INFO - 21/05/15 14:38:17 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_11_36_45/from_1621015605_to_1621017405.csv, range: 0-103596, partition values: [empty row]
[2021-05-15 11:38:17,571] {docker.py:276} INFO - 21/05/15 14:38:17 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_11_36_45/from_1621022805_to_1621024605.csv, range: 0-103481, partition values: [empty row]
[2021-05-15 11:38:18,079] {docker.py:276} INFO - 21/05/15 14:38:18 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-15 11:38:18,080] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 5549 ms on bbe2d2545a9d (executor driver) (5/5)
[2021-05-15 11:38:18,080] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-15 11:38:18,081] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 18.978 s
[2021-05-15 11:38:18,082] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: looking for newly runnable stages
[2021-05-15 11:38:18,082] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: running: Set()
[2021-05-15 11:38:18,083] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-15 11:38:18,084] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: failed: Set()
[2021-05-15 11:38:18,088] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 11:38:18,128] {docker.py:276} INFO - 21/05/15 14:38:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-15 11:38:18,131] {docker.py:276} INFO - 21/05/15 14:38:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-15 11:38:18,131] {docker.py:276} INFO - 21/05/15 14:38:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on bbe2d2545a9d:38027 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-15 11:38:18,132] {docker.py:276} INFO - 21/05/15 14:38:18 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-15 11:38:18,133] {docker.py:276} INFO - 21/05/15 14:38:18 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/15 14:38:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-15 11:38:18,141] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (bbe2d2545a9d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:18,142] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (bbe2d2545a9d, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:18,143] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (bbe2d2545a9d, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:18,143] {docker.py:276} INFO - 21/05/15 14:38:18 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (bbe2d2545a9d, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:18,144] {docker.py:276} INFO - 21/05/15 14:38:18 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
21/05/15 14:38:18 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-15 11:38:18,146] {docker.py:276} INFO - 21/05/15 14:38:18 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-15 11:38:18,152] {docker.py:276} INFO - 21/05/15 14:38:18 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-15 11:38:18,220] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:18,222] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:18,223] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:18,223] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2021-05-15 11:38:18,224] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2021-05-15 11:38:18,225] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-15 11:38:18,225] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Getting 5 (25.1 KiB) non-empty blocks including 5 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:18,226] {docker.py:276} INFO - 21/05/15 14:38:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-15 11:38:18,242] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:18,242] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:18,243] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,244] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,245] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594457286410368961426_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594457286410368961426_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594457286410368961426_0004}; taskId=attempt_202105151437594457286410368961426_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20b43cfc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,245] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:18,246] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:18,246] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,246] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592858168787175095888_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592858168787175095888_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592858168787175095888_0004}; taskId=attempt_202105151437592858168787175095888_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@214f35cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,247] {docker.py:276} INFO - 21/05/15 14:38:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:18,249] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:18,249] {docker.py:276} INFO - 21/05/15 14:38:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:18,249] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592718996847515801256_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592718996847515801256_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592718996847515801256_0004}; taskId=attempt_202105151437592718996847515801256_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6fa73303}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:18,250] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:18,250] {docker.py:276} INFO - 21/05/15 14:38:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591880302696359544555_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591880302696359544555_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591880302696359544555_0004}; taskId=attempt_202105151437591880302696359544555_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58e6a2c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437594457286410368961426_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594457286410368961426_0004_m_000001_289
[2021-05-15 11:38:18,251] {docker.py:276} INFO - 21/05/15 14:38:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:18,252] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437591880302696359544555_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591880302696359544555_0004_m_000003_291
[2021-05-15 11:38:18,252] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437592718996847515801256_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592718996847515801256_0004_m_000002_290
[2021-05-15 11:38:18,252] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437592858168787175095888_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592858168787175095888_0004_m_000000_288
[2021-05-15 11:38:18,274] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Task committer attempt_202105151437594457286410368961426_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594457286410368961426_0004_m_000001_289 : duration 0:00.023s
[2021-05-15 11:38:18,275] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Task committer attempt_202105151437591880302696359544555_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591880302696359544555_0004_m_000003_291 : duration 0:00.023s
[2021-05-15 11:38:18,284] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Task committer attempt_202105151437592718996847515801256_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592718996847515801256_0004_m_000002_290 : duration 0:00.031s
[2021-05-15 11:38:18,292] {docker.py:276} INFO - 21/05/15 14:38:18 INFO StagingCommitter: Task committer attempt_202105151437592858168787175095888_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592858168787175095888_0004_m_000000_288 : duration 0:00.040s
[2021-05-15 11:38:20,407] {docker.py:276} INFO - 21/05/15 14:38:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437592858168787175095888_0004_m_000000_288: needsTaskCommit() Task attempt_202105151437592858168787175095888_0004_m_000000_288
[2021-05-15 11:38:20,407] {docker.py:276} INFO - 21/05/15 14:38:20 INFO StagingCommitter: Task committer attempt_202105151437592858168787175095888_0004_m_000000_288: needsTaskCommit() Task attempt_202105151437592858168787175095888_0004_m_000000_288: duration 0:00.001s
[2021-05-15 11:38:20,408] {docker.py:276} INFO - 21/05/15 14:38:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592858168787175095888_0004_m_000000_288
[2021-05-15 11:38:20,413] {docker.py:276} INFO - 21/05/15 14:38:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 4630 bytes result sent to driver
[2021-05-15 11:38:20,414] {docker.py:276} INFO - 21/05/15 14:38:20 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (bbe2d2545a9d, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:20,415] {docker.py:276} INFO - 21/05/15 14:38:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 2243 ms on bbe2d2545a9d (executor driver) (1/200)
[2021-05-15 11:38:20,416] {docker.py:276} INFO - 21/05/15 14:38:20 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-15 11:38:20,423] {docker.py:276} INFO - 21/05/15 14:38:20 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:20,425] {docker.py:276} INFO - 21/05/15 14:38:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:20,426] {docker.py:276} INFO - 21/05/15 14:38:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759944460604243380902_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759944460604243380902_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759944460604243380902_0004}; taskId=attempt_20210515143759944460604243380902_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5aaeb36d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:20,426] {docker.py:276} INFO - 21/05/15 14:38:20 INFO StagingCommitter: Starting: Task committer attempt_20210515143759944460604243380902_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759944460604243380902_0004_m_000004_292
[2021-05-15 11:38:20,431] {docker.py:276} INFO - 21/05/15 14:38:20 INFO StagingCommitter: Task committer attempt_20210515143759944460604243380902_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759944460604243380902_0004_m_000004_292 : duration 0:00.006s
[2021-05-15 11:38:21,129] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437592718996847515801256_0004_m_000002_290: needsTaskCommit() Task attempt_202105151437592718996847515801256_0004_m_000002_290
[2021-05-15 11:38:21,130] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437592718996847515801256_0004_m_000002_290: needsTaskCommit() Task attempt_202105151437592718996847515801256_0004_m_000002_290: duration 0:00.001s
[2021-05-15 11:38:21,130] {docker.py:276} INFO - 21/05/15 14:38:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592718996847515801256_0004_m_000002_290
[2021-05-15 11:38:21,131] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-15 11:38:21,132] {docker.py:276} INFO - 21/05/15 14:38:21 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (bbe2d2545a9d, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:21,133] {docker.py:276} INFO - 21/05/15 14:38:21 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 2959 ms on bbe2d2545a9d (executor driver) (2/200)
[2021-05-15 11:38:21,133] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-15 11:38:21,142] {docker.py:276} INFO - 21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:21,145] {docker.py:276} INFO - 21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2021-05-15 11:38:21,147] {docker.py:276} INFO - 21/05/15 14:38:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:21,148] {docker.py:276} INFO - 21/05/15 14:38:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:21,148] {docker.py:276} INFO - 21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:21,148] {docker.py:276} INFO - 21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597836404394482359657_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597836404394482359657_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597836404394482359657_0004}; taskId=attempt_202105151437597836404394482359657_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a12348e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:21,149] {docker.py:276} INFO - 21/05/15 14:38:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:21,149] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437597836404394482359657_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597836404394482359657_0004_m_000005_293
[2021-05-15 11:38:21,156] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437597836404394482359657_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597836404394482359657_0004_m_000005_293 : duration 0:00.008s
[2021-05-15 11:38:21,453] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437594457286410368961426_0004_m_000001_289: needsTaskCommit() Task attempt_202105151437594457286410368961426_0004_m_000001_289
21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437594457286410368961426_0004_m_000001_289: needsTaskCommit() Task attempt_202105151437594457286410368961426_0004_m_000001_289: duration 0:00.002s
[2021-05-15 11:38:21,454] {docker.py:276} INFO - 21/05/15 14:38:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594457286410368961426_0004_m_000001_289
[2021-05-15 11:38:21,455] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-15 11:38:21,456] {docker.py:276} INFO - 21/05/15 14:38:21 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (bbe2d2545a9d, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:21,458] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
21/05/15 14:38:21 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 3284 ms on bbe2d2545a9d (executor driver) (3/200)
[2021-05-15 11:38:21,469] {docker.py:276} INFO - 21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:21,471] {docker.py:276} INFO - 21/05/15 14:38:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:21,472] {docker.py:276} INFO - 21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592947954309570056718_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592947954309570056718_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592947954309570056718_0004}; taskId=attempt_202105151437592947954309570056718_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b4267f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:21,472] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437592947954309570056718_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592947954309570056718_0004_m_000006_294
[2021-05-15 11:38:21,478] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437592947954309570056718_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592947954309570056718_0004_m_000006_294 : duration 0:00.007s
[2021-05-15 11:38:21,641] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437591880302696359544555_0004_m_000003_291: needsTaskCommit() Task attempt_202105151437591880302696359544555_0004_m_000003_291
[2021-05-15 11:38:21,643] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437591880302696359544555_0004_m_000003_291: needsTaskCommit() Task attempt_202105151437591880302696359544555_0004_m_000003_291: duration 0:00.003s
[2021-05-15 11:38:21,644] {docker.py:276} INFO - 21/05/15 14:38:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591880302696359544555_0004_m_000003_291
[2021-05-15 11:38:21,645] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 4587 bytes result sent to driver
[2021-05-15 11:38:21,646] {docker.py:276} INFO - 21/05/15 14:38:21 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (bbe2d2545a9d, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:21,646] {docker.py:276} INFO - 21/05/15 14:38:21 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-15 11:38:21,647] {docker.py:276} INFO - 21/05/15 14:38:21 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 3474 ms on bbe2d2545a9d (executor driver) (4/200)
[2021-05-15 11:38:21,656] {docker.py:276} INFO - 21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:21,657] {docker.py:276} INFO - 21/05/15 14:38:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:38:21,660] {docker.py:276} INFO - 21/05/15 14:38:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:21,660] {docker.py:276} INFO - 21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593029850196998260492_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593029850196998260492_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593029850196998260492_0004}; taskId=attempt_202105151437593029850196998260492_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35da050d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:21,661] {docker.py:276} INFO - 21/05/15 14:38:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:21,661] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437593029850196998260492_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593029850196998260492_0004_m_000007_295
[2021-05-15 11:38:21,666] {docker.py:276} INFO - 21/05/15 14:38:21 INFO StagingCommitter: Task committer attempt_202105151437593029850196998260492_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593029850196998260492_0004_m_000007_295 : duration 0:00.005s
[2021-05-15 11:38:23,643] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_20210515143759944460604243380902_0004_m_000004_292: needsTaskCommit() Task attempt_20210515143759944460604243380902_0004_m_000004_292
[2021-05-15 11:38:23,643] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_20210515143759944460604243380902_0004_m_000004_292: needsTaskCommit() Task attempt_20210515143759944460604243380902_0004_m_000004_292: duration 0:00.001s
21/05/15 14:38:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759944460604243380902_0004_m_000004_292
[2021-05-15 11:38:23,645] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 4544 bytes result sent to driver
[2021-05-15 11:38:23,646] {docker.py:276} INFO - 21/05/15 14:38:23 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (bbe2d2545a9d, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:23,646] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-15 11:38:23,647] {docker.py:276} INFO - 21/05/15 14:38:23 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 3237 ms on bbe2d2545a9d (executor driver) (5/200)
[2021-05-15 11:38:23,657] {docker.py:276} INFO - 21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:23,657] {docker.py:276} INFO - 21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:23,662] {docker.py:276} INFO - 21/05/15 14:38:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:23,663] {docker.py:276} INFO - 21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:23,664] {docker.py:276} INFO - 21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593773927827070466247_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593773927827070466247_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593773927827070466247_0004}; taskId=attempt_202105151437593773927827070466247_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f7a367e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:23,664] {docker.py:276} INFO - 21/05/15 14:38:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:23,664] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437593773927827070466247_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593773927827070466247_0004_m_000008_296
[2021-05-15 11:38:23,670] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_202105151437593773927827070466247_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593773927827070466247_0004_m_000008_296 : duration 0:00.006s
[2021-05-15 11:38:23,755] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437593029850196998260492_0004_m_000007_295: needsTaskCommit() Task attempt_202105151437593029850196998260492_0004_m_000007_295
[2021-05-15 11:38:23,755] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_202105151437593029850196998260492_0004_m_000007_295: needsTaskCommit() Task attempt_202105151437593029850196998260492_0004_m_000007_295: duration 0:00.001s
[2021-05-15 11:38:23,756] {docker.py:276} INFO - 21/05/15 14:38:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593029850196998260492_0004_m_000007_295
[2021-05-15 11:38:23,757] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-15 11:38:23,758] {docker.py:276} INFO - 21/05/15 14:38:23 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (bbe2d2545a9d, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:23,759] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-15 11:38:23,759] {docker.py:276} INFO - 21/05/15 14:38:23 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 2116 ms on bbe2d2545a9d (executor driver) (6/200)
[2021-05-15 11:38:23,769] {docker.py:276} INFO - 21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:23,770] {docker.py:276} INFO - 21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:23,772] {docker.py:276} INFO - 21/05/15 14:38:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:23,772] {docker.py:276} INFO - 21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:23,773] {docker.py:276} INFO - 21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759864014776027090755_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759864014776027090755_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759864014776027090755_0004}; taskId=attempt_20210515143759864014776027090755_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11a50b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:23,773] {docker.py:276} INFO - 21/05/15 14:38:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:23,773] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_20210515143759864014776027090755_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759864014776027090755_0004_m_000009_297
[2021-05-15 11:38:23,777] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_20210515143759864014776027090755_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759864014776027090755_0004_m_000009_297 : duration 0:00.004s
[2021-05-15 11:38:23,906] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437597836404394482359657_0004_m_000005_293: needsTaskCommit() Task attempt_202105151437597836404394482359657_0004_m_000005_293
[2021-05-15 11:38:23,907] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_202105151437597836404394482359657_0004_m_000005_293: needsTaskCommit() Task attempt_202105151437597836404394482359657_0004_m_000005_293: duration 0:00.001s
21/05/15 14:38:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597836404394482359657_0004_m_000005_293
[2021-05-15 11:38:23,907] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 4544 bytes result sent to driver
[2021-05-15 11:38:23,909] {docker.py:276} INFO - 21/05/15 14:38:23 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (bbe2d2545a9d, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:23,910] {docker.py:276} INFO - 21/05/15 14:38:23 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
21/05/15 14:38:23 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 2782 ms on bbe2d2545a9d (executor driver) (7/200)
[2021-05-15 11:38:23,922] {docker.py:276} INFO - 21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:23,924] {docker.py:276} INFO - 21/05/15 14:38:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593377155181754256280_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593377155181754256280_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593377155181754256280_0004}; taskId=attempt_202105151437593377155181754256280_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7888e4a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:23,924] {docker.py:276} INFO - 21/05/15 14:38:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437593377155181754256280_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593377155181754256280_0004_m_000010_298
[2021-05-15 11:38:23,928] {docker.py:276} INFO - 21/05/15 14:38:23 INFO StagingCommitter: Task committer attempt_202105151437593377155181754256280_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593377155181754256280_0004_m_000010_298 : duration 0:00.004s
[2021-05-15 11:38:24,191] {docker.py:276} INFO - 21/05/15 14:38:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437592947954309570056718_0004_m_000006_294: needsTaskCommit() Task attempt_202105151437592947954309570056718_0004_m_000006_294
[2021-05-15 11:38:24,192] {docker.py:276} INFO - 21/05/15 14:38:24 INFO StagingCommitter: Task committer attempt_202105151437592947954309570056718_0004_m_000006_294: needsTaskCommit() Task attempt_202105151437592947954309570056718_0004_m_000006_294: duration 0:00.002s
[2021-05-15 11:38:24,193] {docker.py:276} INFO - 21/05/15 14:38:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592947954309570056718_0004_m_000006_294
[2021-05-15 11:38:24,194] {docker.py:276} INFO - 21/05/15 14:38:24 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 4587 bytes result sent to driver
[2021-05-15 11:38:24,196] {docker.py:276} INFO - 21/05/15 14:38:24 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (bbe2d2545a9d, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:24,197] {docker.py:276} INFO - 21/05/15 14:38:24 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
21/05/15 14:38:24 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 2744 ms on bbe2d2545a9d (executor driver) (8/200)
[2021-05-15 11:38:24,208] {docker.py:276} INFO - 21/05/15 14:38:24 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:24,210] {docker.py:276} INFO - 21/05/15 14:38:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:24,211] {docker.py:276} INFO - 21/05/15 14:38:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593421997368410700550_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593421997368410700550_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593421997368410700550_0004}; taskId=attempt_202105151437593421997368410700550_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dc1aebb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:24,211] {docker.py:276} INFO - 21/05/15 14:38:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437593421997368410700550_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593421997368410700550_0004_m_000011_299
[2021-05-15 11:38:24,216] {docker.py:276} INFO - 21/05/15 14:38:24 INFO StagingCommitter: Task committer attempt_202105151437593421997368410700550_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593421997368410700550_0004_m_000011_299 : duration 0:00.005s
[2021-05-15 11:38:25,667] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Starting: Task committer attempt_20210515143759864014776027090755_0004_m_000009_297: needsTaskCommit() Task attempt_20210515143759864014776027090755_0004_m_000009_297
21/05/15 14:38:25 INFO StagingCommitter: Task committer attempt_20210515143759864014776027090755_0004_m_000009_297: needsTaskCommit() Task attempt_20210515143759864014776027090755_0004_m_000009_297: duration 0:00.002s
21/05/15 14:38:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759864014776027090755_0004_m_000009_297
[2021-05-15 11:38:25,668] {docker.py:276} INFO - 21/05/15 14:38:25 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 4587 bytes result sent to driver
[2021-05-15 11:38:25,670] {docker.py:276} INFO - 21/05/15 14:38:25 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (bbe2d2545a9d, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:25,677] {docker.py:276} INFO - 21/05/15 14:38:25 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 1915 ms on bbe2d2545a9d (executor driver) (9/200)
[2021-05-15 11:38:25,678] {docker.py:276} INFO - 21/05/15 14:38:25 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-15 11:38:25,689] {docker.py:276} INFO - 21/05/15 14:38:25 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:25,689] {docker.py:276} INFO - 21/05/15 14:38:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:25,692] {docker.py:276} INFO - 21/05/15 14:38:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:25,692] {docker.py:276} INFO - 21/05/15 14:38:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598883883926356525846_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598883883926356525846_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598883883926356525846_0004}; taskId=attempt_202105151437598883883926356525846_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4900ef7e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:25,692] {docker.py:276} INFO - 21/05/15 14:38:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437598883883926356525846_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598883883926356525846_0004_m_000012_300
[2021-05-15 11:38:25,696] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Task committer attempt_202105151437598883883926356525846_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598883883926356525846_0004_m_000012_300 : duration 0:00.004s
[2021-05-15 11:38:25,850] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437593773927827070466247_0004_m_000008_296: needsTaskCommit() Task attempt_202105151437593773927827070466247_0004_m_000008_296
[2021-05-15 11:38:25,852] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Task committer attempt_202105151437593773927827070466247_0004_m_000008_296: needsTaskCommit() Task attempt_202105151437593773927827070466247_0004_m_000008_296: duration 0:00.004s
21/05/15 14:38:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593773927827070466247_0004_m_000008_296
[2021-05-15 11:38:25,854] {docker.py:276} INFO - 21/05/15 14:38:25 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 4587 bytes result sent to driver
[2021-05-15 11:38:25,855] {docker.py:276} INFO - 21/05/15 14:38:25 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (bbe2d2545a9d, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:25,856] {docker.py:276} INFO - 21/05/15 14:38:25 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
21/05/15 14:38:25 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 2213 ms on bbe2d2545a9d (executor driver) (10/200)
[2021-05-15 11:38:25,868] {docker.py:276} INFO - 21/05/15 14:38:25 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:25,870] {docker.py:276} INFO - 21/05/15 14:38:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:25,871] {docker.py:276} INFO - 21/05/15 14:38:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:25,871] {docker.py:276} INFO - 21/05/15 14:38:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596957430985616798617_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596957430985616798617_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596957430985616798617_0004}; taskId=attempt_202105151437596957430985616798617_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17022a16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:25,871] {docker.py:276} INFO - 21/05/15 14:38:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:25,872] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437596957430985616798617_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596957430985616798617_0004_m_000013_301
[2021-05-15 11:38:25,874] {docker.py:276} INFO - 21/05/15 14:38:25 INFO StagingCommitter: Task committer attempt_202105151437596957430985616798617_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596957430985616798617_0004_m_000013_301 : duration 0:00.004s
[2021-05-15 11:38:26,571] {docker.py:276} INFO - 21/05/15 14:38:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437593377155181754256280_0004_m_000010_298: needsTaskCommit() Task attempt_202105151437593377155181754256280_0004_m_000010_298
[2021-05-15 11:38:26,572] {docker.py:276} INFO - 21/05/15 14:38:26 INFO StagingCommitter: Task committer attempt_202105151437593377155181754256280_0004_m_000010_298: needsTaskCommit() Task attempt_202105151437593377155181754256280_0004_m_000010_298: duration 0:00.001s
21/05/15 14:38:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593377155181754256280_0004_m_000010_298
[2021-05-15 11:38:26,573] {docker.py:276} INFO - 21/05/15 14:38:26 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 4587 bytes result sent to driver
[2021-05-15 11:38:26,574] {docker.py:276} INFO - 21/05/15 14:38:26 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (bbe2d2545a9d, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:26,575] {docker.py:276} INFO - 21/05/15 14:38:26 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 2670 ms on bbe2d2545a9d (executor driver) (11/200)
[2021-05-15 11:38:26,576] {docker.py:276} INFO - 21/05/15 14:38:26 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-15 11:38:26,585] {docker.py:276} INFO - 21/05/15 14:38:26 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:26,585] {docker.py:276} INFO - 21/05/15 14:38:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:26,588] {docker.py:276} INFO - 21/05/15 14:38:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598755432296952933914_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598755432296952933914_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598755432296952933914_0004}; taskId=attempt_202105151437598755432296952933914_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55650eec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:26,589] {docker.py:276} INFO - 21/05/15 14:38:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437598755432296952933914_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598755432296952933914_0004_m_000014_302
[2021-05-15 11:38:26,593] {docker.py:276} INFO - 21/05/15 14:38:26 INFO StagingCommitter: Task committer attempt_202105151437598755432296952933914_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598755432296952933914_0004_m_000014_302 : duration 0:00.005s
[2021-05-15 11:38:27,035] {docker.py:276} INFO - 21/05/15 14:38:27 INFO StagingCommitter: Starting: Task committer attempt_202105151437593421997368410700550_0004_m_000011_299: needsTaskCommit() Task attempt_202105151437593421997368410700550_0004_m_000011_299
[2021-05-15 11:38:27,035] {docker.py:276} INFO - 21/05/15 14:38:27 INFO StagingCommitter: Task committer attempt_202105151437593421997368410700550_0004_m_000011_299: needsTaskCommit() Task attempt_202105151437593421997368410700550_0004_m_000011_299: duration 0:00.004s
[2021-05-15 11:38:27,036] {docker.py:276} INFO - 21/05/15 14:38:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593421997368410700550_0004_m_000011_299
[2021-05-15 11:38:27,037] {docker.py:276} INFO - 21/05/15 14:38:27 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-15 11:38:27,039] {docker.py:276} INFO - 21/05/15 14:38:27 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (bbe2d2545a9d, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:27,040] {docker.py:276} INFO - 21/05/15 14:38:27 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 2849 ms on bbe2d2545a9d (executor driver) (12/200)
[2021-05-15 11:38:27,041] {docker.py:276} INFO - 21/05/15 14:38:27 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-15 11:38:27,052] {docker.py:276} INFO - 21/05/15 14:38:27 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:27,053] {docker.py:276} INFO - 21/05/15 14:38:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:27,055] {docker.py:276} INFO - 21/05/15 14:38:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596002438435891057143_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596002438435891057143_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596002438435891057143_0004}; taskId=attempt_202105151437596002438435891057143_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8efa8cb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:27,055] {docker.py:276} INFO - 21/05/15 14:38:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:27 INFO StagingCommitter: Starting: Task committer attempt_202105151437596002438435891057143_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596002438435891057143_0004_m_000015_303
[2021-05-15 11:38:27,059] {docker.py:276} INFO - 21/05/15 14:38:27 INFO StagingCommitter: Task committer attempt_202105151437596002438435891057143_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596002438435891057143_0004_m_000015_303 : duration 0:00.004s
[2021-05-15 11:38:28,433] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437596957430985616798617_0004_m_000013_301: needsTaskCommit() Task attempt_202105151437596957430985616798617_0004_m_000013_301
[2021-05-15 11:38:28,434] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Task committer attempt_202105151437596957430985616798617_0004_m_000013_301: needsTaskCommit() Task attempt_202105151437596957430985616798617_0004_m_000013_301: duration 0:00.003s
[2021-05-15 11:38:28,435] {docker.py:276} INFO - 21/05/15 14:38:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596957430985616798617_0004_m_000013_301
[2021-05-15 11:38:28,435] {docker.py:276} INFO - 21/05/15 14:38:28 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-15 11:38:28,437] {docker.py:276} INFO - 21/05/15 14:38:28 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (bbe2d2545a9d, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:28,438] {docker.py:276} INFO - 21/05/15 14:38:28 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 2587 ms on bbe2d2545a9d (executor driver) (13/200)
[2021-05-15 11:38:28,439] {docker.py:276} INFO - 21/05/15 14:38:28 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-15 11:38:28,450] {docker.py:276} INFO - 21/05/15 14:38:28 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:28,452] {docker.py:276} INFO - 21/05/15 14:38:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:28,453] {docker.py:276} INFO - 21/05/15 14:38:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597005384220929796549_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597005384220929796549_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597005384220929796549_0004}; taskId=attempt_202105151437597005384220929796549_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@255cecdf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437597005384220929796549_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597005384220929796549_0004_m_000016_304
[2021-05-15 11:38:28,457] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Task committer attempt_202105151437597005384220929796549_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597005384220929796549_0004_m_000016_304 : duration 0:00.004s
[2021-05-15 11:38:28,516] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437598883883926356525846_0004_m_000012_300: needsTaskCommit() Task attempt_202105151437598883883926356525846_0004_m_000012_300
[2021-05-15 11:38:28,516] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Task committer attempt_202105151437598883883926356525846_0004_m_000012_300: needsTaskCommit() Task attempt_202105151437598883883926356525846_0004_m_000012_300: duration 0:00.002s
[2021-05-15 11:38:28,517] {docker.py:276} INFO - 21/05/15 14:38:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598883883926356525846_0004_m_000012_300
[2021-05-15 11:38:28,520] {docker.py:276} INFO - 21/05/15 14:38:28 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-15 11:38:28,522] {docker.py:276} INFO - 21/05/15 14:38:28 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (bbe2d2545a9d, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:28,523] {docker.py:276} INFO - 21/05/15 14:38:28 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
[2021-05-15 11:38:28,523] {docker.py:276} INFO - 21/05/15 14:38:28 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 2857 ms on bbe2d2545a9d (executor driver) (14/200)
[2021-05-15 11:38:28,532] {docker.py:276} INFO - 21/05/15 14:38:28 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:28,533] {docker.py:276} INFO - 21/05/15 14:38:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:28,535] {docker.py:276} INFO - 21/05/15 14:38:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:28,536] {docker.py:276} INFO - 21/05/15 14:38:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:28,536] {docker.py:276} INFO - 21/05/15 14:38:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:28,536] {docker.py:276} INFO - 21/05/15 14:38:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595642659740882663320_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595642659740882663320_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595642659740882663320_0004}; taskId=attempt_202105151437595642659740882663320_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53be6544}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:28,537] {docker.py:276} INFO - 21/05/15 14:38:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:28,537] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437595642659740882663320_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595642659740882663320_0004_m_000017_305
[2021-05-15 11:38:28,541] {docker.py:276} INFO - 21/05/15 14:38:28 INFO StagingCommitter: Task committer attempt_202105151437595642659740882663320_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595642659740882663320_0004_m_000017_305 : duration 0:00.004s
[2021-05-15 11:38:29,664] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437598755432296952933914_0004_m_000014_302: needsTaskCommit() Task attempt_202105151437598755432296952933914_0004_m_000014_302
[2021-05-15 11:38:29,665] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Task committer attempt_202105151437598755432296952933914_0004_m_000014_302: needsTaskCommit() Task attempt_202105151437598755432296952933914_0004_m_000014_302: duration 0:00.002s
[2021-05-15 11:38:29,666] {docker.py:276} INFO - 21/05/15 14:38:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598755432296952933914_0004_m_000014_302
[2021-05-15 11:38:29,668] {docker.py:276} INFO - 21/05/15 14:38:29 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-15 11:38:29,668] {docker.py:276} INFO - 21/05/15 14:38:29 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (bbe2d2545a9d, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:29,669] {docker.py:276} INFO - 21/05/15 14:38:29 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
21/05/15 14:38:29 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 3098 ms on bbe2d2545a9d (executor driver) (15/200)
[2021-05-15 11:38:29,681] {docker.py:276} INFO - 21/05/15 14:38:29 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:29,681] {docker.py:276} INFO - 21/05/15 14:38:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:29,683] {docker.py:276} INFO - 21/05/15 14:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:29,684] {docker.py:276} INFO - 21/05/15 14:38:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:29,684] {docker.py:276} INFO - 21/05/15 14:38:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596471203820577663549_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596471203820577663549_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596471203820577663549_0004}; taskId=attempt_202105151437596471203820577663549_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b101c2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:29,684] {docker.py:276} INFO - 21/05/15 14:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:29,685] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437596471203820577663549_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596471203820577663549_0004_m_000018_306
[2021-05-15 11:38:29,688] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Task committer attempt_202105151437596471203820577663549_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596471203820577663549_0004_m_000018_306 : duration 0:00.005s
[2021-05-15 11:38:29,767] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437596002438435891057143_0004_m_000015_303: needsTaskCommit() Task attempt_202105151437596002438435891057143_0004_m_000015_303
21/05/15 14:38:29 INFO StagingCommitter: Task committer attempt_202105151437596002438435891057143_0004_m_000015_303: needsTaskCommit() Task attempt_202105151437596002438435891057143_0004_m_000015_303: duration 0:00.001s
[2021-05-15 11:38:29,767] {docker.py:276} INFO - 21/05/15 14:38:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596002438435891057143_0004_m_000015_303
[2021-05-15 11:38:29,768] {docker.py:276} INFO - 21/05/15 14:38:29 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 4544 bytes result sent to driver
[2021-05-15 11:38:29,769] {docker.py:276} INFO - 21/05/15 14:38:29 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (bbe2d2545a9d, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:29,770] {docker.py:276} INFO - 21/05/15 14:38:29 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-15 11:38:29,770] {docker.py:276} INFO - 21/05/15 14:38:29 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 2735 ms on bbe2d2545a9d (executor driver) (16/200)
[2021-05-15 11:38:29,780] {docker.py:276} INFO - 21/05/15 14:38:29 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:29,782] {docker.py:276} INFO - 21/05/15 14:38:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:29,783] {docker.py:276} INFO - 21/05/15 14:38:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592760631763824423539_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592760631763824423539_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592760631763824423539_0004}; taskId=attempt_202105151437592760631763824423539_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53356252}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:29,783] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437592760631763824423539_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592760631763824423539_0004_m_000019_307
[2021-05-15 11:38:29,788] {docker.py:276} INFO - 21/05/15 14:38:29 INFO StagingCommitter: Task committer attempt_202105151437592760631763824423539_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592760631763824423539_0004_m_000019_307 : duration 0:00.005s
[2021-05-15 11:38:30,698] {docker.py:276} INFO - 21/05/15 14:38:30 INFO StagingCommitter: Starting: Task committer attempt_202105151437595642659740882663320_0004_m_000017_305: needsTaskCommit() Task attempt_202105151437595642659740882663320_0004_m_000017_305
21/05/15 14:38:30 INFO StagingCommitter: Task committer attempt_202105151437595642659740882663320_0004_m_000017_305: needsTaskCommit() Task attempt_202105151437595642659740882663320_0004_m_000017_305: duration 0:00.002s
21/05/15 14:38:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595642659740882663320_0004_m_000017_305
[2021-05-15 11:38:30,700] {docker.py:276} INFO - 21/05/15 14:38:30 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-15 11:38:30,701] {docker.py:276} INFO - 21/05/15 14:38:30 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (bbe2d2545a9d, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:30,702] {docker.py:276} INFO - 21/05/15 14:38:30 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 2184 ms on bbe2d2545a9d (executor driver) (17/200)
[2021-05-15 11:38:30,703] {docker.py:276} INFO - 21/05/15 14:38:30 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-15 11:38:30,713] {docker.py:276} INFO - 21/05/15 14:38:30 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:30,715] {docker.py:276} INFO - 21/05/15 14:38:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:30,716] {docker.py:276} INFO - 21/05/15 14:38:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592551675571481311020_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592551675571481311020_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592551675571481311020_0004}; taskId=attempt_202105151437592551675571481311020_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d09fce5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:30 INFO StagingCommitter: Starting: Task committer attempt_202105151437592551675571481311020_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592551675571481311020_0004_m_000020_308
[2021-05-15 11:38:30,721] {docker.py:276} INFO - 21/05/15 14:38:30 INFO StagingCommitter: Task committer attempt_202105151437592551675571481311020_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592551675571481311020_0004_m_000020_308 : duration 0:00.005s
[2021-05-15 11:38:31,303] {docker.py:276} INFO - 21/05/15 14:38:31 INFO StagingCommitter: Starting: Task committer attempt_202105151437597005384220929796549_0004_m_000016_304: needsTaskCommit() Task attempt_202105151437597005384220929796549_0004_m_000016_304
[2021-05-15 11:38:31,304] {docker.py:276} INFO - 21/05/15 14:38:31 INFO StagingCommitter: Task committer attempt_202105151437597005384220929796549_0004_m_000016_304: needsTaskCommit() Task attempt_202105151437597005384220929796549_0004_m_000016_304: duration 0:00.004s
[2021-05-15 11:38:31,304] {docker.py:276} INFO - 21/05/15 14:38:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597005384220929796549_0004_m_000016_304
[2021-05-15 11:38:31,307] {docker.py:276} INFO - 21/05/15 14:38:31 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 4587 bytes result sent to driver
[2021-05-15 11:38:31,308] {docker.py:276} INFO - 21/05/15 14:38:31 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (bbe2d2545a9d, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:31,309] {docker.py:276} INFO - 21/05/15 14:38:31 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-15 11:38:31,310] {docker.py:276} INFO - 21/05/15 14:38:31 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 2876 ms on bbe2d2545a9d (executor driver) (18/200)
[2021-05-15 11:38:31,319] {docker.py:276} INFO - 21/05/15 14:38:31 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:31,321] {docker.py:276} INFO - 21/05/15 14:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596714923624666756357_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596714923624666756357_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596714923624666756357_0004}; taskId=attempt_202105151437596714923624666756357_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d5d3410}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:31,322] {docker.py:276} INFO - 21/05/15 14:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:31 INFO StagingCommitter: Starting: Task committer attempt_202105151437596714923624666756357_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596714923624666756357_0004_m_000021_309
[2021-05-15 11:38:31,326] {docker.py:276} INFO - 21/05/15 14:38:31 INFO StagingCommitter: Task committer attempt_202105151437596714923624666756357_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596714923624666756357_0004_m_000021_309 : duration 0:00.004s
[2021-05-15 11:38:32,008] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437596471203820577663549_0004_m_000018_306: needsTaskCommit() Task attempt_202105151437596471203820577663549_0004_m_000018_306
[2021-05-15 11:38:32,008] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Task committer attempt_202105151437596471203820577663549_0004_m_000018_306: needsTaskCommit() Task attempt_202105151437596471203820577663549_0004_m_000018_306: duration 0:00.002s
[2021-05-15 11:38:32,008] {docker.py:276} INFO - 21/05/15 14:38:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596471203820577663549_0004_m_000018_306
[2021-05-15 11:38:32,009] {docker.py:276} INFO - 21/05/15 14:38:32 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-15 11:38:32,010] {docker.py:276} INFO - 21/05/15 14:38:32 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (bbe2d2545a9d, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:32,012] {docker.py:276} INFO - 21/05/15 14:38:32 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 2347 ms on bbe2d2545a9d (executor driver) (19/200)
[2021-05-15 11:38:32,012] {docker.py:276} INFO - 21/05/15 14:38:32 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-15 11:38:32,020] {docker.py:276} INFO - 21/05/15 14:38:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:32,023] {docker.py:276} INFO - 21/05/15 14:38:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:32,023] {docker.py:276} INFO - 21/05/15 14:38:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592309476439092590200_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592309476439092590200_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592309476439092590200_0004}; taskId=attempt_202105151437592309476439092590200_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8bf9ba9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:32,023] {docker.py:276} INFO - 21/05/15 14:38:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437592309476439092590200_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592309476439092590200_0004_m_000022_310
[2021-05-15 11:38:32,026] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Task committer attempt_202105151437592309476439092590200_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592309476439092590200_0004_m_000022_310 : duration 0:00.004s
[2021-05-15 11:38:32,529] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437592760631763824423539_0004_m_000019_307: needsTaskCommit() Task attempt_202105151437592760631763824423539_0004_m_000019_307
[2021-05-15 11:38:32,530] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Task committer attempt_202105151437592760631763824423539_0004_m_000019_307: needsTaskCommit() Task attempt_202105151437592760631763824423539_0004_m_000019_307: duration 0:00.001s
21/05/15 14:38:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592760631763824423539_0004_m_000019_307
[2021-05-15 11:38:32,530] {docker.py:276} INFO - 21/05/15 14:38:32 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-15 11:38:32,531] {docker.py:276} INFO - 21/05/15 14:38:32 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (bbe2d2545a9d, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:32,532] {docker.py:276} INFO - 21/05/15 14:38:32 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-15 11:38:32,540] {docker.py:276} INFO - 21/05/15 14:38:32 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 2774 ms on bbe2d2545a9d (executor driver) (20/200)
[2021-05-15 11:38:32,541] {docker.py:276} INFO - 21/05/15 14:38:32 INFO ShuffleBlockFetcherIterator: Getting 5 (25.7 KiB) non-empty blocks including 5 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:32,544] {docker.py:276} INFO - 21/05/15 14:38:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:32,546] {docker.py:276} INFO - 21/05/15 14:38:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:32,547] {docker.py:276} INFO - 21/05/15 14:38:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592659413792812512626_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592659413792812512626_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592659413792812512626_0004}; taskId=attempt_202105151437592659413792812512626_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45ebf60d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:32,547] {docker.py:276} INFO - 21/05/15 14:38:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437592659413792812512626_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592659413792812512626_0004_m_000023_311
[2021-05-15 11:38:32,552] {docker.py:276} INFO - 21/05/15 14:38:32 INFO StagingCommitter: Task committer attempt_202105151437592659413792812512626_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592659413792812512626_0004_m_000023_311 : duration 0:00.005s
[2021-05-15 11:38:33,489] {docker.py:276} INFO - 21/05/15 14:38:33 INFO StagingCommitter: Starting: Task committer attempt_202105151437592551675571481311020_0004_m_000020_308: needsTaskCommit() Task attempt_202105151437592551675571481311020_0004_m_000020_308
[2021-05-15 11:38:33,491] {docker.py:276} INFO - 21/05/15 14:38:33 INFO StagingCommitter: Task committer attempt_202105151437592551675571481311020_0004_m_000020_308: needsTaskCommit() Task attempt_202105151437592551675571481311020_0004_m_000020_308: duration 0:00.002s
21/05/15 14:38:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592551675571481311020_0004_m_000020_308
[2021-05-15 11:38:33,492] {docker.py:276} INFO - 21/05/15 14:38:33 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 4544 bytes result sent to driver
[2021-05-15 11:38:33,494] {docker.py:276} INFO - 21/05/15 14:38:33 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (bbe2d2545a9d, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:33,495] {docker.py:276} INFO - 21/05/15 14:38:33 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
21/05/15 14:38:33 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 2797 ms on bbe2d2545a9d (executor driver) (21/200)
[2021-05-15 11:38:33,506] {docker.py:276} INFO - 21/05/15 14:38:33 INFO ShuffleBlockFetcherIterator: Getting 5 (26.4 KiB) non-empty blocks including 5 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:33,506] {docker.py:276} INFO - 21/05/15 14:38:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:33,508] {docker.py:276} INFO - 21/05/15 14:38:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:33,508] {docker.py:276} INFO - 21/05/15 14:38:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596636276131409637559_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596636276131409637559_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596636276131409637559_0004}; taskId=attempt_202105151437596636276131409637559_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@294895ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:33,509] {docker.py:276} INFO - 21/05/15 14:38:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:33,509] {docker.py:276} INFO - 21/05/15 14:38:33 INFO StagingCommitter: Starting: Task committer attempt_202105151437596636276131409637559_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596636276131409637559_0004_m_000024_312
[2021-05-15 11:38:33,514] {docker.py:276} INFO - 21/05/15 14:38:33 INFO StagingCommitter: Task committer attempt_202105151437596636276131409637559_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596636276131409637559_0004_m_000024_312 : duration 0:00.005s
[2021-05-15 11:38:34,237] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437592309476439092590200_0004_m_000022_310: needsTaskCommit() Task attempt_202105151437592309476439092590200_0004_m_000022_310
[2021-05-15 11:38:34,238] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_202105151437592309476439092590200_0004_m_000022_310: needsTaskCommit() Task attempt_202105151437592309476439092590200_0004_m_000022_310: duration 0:00.003s
21/05/15 14:38:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592309476439092590200_0004_m_000022_310
[2021-05-15 11:38:34,242] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-15 11:38:34,244] {docker.py:276} INFO - 21/05/15 14:38:34 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (bbe2d2545a9d, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:34,245] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
21/05/15 14:38:34 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 2237 ms on bbe2d2545a9d (executor driver) (22/200)
[2021-05-15 11:38:34,255] {docker.py:276} INFO - 21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:34,258] {docker.py:276} INFO - 21/05/15 14:38:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:34,258] {docker.py:276} INFO - 21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594230998084309307516_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594230998084309307516_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594230998084309307516_0004}; taskId=attempt_202105151437594230998084309307516_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5381cb9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437594230998084309307516_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594230998084309307516_0004_m_000025_313
[2021-05-15 11:38:34,262] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_202105151437594230998084309307516_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594230998084309307516_0004_m_000025_313 : duration 0:00.003s
[2021-05-15 11:38:34,586] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437596714923624666756357_0004_m_000021_309: needsTaskCommit() Task attempt_202105151437596714923624666756357_0004_m_000021_309
[2021-05-15 11:38:34,587] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_202105151437596714923624666756357_0004_m_000021_309: needsTaskCommit() Task attempt_202105151437596714923624666756357_0004_m_000021_309: duration 0:00.003s
21/05/15 14:38:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596714923624666756357_0004_m_000021_309
[2021-05-15 11:38:34,588] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-15 11:38:34,590] {docker.py:276} INFO - 21/05/15 14:38:34 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (bbe2d2545a9d, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:34,592] {docker.py:276} INFO - 21/05/15 14:38:34 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 3287 ms on bbe2d2545a9d (executor driver) (23/200)
[2021-05-15 11:38:34,593] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-15 11:38:34,606] {docker.py:276} INFO - 21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:34,607] {docker.py:276} INFO - 21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:34,609] {docker.py:276} INFO - 21/05/15 14:38:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:34,609] {docker.py:276} INFO - 21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:34,609] {docker.py:276} INFO - 21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759564002648315744158_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759564002648315744158_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759564002648315744158_0004}; taskId=attempt_20210515143759564002648315744158_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@728d9df8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_20210515143759564002648315744158_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759564002648315744158_0004_m_000026_314
[2021-05-15 11:38:34,613] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_20210515143759564002648315744158_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759564002648315744158_0004_m_000026_314 : duration 0:00.004s
[2021-05-15 11:38:34,775] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437592659413792812512626_0004_m_000023_311: needsTaskCommit() Task attempt_202105151437592659413792812512626_0004_m_000023_311
[2021-05-15 11:38:34,777] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_202105151437592659413792812512626_0004_m_000023_311: needsTaskCommit() Task attempt_202105151437592659413792812512626_0004_m_000023_311: duration 0:00.003s
[2021-05-15 11:38:34,777] {docker.py:276} INFO - 21/05/15 14:38:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592659413792812512626_0004_m_000023_311
[2021-05-15 11:38:34,779] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
[2021-05-15 11:38:34,781] {docker.py:276} INFO - 21/05/15 14:38:34 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (bbe2d2545a9d, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:34,782] {docker.py:276} INFO - 21/05/15 14:38:34 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
21/05/15 14:38:34 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 2253 ms on bbe2d2545a9d (executor driver) (24/200)
[2021-05-15 11:38:34,795] {docker.py:276} INFO - 21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:34,797] {docker.py:276} INFO - 21/05/15 14:38:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:34,797] {docker.py:276} INFO - 21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597973954583699337519_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597973954583699337519_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597973954583699337519_0004}; taskId=attempt_202105151437597973954583699337519_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52bf4096}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:34,798] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437597973954583699337519_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597973954583699337519_0004_m_000027_315
[2021-05-15 11:38:34,801] {docker.py:276} INFO - 21/05/15 14:38:34 INFO StagingCommitter: Task committer attempt_202105151437597973954583699337519_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597973954583699337519_0004_m_000027_315 : duration 0:00.004s
[2021-05-15 11:38:36,331] {docker.py:276} INFO - 21/05/15 14:38:36 INFO StagingCommitter: Starting: Task committer attempt_202105151437596636276131409637559_0004_m_000024_312: needsTaskCommit() Task attempt_202105151437596636276131409637559_0004_m_000024_312
21/05/15 14:38:36 INFO StagingCommitter: Task committer attempt_202105151437596636276131409637559_0004_m_000024_312: needsTaskCommit() Task attempt_202105151437596636276131409637559_0004_m_000024_312: duration 0:00.003s
21/05/15 14:38:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596636276131409637559_0004_m_000024_312
[2021-05-15 11:38:36,333] {docker.py:276} INFO - 21/05/15 14:38:36 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-15 11:38:36,334] {docker.py:276} INFO - 21/05/15 14:38:36 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (bbe2d2545a9d, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:36,335] {docker.py:276} INFO - 21/05/15 14:38:36 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 2846 ms on bbe2d2545a9d (executor driver) (25/200)
[2021-05-15 11:38:36,336] {docker.py:276} INFO - 21/05/15 14:38:36 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-15 11:38:36,347] {docker.py:276} INFO - 21/05/15 14:38:36 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:36,347] {docker.py:276} INFO - 21/05/15 14:38:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:38:36,350] {docker.py:276} INFO - 21/05/15 14:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:36,351] {docker.py:276} INFO - 21/05/15 14:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:36,351] {docker.py:276} INFO - 21/05/15 14:38:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:36,352] {docker.py:276} INFO - 21/05/15 14:38:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593447864381685769299_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593447864381685769299_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593447864381685769299_0004}; taskId=attempt_202105151437593447864381685769299_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60505538}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:36,352] {docker.py:276} INFO - 21/05/15 14:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:36 INFO StagingCommitter: Starting: Task committer attempt_202105151437593447864381685769299_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593447864381685769299_0004_m_000028_316
[2021-05-15 11:38:36,355] {docker.py:276} INFO - 21/05/15 14:38:36 INFO StagingCommitter: Task committer attempt_202105151437593447864381685769299_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593447864381685769299_0004_m_000028_316 : duration 0:00.003s
[2021-05-15 11:38:37,076] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437594230998084309307516_0004_m_000025_313: needsTaskCommit() Task attempt_202105151437594230998084309307516_0004_m_000025_313
[2021-05-15 11:38:37,077] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_202105151437594230998084309307516_0004_m_000025_313: needsTaskCommit() Task attempt_202105151437594230998084309307516_0004_m_000025_313: duration 0:00.002s
21/05/15 14:38:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594230998084309307516_0004_m_000025_313
[2021-05-15 11:38:37,079] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-15 11:38:37,081] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (bbe2d2545a9d, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:37,083] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 2842 ms on bbe2d2545a9d (executor driver) (26/200)
[2021-05-15 11:38:37,083] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-15 11:38:37,094] {docker.py:276} INFO - 21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Getting 5 (25.7 KiB) non-empty blocks including 5 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:37,097] {docker.py:276} INFO - 21/05/15 14:38:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:37,097] {docker.py:276} INFO - 21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596072999206628001177_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596072999206628001177_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596072999206628001177_0004}; taskId=attempt_202105151437596072999206628001177_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b5b9855}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:37,098] {docker.py:276} INFO - 21/05/15 14:38:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437596072999206628001177_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596072999206628001177_0004_m_000029_317
[2021-05-15 11:38:37,101] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_202105151437596072999206628001177_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596072999206628001177_0004_m_000029_317 : duration 0:00.003s
[2021-05-15 11:38:37,345] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_20210515143759564002648315744158_0004_m_000026_314: needsTaskCommit() Task attempt_20210515143759564002648315744158_0004_m_000026_314
[2021-05-15 11:38:37,346] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_20210515143759564002648315744158_0004_m_000026_314: needsTaskCommit() Task attempt_20210515143759564002648315744158_0004_m_000026_314: duration 0:00.003s
21/05/15 14:38:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759564002648315744158_0004_m_000026_314
[2021-05-15 11:38:37,347] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 4587 bytes result sent to driver
[2021-05-15 11:38:37,348] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (bbe2d2545a9d, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:37,349] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 2763 ms on bbe2d2545a9d (executor driver) (27/200)
[2021-05-15 11:38:37,350] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
[2021-05-15 11:38:37,359] {docker.py:276} INFO - 21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:37,362] {docker.py:276} INFO - 21/05/15 14:38:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:37,362] {docker.py:276} INFO - 21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:37,362] {docker.py:276} INFO - 21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597321076056292545026_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597321076056292545026_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597321076056292545026_0004}; taskId=attempt_202105151437597321076056292545026_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3348cb73}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:37,363] {docker.py:276} INFO - 21/05/15 14:38:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437597321076056292545026_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597321076056292545026_0004_m_000030_318
[2021-05-15 11:38:37,365] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_202105151437597321076056292545026_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597321076056292545026_0004_m_000030_318 : duration 0:00.002s
[2021-05-15 11:38:37,494] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437597973954583699337519_0004_m_000027_315: needsTaskCommit() Task attempt_202105151437597973954583699337519_0004_m_000027_315
[2021-05-15 11:38:37,495] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_202105151437597973954583699337519_0004_m_000027_315: needsTaskCommit() Task attempt_202105151437597973954583699337519_0004_m_000027_315: duration 0:00.002s
21/05/15 14:38:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597973954583699337519_0004_m_000027_315
[2021-05-15 11:38:37,496] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 4587 bytes result sent to driver
[2021-05-15 11:38:37,498] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (bbe2d2545a9d, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:37,498] {docker.py:276} INFO - 21/05/15 14:38:37 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
[2021-05-15 11:38:37,499] {docker.py:276} INFO - 21/05/15 14:38:37 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 2721 ms on bbe2d2545a9d (executor driver) (28/200)
[2021-05-15 11:38:37,507] {docker.py:276} INFO - 21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:37,508] {docker.py:276} INFO - 21/05/15 14:38:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:37,509] {docker.py:276} INFO - 21/05/15 14:38:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:37,510] {docker.py:276} INFO - 21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:37,510] {docker.py:276} INFO - 21/05/15 14:38:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593492622926634124751_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593492622926634124751_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593492622926634124751_0004}; taskId=attempt_202105151437593492622926634124751_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@787286a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:37,510] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437593492622926634124751_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593492622926634124751_0004_m_000031_319
[2021-05-15 11:38:37,513] {docker.py:276} INFO - 21/05/15 14:38:37 INFO StagingCommitter: Task committer attempt_202105151437593492622926634124751_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593492622926634124751_0004_m_000031_319 : duration 0:00.003s
[2021-05-15 11:38:39,169] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Starting: Task committer attempt_202105151437593447864381685769299_0004_m_000028_316: needsTaskCommit() Task attempt_202105151437593447864381685769299_0004_m_000028_316
[2021-05-15 11:38:39,169] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Task committer attempt_202105151437593447864381685769299_0004_m_000028_316: needsTaskCommit() Task attempt_202105151437593447864381685769299_0004_m_000028_316: duration 0:00.002s
[2021-05-15 11:38:39,170] {docker.py:276} INFO - 21/05/15 14:38:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593447864381685769299_0004_m_000028_316
[2021-05-15 11:38:39,172] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 4587 bytes result sent to driver
[2021-05-15 11:38:39,173] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (bbe2d2545a9d, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:39,175] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 2845 ms on bbe2d2545a9d (executor driver) (29/200)
[2021-05-15 11:38:39,175] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-15 11:38:39,185] {docker.py:276} INFO - 21/05/15 14:38:39 INFO ShuffleBlockFetcherIterator: Getting 5 (25.9 KiB) non-empty blocks including 5 (25.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:39,187] {docker.py:276} INFO - 21/05/15 14:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:39,188] {docker.py:276} INFO - 21/05/15 14:38:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:39,188] {docker.py:276} INFO - 21/05/15 14:38:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597065058841595238258_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597065058841595238258_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597065058841595238258_0004}; taskId=attempt_202105151437597065058841595238258_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ead3d01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:39 INFO StagingCommitter: Starting: Task committer attempt_202105151437597065058841595238258_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597065058841595238258_0004_m_000032_320
[2021-05-15 11:38:39,191] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Task committer attempt_202105151437597065058841595238258_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597065058841595238258_0004_m_000032_320 : duration 0:00.003s
[2021-05-15 11:38:39,760] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Starting: Task committer attempt_202105151437596072999206628001177_0004_m_000029_317: needsTaskCommit() Task attempt_202105151437596072999206628001177_0004_m_000029_317
[2021-05-15 11:38:39,760] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Task committer attempt_202105151437596072999206628001177_0004_m_000029_317: needsTaskCommit() Task attempt_202105151437596072999206628001177_0004_m_000029_317: duration 0:00.001s
21/05/15 14:38:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596072999206628001177_0004_m_000029_317
[2021-05-15 11:38:39,762] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 4587 bytes result sent to driver
[2021-05-15 11:38:39,763] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (bbe2d2545a9d, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:39,764] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-15 11:38:39,764] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 2686 ms on bbe2d2545a9d (executor driver) (30/200)
[2021-05-15 11:38:39,773] {docker.py:276} INFO - 21/05/15 14:38:39 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:39,773] {docker.py:276} INFO - 21/05/15 14:38:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:39,775] {docker.py:276} INFO - 21/05/15 14:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:39,776] {docker.py:276} INFO - 21/05/15 14:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:39,776] {docker.py:276} INFO - 21/05/15 14:38:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:39,776] {docker.py:276} INFO - 21/05/15 14:38:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594646500338581909537_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594646500338581909537_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594646500338581909537_0004}; taskId=attempt_202105151437594646500338581909537_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31ed55bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:39 INFO StagingCommitter: Starting: Task committer attempt_202105151437594646500338581909537_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594646500338581909537_0004_m_000033_321
[2021-05-15 11:38:39,778] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Task committer attempt_202105151437594646500338581909537_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594646500338581909537_0004_m_000033_321 : duration 0:00.003s
[2021-05-15 11:38:39,969] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Starting: Task committer attempt_202105151437597321076056292545026_0004_m_000030_318: needsTaskCommit() Task attempt_202105151437597321076056292545026_0004_m_000030_318
[2021-05-15 11:38:39,970] {docker.py:276} INFO - 21/05/15 14:38:39 INFO StagingCommitter: Task committer attempt_202105151437597321076056292545026_0004_m_000030_318: needsTaskCommit() Task attempt_202105151437597321076056292545026_0004_m_000030_318: duration 0:00.001s
21/05/15 14:38:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597321076056292545026_0004_m_000030_318
[2021-05-15 11:38:39,971] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
[2021-05-15 11:38:39,972] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (bbe2d2545a9d, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:39,973] {docker.py:276} INFO - 21/05/15 14:38:39 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-15 11:38:39,974] {docker.py:276} INFO - 21/05/15 14:38:39 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 2628 ms on bbe2d2545a9d (executor driver) (31/200)
[2021-05-15 11:38:39,983] {docker.py:276} INFO - 21/05/15 14:38:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:39,983] {docker.py:276} INFO - 21/05/15 14:38:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:39,986] {docker.py:276} INFO - 21/05/15 14:38:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:39,986] {docker.py:276} INFO - 21/05/15 14:38:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:39,987] {docker.py:276} INFO - 21/05/15 14:38:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592307988067888000271_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592307988067888000271_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592307988067888000271_0004}; taskId=attempt_202105151437592307988067888000271_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@441f7913}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437592307988067888000271_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592307988067888000271_0004_m_000034_322
[2021-05-15 11:38:39,990] {docker.py:276} INFO - 21/05/15 14:38:40 INFO StagingCommitter: Task committer attempt_202105151437592307988067888000271_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592307988067888000271_0004_m_000034_322 : duration 0:00.004s
[2021-05-15 11:38:40,096] {docker.py:276} INFO - 21/05/15 14:38:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437593492622926634124751_0004_m_000031_319: needsTaskCommit() Task attempt_202105151437593492622926634124751_0004_m_000031_319
[2021-05-15 11:38:40,097] {docker.py:276} INFO - 21/05/15 14:38:40 INFO StagingCommitter: Task committer attempt_202105151437593492622926634124751_0004_m_000031_319: needsTaskCommit() Task attempt_202105151437593492622926634124751_0004_m_000031_319: duration 0:00.002s
[2021-05-15 11:38:40,098] {docker.py:276} INFO - 21/05/15 14:38:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593492622926634124751_0004_m_000031_319
[2021-05-15 11:38:40,099] {docker.py:276} INFO - 21/05/15 14:38:40 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
[2021-05-15 11:38:40,101] {docker.py:276} INFO - 21/05/15 14:38:40 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (bbe2d2545a9d, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:40,103] {docker.py:276} INFO - 21/05/15 14:38:40 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
[2021-05-15 11:38:40,103] {docker.py:276} INFO - 21/05/15 14:38:40 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 2609 ms on bbe2d2545a9d (executor driver) (32/200)
[2021-05-15 11:38:40,113] {docker.py:276} INFO - 21/05/15 14:38:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:38:40,116] {docker.py:276} INFO - 21/05/15 14:38:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592614000003468608112_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592614000003468608112_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592614000003468608112_0004}; taskId=attempt_202105151437592614000003468608112_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d8b6432}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437592614000003468608112_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592614000003468608112_0004_m_000035_323
[2021-05-15 11:38:40,119] {docker.py:276} INFO - 21/05/15 14:38:40 INFO StagingCommitter: Task committer attempt_202105151437592614000003468608112_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592614000003468608112_0004_m_000035_323 : duration 0:00.003s
[2021-05-15 11:38:41,866] {docker.py:276} INFO - 21/05/15 14:38:41 INFO StagingCommitter: Starting: Task committer attempt_202105151437597065058841595238258_0004_m_000032_320: needsTaskCommit() Task attempt_202105151437597065058841595238258_0004_m_000032_320
[2021-05-15 11:38:41,868] {docker.py:276} INFO - 21/05/15 14:38:41 INFO StagingCommitter: Task committer attempt_202105151437597065058841595238258_0004_m_000032_320: needsTaskCommit() Task attempt_202105151437597065058841595238258_0004_m_000032_320: duration 0:00.008s
[2021-05-15 11:38:41,869] {docker.py:276} INFO - 21/05/15 14:38:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597065058841595238258_0004_m_000032_320
[2021-05-15 11:38:41,870] {docker.py:276} INFO - 21/05/15 14:38:41 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 4544 bytes result sent to driver
[2021-05-15 11:38:41,871] {docker.py:276} INFO - 21/05/15 14:38:41 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (bbe2d2545a9d, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:41,873] {docker.py:276} INFO - 21/05/15 14:38:41 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 2703 ms on bbe2d2545a9d (executor driver) (33/200)
[2021-05-15 11:38:41,874] {docker.py:276} INFO - 21/05/15 14:38:41 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-15 11:38:41,883] {docker.py:276} INFO - 21/05/15 14:38:41 INFO ShuffleBlockFetcherIterator: Getting 5 (25.1 KiB) non-empty blocks including 5 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:41,884] {docker.py:276} INFO - 21/05/15 14:38:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:41,886] {docker.py:276} INFO - 21/05/15 14:38:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:41,886] {docker.py:276} INFO - 21/05/15 14:38:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594156453509307250437_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594156453509307250437_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594156453509307250437_0004}; taskId=attempt_202105151437594156453509307250437_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b9c1661}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:41,886] {docker.py:276} INFO - 21/05/15 14:38:41 INFO StagingCommitter: Starting: Task committer attempt_202105151437594156453509307250437_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594156453509307250437_0004_m_000036_324
[2021-05-15 11:38:41,889] {docker.py:276} INFO - 21/05/15 14:38:41 INFO StagingCommitter: Task committer attempt_202105151437594156453509307250437_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594156453509307250437_0004_m_000036_324 : duration 0:00.003s
[2021-05-15 11:38:42,385] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437594646500338581909537_0004_m_000033_321: needsTaskCommit() Task attempt_202105151437594646500338581909537_0004_m_000033_321
[2021-05-15 11:38:42,386] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437594646500338581909537_0004_m_000033_321: needsTaskCommit() Task attempt_202105151437594646500338581909537_0004_m_000033_321: duration 0:00.002s
[2021-05-15 11:38:42,387] {docker.py:276} INFO - 21/05/15 14:38:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594646500338581909537_0004_m_000033_321
[2021-05-15 11:38:42,388] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 4544 bytes result sent to driver
[2021-05-15 11:38:42,390] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (bbe2d2545a9d, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:42,391] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 2631 ms on bbe2d2545a9d (executor driver) (34/200)
[2021-05-15 11:38:42,392] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
[2021-05-15 11:38:42,401] {docker.py:276} INFO - 21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:42,402] {docker.py:276} INFO - 21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:42,404] {docker.py:276} INFO - 21/05/15 14:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:42,404] {docker.py:276} INFO - 21/05/15 14:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:42,405] {docker.py:276} INFO - 21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:42,405] {docker.py:276} INFO - 21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594899594085135633962_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594899594085135633962_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594899594085135633962_0004}; taskId=attempt_202105151437594899594085135633962_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c335c29}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:42,405] {docker.py:276} INFO - 21/05/15 14:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:42,406] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437594899594085135633962_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594899594085135633962_0004_m_000037_325
[2021-05-15 11:38:42,408] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437594899594085135633962_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594899594085135633962_0004_m_000037_325 : duration 0:00.003s
[2021-05-15 11:38:42,670] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437592307988067888000271_0004_m_000034_322: needsTaskCommit() Task attempt_202105151437592307988067888000271_0004_m_000034_322
[2021-05-15 11:38:42,671] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437592307988067888000271_0004_m_000034_322: needsTaskCommit() Task attempt_202105151437592307988067888000271_0004_m_000034_322: duration 0:00.000s
[2021-05-15 11:38:42,672] {docker.py:276} INFO - 21/05/15 14:38:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592307988067888000271_0004_m_000034_322
[2021-05-15 11:38:42,673] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 4544 bytes result sent to driver
[2021-05-15 11:38:42,675] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (bbe2d2545a9d, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:42,676] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 2706 ms on bbe2d2545a9d (executor driver) (35/200)
[2021-05-15 11:38:42,676] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
[2021-05-15 11:38:42,686] {docker.py:276} INFO - 21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:42,687] {docker.py:276} INFO - 21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:42,689] {docker.py:276} INFO - 21/05/15 14:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:42,690] {docker.py:276} INFO - 21/05/15 14:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:42,690] {docker.py:276} INFO - 21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596285291571672370832_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596285291571672370832_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596285291571672370832_0004}; taskId=attempt_202105151437596285291571672370832_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6becc49c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:42,690] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437596285291571672370832_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596285291571672370832_0004_m_000038_326
[2021-05-15 11:38:42,694] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437596285291571672370832_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596285291571672370832_0004_m_000038_326 : duration 0:00.003s
[2021-05-15 11:38:42,925] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437592614000003468608112_0004_m_000035_323: needsTaskCommit() Task attempt_202105151437592614000003468608112_0004_m_000035_323
[2021-05-15 11:38:42,926] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437592614000003468608112_0004_m_000035_323: needsTaskCommit() Task attempt_202105151437592614000003468608112_0004_m_000035_323: duration 0:00.001s
21/05/15 14:38:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592614000003468608112_0004_m_000035_323
[2021-05-15 11:38:42,927] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-15 11:38:42,930] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (bbe2d2545a9d, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:42,931] {docker.py:276} INFO - 21/05/15 14:38:42 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-15 11:38:42,932] {docker.py:276} INFO - 21/05/15 14:38:42 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 2835 ms on bbe2d2545a9d (executor driver) (36/200)
[2021-05-15 11:38:42,942] {docker.py:276} INFO - 21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Getting 5 (24.6 KiB) non-empty blocks including 5 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:42,944] {docker.py:276} INFO - 21/05/15 14:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596175677488438555234_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596175677488438555234_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596175677488438555234_0004}; taskId=attempt_202105151437596175677488438555234_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59546345}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437596175677488438555234_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596175677488438555234_0004_m_000039_327
[2021-05-15 11:38:42,947] {docker.py:276} INFO - 21/05/15 14:38:42 INFO StagingCommitter: Task committer attempt_202105151437596175677488438555234_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596175677488438555234_0004_m_000039_327 : duration 0:00.003s
[2021-05-15 11:38:44,595] {docker.py:276} INFO - 21/05/15 14:38:44 INFO StagingCommitter: Starting: Task committer attempt_202105151437594156453509307250437_0004_m_000036_324: needsTaskCommit() Task attempt_202105151437594156453509307250437_0004_m_000036_324
[2021-05-15 11:38:44,596] {docker.py:276} INFO - 21/05/15 14:38:44 INFO StagingCommitter: Task committer attempt_202105151437594156453509307250437_0004_m_000036_324: needsTaskCommit() Task attempt_202105151437594156453509307250437_0004_m_000036_324: duration 0:00.000s
21/05/15 14:38:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594156453509307250437_0004_m_000036_324
[2021-05-15 11:38:44,599] {docker.py:276} INFO - 21/05/15 14:38:44 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 4544 bytes result sent to driver
[2021-05-15 11:38:44,600] {docker.py:276} INFO - 21/05/15 14:38:44 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (bbe2d2545a9d, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:44,601] {docker.py:276} INFO - 21/05/15 14:38:44 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-15 11:38:44,602] {docker.py:276} INFO - 21/05/15 14:38:44 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 2733 ms on bbe2d2545a9d (executor driver) (37/200)
[2021-05-15 11:38:44,610] {docker.py:276} INFO - 21/05/15 14:38:44 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:44,611] {docker.py:276} INFO - 21/05/15 14:38:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:44,613] {docker.py:276} INFO - 21/05/15 14:38:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:44,613] {docker.py:276} INFO - 21/05/15 14:38:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597574243953022301451_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597574243953022301451_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597574243953022301451_0004}; taskId=attempt_202105151437597574243953022301451_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2200cc44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:44,614] {docker.py:276} INFO - 21/05/15 14:38:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:44,614] {docker.py:276} INFO - 21/05/15 14:38:44 INFO StagingCommitter: Starting: Task committer attempt_202105151437597574243953022301451_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597574243953022301451_0004_m_000040_328
[2021-05-15 11:38:44,617] {docker.py:276} INFO - 21/05/15 14:38:44 INFO StagingCommitter: Task committer attempt_202105151437597574243953022301451_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597574243953022301451_0004_m_000040_328 : duration 0:00.003s
[2021-05-15 11:38:45,018] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437596285291571672370832_0004_m_000038_326: needsTaskCommit() Task attempt_202105151437596285291571672370832_0004_m_000038_326
21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437596285291571672370832_0004_m_000038_326: needsTaskCommit() Task attempt_202105151437596285291571672370832_0004_m_000038_326: duration 0:00.001s
21/05/15 14:38:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596285291571672370832_0004_m_000038_326
[2021-05-15 11:38:45,020] {docker.py:276} INFO - 21/05/15 14:38:45 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 4544 bytes result sent to driver
[2021-05-15 11:38:45,021] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (bbe2d2545a9d, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:45,022] {docker.py:276} INFO - 21/05/15 14:38:45 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-15 11:38:45,023] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 2351 ms on bbe2d2545a9d (executor driver) (38/200)
[2021-05-15 11:38:45,033] {docker.py:276} INFO - 21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:45,033] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437594899594085135633962_0004_m_000037_325: needsTaskCommit() Task attempt_202105151437594899594085135633962_0004_m_000037_325
[2021-05-15 11:38:45,033] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437594899594085135633962_0004_m_000037_325: needsTaskCommit() Task attempt_202105151437594899594085135633962_0004_m_000037_325: duration 0:00.001s
21/05/15 14:38:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594899594085135633962_0004_m_000037_325
[2021-05-15 11:38:45,036] {docker.py:276} INFO - 21/05/15 14:38:45 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 4544 bytes result sent to driver
[2021-05-15 11:38:45,037] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (bbe2d2545a9d, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:45,037] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 2651 ms on bbe2d2545a9d (executor driver) (39/200)
[2021-05-15 11:38:45,038] {docker.py:276} INFO - 21/05/15 14:38:45 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-15 11:38:45,038] {docker.py:276} INFO - 21/05/15 14:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:45,038] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:45,039] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595803550074446268722_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595803550074446268722_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595803550074446268722_0004}; taskId=attempt_202105151437595803550074446268722_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41a8f1a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:45,039] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437595803550074446268722_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595803550074446268722_0004_m_000041_329
[2021-05-15 11:38:45,055] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437595803550074446268722_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595803550074446268722_0004_m_000041_329 : duration 0:00.018s
[2021-05-15 11:38:45,060] {docker.py:276} INFO - 21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:45,062] {docker.py:276} INFO - 21/05/15 14:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:45,063] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:45,064] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596995187601752277563_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596995187601752277563_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596995187601752277563_0004}; taskId=attempt_202105151437596995187601752277563_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b3eb23d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:45,064] {docker.py:276} INFO - 21/05/15 14:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:45,064] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437596995187601752277563_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596995187601752277563_0004_m_000042_330
[2021-05-15 11:38:45,066] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437596995187601752277563_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596995187601752277563_0004_m_000042_330 : duration 0:00.003s
[2021-05-15 11:38:45,574] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437596175677488438555234_0004_m_000039_327: needsTaskCommit() Task attempt_202105151437596175677488438555234_0004_m_000039_327
[2021-05-15 11:38:45,575] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437596175677488438555234_0004_m_000039_327: needsTaskCommit() Task attempt_202105151437596175677488438555234_0004_m_000039_327: duration 0:00.000s
[2021-05-15 11:38:45,576] {docker.py:276} INFO - 21/05/15 14:38:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596175677488438555234_0004_m_000039_327
[2021-05-15 11:38:45,577] {docker.py:276} INFO - 21/05/15 14:38:45 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 4587 bytes result sent to driver
[2021-05-15 11:38:45,579] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (bbe2d2545a9d, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:45,579] {docker.py:276} INFO - 21/05/15 14:38:45 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 2654 ms on bbe2d2545a9d (executor driver) (40/200)
21/05/15 14:38:45 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
[2021-05-15 11:38:45,588] {docker.py:276} INFO - 21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:45,588] {docker.py:276} INFO - 21/05/15 14:38:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:45,590] {docker.py:276} INFO - 21/05/15 14:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:45,591] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:45,591] {docker.py:276} INFO - 21/05/15 14:38:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591520313787362170269_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591520313787362170269_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591520313787362170269_0004}; taskId=attempt_202105151437591520313787362170269_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39f432e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:45,591] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437591520313787362170269_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591520313787362170269_0004_m_000043_331
[2021-05-15 11:38:45,594] {docker.py:276} INFO - 21/05/15 14:38:45 INFO StagingCommitter: Task committer attempt_202105151437591520313787362170269_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591520313787362170269_0004_m_000043_331 : duration 0:00.003s
[2021-05-15 11:38:47,484] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437597574243953022301451_0004_m_000040_328: needsTaskCommit() Task attempt_202105151437597574243953022301451_0004_m_000040_328
[2021-05-15 11:38:47,486] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437597574243953022301451_0004_m_000040_328: needsTaskCommit() Task attempt_202105151437597574243953022301451_0004_m_000040_328: duration 0:00.001s
[2021-05-15 11:38:47,487] {docker.py:276} INFO - 21/05/15 14:38:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597574243953022301451_0004_m_000040_328
[2021-05-15 11:38:47,488] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 4587 bytes result sent to driver
[2021-05-15 11:38:47,489] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (bbe2d2545a9d, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:47,489] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 2892 ms on bbe2d2545a9d (executor driver) (41/200)
[2021-05-15 11:38:47,490] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-15 11:38:47,500] {docker.py:276} INFO - 21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:47,501] {docker.py:276} INFO - 21/05/15 14:38:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591398505367396440490_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591398505367396440490_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591398505367396440490_0004}; taskId=attempt_202105151437591398505367396440490_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a7a37c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437591398505367396440490_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591398505367396440490_0004_m_000044_332
[2021-05-15 11:38:47,505] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437591398505367396440490_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591398505367396440490_0004_m_000044_332 : duration 0:00.003s
[2021-05-15 11:38:47,639] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437596995187601752277563_0004_m_000042_330: needsTaskCommit() Task attempt_202105151437596995187601752277563_0004_m_000042_330
[2021-05-15 11:38:47,640] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437596995187601752277563_0004_m_000042_330: needsTaskCommit() Task attempt_202105151437596995187601752277563_0004_m_000042_330: duration 0:00.001s
21/05/15 14:38:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596995187601752277563_0004_m_000042_330
[2021-05-15 11:38:47,640] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 4587 bytes result sent to driver
[2021-05-15 11:38:47,642] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (bbe2d2545a9d, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:47,644] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 2611 ms on bbe2d2545a9d (executor driver) (42/200)
[2021-05-15 11:38:47,645] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-15 11:38:47,655] {docker.py:276} INFO - 21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:47,657] {docker.py:276} INFO - 21/05/15 14:38:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596359884151842628602_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596359884151842628602_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596359884151842628602_0004}; taskId=attempt_202105151437596359884151842628602_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@196840e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:47,657] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437596359884151842628602_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596359884151842628602_0004_m_000045_333
[2021-05-15 11:38:47,660] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437596359884151842628602_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596359884151842628602_0004_m_000045_333 : duration 0:00.003s
[2021-05-15 11:38:47,776] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437595803550074446268722_0004_m_000041_329: needsTaskCommit() Task attempt_202105151437595803550074446268722_0004_m_000041_329
[2021-05-15 11:38:47,777] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437595803550074446268722_0004_m_000041_329: needsTaskCommit() Task attempt_202105151437595803550074446268722_0004_m_000041_329: duration 0:00.001s
21/05/15 14:38:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595803550074446268722_0004_m_000041_329
[2021-05-15 11:38:47,779] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 4587 bytes result sent to driver
[2021-05-15 11:38:47,780] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (bbe2d2545a9d, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:47,782] {docker.py:276} INFO - 21/05/15 14:38:47 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 2764 ms on bbe2d2545a9d (executor driver) (43/200)
[2021-05-15 11:38:47,782] {docker.py:276} INFO - 21/05/15 14:38:47 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-15 11:38:47,792] {docker.py:276} INFO - 21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:47,793] {docker.py:276} INFO - 21/05/15 14:38:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:47,795] {docker.py:276} INFO - 21/05/15 14:38:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:47,795] {docker.py:276} INFO - 21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:47,796] {docker.py:276} INFO - 21/05/15 14:38:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597280678803537698299_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597280678803537698299_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597280678803537698299_0004}; taskId=attempt_202105151437597280678803537698299_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52e444e0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:47,796] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Starting: Task committer attempt_202105151437597280678803537698299_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597280678803537698299_0004_m_000046_334
[2021-05-15 11:38:47,799] {docker.py:276} INFO - 21/05/15 14:38:47 INFO StagingCommitter: Task committer attempt_202105151437597280678803537698299_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597280678803537698299_0004_m_000046_334 : duration 0:00.003s
[2021-05-15 11:38:48,165] {docker.py:276} INFO - 21/05/15 14:38:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437591520313787362170269_0004_m_000043_331: needsTaskCommit() Task attempt_202105151437591520313787362170269_0004_m_000043_331
[2021-05-15 11:38:48,166] {docker.py:276} INFO - 21/05/15 14:38:48 INFO StagingCommitter: Task committer attempt_202105151437591520313787362170269_0004_m_000043_331: needsTaskCommit() Task attempt_202105151437591520313787362170269_0004_m_000043_331: duration 0:00.002s
21/05/15 14:38:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591520313787362170269_0004_m_000043_331
[2021-05-15 11:38:48,168] {docker.py:276} INFO - 21/05/15 14:38:48 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 4544 bytes result sent to driver
[2021-05-15 11:38:48,170] {docker.py:276} INFO - 21/05/15 14:38:48 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (bbe2d2545a9d, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:48,172] {docker.py:276} INFO - 21/05/15 14:38:48 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 2596 ms on bbe2d2545a9d (executor driver) (44/200)
21/05/15 14:38:48 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-15 11:38:48,181] {docker.py:276} INFO - 21/05/15 14:38:48 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:48,182] {docker.py:276} INFO - 21/05/15 14:38:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595087990276797013423_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595087990276797013423_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595087990276797013423_0004}; taskId=attempt_202105151437595087990276797013423_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cbed728}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:48,183] {docker.py:276} INFO - 21/05/15 14:38:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437595087990276797013423_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595087990276797013423_0004_m_000047_335
[2021-05-15 11:38:48,185] {docker.py:276} INFO - 21/05/15 14:38:48 INFO StagingCommitter: Task committer attempt_202105151437595087990276797013423_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595087990276797013423_0004_m_000047_335 : duration 0:00.002s
[2021-05-15 11:38:50,322] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437591398505367396440490_0004_m_000044_332: needsTaskCommit() Task attempt_202105151437591398505367396440490_0004_m_000044_332
[2021-05-15 11:38:50,324] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437591398505367396440490_0004_m_000044_332: needsTaskCommit() Task attempt_202105151437591398505367396440490_0004_m_000044_332: duration 0:00.000s
21/05/15 14:38:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591398505367396440490_0004_m_000044_332
[2021-05-15 11:38:50,325] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 4544 bytes result sent to driver
[2021-05-15 11:38:50,326] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (bbe2d2545a9d, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:50,328] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 2809 ms on bbe2d2545a9d (executor driver) (45/200)
[2021-05-15 11:38:50,328] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
[2021-05-15 11:38:50,338] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:38:50,342] {docker.py:276} INFO - 21/05/15 14:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:50,343] {docker.py:276} INFO - 21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598749678227230276486_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598749678227230276486_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598749678227230276486_0004}; taskId=attempt_202105151437598749678227230276486_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cbfcf85}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:50,343] {docker.py:276} INFO - 21/05/15 14:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437598749678227230276486_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598749678227230276486_0004_m_000048_336
[2021-05-15 11:38:50,346] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437598749678227230276486_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598749678227230276486_0004_m_000048_336 : duration 0:00.003s
[2021-05-15 11:38:50,469] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437597280678803537698299_0004_m_000046_334: needsTaskCommit() Task attempt_202105151437597280678803537698299_0004_m_000046_334
[2021-05-15 11:38:50,470] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437596359884151842628602_0004_m_000045_333: needsTaskCommit() Task attempt_202105151437596359884151842628602_0004_m_000045_333
[2021-05-15 11:38:50,471] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437597280678803537698299_0004_m_000046_334: needsTaskCommit() Task attempt_202105151437597280678803537698299_0004_m_000046_334: duration 0:00.001s
[2021-05-15 11:38:50,472] {docker.py:276} INFO - 21/05/15 14:38:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597280678803537698299_0004_m_000046_334
[2021-05-15 11:38:50,473] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437596359884151842628602_0004_m_000045_333: needsTaskCommit() Task attempt_202105151437596359884151842628602_0004_m_000045_333: duration 0:00.002s
21/05/15 14:38:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596359884151842628602_0004_m_000045_333
[2021-05-15 11:38:50,474] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 4544 bytes result sent to driver
[2021-05-15 11:38:50,474] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 4544 bytes result sent to driver
[2021-05-15 11:38:50,475] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (bbe2d2545a9d, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:50,477] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (bbe2d2545a9d, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:50,478] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-15 11:38:50,478] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
[2021-05-15 11:38:50,479] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 2668 ms on bbe2d2545a9d (executor driver) (46/200)
[2021-05-15 11:38:50,480] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 2806 ms on bbe2d2545a9d (executor driver) (47/200)
[2021-05-15 11:38:50,489] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Getting 5 (26.0 KiB) non-empty blocks including 5 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:50,489] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:50,490] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:50,490] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:50,492] {docker.py:276} INFO - 21/05/15 14:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:50,493] {docker.py:276} INFO - 21/05/15 14:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:50,493] {docker.py:276} INFO - 21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595523817290052987223_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595523817290052987223_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595523817290052987223_0004}; taskId=attempt_202105151437595523817290052987223_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@769e2c12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437595523817290052987223_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595523817290052987223_0004_m_000050_338
[2021-05-15 11:38:50,494] {docker.py:276} INFO - 21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595839385075416433251_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595839385075416433251_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595839385075416433251_0004}; taskId=attempt_202105151437595839385075416433251_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f0f560}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:50,494] {docker.py:276} INFO - 21/05/15 14:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437595839385075416433251_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595839385075416433251_0004_m_000049_337
[2021-05-15 11:38:50,496] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437595523817290052987223_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595523817290052987223_0004_m_000050_338 : duration 0:00.003s
[2021-05-15 11:38:50,496] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437595839385075416433251_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595839385075416433251_0004_m_000049_337 : duration 0:00.002s
[2021-05-15 11:38:50,524] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437595087990276797013423_0004_m_000047_335: needsTaskCommit() Task attempt_202105151437595087990276797013423_0004_m_000047_335
[2021-05-15 11:38:50,525] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437595087990276797013423_0004_m_000047_335: needsTaskCommit() Task attempt_202105151437595087990276797013423_0004_m_000047_335: duration 0:00.000s
21/05/15 14:38:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595087990276797013423_0004_m_000047_335
[2021-05-15 11:38:50,526] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-15 11:38:50,527] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (bbe2d2545a9d, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:50,528] {docker.py:276} INFO - 21/05/15 14:38:50 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-15 11:38:50,528] {docker.py:276} INFO - 21/05/15 14:38:50 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 2327 ms on bbe2d2545a9d (executor driver) (48/200)
[2021-05-15 11:38:50,536] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:50,537] {docker.py:276} INFO - 21/05/15 14:38:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:50,538] {docker.py:276} INFO - 21/05/15 14:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:50,538] {docker.py:276} INFO - 21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593209653760244223800_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593209653760244223800_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593209653760244223800_0004}; taskId=attempt_202105151437593209653760244223800_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43b6ea88}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:50,539] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Starting: Task committer attempt_202105151437593209653760244223800_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593209653760244223800_0004_m_000051_339
[2021-05-15 11:38:50,543] {docker.py:276} INFO - 21/05/15 14:38:50 INFO StagingCommitter: Task committer attempt_202105151437593209653760244223800_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593209653760244223800_0004_m_000051_339 : duration 0:00.005s
[2021-05-15 11:38:53,099] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437595839385075416433251_0004_m_000049_337: needsTaskCommit() Task attempt_202105151437595839385075416433251_0004_m_000049_337
[2021-05-15 11:38:53,100] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437595839385075416433251_0004_m_000049_337: needsTaskCommit() Task attempt_202105151437595839385075416433251_0004_m_000049_337: duration 0:00.001s
21/05/15 14:38:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595839385075416433251_0004_m_000049_337
[2021-05-15 11:38:53,103] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-15 11:38:53,104] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (bbe2d2545a9d, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:53,105] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 2633 ms on bbe2d2545a9d (executor driver) (49/200)
[2021-05-15 11:38:53,106] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-15 11:38:53,115] {docker.py:276} INFO - 21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:53,118] {docker.py:276} INFO - 21/05/15 14:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596712409520928547223_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596712409520928547223_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596712409520928547223_0004}; taskId=attempt_202105151437596712409520928547223_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d1b6894}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:53,118] {docker.py:276} INFO - 21/05/15 14:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437596712409520928547223_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596712409520928547223_0004_m_000052_340
[2021-05-15 11:38:53,121] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437596712409520928547223_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596712409520928547223_0004_m_000052_340 : duration 0:00.004s
[2021-05-15 11:38:53,202] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437593209653760244223800_0004_m_000051_339: needsTaskCommit() Task attempt_202105151437593209653760244223800_0004_m_000051_339
21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437593209653760244223800_0004_m_000051_339: needsTaskCommit() Task attempt_202105151437593209653760244223800_0004_m_000051_339: duration 0:00.001s
21/05/15 14:38:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593209653760244223800_0004_m_000051_339
[2021-05-15 11:38:53,204] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-15 11:38:53,205] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (bbe2d2545a9d, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:53,207] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 2682 ms on bbe2d2545a9d (executor driver) (50/200)
[2021-05-15 11:38:53,208] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-15 11:38:53,217] {docker.py:276} INFO - 21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:53,220] {docker.py:276} INFO - 21/05/15 14:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:53,221] {docker.py:276} INFO - 21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596781153742051961851_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596781153742051961851_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596781153742051961851_0004}; taskId=attempt_202105151437596781153742051961851_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a66cf6f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:53,221] {docker.py:276} INFO - 21/05/15 14:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437596781153742051961851_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596781153742051961851_0004_m_000053_341
[2021-05-15 11:38:53,234] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437596781153742051961851_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596781153742051961851_0004_m_000053_341 : duration 0:00.013s
[2021-05-15 11:38:53,323] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437595523817290052987223_0004_m_000050_338: needsTaskCommit() Task attempt_202105151437595523817290052987223_0004_m_000050_338
[2021-05-15 11:38:53,324] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437595523817290052987223_0004_m_000050_338: needsTaskCommit() Task attempt_202105151437595523817290052987223_0004_m_000050_338: duration 0:00.000s
21/05/15 14:38:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595523817290052987223_0004_m_000050_338
[2021-05-15 11:38:53,325] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 4587 bytes result sent to driver
[2021-05-15 11:38:53,326] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (bbe2d2545a9d, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:53,328] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-15 11:38:53,329] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 2856 ms on bbe2d2545a9d (executor driver) (51/200)
[2021-05-15 11:38:53,337] {docker.py:276} INFO - 21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Getting 5 (24.6 KiB) non-empty blocks including 5 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:53,339] {docker.py:276} INFO - 21/05/15 14:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:53,340] {docker.py:276} INFO - 21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591866736722261841394_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591866736722261841394_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591866736722261841394_0004}; taskId=attempt_202105151437591866736722261841394_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2feeaf81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437591866736722261841394_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591866736722261841394_0004_m_000054_342
[2021-05-15 11:38:53,343] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437591866736722261841394_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591866736722261841394_0004_m_000054_342 : duration 0:00.003s
[2021-05-15 11:38:53,623] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437598749678227230276486_0004_m_000048_336: needsTaskCommit() Task attempt_202105151437598749678227230276486_0004_m_000048_336
[2021-05-15 11:38:53,623] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437598749678227230276486_0004_m_000048_336: needsTaskCommit() Task attempt_202105151437598749678227230276486_0004_m_000048_336: duration 0:00.001s
21/05/15 14:38:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598749678227230276486_0004_m_000048_336
[2021-05-15 11:38:53,624] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 4587 bytes result sent to driver
[2021-05-15 11:38:53,626] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (bbe2d2545a9d, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:53,627] {docker.py:276} INFO - 21/05/15 14:38:53 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 3304 ms on bbe2d2545a9d (executor driver) (52/200)
[2021-05-15 11:38:53,627] {docker.py:276} INFO - 21/05/15 14:38:53 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-15 11:38:53,636] {docker.py:276} INFO - 21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:53,636] {docker.py:276} INFO - 21/05/15 14:38:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:53,638] {docker.py:276} INFO - 21/05/15 14:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:53,639] {docker.py:276} INFO - 21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:53,639] {docker.py:276} INFO - 21/05/15 14:38:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591548245656209939733_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591548245656209939733_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591548245656209939733_0004}; taskId=attempt_202105151437591548245656209939733_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@663a0603}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:53,639] {docker.py:276} INFO - 21/05/15 14:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:53 INFO StagingCommitter: Starting: Task committer attempt_202105151437591548245656209939733_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591548245656209939733_0004_m_000055_343
[2021-05-15 11:38:53,641] {docker.py:276} INFO - 21/05/15 14:38:53 INFO StagingCommitter: Task committer attempt_202105151437591548245656209939733_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591548245656209939733_0004_m_000055_343 : duration 0:00.003s
[2021-05-15 11:38:55,868] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Starting: Task committer attempt_202105151437596712409520928547223_0004_m_000052_340: needsTaskCommit() Task attempt_202105151437596712409520928547223_0004_m_000052_340
[2021-05-15 11:38:55,869] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Task committer attempt_202105151437596712409520928547223_0004_m_000052_340: needsTaskCommit() Task attempt_202105151437596712409520928547223_0004_m_000052_340: duration 0:00.001s
21/05/15 14:38:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596712409520928547223_0004_m_000052_340
[2021-05-15 11:38:55,871] {docker.py:276} INFO - 21/05/15 14:38:55 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 4587 bytes result sent to driver
[2021-05-15 11:38:55,872] {docker.py:276} INFO - 21/05/15 14:38:55 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (bbe2d2545a9d, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:55,873] {docker.py:276} INFO - 21/05/15 14:38:55 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 2772 ms on bbe2d2545a9d (executor driver) (53/200)
[2021-05-15 11:38:55,874] {docker.py:276} INFO - 21/05/15 14:38:55 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-15 11:38:55,883] {docker.py:276} INFO - 21/05/15 14:38:55 INFO ShuffleBlockFetcherIterator: Getting 5 (23.9 KiB) non-empty blocks including 5 (23.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:55,885] {docker.py:276} INFO - 21/05/15 14:38:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595621297672474691284_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595621297672474691284_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595621297672474691284_0004}; taskId=attempt_202105151437595621297672474691284_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1818e36b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:55,885] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Starting: Task committer attempt_202105151437595621297672474691284_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595621297672474691284_0004_m_000056_344
[2021-05-15 11:38:55,888] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Task committer attempt_202105151437595621297672474691284_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595621297672474691284_0004_m_000056_344 : duration 0:00.003s
[2021-05-15 11:38:55,967] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Starting: Task committer attempt_202105151437596781153742051961851_0004_m_000053_341: needsTaskCommit() Task attempt_202105151437596781153742051961851_0004_m_000053_341
[2021-05-15 11:38:55,968] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Task committer attempt_202105151437596781153742051961851_0004_m_000053_341: needsTaskCommit() Task attempt_202105151437596781153742051961851_0004_m_000053_341: duration 0:00.001s
21/05/15 14:38:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596781153742051961851_0004_m_000053_341
[2021-05-15 11:38:55,969] {docker.py:276} INFO - 21/05/15 14:38:55 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 4587 bytes result sent to driver
[2021-05-15 11:38:55,971] {docker.py:276} INFO - 21/05/15 14:38:55 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (bbe2d2545a9d, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:55,972] {docker.py:276} INFO - 21/05/15 14:38:55 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
21/05/15 14:38:55 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 2770 ms on bbe2d2545a9d (executor driver) (54/200)
[2021-05-15 11:38:55,981] {docker.py:276} INFO - 21/05/15 14:38:55 INFO ShuffleBlockFetcherIterator: Getting 5 (25.7 KiB) non-empty blocks including 5 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:55,983] {docker.py:276} INFO - 21/05/15 14:38:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597893478781051983726_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597893478781051983726_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597893478781051983726_0004}; taskId=attempt_202105151437597893478781051983726_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11722468}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:55 INFO StagingCommitter: Starting: Task committer attempt_202105151437597893478781051983726_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597893478781051983726_0004_m_000057_345
[2021-05-15 11:38:55,986] {docker.py:276} INFO - 21/05/15 14:38:55 INFO StagingCommitter: Task committer attempt_202105151437597893478781051983726_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597893478781051983726_0004_m_000057_345 : duration 0:00.003s
[2021-05-15 11:38:56,144] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437591548245656209939733_0004_m_000055_343: needsTaskCommit() Task attempt_202105151437591548245656209939733_0004_m_000055_343
[2021-05-15 11:38:56,145] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Task committer attempt_202105151437591548245656209939733_0004_m_000055_343: needsTaskCommit() Task attempt_202105151437591548245656209939733_0004_m_000055_343: duration 0:00.001s
21/05/15 14:38:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591548245656209939733_0004_m_000055_343
[2021-05-15 11:38:56,147] {docker.py:276} INFO - 21/05/15 14:38:56 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 4544 bytes result sent to driver
[2021-05-15 11:38:56,148] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437591866736722261841394_0004_m_000054_342: needsTaskCommit() Task attempt_202105151437591866736722261841394_0004_m_000054_342
21/05/15 14:38:56 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (bbe2d2545a9d, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:56,150] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Task committer attempt_202105151437591866736722261841394_0004_m_000054_342: needsTaskCommit() Task attempt_202105151437591866736722261841394_0004_m_000054_342: duration 0:00.001s
21/05/15 14:38:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591866736722261841394_0004_m_000054_342
[2021-05-15 11:38:56,151] {docker.py:276} INFO - 21/05/15 14:38:56 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 2528 ms on bbe2d2545a9d (executor driver) (55/200)
21/05/15 14:38:56 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-15 11:38:56,155] {docker.py:276} INFO - 21/05/15 14:38:56 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 4544 bytes result sent to driver
[2021-05-15 11:38:56,156] {docker.py:276} INFO - 21/05/15 14:38:56 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (bbe2d2545a9d, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:56,158] {docker.py:276} INFO - 21/05/15 14:38:56 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 2835 ms on bbe2d2545a9d (executor driver) (56/200)
[2021-05-15 11:38:56,159] {docker.py:276} INFO - 21/05/15 14:38:56 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-15 11:38:56,167] {docker.py:276} INFO - 21/05/15 14:38:56 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:56,168] {docker.py:276} INFO - 21/05/15 14:38:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:56,170] {docker.py:276} INFO - 21/05/15 14:38:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:56,170] {docker.py:276} INFO - 21/05/15 14:38:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:56,170] {docker.py:276} INFO - 21/05/15 14:38:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:56,171] {docker.py:276} INFO - 21/05/15 14:38:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599206448013592999763_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599206448013592999763_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599206448013592999763_0004}; taskId=attempt_202105151437599206448013592999763_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12d2f6ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:56 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:56,171] {docker.py:276} INFO - 21/05/15 14:38:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 14:38:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437599206448013592999763_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599206448013592999763_0004_m_000058_346
[2021-05-15 11:38:56,173] {docker.py:276} INFO - 21/05/15 14:38:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:56,174] {docker.py:276} INFO - 21/05/15 14:38:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595022442071239410755_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595022442071239410755_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595022442071239410755_0004}; taskId=attempt_202105151437595022442071239410755_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d23342c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:56,174] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437595022442071239410755_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595022442071239410755_0004_m_000059_347
[2021-05-15 11:38:56,175] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Task committer attempt_202105151437599206448013592999763_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599206448013592999763_0004_m_000058_346 : duration 0:00.003s
[2021-05-15 11:38:56,177] {docker.py:276} INFO - 21/05/15 14:38:56 INFO StagingCommitter: Task committer attempt_202105151437595022442071239410755_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595022442071239410755_0004_m_000059_347 : duration 0:00.004s
[2021-05-15 11:38:58,684] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437597893478781051983726_0004_m_000057_345: needsTaskCommit() Task attempt_202105151437597893478781051983726_0004_m_000057_345
21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437595621297672474691284_0004_m_000056_344: needsTaskCommit() Task attempt_202105151437595621297672474691284_0004_m_000056_344
21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437597893478781051983726_0004_m_000057_345: needsTaskCommit() Task attempt_202105151437597893478781051983726_0004_m_000057_345: duration 0:00.001s
21/05/15 14:38:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597893478781051983726_0004_m_000057_345
[2021-05-15 11:38:58,685] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437595621297672474691284_0004_m_000056_344: needsTaskCommit() Task attempt_202105151437595621297672474691284_0004_m_000056_344: duration 0:00.000s
21/05/15 14:38:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595621297672474691284_0004_m_000056_344
[2021-05-15 11:38:58,686] {docker.py:276} INFO - 21/05/15 14:38:58 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 4544 bytes result sent to driver
21/05/15 14:38:58 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 4544 bytes result sent to driver
[2021-05-15 11:38:58,687] {docker.py:276} INFO - 21/05/15 14:38:58 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (bbe2d2545a9d, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:58,689] {docker.py:276} INFO - 21/05/15 14:38:58 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
21/05/15 14:38:58 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (bbe2d2545a9d, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:38:58 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 2722 ms on bbe2d2545a9d (executor driver) (57/200)
21/05/15 14:38:58 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-15 11:38:58,691] {docker.py:276} INFO - 21/05/15 14:38:58 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 2822 ms on bbe2d2545a9d (executor driver) (58/200)
[2021-05-15 11:38:58,699] {docker.py:276} INFO - 21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:58,700] {docker.py:276} INFO - 21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Getting 5 (23.6 KiB) non-empty blocks including 5 (23.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:38:58,701] {docker.py:276} INFO - 21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:58,701] {docker.py:276} INFO - 21/05/15 14:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:58,702] {docker.py:276} INFO - 21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:58,702] {docker.py:276} INFO - 21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598408828282400149720_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598408828282400149720_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598408828282400149720_0004}; taskId=attempt_202105151437598408828282400149720_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@dc4eed8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437598408828282400149720_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598408828282400149720_0004_m_000061_349
[2021-05-15 11:38:58,703] {docker.py:276} INFO - 21/05/15 14:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:38:58,703] {docker.py:276} INFO - 21/05/15 14:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:38:58,703] {docker.py:276} INFO - 21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:58,704] {docker.py:276} INFO - 21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593275310430845155252_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593275310430845155252_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593275310430845155252_0004}; taskId=attempt_202105151437593275310430845155252_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c5870e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:58,704] {docker.py:276} INFO - 21/05/15 14:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:58,704] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437593275310430845155252_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593275310430845155252_0004_m_000060_348
[2021-05-15 11:38:58,705] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437598408828282400149720_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598408828282400149720_0004_m_000061_349 : duration 0:00.003s
[2021-05-15 11:38:58,707] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437593275310430845155252_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593275310430845155252_0004_m_000060_348 : duration 0:00.004s
[2021-05-15 11:38:58,829] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437599206448013592999763_0004_m_000058_346: needsTaskCommit() Task attempt_202105151437599206448013592999763_0004_m_000058_346
[2021-05-15 11:38:58,830] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437599206448013592999763_0004_m_000058_346: needsTaskCommit() Task attempt_202105151437599206448013592999763_0004_m_000058_346: duration 0:00.001s
[2021-05-15 11:38:58,831] {docker.py:276} INFO - 21/05/15 14:38:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599206448013592999763_0004_m_000058_346
[2021-05-15 11:38:58,832] {docker.py:276} INFO - 21/05/15 14:38:58 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 4544 bytes result sent to driver
[2021-05-15 11:38:58,835] {docker.py:276} INFO - 21/05/15 14:38:58 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (bbe2d2545a9d, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:58,836] {docker.py:276} INFO - 21/05/15 14:38:58 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 2690 ms on bbe2d2545a9d (executor driver) (59/200)
21/05/15 14:38:58 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-15 11:38:58,845] {docker.py:276} INFO - 21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:58,847] {docker.py:276} INFO - 21/05/15 14:38:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:38:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593618213957289563215_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593618213957289563215_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593618213957289563215_0004}; taskId=attempt_202105151437593618213957289563215_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a5e045f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:58,847] {docker.py:276} INFO - 21/05/15 14:38:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437593618213957289563215_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593618213957289563215_0004_m_000062_350
[2021-05-15 11:38:58,850] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437593618213957289563215_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593618213957289563215_0004_m_000062_350 : duration 0:00.003s
[2021-05-15 11:38:58,986] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Starting: Task committer attempt_202105151437595022442071239410755_0004_m_000059_347: needsTaskCommit() Task attempt_202105151437595022442071239410755_0004_m_000059_347
[2021-05-15 11:38:58,987] {docker.py:276} INFO - 21/05/15 14:38:58 INFO StagingCommitter: Task committer attempt_202105151437595022442071239410755_0004_m_000059_347: needsTaskCommit() Task attempt_202105151437595022442071239410755_0004_m_000059_347: duration 0:00.001s
21/05/15 14:38:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595022442071239410755_0004_m_000059_347
[2021-05-15 11:38:58,989] {docker.py:276} INFO - 21/05/15 14:38:58 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 4544 bytes result sent to driver
[2021-05-15 11:38:58,991] {docker.py:276} INFO - 21/05/15 14:38:59 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (bbe2d2545a9d, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:38:58,992] {docker.py:276} INFO - 21/05/15 14:38:59 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 2839 ms on bbe2d2545a9d (executor driver) (60/200)
21/05/15 14:38:59 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
[2021-05-15 11:38:59,002] {docker.py:276} INFO - 21/05/15 14:38:59 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:38:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:38:59,004] {docker.py:276} INFO - 21/05/15 14:38:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:38:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:38:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:38:59,005] {docker.py:276} INFO - 21/05/15 14:38:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598248203162042063136_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598248203162042063136_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598248203162042063136_0004}; taskId=attempt_202105151437598248203162042063136_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@668349a4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:38:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:38:59,005] {docker.py:276} INFO - 21/05/15 14:38:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437598248203162042063136_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598248203162042063136_0004_m_000063_351
[2021-05-15 11:38:59,008] {docker.py:276} INFO - 21/05/15 14:38:59 INFO StagingCommitter: Task committer attempt_202105151437598248203162042063136_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598248203162042063136_0004_m_000063_351 : duration 0:00.003s
[2021-05-15 11:39:01,333] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437593275310430845155252_0004_m_000060_348: needsTaskCommit() Task attempt_202105151437593275310430845155252_0004_m_000060_348
[2021-05-15 11:39:01,334] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437593275310430845155252_0004_m_000060_348: needsTaskCommit() Task attempt_202105151437593275310430845155252_0004_m_000060_348: duration 0:00.002s
21/05/15 14:39:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593275310430845155252_0004_m_000060_348
[2021-05-15 11:39:01,337] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 4544 bytes result sent to driver
[2021-05-15 11:39:01,338] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (bbe2d2545a9d, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:01,339] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 2655 ms on bbe2d2545a9d (executor driver) (61/200)
[2021-05-15 11:39:01,341] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-15 11:39:01,349] {docker.py:276} INFO - 21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Getting 5 (22.2 KiB) non-empty blocks including 5 (22.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:01,351] {docker.py:276} INFO - 21/05/15 14:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:01,352] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598753084018364685393_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598753084018364685393_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598753084018364685393_0004}; taskId=attempt_202105151437598753084018364685393_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27068948}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:01,352] {docker.py:276} INFO - 21/05/15 14:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437598753084018364685393_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598753084018364685393_0004_m_000064_352
[2021-05-15 11:39:01,355] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437598753084018364685393_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598753084018364685393_0004_m_000064_352 : duration 0:00.003s
[2021-05-15 11:39:01,365] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437598408828282400149720_0004_m_000061_349: needsTaskCommit() Task attempt_202105151437598408828282400149720_0004_m_000061_349
[2021-05-15 11:39:01,366] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437598408828282400149720_0004_m_000061_349: needsTaskCommit() Task attempt_202105151437598408828282400149720_0004_m_000061_349: duration 0:00.001s
21/05/15 14:39:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598408828282400149720_0004_m_000061_349
[2021-05-15 11:39:01,367] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-15 11:39:01,368] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (bbe2d2545a9d, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:01,369] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 2684 ms on bbe2d2545a9d (executor driver) (62/200)
[2021-05-15 11:39:01,369] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-15 11:39:01,376] {docker.py:276} INFO - 21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:01,378] {docker.py:276} INFO - 21/05/15 14:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:01,378] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591224495292142500648_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224495292142500648_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591224495292142500648_0004}; taskId=attempt_202105151437591224495292142500648_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a168d2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:01,379] {docker.py:276} INFO - 21/05/15 14:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:01,379] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437591224495292142500648_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224495292142500648_0004_m_000065_353
[2021-05-15 11:39:01,382] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437591224495292142500648_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224495292142500648_0004_m_000065_353 : duration 0:00.003s
[2021-05-15 11:39:01,559] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437593618213957289563215_0004_m_000062_350: needsTaskCommit() Task attempt_202105151437593618213957289563215_0004_m_000062_350
[2021-05-15 11:39:01,559] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437593618213957289563215_0004_m_000062_350: needsTaskCommit() Task attempt_202105151437593618213957289563215_0004_m_000062_350: duration 0:00.001s
[2021-05-15 11:39:01,560] {docker.py:276} INFO - 21/05/15 14:39:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593618213957289563215_0004_m_000062_350
[2021-05-15 11:39:01,560] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-15 11:39:01,561] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (bbe2d2545a9d, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:01,562] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-15 11:39:01,563] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 2732 ms on bbe2d2545a9d (executor driver) (63/200)
[2021-05-15 11:39:01,567] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437598248203162042063136_0004_m_000063_351: needsTaskCommit() Task attempt_202105151437598248203162042063136_0004_m_000063_351
[2021-05-15 11:39:01,567] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437598248203162042063136_0004_m_000063_351: needsTaskCommit() Task attempt_202105151437598248203162042063136_0004_m_000063_351: duration 0:00.000s
21/05/15 14:39:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598248203162042063136_0004_m_000063_351
[2021-05-15 11:39:01,568] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-15 11:39:01,569] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (bbe2d2545a9d, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:01,569] {docker.py:276} INFO - 21/05/15 14:39:01 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-15 11:39:01,570] {docker.py:276} INFO - 21/05/15 14:39:01 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 2583 ms on bbe2d2545a9d (executor driver) (64/200)
[2021-05-15 11:39:01,584] {docker.py:276} INFO - 21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Getting 5 (25.9 KiB) non-empty blocks including 5 (25.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:01,586] {docker.py:276} INFO - 21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:01,587] {docker.py:276} INFO - 21/05/15 14:39:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:01,587] {docker.py:276} INFO - 21/05/15 14:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:39:01,587] {docker.py:276} INFO - 21/05/15 14:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:01,587] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:01,588] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596286066364857555104_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596286066364857555104_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596286066364857555104_0004}; taskId=attempt_202105151437596286066364857555104_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c67316d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:01,588] {docker.py:276} INFO - 21/05/15 14:39:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:01,588] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:01,589] {docker.py:276} INFO - 21/05/15 14:39:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591741033054352216051_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591741033054352216051_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591741033054352216051_0004}; taskId=attempt_202105151437591741033054352216051_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63bb33a2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:01,589] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437591741033054352216051_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591741033054352216051_0004_m_000067_355
[2021-05-15 11:39:01,589] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Starting: Task committer attempt_202105151437596286066364857555104_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596286066364857555104_0004_m_000066_354
[2021-05-15 11:39:01,591] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437591741033054352216051_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591741033054352216051_0004_m_000067_355 : duration 0:00.002s
[2021-05-15 11:39:01,594] {docker.py:276} INFO - 21/05/15 14:39:01 INFO StagingCommitter: Task committer attempt_202105151437596286066364857555104_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596286066364857555104_0004_m_000066_354 : duration 0:00.006s
[2021-05-15 11:39:03,926] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Starting: Task committer attempt_202105151437591224495292142500648_0004_m_000065_353: needsTaskCommit() Task attempt_202105151437591224495292142500648_0004_m_000065_353
[2021-05-15 11:39:03,927] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Task committer attempt_202105151437591224495292142500648_0004_m_000065_353: needsTaskCommit() Task attempt_202105151437591224495292142500648_0004_m_000065_353: duration 0:00.001s
21/05/15 14:39:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591224495292142500648_0004_m_000065_353
[2021-05-15 11:39:03,931] {docker.py:276} INFO - 21/05/15 14:39:03 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 4587 bytes result sent to driver
[2021-05-15 11:39:03,933] {docker.py:276} INFO - 21/05/15 14:39:03 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (bbe2d2545a9d, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:03,933] {docker.py:276} INFO - 21/05/15 14:39:03 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-15 11:39:03,934] {docker.py:276} INFO - 21/05/15 14:39:03 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 2569 ms on bbe2d2545a9d (executor driver) (65/200)
[2021-05-15 11:39:03,938] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Starting: Task committer attempt_202105151437598753084018364685393_0004_m_000064_352: needsTaskCommit() Task attempt_202105151437598753084018364685393_0004_m_000064_352
[2021-05-15 11:39:03,938] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Task committer attempt_202105151437598753084018364685393_0004_m_000064_352: needsTaskCommit() Task attempt_202105151437598753084018364685393_0004_m_000064_352: duration 0:00.000s
21/05/15 14:39:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598753084018364685393_0004_m_000064_352
[2021-05-15 11:39:03,940] {docker.py:276} INFO - 21/05/15 14:39:03 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 4587 bytes result sent to driver
[2021-05-15 11:39:03,941] {docker.py:276} INFO - 21/05/15 14:39:03 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (bbe2d2545a9d, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:03,942] {docker.py:276} INFO - 21/05/15 14:39:03 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 2608 ms on bbe2d2545a9d (executor driver) (66/200)
[2021-05-15 11:39:03,942] {docker.py:276} INFO - 21/05/15 14:39:03 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-15 11:39:03,944] {docker.py:276} INFO - 21/05/15 14:39:03 INFO ShuffleBlockFetcherIterator: Getting 5 (25.4 KiB) non-empty blocks including 5 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:03,945] {docker.py:276} INFO - 21/05/15 14:39:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598387898761704930703_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598387898761704930703_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598387898761704930703_0004}; taskId=attempt_202105151437598387898761704930703_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3488917f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:03,946] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Starting: Task committer attempt_202105151437598387898761704930703_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598387898761704930703_0004_m_000068_356
[2021-05-15 11:39:03,948] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Task committer attempt_202105151437598387898761704930703_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598387898761704930703_0004_m_000068_356 : duration 0:00.002s
[2021-05-15 11:39:03,951] {docker.py:276} INFO - 21/05/15 14:39:03 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:03,951] {docker.py:276} INFO - 21/05/15 14:39:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:03,953] {docker.py:276} INFO - 21/05/15 14:39:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:03,953] {docker.py:276} INFO - 21/05/15 14:39:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759902758075763077767_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759902758075763077767_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759902758075763077767_0004}; taskId=attempt_20210515143759902758075763077767_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5015fee4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:03,954] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Starting: Task committer attempt_20210515143759902758075763077767_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759902758075763077767_0004_m_000069_357
[2021-05-15 11:39:03,956] {docker.py:276} INFO - 21/05/15 14:39:03 INFO StagingCommitter: Task committer attempt_20210515143759902758075763077767_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759902758075763077767_0004_m_000069_357 : duration 0:00.003s
[2021-05-15 11:39:04,333] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Starting: Task committer attempt_202105151437591741033054352216051_0004_m_000067_355: needsTaskCommit() Task attempt_202105151437591741033054352216051_0004_m_000067_355
[2021-05-15 11:39:04,334] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Task committer attempt_202105151437591741033054352216051_0004_m_000067_355: needsTaskCommit() Task attempt_202105151437591741033054352216051_0004_m_000067_355: duration 0:00.001s
21/05/15 14:39:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591741033054352216051_0004_m_000067_355
[2021-05-15 11:39:04,338] {docker.py:276} INFO - 21/05/15 14:39:04 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 4587 bytes result sent to driver
[2021-05-15 11:39:04,339] {docker.py:276} INFO - 21/05/15 14:39:04 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (bbe2d2545a9d, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:04,340] {docker.py:276} INFO - 21/05/15 14:39:04 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-15 11:39:04,342] {docker.py:276} INFO - 21/05/15 14:39:04 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 2776 ms on bbe2d2545a9d (executor driver) (67/200)
[2021-05-15 11:39:04,350] {docker.py:276} INFO - 21/05/15 14:39:04 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:04,352] {docker.py:276} INFO - 21/05/15 14:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:04,353] {docker.py:276} INFO - 21/05/15 14:39:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:04,353] {docker.py:276} INFO - 21/05/15 14:39:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759483973699337644118_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759483973699337644118_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759483973699337644118_0004}; taskId=attempt_20210515143759483973699337644118_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47972e44}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:04,354] {docker.py:276} INFO - 21/05/15 14:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:04,354] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Starting: Task committer attempt_20210515143759483973699337644118_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759483973699337644118_0004_m_000070_358
[2021-05-15 11:39:04,356] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Task committer attempt_20210515143759483973699337644118_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759483973699337644118_0004_m_000070_358 : duration 0:00.003s
[2021-05-15 11:39:04,403] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Starting: Task committer attempt_202105151437596286066364857555104_0004_m_000066_354: needsTaskCommit() Task attempt_202105151437596286066364857555104_0004_m_000066_354
[2021-05-15 11:39:04,404] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Task committer attempt_202105151437596286066364857555104_0004_m_000066_354: needsTaskCommit() Task attempt_202105151437596286066364857555104_0004_m_000066_354: duration 0:00.001s
21/05/15 14:39:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596286066364857555104_0004_m_000066_354
[2021-05-15 11:39:04,407] {docker.py:276} INFO - 21/05/15 14:39:04 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 4587 bytes result sent to driver
[2021-05-15 11:39:04,407] {docker.py:276} INFO - 21/05/15 14:39:04 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (bbe2d2545a9d, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:04,408] {docker.py:276} INFO - 21/05/15 14:39:04 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
[2021-05-15 11:39:04,409] {docker.py:276} INFO - 21/05/15 14:39:04 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 2852 ms on bbe2d2545a9d (executor driver) (68/200)
[2021-05-15 11:39:04,418] {docker.py:276} INFO - 21/05/15 14:39:04 INFO ShuffleBlockFetcherIterator: Getting 5 (26.4 KiB) non-empty blocks including 5 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:04,419] {docker.py:276} INFO - 21/05/15 14:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:04,420] {docker.py:276} INFO - 21/05/15 14:39:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596693808051658380642_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596693808051658380642_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596693808051658380642_0004}; taskId=attempt_202105151437596693808051658380642_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dcd6413}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:04,420] {docker.py:276} INFO - 21/05/15 14:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:04 INFO StagingCommitter: Starting: Task committer attempt_202105151437596693808051658380642_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596693808051658380642_0004_m_000071_359
[2021-05-15 11:39:04,422] {docker.py:276} INFO - 21/05/15 14:39:04 INFO StagingCommitter: Task committer attempt_202105151437596693808051658380642_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596693808051658380642_0004_m_000071_359 : duration 0:00.003s
[2021-05-15 11:39:06,602] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Starting: Task committer attempt_20210515143759902758075763077767_0004_m_000069_357: needsTaskCommit() Task attempt_20210515143759902758075763077767_0004_m_000069_357
[2021-05-15 11:39:06,603] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Task committer attempt_20210515143759902758075763077767_0004_m_000069_357: needsTaskCommit() Task attempt_20210515143759902758075763077767_0004_m_000069_357: duration 0:00.001s
21/05/15 14:39:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759902758075763077767_0004_m_000069_357
[2021-05-15 11:39:06,604] {docker.py:276} INFO - 21/05/15 14:39:06 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 4544 bytes result sent to driver
[2021-05-15 11:39:06,605] {docker.py:276} INFO - 21/05/15 14:39:06 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (bbe2d2545a9d, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:06,606] {docker.py:276} INFO - 21/05/15 14:39:06 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-15 11:39:06,607] {docker.py:276} INFO - 21/05/15 14:39:06 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 2668 ms on bbe2d2545a9d (executor driver) (69/200)
[2021-05-15 11:39:06,616] {docker.py:276} INFO - 21/05/15 14:39:06 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:06,618] {docker.py:276} INFO - 21/05/15 14:39:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:06,618] {docker.py:276} INFO - 21/05/15 14:39:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596118854746899258005_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596118854746899258005_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596118854746899258005_0004}; taskId=attempt_202105151437596118854746899258005_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34fa988e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:06,619] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Starting: Task committer attempt_202105151437596118854746899258005_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596118854746899258005_0004_m_000072_360
[2021-05-15 11:39:06,621] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Task committer attempt_202105151437596118854746899258005_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596118854746899258005_0004_m_000072_360 : duration 0:00.003s
[2021-05-15 11:39:06,652] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Starting: Task committer attempt_202105151437598387898761704930703_0004_m_000068_356: needsTaskCommit() Task attempt_202105151437598387898761704930703_0004_m_000068_356
[2021-05-15 11:39:06,653] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Task committer attempt_202105151437598387898761704930703_0004_m_000068_356: needsTaskCommit() Task attempt_202105151437598387898761704930703_0004_m_000068_356: duration 0:00.000s
21/05/15 14:39:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598387898761704930703_0004_m_000068_356
[2021-05-15 11:39:06,654] {docker.py:276} INFO - 21/05/15 14:39:06 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 4544 bytes result sent to driver
[2021-05-15 11:39:06,655] {docker.py:276} INFO - 21/05/15 14:39:06 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (bbe2d2545a9d, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:06,656] {docker.py:276} INFO - 21/05/15 14:39:06 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 2727 ms on bbe2d2545a9d (executor driver) (70/200)
[2021-05-15 11:39:06,656] {docker.py:276} INFO - 21/05/15 14:39:06 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-15 11:39:06,664] {docker.py:276} INFO - 21/05/15 14:39:06 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:06,665] {docker.py:276} INFO - 21/05/15 14:39:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:06,666] {docker.py:276} INFO - 21/05/15 14:39:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:06,666] {docker.py:276} INFO - 21/05/15 14:39:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598522830027073322295_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598522830027073322295_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598522830027073322295_0004}; taskId=attempt_202105151437598522830027073322295_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e6f5f2c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:06 INFO StagingCommitter: Starting: Task committer attempt_202105151437598522830027073322295_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598522830027073322295_0004_m_000073_361
[2021-05-15 11:39:06,669] {docker.py:276} INFO - 21/05/15 14:39:06 INFO StagingCommitter: Task committer attempt_202105151437598522830027073322295_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598522830027073322295_0004_m_000073_361 : duration 0:00.003s
[2021-05-15 11:39:07,095] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Starting: Task committer attempt_20210515143759483973699337644118_0004_m_000070_358: needsTaskCommit() Task attempt_20210515143759483973699337644118_0004_m_000070_358
[2021-05-15 11:39:07,096] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Task committer attempt_20210515143759483973699337644118_0004_m_000070_358: needsTaskCommit() Task attempt_20210515143759483973699337644118_0004_m_000070_358: duration 0:00.002s
21/05/15 14:39:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759483973699337644118_0004_m_000070_358
[2021-05-15 11:39:07,098] {docker.py:276} INFO - 21/05/15 14:39:07 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 4544 bytes result sent to driver
[2021-05-15 11:39:07,099] {docker.py:276} INFO - 21/05/15 14:39:07 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (bbe2d2545a9d, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:07,100] {docker.py:276} INFO - 21/05/15 14:39:07 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-15 11:39:07,101] {docker.py:276} INFO - 21/05/15 14:39:07 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 2765 ms on bbe2d2545a9d (executor driver) (71/200)
[2021-05-15 11:39:07,109] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Starting: Task committer attempt_202105151437596693808051658380642_0004_m_000071_359: needsTaskCommit() Task attempt_202105151437596693808051658380642_0004_m_000071_359
[2021-05-15 11:39:07,109] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Task committer attempt_202105151437596693808051658380642_0004_m_000071_359: needsTaskCommit() Task attempt_202105151437596693808051658380642_0004_m_000071_359: duration 0:00.001s
21/05/15 14:39:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596693808051658380642_0004_m_000071_359
[2021-05-15 11:39:07,110] {docker.py:276} INFO - 21/05/15 14:39:07 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 4544 bytes result sent to driver
[2021-05-15 11:39:07,111] {docker.py:276} INFO - 21/05/15 14:39:07 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (bbe2d2545a9d, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:07,112] {docker.py:276} INFO - 21/05/15 14:39:07 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:07,112] {docker.py:276} INFO - 21/05/15 14:39:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:39:07,112] {docker.py:276} INFO - 21/05/15 14:39:07 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 2708 ms on bbe2d2545a9d (executor driver) (72/200)
[2021-05-15 11:39:07,113] {docker.py:276} INFO - 21/05/15 14:39:07 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-15 11:39:07,115] {docker.py:276} INFO - 21/05/15 14:39:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:07,115] {docker.py:276} INFO - 21/05/15 14:39:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:07,116] {docker.py:276} INFO - 21/05/15 14:39:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759569533720486682222_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759569533720486682222_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759569533720486682222_0004}; taskId=attempt_20210515143759569533720486682222_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@688d7838}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:07,116] {docker.py:276} INFO - 21/05/15 14:39:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:07,116] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Starting: Task committer attempt_20210515143759569533720486682222_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759569533720486682222_0004_m_000074_362
[2021-05-15 11:39:07,119] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Task committer attempt_20210515143759569533720486682222_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759569533720486682222_0004_m_000074_362 : duration 0:00.004s
[2021-05-15 11:39:07,122] {docker.py:276} INFO - 21/05/15 14:39:07 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:07,125] {docker.py:276} INFO - 21/05/15 14:39:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:07,126] {docker.py:276} INFO - 21/05/15 14:39:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591369516038298941177_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591369516038298941177_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591369516038298941177_0004}; taskId=attempt_202105151437591369516038298941177_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c0888e5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:07,126] {docker.py:276} INFO - 21/05/15 14:39:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:07 INFO StagingCommitter: Starting: Task committer attempt_202105151437591369516038298941177_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591369516038298941177_0004_m_000075_363
[2021-05-15 11:39:07,129] {docker.py:276} INFO - 21/05/15 14:39:07 INFO StagingCommitter: Task committer attempt_202105151437591369516038298941177_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591369516038298941177_0004_m_000075_363 : duration 0:00.003s
[2021-05-15 11:39:09,392] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437598522830027073322295_0004_m_000073_361: needsTaskCommit() Task attempt_202105151437598522830027073322295_0004_m_000073_361
[2021-05-15 11:39:09,393] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437598522830027073322295_0004_m_000073_361: needsTaskCommit() Task attempt_202105151437598522830027073322295_0004_m_000073_361: duration 0:00.001s
[2021-05-15 11:39:09,394] {docker.py:276} INFO - 21/05/15 14:39:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598522830027073322295_0004_m_000073_361
[2021-05-15 11:39:09,395] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 4544 bytes result sent to driver
[2021-05-15 11:39:09,397] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (bbe2d2545a9d, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:09,398] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 2746 ms on bbe2d2545a9d (executor driver) (73/200)
[2021-05-15 11:39:09,399] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
[2021-05-15 11:39:09,408] {docker.py:276} INFO - 21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:09,409] {docker.py:276} INFO - 21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:39:09,412] {docker.py:276} INFO - 21/05/15 14:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:09,412] {docker.py:276} INFO - 21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:09,413] {docker.py:276} INFO - 21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598113613518988915461_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598113613518988915461_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598113613518988915461_0004}; taskId=attempt_202105151437598113613518988915461_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7af67290}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:09,413] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437598113613518988915461_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598113613518988915461_0004_m_000076_364
[2021-05-15 11:39:09,416] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437598113613518988915461_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598113613518988915461_0004_m_000076_364 : duration 0:00.003s
[2021-05-15 11:39:09,442] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437596118854746899258005_0004_m_000072_360: needsTaskCommit() Task attempt_202105151437596118854746899258005_0004_m_000072_360
[2021-05-15 11:39:09,443] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437596118854746899258005_0004_m_000072_360: needsTaskCommit() Task attempt_202105151437596118854746899258005_0004_m_000072_360: duration 0:00.000s
21/05/15 14:39:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596118854746899258005_0004_m_000072_360
[2021-05-15 11:39:09,443] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 4544 bytes result sent to driver
[2021-05-15 11:39:09,444] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (bbe2d2545a9d, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:09,445] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 2843 ms on bbe2d2545a9d (executor driver) (74/200)
[2021-05-15 11:39:09,445] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
[2021-05-15 11:39:09,453] {docker.py:276} INFO - 21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Getting 5 (26.0 KiB) non-empty blocks including 5 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:09,455] {docker.py:276} INFO - 21/05/15 14:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591975272556721977922_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591975272556721977922_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591975272556721977922_0004}; taskId=attempt_202105151437591975272556721977922_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b23f9fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:09,455] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437591975272556721977922_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591975272556721977922_0004_m_000077_365
[2021-05-15 11:39:09,457] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437591975272556721977922_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591975272556721977922_0004_m_000077_365 : duration 0:00.003s
[2021-05-15 11:39:09,775] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437591369516038298941177_0004_m_000075_363: needsTaskCommit() Task attempt_202105151437591369516038298941177_0004_m_000075_363
[2021-05-15 11:39:09,776] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437591369516038298941177_0004_m_000075_363: needsTaskCommit() Task attempt_202105151437591369516038298941177_0004_m_000075_363: duration 0:00.000s
21/05/15 14:39:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591369516038298941177_0004_m_000075_363
[2021-05-15 11:39:09,777] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 4544 bytes result sent to driver
[2021-05-15 11:39:09,778] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (bbe2d2545a9d, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:09,779] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-15 11:39:09,781] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 2672 ms on bbe2d2545a9d (executor driver) (75/200)
[2021-05-15 11:39:09,789] {docker.py:276} INFO - 21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:09,792] {docker.py:276} INFO - 21/05/15 14:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:09,792] {docker.py:276} INFO - 21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597680483244759385275_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597680483244759385275_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597680483244759385275_0004}; taskId=attempt_202105151437597680483244759385275_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e50dec6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:09,792] {docker.py:276} INFO - 21/05/15 14:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437597680483244759385275_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597680483244759385275_0004_m_000078_366
[2021-05-15 11:39:09,795] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437597680483244759385275_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597680483244759385275_0004_m_000078_366 : duration 0:00.003s
[2021-05-15 11:39:09,855] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_20210515143759569533720486682222_0004_m_000074_362: needsTaskCommit() Task attempt_20210515143759569533720486682222_0004_m_000074_362
[2021-05-15 11:39:09,857] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_20210515143759569533720486682222_0004_m_000074_362: needsTaskCommit() Task attempt_20210515143759569533720486682222_0004_m_000074_362: duration 0:00.002s
[2021-05-15 11:39:09,857] {docker.py:276} INFO - 21/05/15 14:39:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759569533720486682222_0004_m_000074_362
[2021-05-15 11:39:09,859] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 4544 bytes result sent to driver
[2021-05-15 11:39:09,860] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (bbe2d2545a9d, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:09,861] {docker.py:276} INFO - 21/05/15 14:39:09 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 2766 ms on bbe2d2545a9d (executor driver) (76/200)
[2021-05-15 11:39:09,862] {docker.py:276} INFO - 21/05/15 14:39:09 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
[2021-05-15 11:39:09,871] {docker.py:276} INFO - 21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:09,873] {docker.py:276} INFO - 21/05/15 14:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:09,873] {docker.py:276} INFO - 21/05/15 14:39:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593606638569606210646_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593606638569606210646_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593606638569606210646_0004}; taskId=attempt_202105151437593606638569606210646_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bd5a33d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:09 INFO StagingCommitter: Starting: Task committer attempt_202105151437593606638569606210646_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593606638569606210646_0004_m_000079_367
[2021-05-15 11:39:09,877] {docker.py:276} INFO - 21/05/15 14:39:09 INFO StagingCommitter: Task committer attempt_202105151437593606638569606210646_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593606638569606210646_0004_m_000079_367 : duration 0:00.004s
[2021-05-15 11:39:12,125] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437591975272556721977922_0004_m_000077_365: needsTaskCommit() Task attempt_202105151437591975272556721977922_0004_m_000077_365
[2021-05-15 11:39:12,126] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437591975272556721977922_0004_m_000077_365: needsTaskCommit() Task attempt_202105151437591975272556721977922_0004_m_000077_365: duration 0:00.001s
21/05/15 14:39:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591975272556721977922_0004_m_000077_365
[2021-05-15 11:39:12,128] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 4587 bytes result sent to driver
[2021-05-15 11:39:12,129] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (bbe2d2545a9d, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:12,131] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 2690 ms on bbe2d2545a9d (executor driver) (77/200)
[2021-05-15 11:39:12,132] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-15 11:39:12,142] {docker.py:276} INFO - 21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:12,144] {docker.py:276} INFO - 21/05/15 14:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:39:12,145] {docker.py:276} INFO - 21/05/15 14:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:12,145] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759689991815061885659_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759689991815061885659_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759689991815061885659_0004}; taskId=attempt_20210515143759689991815061885659_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76f20181}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:12,146] {docker.py:276} INFO - 21/05/15 14:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:12,146] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_20210515143759689991815061885659_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759689991815061885659_0004_m_000080_368
[2021-05-15 11:39:12,148] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_20210515143759689991815061885659_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759689991815061885659_0004_m_000080_368 : duration 0:00.003s
[2021-05-15 11:39:12,267] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437598113613518988915461_0004_m_000076_364: needsTaskCommit() Task attempt_202105151437598113613518988915461_0004_m_000076_364
[2021-05-15 11:39:12,268] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437598113613518988915461_0004_m_000076_364: needsTaskCommit() Task attempt_202105151437598113613518988915461_0004_m_000076_364: duration 0:00.001s
21/05/15 14:39:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598113613518988915461_0004_m_000076_364
[2021-05-15 11:39:12,270] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-15 11:39:12,271] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (bbe2d2545a9d, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:12,273] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-15 11:39:12,273] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 2880 ms on bbe2d2545a9d (executor driver) (78/200)
[2021-05-15 11:39:12,283] {docker.py:276} INFO - 21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:12,285] {docker.py:276} INFO - 21/05/15 14:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:12,285] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597043532952370874772_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597043532952370874772_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597043532952370874772_0004}; taskId=attempt_202105151437597043532952370874772_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@609077dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:12,286] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437597043532952370874772_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597043532952370874772_0004_m_000081_369
[2021-05-15 11:39:12,289] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437597043532952370874772_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597043532952370874772_0004_m_000081_369 : duration 0:00.004s
[2021-05-15 11:39:12,469] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437597680483244759385275_0004_m_000078_366: needsTaskCommit() Task attempt_202105151437597680483244759385275_0004_m_000078_366
[2021-05-15 11:39:12,470] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437597680483244759385275_0004_m_000078_366: needsTaskCommit() Task attempt_202105151437597680483244759385275_0004_m_000078_366: duration 0:00.001s
21/05/15 14:39:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597680483244759385275_0004_m_000078_366
[2021-05-15 11:39:12,471] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 4587 bytes result sent to driver
[2021-05-15 11:39:12,472] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (bbe2d2545a9d, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:12,473] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-15 11:39:12,474] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 2699 ms on bbe2d2545a9d (executor driver) (79/200)
[2021-05-15 11:39:12,482] {docker.py:276} INFO - 21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:12,483] {docker.py:276} INFO - 21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:12,485] {docker.py:276} INFO - 21/05/15 14:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:12,485] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:12,485] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593639281182087039282_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593639281182087039282_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593639281182087039282_0004}; taskId=attempt_202105151437593639281182087039282_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e58ef2f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:12,486] {docker.py:276} INFO - 21/05/15 14:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:12,486] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437593639281182087039282_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593639281182087039282_0004_m_000082_370
[2021-05-15 11:39:12,488] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437593639281182087039282_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593639281182087039282_0004_m_000082_370 : duration 0:00.003s
[2021-05-15 11:39:12,521] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_202105151437593606638569606210646_0004_m_000079_367: needsTaskCommit() Task attempt_202105151437593606638569606210646_0004_m_000079_367
[2021-05-15 11:39:12,521] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_202105151437593606638569606210646_0004_m_000079_367: needsTaskCommit() Task attempt_202105151437593606638569606210646_0004_m_000079_367: duration 0:00.001s
21/05/15 14:39:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593606638569606210646_0004_m_000079_367
[2021-05-15 11:39:12,523] {docker.py:276} INFO - 21/05/15 14:39:12 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 4587 bytes result sent to driver
[2021-05-15 11:39:12,524] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (bbe2d2545a9d, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:12,525] {docker.py:276} INFO - 21/05/15 14:39:12 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 2669 ms on bbe2d2545a9d (executor driver) (80/200)
21/05/15 14:39:12 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-15 11:39:12,535] {docker.py:276} INFO - 21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:12,537] {docker.py:276} INFO - 21/05/15 14:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:12,538] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:12,538] {docker.py:276} INFO - 21/05/15 14:39:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759242273278696655944_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759242273278696655944_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759242273278696655944_0004}; taskId=attempt_20210515143759242273278696655944_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57c7e35f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:12,538] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Starting: Task committer attempt_20210515143759242273278696655944_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759242273278696655944_0004_m_000083_371
[2021-05-15 11:39:12,542] {docker.py:276} INFO - 21/05/15 14:39:12 INFO StagingCommitter: Task committer attempt_20210515143759242273278696655944_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759242273278696655944_0004_m_000083_371 : duration 0:00.004s
[2021-05-15 11:39:14,897] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Starting: Task committer attempt_20210515143759689991815061885659_0004_m_000080_368: needsTaskCommit() Task attempt_20210515143759689991815061885659_0004_m_000080_368
[2021-05-15 11:39:14,898] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Task committer attempt_20210515143759689991815061885659_0004_m_000080_368: needsTaskCommit() Task attempt_20210515143759689991815061885659_0004_m_000080_368: duration 0:00.001s
21/05/15 14:39:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759689991815061885659_0004_m_000080_368
[2021-05-15 11:39:14,900] {docker.py:276} INFO - 21/05/15 14:39:14 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 4544 bytes result sent to driver
[2021-05-15 11:39:14,901] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Starting: Task committer attempt_202105151437597043532952370874772_0004_m_000081_369: needsTaskCommit() Task attempt_202105151437597043532952370874772_0004_m_000081_369
[2021-05-15 11:39:14,902] {docker.py:276} INFO - 21/05/15 14:39:14 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (bbe2d2545a9d, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:14,903] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Task committer attempt_202105151437597043532952370874772_0004_m_000081_369: needsTaskCommit() Task attempt_202105151437597043532952370874772_0004_m_000081_369: duration 0:00.001s
21/05/15 14:39:14 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 2777 ms on bbe2d2545a9d (executor driver) (81/200)
21/05/15 14:39:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597043532952370874772_0004_m_000081_369
[2021-05-15 11:39:14,906] {docker.py:276} INFO - 21/05/15 14:39:14 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
21/05/15 14:39:14 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 4544 bytes result sent to driver
[2021-05-15 11:39:14,907] {docker.py:276} INFO - 21/05/15 14:39:14 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (bbe2d2545a9d, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:14,909] {docker.py:276} INFO - 21/05/15 14:39:14 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 2640 ms on bbe2d2545a9d (executor driver) (82/200)
[2021-05-15 11:39:14,909] {docker.py:276} INFO - 21/05/15 14:39:14 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-15 11:39:14,915] {docker.py:276} INFO - 21/05/15 14:39:14 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:14,917] {docker.py:276} INFO - 21/05/15 14:39:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:14 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:14,918] {docker.py:276} INFO - 21/05/15 14:39:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 14:39:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594155378050694635343_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594155378050694635343_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594155378050694635343_0004}; taskId=attempt_202105151437594155378050694635343_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77600cac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:14 INFO StagingCommitter: Starting: Task committer attempt_202105151437594155378050694635343_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594155378050694635343_0004_m_000084_372
[2021-05-15 11:39:14,919] {docker.py:276} INFO - 21/05/15 14:39:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:14,920] {docker.py:276} INFO - 21/05/15 14:39:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593920137456600678486_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593920137456600678486_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593920137456600678486_0004}; taskId=attempt_202105151437593920137456600678486_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71acdb60}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:14 INFO StagingCommitter: Starting: Task committer attempt_202105151437593920137456600678486_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593920137456600678486_0004_m_000085_373
[2021-05-15 11:39:14,921] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Task committer attempt_202105151437594155378050694635343_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594155378050694635343_0004_m_000084_372 : duration 0:00.003s
[2021-05-15 11:39:14,923] {docker.py:276} INFO - 21/05/15 14:39:14 INFO StagingCommitter: Task committer attempt_202105151437593920137456600678486_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593920137456600678486_0004_m_000085_373 : duration 0:00.003s
[2021-05-15 11:39:15,110] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Starting: Task committer attempt_202105151437593639281182087039282_0004_m_000082_370: needsTaskCommit() Task attempt_202105151437593639281182087039282_0004_m_000082_370
[2021-05-15 11:39:15,111] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Task committer attempt_202105151437593639281182087039282_0004_m_000082_370: needsTaskCommit() Task attempt_202105151437593639281182087039282_0004_m_000082_370: duration 0:00.001s
21/05/15 14:39:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593639281182087039282_0004_m_000082_370
[2021-05-15 11:39:15,114] {docker.py:276} INFO - 21/05/15 14:39:15 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-15 11:39:15,116] {docker.py:276} INFO - 21/05/15 14:39:15 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (bbe2d2545a9d, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:15,116] {docker.py:276} INFO - 21/05/15 14:39:15 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-15 11:39:15,117] {docker.py:276} INFO - 21/05/15 14:39:15 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 2648 ms on bbe2d2545a9d (executor driver) (83/200)
[2021-05-15 11:39:15,121] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Starting: Task committer attempt_20210515143759242273278696655944_0004_m_000083_371: needsTaskCommit() Task attempt_20210515143759242273278696655944_0004_m_000083_371
[2021-05-15 11:39:15,122] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Task committer attempt_20210515143759242273278696655944_0004_m_000083_371: needsTaskCommit() Task attempt_20210515143759242273278696655944_0004_m_000083_371: duration 0:00.001s
[2021-05-15 11:39:15,122] {docker.py:276} INFO - 21/05/15 14:39:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759242273278696655944_0004_m_000083_371
[2021-05-15 11:39:15,123] {docker.py:276} INFO - 21/05/15 14:39:15 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-15 11:39:15,124] {docker.py:276} INFO - 21/05/15 14:39:15 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (bbe2d2545a9d, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:15,125] {docker.py:276} INFO - 21/05/15 14:39:15 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 2605 ms on bbe2d2545a9d (executor driver) (84/200)
[2021-05-15 11:39:15,126] {docker.py:276} INFO - 21/05/15 14:39:15 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-15 11:39:15,133] {docker.py:276} INFO - 21/05/15 14:39:15 INFO ShuffleBlockFetcherIterator: Getting 5 (26.0 KiB) non-empty blocks including 5 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:15,134] {docker.py:276} INFO - 21/05/15 14:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596189049844938775924_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596189049844938775924_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596189049844938775924_0004}; taskId=attempt_202105151437596189049844938775924_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51e33454}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:15,135] {docker.py:276} INFO - 21/05/15 14:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:15 INFO StagingCommitter: Starting: Task committer attempt_202105151437596189049844938775924_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596189049844938775924_0004_m_000086_374
[2021-05-15 11:39:15,136] {docker.py:276} INFO - 21/05/15 14:39:15 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:15,136] {docker.py:276} INFO - 21/05/15 14:39:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:15,138] {docker.py:276} INFO - 21/05/15 14:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:15,138] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Task committer attempt_202105151437596189049844938775924_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596189049844938775924_0004_m_000086_374 : duration 0:00.003s
[2021-05-15 11:39:15,139] {docker.py:276} INFO - 21/05/15 14:39:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:15,139] {docker.py:276} INFO - 21/05/15 14:39:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592207170750643926005_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592207170750643926005_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592207170750643926005_0004}; taskId=attempt_202105151437592207170750643926005_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29bdeaac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:15,139] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Starting: Task committer attempt_202105151437592207170750643926005_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592207170750643926005_0004_m_000087_375
[2021-05-15 11:39:15,142] {docker.py:276} INFO - 21/05/15 14:39:15 INFO StagingCommitter: Task committer attempt_202105151437592207170750643926005_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592207170750643926005_0004_m_000087_375 : duration 0:00.004s
[2021-05-15 11:39:17,614] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437593920137456600678486_0004_m_000085_373: needsTaskCommit() Task attempt_202105151437593920137456600678486_0004_m_000085_373
[2021-05-15 11:39:17,615] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437593920137456600678486_0004_m_000085_373: needsTaskCommit() Task attempt_202105151437593920137456600678486_0004_m_000085_373: duration 0:00.000s
[2021-05-15 11:39:17,615] {docker.py:276} INFO - 21/05/15 14:39:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593920137456600678486_0004_m_000085_373
[2021-05-15 11:39:17,616] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 4544 bytes result sent to driver
[2021-05-15 11:39:17,617] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (bbe2d2545a9d, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:17,618] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-15 11:39:17,618] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 2715 ms on bbe2d2545a9d (executor driver) (85/200)
[2021-05-15 11:39:17,626] {docker.py:276} INFO - 21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Getting 5 (25.1 KiB) non-empty blocks including 5 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:17,628] {docker.py:276} INFO - 21/05/15 14:39:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:17,628] {docker.py:276} INFO - 21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595811761614593291674_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595811761614593291674_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595811761614593291674_0004}; taskId=attempt_202105151437595811761614593291674_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a5dc562}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437594155378050694635343_0004_m_000084_372: needsTaskCommit() Task attempt_202105151437594155378050694635343_0004_m_000084_372
[2021-05-15 11:39:17,628] {docker.py:276} INFO - 21/05/15 14:39:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:17,629] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437594155378050694635343_0004_m_000084_372: needsTaskCommit() Task attempt_202105151437594155378050694635343_0004_m_000084_372: duration 0:00.001s
21/05/15 14:39:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594155378050694635343_0004_m_000084_372
[2021-05-15 11:39:17,629] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437595811761614593291674_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595811761614593291674_0004_m_000088_376
[2021-05-15 11:39:17,630] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-15 11:39:17,631] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (bbe2d2545a9d, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:17,632] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 2735 ms on bbe2d2545a9d (executor driver) (86/200)
[2021-05-15 11:39:17,632] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
[2021-05-15 11:39:17,633] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437595811761614593291674_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595811761614593291674_0004_m_000088_376 : duration 0:00.004s
[2021-05-15 11:39:17,641] {docker.py:276} INFO - 21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:17,642] {docker.py:276} INFO - 21/05/15 14:39:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595597035436632989846_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595597035436632989846_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595597035436632989846_0004}; taskId=attempt_202105151437595597035436632989846_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53399706}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:17,643] {docker.py:276} INFO - 21/05/15 14:39:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437595597035436632989846_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595597035436632989846_0004_m_000089_377
[2021-05-15 11:39:17,645] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437595597035436632989846_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595597035436632989846_0004_m_000089_377 : duration 0:00.002s
[2021-05-15 11:39:17,711] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437596189049844938775924_0004_m_000086_374: needsTaskCommit() Task attempt_202105151437596189049844938775924_0004_m_000086_374
[2021-05-15 11:39:17,712] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437596189049844938775924_0004_m_000086_374: needsTaskCommit() Task attempt_202105151437596189049844938775924_0004_m_000086_374: duration 0:00.001s
21/05/15 14:39:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596189049844938775924_0004_m_000086_374
[2021-05-15 11:39:17,712] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 4544 bytes result sent to driver
[2021-05-15 11:39:17,713] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (bbe2d2545a9d, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:17,715] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
21/05/15 14:39:17 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 2602 ms on bbe2d2545a9d (executor driver) (87/200)
[2021-05-15 11:39:17,725] {docker.py:276} INFO - 21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:17,727] {docker.py:276} INFO - 21/05/15 14:39:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:17,727] {docker.py:276} INFO - 21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759233430096599898679_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759233430096599898679_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759233430096599898679_0004}; taskId=attempt_20210515143759233430096599898679_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b8e9a34}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:17,727] {docker.py:276} INFO - 21/05/15 14:39:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_20210515143759233430096599898679_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759233430096599898679_0004_m_000090_378
[2021-05-15 11:39:17,730] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_20210515143759233430096599898679_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759233430096599898679_0004_m_000090_378 : duration 0:00.003s
[2021-05-15 11:39:17,947] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437592207170750643926005_0004_m_000087_375: needsTaskCommit() Task attempt_202105151437592207170750643926005_0004_m_000087_375
[2021-05-15 11:39:17,948] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437592207170750643926005_0004_m_000087_375: needsTaskCommit() Task attempt_202105151437592207170750643926005_0004_m_000087_375: duration 0:00.000s
21/05/15 14:39:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592207170750643926005_0004_m_000087_375
[2021-05-15 11:39:17,949] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-15 11:39:17,951] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (bbe2d2545a9d, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:17,953] {docker.py:276} INFO - 21/05/15 14:39:17 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-15 11:39:17,953] {docker.py:276} INFO - 21/05/15 14:39:17 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 2832 ms on bbe2d2545a9d (executor driver) (88/200)
[2021-05-15 11:39:17,963] {docker.py:276} INFO - 21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Getting 5 (25.0 KiB) non-empty blocks including 5 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:17,964] {docker.py:276} INFO - 21/05/15 14:39:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592711343218415238897_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592711343218415238897_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592711343218415238897_0004}; taskId=attempt_202105151437592711343218415238897_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@675d83a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:17 INFO StagingCommitter: Starting: Task committer attempt_202105151437592711343218415238897_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592711343218415238897_0004_m_000091_379
[2021-05-15 11:39:17,967] {docker.py:276} INFO - 21/05/15 14:39:17 INFO StagingCommitter: Task committer attempt_202105151437592711343218415238897_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592711343218415238897_0004_m_000091_379 : duration 0:00.003s
[2021-05-15 11:39:20,288] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437595811761614593291674_0004_m_000088_376: needsTaskCommit() Task attempt_202105151437595811761614593291674_0004_m_000088_376
[2021-05-15 11:39:20,289] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437595811761614593291674_0004_m_000088_376: needsTaskCommit() Task attempt_202105151437595811761614593291674_0004_m_000088_376: duration 0:00.000s
[2021-05-15 11:39:20,290] {docker.py:276} INFO - 21/05/15 14:39:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595811761614593291674_0004_m_000088_376
[2021-05-15 11:39:20,290] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 4587 bytes result sent to driver
[2021-05-15 11:39:20,291] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (bbe2d2545a9d, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:20,291] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
[2021-05-15 11:39:20,292] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 2643 ms on bbe2d2545a9d (executor driver) (89/200)
[2021-05-15 11:39:20,301] {docker.py:276} INFO - 21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:20,303] {docker.py:276} INFO - 21/05/15 14:39:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592226732207650893690_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592226732207650893690_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592226732207650893690_0004}; taskId=attempt_202105151437592226732207650893690_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dfbb6a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:20,303] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437592226732207650893690_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592226732207650893690_0004_m_000092_380
[2021-05-15 11:39:20,306] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437592226732207650893690_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592226732207650893690_0004_m_000092_380 : duration 0:00.003s
[2021-05-15 11:39:20,316] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437595597035436632989846_0004_m_000089_377: needsTaskCommit() Task attempt_202105151437595597035436632989846_0004_m_000089_377
[2021-05-15 11:39:20,317] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437595597035436632989846_0004_m_000089_377: needsTaskCommit() Task attempt_202105151437595597035436632989846_0004_m_000089_377: duration 0:00.001s
21/05/15 14:39:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595597035436632989846_0004_m_000089_377
[2021-05-15 11:39:20,318] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 4587 bytes result sent to driver
[2021-05-15 11:39:20,319] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (bbe2d2545a9d, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:20,319] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-15 11:39:20,320] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 2658 ms on bbe2d2545a9d (executor driver) (90/200)
[2021-05-15 11:39:20,326] {docker.py:276} INFO - 21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:20,328] {docker.py:276} INFO - 21/05/15 14:39:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591224924738255105808_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224924738255105808_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591224924738255105808_0004}; taskId=attempt_202105151437591224924738255105808_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17637a2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437591224924738255105808_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224924738255105808_0004_m_000093_381
[2021-05-15 11:39:20,331] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437591224924738255105808_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591224924738255105808_0004_m_000093_381 : duration 0:00.003s
[2021-05-15 11:39:20,339] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_20210515143759233430096599898679_0004_m_000090_378: needsTaskCommit() Task attempt_20210515143759233430096599898679_0004_m_000090_378
[2021-05-15 11:39:20,339] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_20210515143759233430096599898679_0004_m_000090_378: needsTaskCommit() Task attempt_20210515143759233430096599898679_0004_m_000090_378: duration 0:00.001s
21/05/15 14:39:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759233430096599898679_0004_m_000090_378
[2021-05-15 11:39:20,340] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 4587 bytes result sent to driver
[2021-05-15 11:39:20,341] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (bbe2d2545a9d, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:20,341] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 2598 ms on bbe2d2545a9d (executor driver) (91/200)
[2021-05-15 11:39:20,342] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
[2021-05-15 11:39:20,350] {docker.py:276} INFO - 21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:20,352] {docker.py:276} INFO - 21/05/15 14:39:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593390987838752742693_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593390987838752742693_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593390987838752742693_0004}; taskId=attempt_202105151437593390987838752742693_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32ad6364}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:20,352] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437593390987838752742693_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593390987838752742693_0004_m_000094_382
[2021-05-15 11:39:20,355] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437593390987838752742693_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593390987838752742693_0004_m_000094_382 : duration 0:00.003s
[2021-05-15 11:39:20,625] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437592711343218415238897_0004_m_000091_379: needsTaskCommit() Task attempt_202105151437592711343218415238897_0004_m_000091_379
[2021-05-15 11:39:20,625] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437592711343218415238897_0004_m_000091_379: needsTaskCommit() Task attempt_202105151437592711343218415238897_0004_m_000091_379: duration 0:00.001s
21/05/15 14:39:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592711343218415238897_0004_m_000091_379
[2021-05-15 11:39:20,627] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 4587 bytes result sent to driver
[2021-05-15 11:39:20,628] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (bbe2d2545a9d, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:20,629] {docker.py:276} INFO - 21/05/15 14:39:20 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-15 11:39:20,630] {docker.py:276} INFO - 21/05/15 14:39:20 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 2649 ms on bbe2d2545a9d (executor driver) (92/200)
[2021-05-15 11:39:20,640] {docker.py:276} INFO - 21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Getting 5 (25.9 KiB) non-empty blocks including 5 (25.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:20,640] {docker.py:276} INFO - 21/05/15 14:39:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:20,641] {docker.py:276} INFO - 21/05/15 14:39:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591879855241370686252_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591879855241370686252_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591879855241370686252_0004}; taskId=attempt_202105151437591879855241370686252_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48586fae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:20,642] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Starting: Task committer attempt_202105151437591879855241370686252_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591879855241370686252_0004_m_000095_383
[2021-05-15 11:39:20,645] {docker.py:276} INFO - 21/05/15 14:39:20 INFO StagingCommitter: Task committer attempt_202105151437591879855241370686252_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591879855241370686252_0004_m_000095_383 : duration 0:00.003s
[2021-05-15 11:39:22,915] {docker.py:276} INFO - 21/05/15 14:39:22 INFO StagingCommitter: Starting: Task committer attempt_202105151437592226732207650893690_0004_m_000092_380: needsTaskCommit() Task attempt_202105151437592226732207650893690_0004_m_000092_380
[2021-05-15 11:39:22,916] {docker.py:276} INFO - 21/05/15 14:39:22 INFO StagingCommitter: Task committer attempt_202105151437592226732207650893690_0004_m_000092_380: needsTaskCommit() Task attempt_202105151437592226732207650893690_0004_m_000092_380: duration 0:00.001s
21/05/15 14:39:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592226732207650893690_0004_m_000092_380
[2021-05-15 11:39:22,917] {docker.py:276} INFO - 21/05/15 14:39:22 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-15 11:39:22,919] {docker.py:276} INFO - 21/05/15 14:39:22 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (bbe2d2545a9d, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:22,920] {docker.py:276} INFO - 21/05/15 14:39:22 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 2633 ms on bbe2d2545a9d (executor driver) (93/200)
[2021-05-15 11:39:22,921] {docker.py:276} INFO - 21/05/15 14:39:22 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-15 11:39:22,934] {docker.py:276} INFO - 21/05/15 14:39:22 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:22,934] {docker.py:276} INFO - 21/05/15 14:39:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:22,936] {docker.py:276} INFO - 21/05/15 14:39:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:39:22,936] {docker.py:276} INFO - 21/05/15 14:39:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:22,937] {docker.py:276} INFO - 21/05/15 14:39:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:22,937] {docker.py:276} INFO - 21/05/15 14:39:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759995163576027967775_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759995163576027967775_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759995163576027967775_0004}; taskId=attempt_20210515143759995163576027967775_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3eb85d72}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:22,937] {docker.py:276} INFO - 21/05/15 14:39:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:22,938] {docker.py:276} INFO - 21/05/15 14:39:22 INFO StagingCommitter: Starting: Task committer attempt_20210515143759995163576027967775_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759995163576027967775_0004_m_000096_384
[2021-05-15 11:39:22,940] {docker.py:276} INFO - 21/05/15 14:39:22 INFO StagingCommitter: Task committer attempt_20210515143759995163576027967775_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759995163576027967775_0004_m_000096_384 : duration 0:00.002s
[2021-05-15 11:39:23,025] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437593390987838752742693_0004_m_000094_382: needsTaskCommit() Task attempt_202105151437593390987838752742693_0004_m_000094_382
[2021-05-15 11:39:23,026] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437593390987838752742693_0004_m_000094_382: needsTaskCommit() Task attempt_202105151437593390987838752742693_0004_m_000094_382: duration 0:00.001s
21/05/15 14:39:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593390987838752742693_0004_m_000094_382
[2021-05-15 11:39:23,027] {docker.py:276} INFO - 21/05/15 14:39:23 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-15 11:39:23,028] {docker.py:276} INFO - 21/05/15 14:39:23 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (bbe2d2545a9d, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:23,029] {docker.py:276} INFO - 21/05/15 14:39:23 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
[2021-05-15 11:39:23,030] {docker.py:276} INFO - 21/05/15 14:39:23 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 2692 ms on bbe2d2545a9d (executor driver) (94/200)
[2021-05-15 11:39:23,040] {docker.py:276} INFO - 21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:23,040] {docker.py:276} INFO - 21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:23,042] {docker.py:276} INFO - 21/05/15 14:39:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:23,042] {docker.py:276} INFO - 21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594344739933344413208_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594344739933344413208_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594344739933344413208_0004}; taskId=attempt_202105151437594344739933344413208_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22bafd3e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:23,043] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437594344739933344413208_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594344739933344413208_0004_m_000097_385
[2021-05-15 11:39:23,046] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437594344739933344413208_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594344739933344413208_0004_m_000097_385 : duration 0:00.004s
[2021-05-15 11:39:23,127] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437591224924738255105808_0004_m_000093_381: needsTaskCommit() Task attempt_202105151437591224924738255105808_0004_m_000093_381
[2021-05-15 11:39:23,128] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437591224924738255105808_0004_m_000093_381: needsTaskCommit() Task attempt_202105151437591224924738255105808_0004_m_000093_381: duration 0:00.001s
21/05/15 14:39:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591224924738255105808_0004_m_000093_381
[2021-05-15 11:39:23,130] {docker.py:276} INFO - 21/05/15 14:39:23 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 4544 bytes result sent to driver
[2021-05-15 11:39:23,130] {docker.py:276} INFO - 21/05/15 14:39:23 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (bbe2d2545a9d, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:23,131] {docker.py:276} INFO - 21/05/15 14:39:23 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 2816 ms on bbe2d2545a9d (executor driver) (95/200)
21/05/15 14:39:23 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-15 11:39:23,141] {docker.py:276} INFO - 21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:23,142] {docker.py:276} INFO - 21/05/15 14:39:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:23,143] {docker.py:276} INFO - 21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591362631110648663264_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591362631110648663264_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591362631110648663264_0004}; taskId=attempt_202105151437591362631110648663264_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f84dcd2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:23,143] {docker.py:276} INFO - 21/05/15 14:39:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437591362631110648663264_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591362631110648663264_0004_m_000098_386
[2021-05-15 11:39:23,146] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437591362631110648663264_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591362631110648663264_0004_m_000098_386 : duration 0:00.003s
[2021-05-15 11:39:23,308] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437591879855241370686252_0004_m_000095_383: needsTaskCommit() Task attempt_202105151437591879855241370686252_0004_m_000095_383
[2021-05-15 11:39:23,309] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437591879855241370686252_0004_m_000095_383: needsTaskCommit() Task attempt_202105151437591879855241370686252_0004_m_000095_383: duration 0:00.001s
21/05/15 14:39:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591879855241370686252_0004_m_000095_383
[2021-05-15 11:39:23,311] {docker.py:276} INFO - 21/05/15 14:39:23 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-15 11:39:23,315] {docker.py:276} INFO - 21/05/15 14:39:23 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (bbe2d2545a9d, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:23,316] {docker.py:276} INFO - 21/05/15 14:39:23 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
21/05/15 14:39:23 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 2692 ms on bbe2d2545a9d (executor driver) (96/200)
[2021-05-15 11:39:23,326] {docker.py:276} INFO - 21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:23,326] {docker.py:276} INFO - 21/05/15 14:39:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:23,328] {docker.py:276} INFO - 21/05/15 14:39:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:23,328] {docker.py:276} INFO - 21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:23,328] {docker.py:276} INFO - 21/05/15 14:39:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592115348773641106623_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592115348773641106623_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592115348773641106623_0004}; taskId=attempt_202105151437592115348773641106623_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37ca1ead}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:23,329] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437592115348773641106623_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592115348773641106623_0004_m_000099_387
[2021-05-15 11:39:23,331] {docker.py:276} INFO - 21/05/15 14:39:23 INFO StagingCommitter: Task committer attempt_202105151437592115348773641106623_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592115348773641106623_0004_m_000099_387 : duration 0:00.003s
[2021-05-15 11:39:25,592] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_20210515143759995163576027967775_0004_m_000096_384: needsTaskCommit() Task attempt_20210515143759995163576027967775_0004_m_000096_384
[2021-05-15 11:39:25,593] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_20210515143759995163576027967775_0004_m_000096_384: needsTaskCommit() Task attempt_20210515143759995163576027967775_0004_m_000096_384: duration 0:00.001s
21/05/15 14:39:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759995163576027967775_0004_m_000096_384
[2021-05-15 11:39:25,603] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
21/05/15 14:39:25 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (bbe2d2545a9d, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 14:39:25 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
21/05/15 14:39:25 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 2686 ms on bbe2d2545a9d (executor driver) (97/200)
[2021-05-15 11:39:25,611] {docker.py:276} INFO - 21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:25,613] {docker.py:276} INFO - 21/05/15 14:39:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:25,613] {docker.py:276} INFO - 21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592451966141114658810_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592451966141114658810_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592451966141114658810_0004}; taskId=attempt_202105151437592451966141114658810_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c7433f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437592451966141114658810_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592451966141114658810_0004_m_000100_388
[2021-05-15 11:39:25,616] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437592451966141114658810_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592451966141114658810_0004_m_000100_388 : duration 0:00.003s
[2021-05-15 11:39:25,727] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437594344739933344413208_0004_m_000097_385: needsTaskCommit() Task attempt_202105151437594344739933344413208_0004_m_000097_385
[2021-05-15 11:39:25,727] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437594344739933344413208_0004_m_000097_385: needsTaskCommit() Task attempt_202105151437594344739933344413208_0004_m_000097_385: duration 0:00.000s
21/05/15 14:39:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594344739933344413208_0004_m_000097_385
[2021-05-15 11:39:25,728] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 4544 bytes result sent to driver
[2021-05-15 11:39:25,729] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (bbe2d2545a9d, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:25,730] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 2706 ms on bbe2d2545a9d (executor driver) (98/200)
[2021-05-15 11:39:25,730] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-15 11:39:25,738] {docker.py:276} INFO - 21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:25,740] {docker.py:276} INFO - 21/05/15 14:39:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593109937979160349802_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593109937979160349802_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593109937979160349802_0004}; taskId=attempt_202105151437593109937979160349802_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e239990}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:25,740] {docker.py:276} INFO - 21/05/15 14:39:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437593109937979160349802_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593109937979160349802_0004_m_000101_389
[2021-05-15 11:39:25,743] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437593109937979160349802_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593109937979160349802_0004_m_000101_389 : duration 0:00.003s
[2021-05-15 11:39:25,937] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437592115348773641106623_0004_m_000099_387: needsTaskCommit() Task attempt_202105151437592115348773641106623_0004_m_000099_387
[2021-05-15 11:39:25,937] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437592115348773641106623_0004_m_000099_387: needsTaskCommit() Task attempt_202105151437592115348773641106623_0004_m_000099_387: duration 0:00.000s
21/05/15 14:39:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592115348773641106623_0004_m_000099_387
[2021-05-15 11:39:25,938] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 4544 bytes result sent to driver
[2021-05-15 11:39:25,939] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (bbe2d2545a9d, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:25,940] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-15 11:39:25,941] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 2630 ms on bbe2d2545a9d (executor driver) (99/200)
[2021-05-15 11:39:25,943] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437591362631110648663264_0004_m_000098_386: needsTaskCommit() Task attempt_202105151437591362631110648663264_0004_m_000098_386
[2021-05-15 11:39:25,944] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437591362631110648663264_0004_m_000098_386: needsTaskCommit() Task attempt_202105151437591362631110648663264_0004_m_000098_386: duration 0:00.000s
21/05/15 14:39:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591362631110648663264_0004_m_000098_386
[2021-05-15 11:39:25,946] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 4544 bytes result sent to driver
[2021-05-15 11:39:25,946] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (bbe2d2545a9d, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:25,947] {docker.py:276} INFO - 21/05/15 14:39:25 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 2819 ms on bbe2d2545a9d (executor driver) (100/200)
[2021-05-15 11:39:25,947] {docker.py:276} INFO - 21/05/15 14:39:25 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-15 11:39:25,950] {docker.py:276} INFO - 21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:25,952] {docker.py:276} INFO - 21/05/15 14:39:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:25,953] {docker.py:276} INFO - 21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759903794529836452100_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759903794529836452100_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759903794529836452100_0004}; taskId=attempt_20210515143759903794529836452100_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c180cc5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_20210515143759903794529836452100_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759903794529836452100_0004_m_000102_390
[2021-05-15 11:39:25,955] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_20210515143759903794529836452100_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759903794529836452100_0004_m_000102_390 : duration 0:00.003s
[2021-05-15 11:39:25,961] {docker.py:276} INFO - 21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:25,962] {docker.py:276} INFO - 21/05/15 14:39:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596162647556234495336_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596162647556234495336_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596162647556234495336_0004}; taskId=attempt_202105151437596162647556234495336_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5840a203}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:25 INFO StagingCommitter: Starting: Task committer attempt_202105151437596162647556234495336_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596162647556234495336_0004_m_000103_391
[2021-05-15 11:39:25,965] {docker.py:276} INFO - 21/05/15 14:39:25 INFO StagingCommitter: Task committer attempt_202105151437596162647556234495336_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596162647556234495336_0004_m_000103_391 : duration 0:00.002s
[2021-05-15 11:39:28,334] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437592451966141114658810_0004_m_000100_388: needsTaskCommit() Task attempt_202105151437592451966141114658810_0004_m_000100_388
[2021-05-15 11:39:28,336] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437592451966141114658810_0004_m_000100_388: needsTaskCommit() Task attempt_202105151437592451966141114658810_0004_m_000100_388: duration 0:00.000s
21/05/15 14:39:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592451966141114658810_0004_m_000100_388
[2021-05-15 11:39:28,337] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 4544 bytes result sent to driver
[2021-05-15 11:39:28,339] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (bbe2d2545a9d, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:28,340] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 2745 ms on bbe2d2545a9d (executor driver) (101/200)
[2021-05-15 11:39:28,341] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-15 11:39:28,350] {docker.py:276} INFO - 21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Getting 5 (22.9 KiB) non-empty blocks including 5 (22.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:28,352] {docker.py:276} INFO - 21/05/15 14:39:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:28,352] {docker.py:276} INFO - 21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592682804470612893736_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592682804470612893736_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592682804470612893736_0004}; taskId=attempt_202105151437592682804470612893736_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e0bf044}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:28,353] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437592682804470612893736_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592682804470612893736_0004_m_000104_392
[2021-05-15 11:39:28,355] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437592682804470612893736_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592682804470612893736_0004_m_000104_392 : duration 0:00.003s
[2021-05-15 11:39:28,548] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437593109937979160349802_0004_m_000101_389: needsTaskCommit() Task attempt_202105151437593109937979160349802_0004_m_000101_389
[2021-05-15 11:39:28,548] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437593109937979160349802_0004_m_000101_389: needsTaskCommit() Task attempt_202105151437593109937979160349802_0004_m_000101_389: duration 0:00.000s
21/05/15 14:39:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593109937979160349802_0004_m_000101_389
[2021-05-15 11:39:28,549] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 4544 bytes result sent to driver
[2021-05-15 11:39:28,550] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (bbe2d2545a9d, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:28,551] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 2826 ms on bbe2d2545a9d (executor driver) (102/200)
21/05/15 14:39:28 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-15 11:39:28,569] {docker.py:276} INFO - 21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:28,569] {docker.py:276} INFO - 21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:28,571] {docker.py:276} INFO - 21/05/15 14:39:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:28,571] {docker.py:276} INFO - 21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595473573662593829779_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595473573662593829779_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595473573662593829779_0004}; taskId=attempt_202105151437595473573662593829779_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2200a978}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437595473573662593829779_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595473573662593829779_0004_m_000105_393
[2021-05-15 11:39:28,574] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437595473573662593829779_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595473573662593829779_0004_m_000105_393 : duration 0:00.002s
[2021-05-15 11:39:28,626] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_20210515143759903794529836452100_0004_m_000102_390: needsTaskCommit() Task attempt_20210515143759903794529836452100_0004_m_000102_390
[2021-05-15 11:39:28,629] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_20210515143759903794529836452100_0004_m_000102_390: needsTaskCommit() Task attempt_20210515143759903794529836452100_0004_m_000102_390: duration 0:00.004s
[2021-05-15 11:39:28,630] {docker.py:276} INFO - 21/05/15 14:39:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759903794529836452100_0004_m_000102_390
[2021-05-15 11:39:28,630] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 4587 bytes result sent to driver
[2021-05-15 11:39:28,631] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (bbe2d2545a9d, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:28,632] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 2696 ms on bbe2d2545a9d (executor driver) (103/200)
[2021-05-15 11:39:28,633] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-15 11:39:28,642] {docker.py:276} INFO - 21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Getting 5 (24.7 KiB) non-empty blocks including 5 (24.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:28,644] {docker.py:276} INFO - 21/05/15 14:39:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591418443098390114624_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591418443098390114624_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591418443098390114624_0004}; taskId=attempt_202105151437591418443098390114624_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17fdf571}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437591418443098390114624_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591418443098390114624_0004_m_000106_394
[2021-05-15 11:39:28,647] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437591418443098390114624_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591418443098390114624_0004_m_000106_394 : duration 0:00.003s
[2021-05-15 11:39:28,890] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_202105151437596162647556234495336_0004_m_000103_391: needsTaskCommit() Task attempt_202105151437596162647556234495336_0004_m_000103_391
[2021-05-15 11:39:28,891] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_202105151437596162647556234495336_0004_m_000103_391: needsTaskCommit() Task attempt_202105151437596162647556234495336_0004_m_000103_391: duration 0:00.002s
21/05/15 14:39:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596162647556234495336_0004_m_000103_391
[2021-05-15 11:39:28,893] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 4587 bytes result sent to driver
[2021-05-15 11:39:28,893] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (bbe2d2545a9d, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:28,895] {docker.py:276} INFO - 21/05/15 14:39:28 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-15 11:39:28,896] {docker.py:276} INFO - 21/05/15 14:39:28 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 2954 ms on bbe2d2545a9d (executor driver) (104/200)
[2021-05-15 11:39:28,905] {docker.py:276} INFO - 21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:28,907] {docker.py:276} INFO - 21/05/15 14:39:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759779715938694101749_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759779715938694101749_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759779715938694101749_0004}; taskId=attempt_20210515143759779715938694101749_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21158655}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:28 INFO StagingCommitter: Starting: Task committer attempt_20210515143759779715938694101749_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759779715938694101749_0004_m_000107_395
[2021-05-15 11:39:28,910] {docker.py:276} INFO - 21/05/15 14:39:28 INFO StagingCommitter: Task committer attempt_20210515143759779715938694101749_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759779715938694101749_0004_m_000107_395 : duration 0:00.003s
[2021-05-15 11:39:31,678] {docker.py:276} INFO - 21/05/15 14:39:31 INFO StagingCommitter: Starting: Task committer attempt_202105151437592682804470612893736_0004_m_000104_392: needsTaskCommit() Task attempt_202105151437592682804470612893736_0004_m_000104_392
[2021-05-15 11:39:31,679] {docker.py:276} INFO - 21/05/15 14:39:31 INFO StagingCommitter: Task committer attempt_202105151437592682804470612893736_0004_m_000104_392: needsTaskCommit() Task attempt_202105151437592682804470612893736_0004_m_000104_392: duration 0:00.001s
21/05/15 14:39:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592682804470612893736_0004_m_000104_392
[2021-05-15 11:39:31,680] {docker.py:276} INFO - 21/05/15 14:39:31 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 4587 bytes result sent to driver
[2021-05-15 11:39:31,681] {docker.py:276} INFO - 21/05/15 14:39:31 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (bbe2d2545a9d, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:31,683] {docker.py:276} INFO - 21/05/15 14:39:31 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 3348 ms on bbe2d2545a9d (executor driver) (105/200)
[2021-05-15 11:39:31,684] {docker.py:276} INFO - 21/05/15 14:39:31 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
[2021-05-15 11:39:31,693] {docker.py:276} INFO - 21/05/15 14:39:31 INFO ShuffleBlockFetcherIterator: Getting 5 (26.4 KiB) non-empty blocks including 5 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:31,695] {docker.py:276} INFO - 21/05/15 14:39:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:31,695] {docker.py:276} INFO - 21/05/15 14:39:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:31,695] {docker.py:276} INFO - 21/05/15 14:39:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592649380186025061053_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592649380186025061053_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592649380186025061053_0004}; taskId=attempt_202105151437592649380186025061053_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ebe162b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:31,696] {docker.py:276} INFO - 21/05/15 14:39:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:31,696] {docker.py:276} INFO - 21/05/15 14:39:31 INFO StagingCommitter: Starting: Task committer attempt_202105151437592649380186025061053_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592649380186025061053_0004_m_000108_396
[2021-05-15 11:39:31,699] {docker.py:276} INFO - 21/05/15 14:39:31 INFO StagingCommitter: Task committer attempt_202105151437592649380186025061053_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592649380186025061053_0004_m_000108_396 : duration 0:00.003s
[2021-05-15 11:39:32,020] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437591418443098390114624_0004_m_000106_394: needsTaskCommit() Task attempt_202105151437591418443098390114624_0004_m_000106_394
[2021-05-15 11:39:32,021] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_202105151437591418443098390114624_0004_m_000106_394: needsTaskCommit() Task attempt_202105151437591418443098390114624_0004_m_000106_394: duration 0:00.001s
21/05/15 14:39:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591418443098390114624_0004_m_000106_394
[2021-05-15 11:39:32,023] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 4544 bytes result sent to driver
[2021-05-15 11:39:32,025] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (bbe2d2545a9d, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:32,025] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-15 11:39:32,027] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 3400 ms on bbe2d2545a9d (executor driver) (106/200)
[2021-05-15 11:39:32,035] {docker.py:276} INFO - 21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:32,038] {docker.py:276} INFO - 21/05/15 14:39:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:32,038] {docker.py:276} INFO - 21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592195674991494328483_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592195674991494328483_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592195674991494328483_0004}; taskId=attempt_202105151437592195674991494328483_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a0fad78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:32,038] {docker.py:276} INFO - 21/05/15 14:39:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:32,039] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437592195674991494328483_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592195674991494328483_0004_m_000109_397
[2021-05-15 11:39:32,041] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_202105151437592195674991494328483_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592195674991494328483_0004_m_000109_397 : duration 0:00.003s
[2021-05-15 11:39:32,200] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437595473573662593829779_0004_m_000105_393: needsTaskCommit() Task attempt_202105151437595473573662593829779_0004_m_000105_393
21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_20210515143759779715938694101749_0004_m_000107_395: needsTaskCommit() Task attempt_20210515143759779715938694101749_0004_m_000107_395
[2021-05-15 11:39:32,201] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_20210515143759779715938694101749_0004_m_000107_395: needsTaskCommit() Task attempt_20210515143759779715938694101749_0004_m_000107_395: duration 0:00.001s
21/05/15 14:39:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759779715938694101749_0004_m_000107_395
[2021-05-15 11:39:32,202] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_202105151437595473573662593829779_0004_m_000105_393: needsTaskCommit() Task attempt_202105151437595473573662593829779_0004_m_000105_393: duration 0:00.001s
21/05/15 14:39:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595473573662593829779_0004_m_000105_393
[2021-05-15 11:39:32,203] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
[2021-05-15 11:39:32,204] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (bbe2d2545a9d, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:32,204] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 4587 bytes result sent to driver
[2021-05-15 11:39:32,205] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-15 11:39:32,206] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (bbe2d2545a9d, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:32,207] {docker.py:276} INFO - 21/05/15 14:39:32 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
[2021-05-15 11:39:32,208] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 3660 ms on bbe2d2545a9d (executor driver) (107/200)
[2021-05-15 11:39:32,208] {docker.py:276} INFO - 21/05/15 14:39:32 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 3317 ms on bbe2d2545a9d (executor driver) (108/200)
[2021-05-15 11:39:32,216] {docker.py:276} INFO - 21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:32,218] {docker.py:276} INFO - 21/05/15 14:39:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:32,218] {docker.py:276} INFO - 21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:32,219] {docker.py:276} INFO - 21/05/15 14:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:32,219] {docker.py:276} INFO - 21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591806152683352815561_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591806152683352815561_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591806152683352815561_0004}; taskId=attempt_202105151437591806152683352815561_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e8732f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:32,219] {docker.py:276} INFO - 21/05/15 14:39:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437591806152683352815561_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591806152683352815561_0004_m_000111_399
[2021-05-15 11:39:32,221] {docker.py:276} INFO - 21/05/15 14:39:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:32,221] {docker.py:276} INFO - 21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:32,221] {docker.py:276} INFO - 21/05/15 14:39:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597136777591910221785_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597136777591910221785_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597136777591910221785_0004}; taskId=attempt_202105151437597136777591910221785_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60f93889}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437597136777591910221785_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597136777591910221785_0004_m_000110_398
[2021-05-15 11:39:32,223] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_202105151437591806152683352815561_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591806152683352815561_0004_m_000111_399 : duration 0:00.003s
[2021-05-15 11:39:32,227] {docker.py:276} INFO - 21/05/15 14:39:32 INFO StagingCommitter: Task committer attempt_202105151437597136777591910221785_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597136777591910221785_0004_m_000110_398 : duration 0:00.005s
[2021-05-15 11:39:34,528] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437592649380186025061053_0004_m_000108_396: needsTaskCommit() Task attempt_202105151437592649380186025061053_0004_m_000108_396
[2021-05-15 11:39:34,529] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_202105151437592649380186025061053_0004_m_000108_396: needsTaskCommit() Task attempt_202105151437592649380186025061053_0004_m_000108_396: duration 0:00.001s
21/05/15 14:39:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592649380186025061053_0004_m_000108_396
[2021-05-15 11:39:34,531] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 4544 bytes result sent to driver
[2021-05-15 11:39:34,533] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (bbe2d2545a9d, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:34,534] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 2855 ms on bbe2d2545a9d (executor driver) (109/200)
[2021-05-15 11:39:34,534] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-15 11:39:34,544] {docker.py:276} INFO - 21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:34,545] {docker.py:276} INFO - 21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:34,546] {docker.py:276} INFO - 21/05/15 14:39:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:34,547] {docker.py:276} INFO - 21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051514375910048715376526551_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_2021051514375910048715376526551_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051514375910048715376526551_0004}; taskId=attempt_2021051514375910048715376526551_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cc2df80}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_2021051514375910048715376526551_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_2021051514375910048715376526551_0004_m_000112_400
[2021-05-15 11:39:34,551] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_2021051514375910048715376526551_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_2021051514375910048715376526551_0004_m_000112_400 : duration 0:00.004s
[2021-05-15 11:39:34,724] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437597136777591910221785_0004_m_000110_398: needsTaskCommit() Task attempt_202105151437597136777591910221785_0004_m_000110_398
21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_202105151437597136777591910221785_0004_m_000110_398: needsTaskCommit() Task attempt_202105151437597136777591910221785_0004_m_000110_398: duration 0:00.001s
21/05/15 14:39:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597136777591910221785_0004_m_000110_398
[2021-05-15 11:39:34,727] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 4544 bytes result sent to driver
[2021-05-15 11:39:34,728] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (bbe2d2545a9d, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:34,729] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 2530 ms on bbe2d2545a9d (executor driver) (110/200)
[2021-05-15 11:39:34,729] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-15 11:39:34,739] {docker.py:276} INFO - 21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:34,741] {docker.py:276} INFO - 21/05/15 14:39:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597301618420931239961_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597301618420931239961_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597301618420931239961_0004}; taskId=attempt_202105151437597301618420931239961_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@692d391b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:34,741] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437597301618420931239961_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597301618420931239961_0004_m_000113_401
[2021-05-15 11:39:34,744] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_202105151437597301618420931239961_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597301618420931239961_0004_m_000113_401 : duration 0:00.002s
[2021-05-15 11:39:34,820] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437592195674991494328483_0004_m_000109_397: needsTaskCommit() Task attempt_202105151437592195674991494328483_0004_m_000109_397
[2021-05-15 11:39:34,822] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_202105151437592195674991494328483_0004_m_000109_397: needsTaskCommit() Task attempt_202105151437592195674991494328483_0004_m_000109_397: duration 0:00.001s
21/05/15 14:39:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592195674991494328483_0004_m_000109_397
[2021-05-15 11:39:34,825] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 4544 bytes result sent to driver
[2021-05-15 11:39:34,826] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (bbe2d2545a9d, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:34,827] {docker.py:276} INFO - 21/05/15 14:39:34 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 2805 ms on bbe2d2545a9d (executor driver) (111/200)
[2021-05-15 11:39:34,828] {docker.py:276} INFO - 21/05/15 14:39:34 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-15 11:39:34,837] {docker.py:276} INFO - 21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:34,837] {docker.py:276} INFO - 21/05/15 14:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:34,839] {docker.py:276} INFO - 21/05/15 14:39:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:34,840] {docker.py:276} INFO - 21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:34,840] {docker.py:276} INFO - 21/05/15 14:39:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591780697391767227278_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591780697391767227278_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591780697391767227278_0004}; taskId=attempt_202105151437591780697391767227278_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31bd6d8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:34,840] {docker.py:276} INFO - 21/05/15 14:39:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437591780697391767227278_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591780697391767227278_0004_m_000114_402
[2021-05-15 11:39:34,843] {docker.py:276} INFO - 21/05/15 14:39:34 INFO StagingCommitter: Task committer attempt_202105151437591780697391767227278_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591780697391767227278_0004_m_000114_402 : duration 0:00.002s
[2021-05-15 11:39:35,034] {docker.py:276} INFO - 21/05/15 14:39:35 INFO StagingCommitter: Starting: Task committer attempt_202105151437591806152683352815561_0004_m_000111_399: needsTaskCommit() Task attempt_202105151437591806152683352815561_0004_m_000111_399
21/05/15 14:39:35 INFO StagingCommitter: Task committer attempt_202105151437591806152683352815561_0004_m_000111_399: needsTaskCommit() Task attempt_202105151437591806152683352815561_0004_m_000111_399: duration 0:00.001s
21/05/15 14:39:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591806152683352815561_0004_m_000111_399
[2021-05-15 11:39:35,036] {docker.py:276} INFO - 21/05/15 14:39:35 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 4544 bytes result sent to driver
[2021-05-15 11:39:35,037] {docker.py:276} INFO - 21/05/15 14:39:35 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (bbe2d2545a9d, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:35,039] {docker.py:276} INFO - 21/05/15 14:39:35 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
21/05/15 14:39:35 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 2838 ms on bbe2d2545a9d (executor driver) (112/200)
[2021-05-15 11:39:35,047] {docker.py:276} INFO - 21/05/15 14:39:35 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:35,049] {docker.py:276} INFO - 21/05/15 14:39:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591284395306992934369_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591284395306992934369_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591284395306992934369_0004}; taskId=attempt_202105151437591284395306992934369_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d62b341}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:35 INFO StagingCommitter: Starting: Task committer attempt_202105151437591284395306992934369_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591284395306992934369_0004_m_000115_403
[2021-05-15 11:39:35,052] {docker.py:276} INFO - 21/05/15 14:39:35 INFO StagingCommitter: Task committer attempt_202105151437591284395306992934369_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591284395306992934369_0004_m_000115_403 : duration 0:00.003s
[2021-05-15 11:39:37,436] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437597301618420931239961_0004_m_000113_401: needsTaskCommit() Task attempt_202105151437597301618420931239961_0004_m_000113_401
[2021-05-15 11:39:37,437] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437597301618420931239961_0004_m_000113_401: needsTaskCommit() Task attempt_202105151437597301618420931239961_0004_m_000113_401: duration 0:00.001s
[2021-05-15 11:39:37,438] {docker.py:276} INFO - 21/05/15 14:39:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597301618420931239961_0004_m_000113_401
[2021-05-15 11:39:37,439] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 4544 bytes result sent to driver
[2021-05-15 11:39:37,441] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (bbe2d2545a9d, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:37,441] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 2716 ms on bbe2d2545a9d (executor driver) (113/200)
[2021-05-15 11:39:37,442] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-15 11:39:37,451] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:37,453] {docker.py:276} INFO - 21/05/15 14:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:37,454] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,454] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594327937368428621883_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594327937368428621883_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594327937368428621883_0004}; taskId=attempt_202105151437594327937368428621883_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@560ebf7e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,454] {docker.py:276} INFO - 21/05/15 14:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:37,455] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437594327937368428621883_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594327937368428621883_0004_m_000116_404
[2021-05-15 11:39:37,457] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437594327937368428621883_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594327937368428621883_0004_m_000116_404 : duration 0:00.003s
[2021-05-15 11:39:37,602] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437591780697391767227278_0004_m_000114_402: needsTaskCommit() Task attempt_202105151437591780697391767227278_0004_m_000114_402
[2021-05-15 11:39:37,603] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437591780697391767227278_0004_m_000114_402: needsTaskCommit() Task attempt_202105151437591780697391767227278_0004_m_000114_402: duration 0:00.000s
[2021-05-15 11:39:37,603] {docker.py:276} INFO - 21/05/15 14:39:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591780697391767227278_0004_m_000114_402
[2021-05-15 11:39:37,604] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 4544 bytes result sent to driver
[2021-05-15 11:39:37,605] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (bbe2d2545a9d, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:37,605] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-15 11:39:37,606] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 2784 ms on bbe2d2545a9d (executor driver) (114/200)
[2021-05-15 11:39:37,612] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:37,614] {docker.py:276} INFO - 21/05/15 14:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:39:37,614] {docker.py:276} INFO - 21/05/15 14:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,614] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594534387207744799162_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594534387207744799162_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594534387207744799162_0004}; taskId=attempt_202105151437594534387207744799162_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39ae98b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:37,615] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437594534387207744799162_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594534387207744799162_0004_m_000117_405
[2021-05-15 11:39:37,617] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437594534387207744799162_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594534387207744799162_0004_m_000117_405 : duration 0:00.002s
[2021-05-15 11:39:37,638] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437591284395306992934369_0004_m_000115_403: needsTaskCommit() Task attempt_202105151437591284395306992934369_0004_m_000115_403
21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437591284395306992934369_0004_m_000115_403: needsTaskCommit() Task attempt_202105151437591284395306992934369_0004_m_000115_403: duration 0:00.000s
21/05/15 14:39:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591284395306992934369_0004_m_000115_403
[2021-05-15 11:39:37,639] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 4544 bytes result sent to driver
[2021-05-15 11:39:37,640] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (bbe2d2545a9d, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:37,641] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
[2021-05-15 11:39:37,641] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 2608 ms on bbe2d2545a9d (executor driver) (115/200)
[2021-05-15 11:39:37,648] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:37,649] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:37,651] {docker.py:276} INFO - 21/05/15 14:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:37,652] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,652] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595400246946042231486_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595400246946042231486_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595400246946042231486_0004}; taskId=attempt_202105151437595400246946042231486_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@355f94a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,652] {docker.py:276} INFO - 21/05/15 14:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:37,653] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437595400246946042231486_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595400246946042231486_0004_m_000118_406
[2021-05-15 11:39:37,655] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437595400246946042231486_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595400246946042231486_0004_m_000118_406 : duration 0:00.003s
[2021-05-15 11:39:37,746] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_2021051514375910048715376526551_0004_m_000112_400: needsTaskCommit() Task attempt_2021051514375910048715376526551_0004_m_000112_400
[2021-05-15 11:39:37,749] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_2021051514375910048715376526551_0004_m_000112_400: needsTaskCommit() Task attempt_2021051514375910048715376526551_0004_m_000112_400: duration 0:00.000s
21/05/15 14:39:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051514375910048715376526551_0004_m_000112_400
21/05/15 14:39:37 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 4544 bytes result sent to driver
[2021-05-15 11:39:37,751] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (bbe2d2545a9d, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:37,752] {docker.py:276} INFO - 21/05/15 14:39:37 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-15 11:39:37,753] {docker.py:276} INFO - 21/05/15 14:39:37 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 3224 ms on bbe2d2545a9d (executor driver) (116/200)
[2021-05-15 11:39:37,771] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Getting 5 (26.0 KiB) non-empty blocks including 5 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:37,771] {docker.py:276} INFO - 21/05/15 14:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:37,773] {docker.py:276} INFO - 21/05/15 14:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,774] {docker.py:276} INFO - 21/05/15 14:39:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598085397270427533678_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598085397270427533678_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598085397270427533678_0004}; taskId=attempt_202105151437598085397270427533678_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38549921}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:37,774] {docker.py:276} INFO - 21/05/15 14:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:37 INFO StagingCommitter: Starting: Task committer attempt_202105151437598085397270427533678_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598085397270427533678_0004_m_000119_407
[2021-05-15 11:39:37,777] {docker.py:276} INFO - 21/05/15 14:39:37 INFO StagingCommitter: Task committer attempt_202105151437598085397270427533678_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598085397270427533678_0004_m_000119_407 : duration 0:00.003s
[2021-05-15 11:39:40,236] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437595400246946042231486_0004_m_000118_406: needsTaskCommit() Task attempt_202105151437595400246946042231486_0004_m_000118_406
[2021-05-15 11:39:40,237] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437595400246946042231486_0004_m_000118_406: needsTaskCommit() Task attempt_202105151437595400246946042231486_0004_m_000118_406: duration 0:00.001s
21/05/15 14:39:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595400246946042231486_0004_m_000118_406
[2021-05-15 11:39:40,238] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 4587 bytes result sent to driver
[2021-05-15 11:39:40,239] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (bbe2d2545a9d, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:40,240] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 2603 ms on bbe2d2545a9d (executor driver) (117/200)
[2021-05-15 11:39:40,242] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-15 11:39:40,250] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:40,250] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:40,252] {docker.py:276} INFO - 21/05/15 14:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:40,253] {docker.py:276} INFO - 21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759514436075272547875_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759514436075272547875_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759514436075272547875_0004}; taskId=attempt_20210515143759514436075272547875_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@435c1f00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:40,254] {docker.py:276} INFO - 21/05/15 14:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:40,254] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_20210515143759514436075272547875_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759514436075272547875_0004_m_000120_408
[2021-05-15 11:39:40,255] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437594534387207744799162_0004_m_000117_405: needsTaskCommit() Task attempt_202105151437594534387207744799162_0004_m_000117_405
[2021-05-15 11:39:40,256] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437594534387207744799162_0004_m_000117_405: needsTaskCommit() Task attempt_202105151437594534387207744799162_0004_m_000117_405: duration 0:00.000s
[2021-05-15 11:39:40,256] {docker.py:276} INFO - 21/05/15 14:39:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594534387207744799162_0004_m_000117_405
[2021-05-15 11:39:40,257] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_20210515143759514436075272547875_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759514436075272547875_0004_m_000120_408 : duration 0:00.004s
[2021-05-15 11:39:40,257] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 4587 bytes result sent to driver
[2021-05-15 11:39:40,258] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (bbe2d2545a9d, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:40,258] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
[2021-05-15 11:39:40,260] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 2659 ms on bbe2d2545a9d (executor driver) (118/200)
[2021-05-15 11:39:40,268] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.3 KiB) non-empty blocks including 5 (24.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:40,269] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:40,271] {docker.py:276} INFO - 21/05/15 14:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598256028767672249397_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598256028767672249397_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598256028767672249397_0004}; taskId=attempt_202105151437598256028767672249397_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@57ae2f6a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437598256028767672249397_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598256028767672249397_0004_m_000121_409
[2021-05-15 11:39:40,274] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437598256028767672249397_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598256028767672249397_0004_m_000121_409 : duration 0:00.002s
[2021-05-15 11:39:40,350] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437594327937368428621883_0004_m_000116_404: needsTaskCommit() Task attempt_202105151437594327937368428621883_0004_m_000116_404
[2021-05-15 11:39:40,350] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437594327937368428621883_0004_m_000116_404: needsTaskCommit() Task attempt_202105151437594327937368428621883_0004_m_000116_404: duration 0:00.001s
21/05/15 14:39:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594327937368428621883_0004_m_000116_404
[2021-05-15 11:39:40,351] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 4587 bytes result sent to driver
[2021-05-15 11:39:40,352] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (bbe2d2545a9d, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:40,353] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
21/05/15 14:39:40 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 2917 ms on bbe2d2545a9d (executor driver) (119/200)
[2021-05-15 11:39:40,361] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:40,364] {docker.py:276} INFO - 21/05/15 14:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594406056147102468801_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594406056147102468801_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594406056147102468801_0004}; taskId=attempt_202105151437594406056147102468801_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d6a86b2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:40,364] {docker.py:276} INFO - 21/05/15 14:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437594406056147102468801_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594406056147102468801_0004_m_000122_410
[2021-05-15 11:39:40,367] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437594406056147102468801_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594406056147102468801_0004_m_000122_410 : duration 0:00.003s
[2021-05-15 11:39:40,544] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437598085397270427533678_0004_m_000119_407: needsTaskCommit() Task attempt_202105151437598085397270427533678_0004_m_000119_407
[2021-05-15 11:39:40,545] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437598085397270427533678_0004_m_000119_407: needsTaskCommit() Task attempt_202105151437598085397270427533678_0004_m_000119_407: duration 0:00.000s
21/05/15 14:39:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598085397270427533678_0004_m_000119_407
[2021-05-15 11:39:40,546] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 4587 bytes result sent to driver
[2021-05-15 11:39:40,547] {docker.py:276} INFO - 21/05/15 14:39:40 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (bbe2d2545a9d, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:40,549] {docker.py:276} INFO - 21/05/15 14:39:40 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
21/05/15 14:39:40 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 2802 ms on bbe2d2545a9d (executor driver) (120/200)
[2021-05-15 11:39:40,557] {docker.py:276} INFO - 21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:40,560] {docker.py:276} INFO - 21/05/15 14:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592942265005163476619_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592942265005163476619_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592942265005163476619_0004}; taskId=attempt_202105151437592942265005163476619_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2da0d1ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:40,560] {docker.py:276} INFO - 21/05/15 14:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:40,561] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Starting: Task committer attempt_202105151437592942265005163476619_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592942265005163476619_0004_m_000123_411
[2021-05-15 11:39:40,564] {docker.py:276} INFO - 21/05/15 14:39:40 INFO StagingCommitter: Task committer attempt_202105151437592942265005163476619_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592942265005163476619_0004_m_000123_411 : duration 0:00.004s
[2021-05-15 11:39:42,918] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437598256028767672249397_0004_m_000121_409: needsTaskCommit() Task attempt_202105151437598256028767672249397_0004_m_000121_409
21/05/15 14:39:42 INFO StagingCommitter: Task committer attempt_202105151437598256028767672249397_0004_m_000121_409: needsTaskCommit() Task attempt_202105151437598256028767672249397_0004_m_000121_409: duration 0:00.001s
21/05/15 14:39:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598256028767672249397_0004_m_000121_409
[2021-05-15 11:39:42,920] {docker.py:276} INFO - 21/05/15 14:39:42 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 4544 bytes result sent to driver
[2021-05-15 11:39:42,921] {docker.py:276} INFO - 21/05/15 14:39:42 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (bbe2d2545a9d, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:42,922] {docker.py:276} INFO - 21/05/15 14:39:42 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 2666 ms on bbe2d2545a9d (executor driver) (121/200)
[2021-05-15 11:39:42,923] {docker.py:276} INFO - 21/05/15 14:39:42 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-15 11:39:42,933] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Starting: Task committer attempt_20210515143759514436075272547875_0004_m_000120_408: needsTaskCommit() Task attempt_20210515143759514436075272547875_0004_m_000120_408
[2021-05-15 11:39:42,933] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Task committer attempt_20210515143759514436075272547875_0004_m_000120_408: needsTaskCommit() Task attempt_20210515143759514436075272547875_0004_m_000120_408: duration 0:00.000s
[2021-05-15 11:39:42,934] {docker.py:276} INFO - 21/05/15 14:39:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759514436075272547875_0004_m_000120_408
[2021-05-15 11:39:42,935] {docker.py:276} INFO - 21/05/15 14:39:42 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:42,935] {docker.py:276} INFO - 21/05/15 14:39:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:39:42,936] {docker.py:276} INFO - 21/05/15 14:39:42 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 4544 bytes result sent to driver
[2021-05-15 11:39:42,936] {docker.py:276} INFO - 21/05/15 14:39:42 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (bbe2d2545a9d, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:42,937] {docker.py:276} INFO - 21/05/15 14:39:42 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 2702 ms on bbe2d2545a9d (executor driver) (122/200)
[2021-05-15 11:39:42,937] {docker.py:276} INFO - 21/05/15 14:39:42 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-15 11:39:42,938] {docker.py:276} INFO - 21/05/15 14:39:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:42,939] {docker.py:276} INFO - 21/05/15 14:39:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592240475814748907271_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592240475814748907271_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592240475814748907271_0004}; taskId=attempt_202105151437592240475814748907271_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@cf1a0f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:42,939] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437592240475814748907271_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592240475814748907271_0004_m_000124_412
[2021-05-15 11:39:42,943] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Task committer attempt_202105151437592240475814748907271_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592240475814748907271_0004_m_000124_412 : duration 0:00.004s
[2021-05-15 11:39:42,945] {docker.py:276} INFO - 21/05/15 14:39:42 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:42,948] {docker.py:276} INFO - 21/05/15 14:39:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595336997085028836206_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595336997085028836206_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595336997085028836206_0004}; taskId=attempt_202105151437595336997085028836206_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7be7bea7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:42 INFO StagingCommitter: Starting: Task committer attempt_202105151437595336997085028836206_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595336997085028836206_0004_m_000125_413
[2021-05-15 11:39:42,950] {docker.py:276} INFO - 21/05/15 14:39:42 INFO StagingCommitter: Task committer attempt_202105151437595336997085028836206_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595336997085028836206_0004_m_000125_413 : duration 0:00.003s
[2021-05-15 11:39:43,017] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Starting: Task committer attempt_202105151437594406056147102468801_0004_m_000122_410: needsTaskCommit() Task attempt_202105151437594406056147102468801_0004_m_000122_410
[2021-05-15 11:39:43,018] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Task committer attempt_202105151437594406056147102468801_0004_m_000122_410: needsTaskCommit() Task attempt_202105151437594406056147102468801_0004_m_000122_410: duration 0:00.000s
21/05/15 14:39:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594406056147102468801_0004_m_000122_410
[2021-05-15 11:39:43,019] {docker.py:276} INFO - 21/05/15 14:39:43 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-15 11:39:43,020] {docker.py:276} INFO - 21/05/15 14:39:43 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (bbe2d2545a9d, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:43,021] {docker.py:276} INFO - 21/05/15 14:39:43 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
21/05/15 14:39:43 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 2672 ms on bbe2d2545a9d (executor driver) (123/200)
[2021-05-15 11:39:43,029] {docker.py:276} INFO - 21/05/15 14:39:43 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:43,031] {docker.py:276} INFO - 21/05/15 14:39:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599194551333914358076_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599194551333914358076_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599194551333914358076_0004}; taskId=attempt_202105151437599194551333914358076_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6aa0283c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:43,031] {docker.py:276} INFO - 21/05/15 14:39:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:43 INFO StagingCommitter: Starting: Task committer attempt_202105151437599194551333914358076_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599194551333914358076_0004_m_000126_414
[2021-05-15 11:39:43,033] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Task committer attempt_202105151437599194551333914358076_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599194551333914358076_0004_m_000126_414 : duration 0:00.003s
[2021-05-15 11:39:43,538] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Starting: Task committer attempt_202105151437592942265005163476619_0004_m_000123_411: needsTaskCommit() Task attempt_202105151437592942265005163476619_0004_m_000123_411
[2021-05-15 11:39:43,539] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Task committer attempt_202105151437592942265005163476619_0004_m_000123_411: needsTaskCommit() Task attempt_202105151437592942265005163476619_0004_m_000123_411: duration 0:00.002s
21/05/15 14:39:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592942265005163476619_0004_m_000123_411
[2021-05-15 11:39:43,542] {docker.py:276} INFO - 21/05/15 14:39:43 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 4544 bytes result sent to driver
[2021-05-15 11:39:43,543] {docker.py:276} INFO - 21/05/15 14:39:43 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 2999 ms on bbe2d2545a9d (executor driver) (124/200)
[2021-05-15 11:39:43,544] {docker.py:276} INFO - 21/05/15 14:39:43 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (bbe2d2545a9d, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:43,545] {docker.py:276} INFO - 21/05/15 14:39:43 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
[2021-05-15 11:39:43,555] {docker.py:276} INFO - 21/05/15 14:39:43 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:43,558] {docker.py:276} INFO - 21/05/15 14:39:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:43,558] {docker.py:276} INFO - 21/05/15 14:39:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:43,559] {docker.py:276} INFO - 21/05/15 14:39:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594266695467161767210_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594266695467161767210_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594266695467161767210_0004}; taskId=attempt_202105151437594266695467161767210_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34ed116f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:43,559] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Starting: Task committer attempt_202105151437594266695467161767210_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594266695467161767210_0004_m_000127_415
[2021-05-15 11:39:43,561] {docker.py:276} INFO - 21/05/15 14:39:43 INFO StagingCommitter: Task committer attempt_202105151437594266695467161767210_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594266695467161767210_0004_m_000127_415 : duration 0:00.003s
[2021-05-15 11:39:45,608] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437592240475814748907271_0004_m_000124_412: needsTaskCommit() Task attempt_202105151437592240475814748907271_0004_m_000124_412
21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437592240475814748907271_0004_m_000124_412: needsTaskCommit() Task attempt_202105151437592240475814748907271_0004_m_000124_412: duration 0:00.001s
21/05/15 14:39:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592240475814748907271_0004_m_000124_412
[2021-05-15 11:39:45,611] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 4544 bytes result sent to driver
[2021-05-15 11:39:45,612] {docker.py:276} INFO - 21/05/15 14:39:45 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (bbe2d2545a9d, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:45,613] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
21/05/15 14:39:45 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 2696 ms on bbe2d2545a9d (executor driver) (125/200)
[2021-05-15 11:39:45,623] {docker.py:276} INFO - 21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 11:39:45,625] {docker.py:276} INFO - 21/05/15 14:39:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597942417947761788541_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597942417947761788541_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597942417947761788541_0004}; taskId=attempt_202105151437597942417947761788541_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74b5457}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437597942417947761788541_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597942417947761788541_0004_m_000128_416
[2021-05-15 11:39:45,628] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437597942417947761788541_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597942417947761788541_0004_m_000128_416 : duration 0:00.002s
[2021-05-15 11:39:45,682] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437595336997085028836206_0004_m_000125_413: needsTaskCommit() Task attempt_202105151437595336997085028836206_0004_m_000125_413
[2021-05-15 11:39:45,683] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437595336997085028836206_0004_m_000125_413: needsTaskCommit() Task attempt_202105151437595336997085028836206_0004_m_000125_413: duration 0:00.000s
21/05/15 14:39:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595336997085028836206_0004_m_000125_413
[2021-05-15 11:39:45,684] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 4544 bytes result sent to driver
[2021-05-15 11:39:45,686] {docker.py:276} INFO - 21/05/15 14:39:45 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (bbe2d2545a9d, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:45,687] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-15 11:39:45,687] {docker.py:276} INFO - 21/05/15 14:39:45 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 2754 ms on bbe2d2545a9d (executor driver) (126/200)
[2021-05-15 11:39:45,696] {docker.py:276} INFO - 21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:45,697] {docker.py:276} INFO - 21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:45,698] {docker.py:276} INFO - 21/05/15 14:39:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:45,699] {docker.py:276} INFO - 21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599003376906776376287_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599003376906776376287_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599003376906776376287_0004}; taskId=attempt_202105151437599003376906776376287_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4395c264}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:45,699] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437599003376906776376287_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599003376906776376287_0004_m_000129_417
[2021-05-15 11:39:45,702] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437599003376906776376287_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599003376906776376287_0004_m_000129_417 : duration 0:00.003s
[2021-05-15 11:39:45,797] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437599194551333914358076_0004_m_000126_414: needsTaskCommit() Task attempt_202105151437599194551333914358076_0004_m_000126_414
[2021-05-15 11:39:45,798] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437599194551333914358076_0004_m_000126_414: needsTaskCommit() Task attempt_202105151437599194551333914358076_0004_m_000126_414: duration 0:00.001s
21/05/15 14:39:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599194551333914358076_0004_m_000126_414
[2021-05-15 11:39:45,799] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 4544 bytes result sent to driver
[2021-05-15 11:39:45,800] {docker.py:276} INFO - 21/05/15 14:39:45 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (bbe2d2545a9d, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:45,802] {docker.py:276} INFO - 21/05/15 14:39:45 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 2785 ms on bbe2d2545a9d (executor driver) (127/200)
[2021-05-15 11:39:45,802] {docker.py:276} INFO - 21/05/15 14:39:45 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
[2021-05-15 11:39:45,810] {docker.py:276} INFO - 21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:45,812] {docker.py:276} INFO - 21/05/15 14:39:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:45,812] {docker.py:276} INFO - 21/05/15 14:39:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597010617157568831600_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597010617157568831600_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597010617157568831600_0004}; taskId=attempt_202105151437597010617157568831600_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b7214f5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:45 INFO StagingCommitter: Starting: Task committer attempt_202105151437597010617157568831600_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597010617157568831600_0004_m_000130_418
[2021-05-15 11:39:45,815] {docker.py:276} INFO - 21/05/15 14:39:45 INFO StagingCommitter: Task committer attempt_202105151437597010617157568831600_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597010617157568831600_0004_m_000130_418 : duration 0:00.003s
[2021-05-15 11:39:46,400] {docker.py:276} INFO - 21/05/15 14:39:46 INFO StagingCommitter: Starting: Task committer attempt_202105151437594266695467161767210_0004_m_000127_415: needsTaskCommit() Task attempt_202105151437594266695467161767210_0004_m_000127_415
[2021-05-15 11:39:46,402] {docker.py:276} INFO - 21/05/15 14:39:46 INFO StagingCommitter: Task committer attempt_202105151437594266695467161767210_0004_m_000127_415: needsTaskCommit() Task attempt_202105151437594266695467161767210_0004_m_000127_415: duration 0:00.001s
21/05/15 14:39:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594266695467161767210_0004_m_000127_415
[2021-05-15 11:39:46,403] {docker.py:276} INFO - 21/05/15 14:39:46 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 4544 bytes result sent to driver
[2021-05-15 11:39:46,405] {docker.py:276} INFO - 21/05/15 14:39:46 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (bbe2d2545a9d, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:46,406] {docker.py:276} INFO - 21/05/15 14:39:46 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-15 11:39:46,407] {docker.py:276} INFO - 21/05/15 14:39:46 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 2866 ms on bbe2d2545a9d (executor driver) (128/200)
[2021-05-15 11:39:46,415] {docker.py:276} INFO - 21/05/15 14:39:46 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:46,417] {docker.py:276} INFO - 21/05/15 14:39:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:46,417] {docker.py:276} INFO - 21/05/15 14:39:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598650160797514153930_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598650160797514153930_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598650160797514153930_0004}; taskId=attempt_202105151437598650160797514153930_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d92ea03}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:46 INFO StagingCommitter: Starting: Task committer attempt_202105151437598650160797514153930_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598650160797514153930_0004_m_000131_419
[2021-05-15 11:39:46,420] {docker.py:276} INFO - 21/05/15 14:39:46 INFO StagingCommitter: Task committer attempt_202105151437598650160797514153930_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598650160797514153930_0004_m_000131_419 : duration 0:00.003s
[2021-05-15 11:39:48,237] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437597942417947761788541_0004_m_000128_416: needsTaskCommit() Task attempt_202105151437597942417947761788541_0004_m_000128_416
[2021-05-15 11:39:48,238] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437597942417947761788541_0004_m_000128_416: needsTaskCommit() Task attempt_202105151437597942417947761788541_0004_m_000128_416: duration 0:00.001s
21/05/15 14:39:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597942417947761788541_0004_m_000128_416
[2021-05-15 11:39:48,240] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 4587 bytes result sent to driver
[2021-05-15 11:39:48,241] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (bbe2d2545a9d, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:48,242] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 2634 ms on bbe2d2545a9d (executor driver) (129/200)
21/05/15 14:39:48 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-15 11:39:48,250] {docker.py:276} INFO - 21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:48,252] {docker.py:276} INFO - 21/05/15 14:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:48,252] {docker.py:276} INFO - 21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594250621684517707736_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594250621684517707736_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594250621684517707736_0004}; taskId=attempt_202105151437594250621684517707736_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@223af9b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437594250621684517707736_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594250621684517707736_0004_m_000132_420
[2021-05-15 11:39:48,255] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437594250621684517707736_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594250621684517707736_0004_m_000132_420 : duration 0:00.003s
[2021-05-15 11:39:48,327] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437599003376906776376287_0004_m_000129_417: needsTaskCommit() Task attempt_202105151437599003376906776376287_0004_m_000129_417
[2021-05-15 11:39:48,328] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437599003376906776376287_0004_m_000129_417: needsTaskCommit() Task attempt_202105151437599003376906776376287_0004_m_000129_417: duration 0:00.000s
[2021-05-15 11:39:48,328] {docker.py:276} INFO - 21/05/15 14:39:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599003376906776376287_0004_m_000129_417
[2021-05-15 11:39:48,331] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 4587 bytes result sent to driver
[2021-05-15 11:39:48,332] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (bbe2d2545a9d, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:48,333] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-15 11:39:48,334] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 2652 ms on bbe2d2545a9d (executor driver) (130/200)
[2021-05-15 11:39:48,342] {docker.py:276} INFO - 21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:48,343] {docker.py:276} INFO - 21/05/15 14:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591681739135975102658_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591681739135975102658_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591681739135975102658_0004}; taskId=attempt_202105151437591681739135975102658_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64ac1ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437591681739135975102658_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591681739135975102658_0004_m_000133_421
[2021-05-15 11:39:48,346] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437591681739135975102658_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591681739135975102658_0004_m_000133_421 : duration 0:00.003s
[2021-05-15 11:39:48,501] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437598650160797514153930_0004_m_000131_419: needsTaskCommit() Task attempt_202105151437598650160797514153930_0004_m_000131_419
[2021-05-15 11:39:48,502] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437598650160797514153930_0004_m_000131_419: needsTaskCommit() Task attempt_202105151437598650160797514153930_0004_m_000131_419: duration 0:00.001s
21/05/15 14:39:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598650160797514153930_0004_m_000131_419
[2021-05-15 11:39:48,504] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 4587 bytes result sent to driver
[2021-05-15 11:39:48,505] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (bbe2d2545a9d, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:48,506] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 2104 ms on bbe2d2545a9d (executor driver) (131/200)
21/05/15 14:39:48 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
[2021-05-15 11:39:48,515] {docker.py:276} INFO - 21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:48,517] {docker.py:276} INFO - 21/05/15 14:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593876058406976618074_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593876058406976618074_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593876058406976618074_0004}; taskId=attempt_202105151437593876058406976618074_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40b710d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437593876058406976618074_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593876058406976618074_0004_m_000134_422
[2021-05-15 11:39:48,519] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437593876058406976618074_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593876058406976618074_0004_m_000134_422 : duration 0:00.003s
[2021-05-15 11:39:48,591] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437597010617157568831600_0004_m_000130_418: needsTaskCommit() Task attempt_202105151437597010617157568831600_0004_m_000130_418
[2021-05-15 11:39:48,592] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437597010617157568831600_0004_m_000130_418: needsTaskCommit() Task attempt_202105151437597010617157568831600_0004_m_000130_418: duration 0:00.001s
[2021-05-15 11:39:48,592] {docker.py:276} INFO - 21/05/15 14:39:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597010617157568831600_0004_m_000130_418
[2021-05-15 11:39:48,594] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 4587 bytes result sent to driver
[2021-05-15 11:39:48,595] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (bbe2d2545a9d, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:48,596] {docker.py:276} INFO - 21/05/15 14:39:48 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 2798 ms on bbe2d2545a9d (executor driver) (132/200)
[2021-05-15 11:39:48,597] {docker.py:276} INFO - 21/05/15 14:39:48 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-15 11:39:48,606] {docker.py:276} INFO - 21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:48,608] {docker.py:276} INFO - 21/05/15 14:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593516800572895012479_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593516800572895012479_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593516800572895012479_0004}; taskId=attempt_202105151437593516800572895012479_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@22e30290}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:48 INFO StagingCommitter: Starting: Task committer attempt_202105151437593516800572895012479_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593516800572895012479_0004_m_000135_423
[2021-05-15 11:39:48,612] {docker.py:276} INFO - 21/05/15 14:39:48 INFO StagingCommitter: Task committer attempt_202105151437593516800572895012479_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593516800572895012479_0004_m_000135_423 : duration 0:00.003s
[2021-05-15 11:39:51,625] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_202105151437594250621684517707736_0004_m_000132_420: needsTaskCommit() Task attempt_202105151437594250621684517707736_0004_m_000132_420
[2021-05-15 11:39:51,626] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_202105151437594250621684517707736_0004_m_000132_420: needsTaskCommit() Task attempt_202105151437594250621684517707736_0004_m_000132_420: duration 0:00.000s
21/05/15 14:39:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594250621684517707736_0004_m_000132_420
[2021-05-15 11:39:51,627] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 4544 bytes result sent to driver
[2021-05-15 11:39:51,629] {docker.py:276} INFO - 21/05/15 14:39:51 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (bbe2d2545a9d, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:51,630] {docker.py:276} INFO - 21/05/15 14:39:51 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 3358 ms on bbe2d2545a9d (executor driver) (133/200)
[2021-05-15 11:39:51,631] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-15 11:39:51,640] {docker.py:276} INFO - 21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:51,641] {docker.py:276} INFO - 21/05/15 14:39:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759971151647847938945_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759971151647847938945_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759971151647847938945_0004}; taskId=attempt_20210515143759971151647847938945_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76c39b55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:51,642] {docker.py:276} INFO - 21/05/15 14:39:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_20210515143759971151647847938945_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759971151647847938945_0004_m_000136_424
[2021-05-15 11:39:51,644] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_20210515143759971151647847938945_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759971151647847938945_0004_m_000136_424 : duration 0:00.003s
[2021-05-15 11:39:51,798] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_202105151437593876058406976618074_0004_m_000134_422: needsTaskCommit() Task attempt_202105151437593876058406976618074_0004_m_000134_422
[2021-05-15 11:39:51,799] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_202105151437593876058406976618074_0004_m_000134_422: needsTaskCommit() Task attempt_202105151437593876058406976618074_0004_m_000134_422: duration 0:00.001s
21/05/15 14:39:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593876058406976618074_0004_m_000134_422
[2021-05-15 11:39:51,800] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 4544 bytes result sent to driver
[2021-05-15 11:39:51,801] {docker.py:276} INFO - 21/05/15 14:39:51 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (bbe2d2545a9d, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:51,801] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
[2021-05-15 11:39:51,802] {docker.py:276} INFO - 21/05/15 14:39:51 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 3267 ms on bbe2d2545a9d (executor driver) (134/200)
[2021-05-15 11:39:51,810] {docker.py:276} INFO - 21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Getting 5 (24.1 KiB) non-empty blocks including 5 (24.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:51,812] {docker.py:276} INFO - 21/05/15 14:39:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592516338175432059771_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592516338175432059771_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592516338175432059771_0004}; taskId=attempt_202105151437592516338175432059771_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e7502fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:51,812] {docker.py:276} INFO - 21/05/15 14:39:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:51,812] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_202105151437592516338175432059771_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592516338175432059771_0004_m_000137_425
[2021-05-15 11:39:51,815] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_202105151437592516338175432059771_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592516338175432059771_0004_m_000137_425 : duration 0:00.002s
[2021-05-15 11:39:51,960] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_202105151437593516800572895012479_0004_m_000135_423: needsTaskCommit() Task attempt_202105151437593516800572895012479_0004_m_000135_423
[2021-05-15 11:39:51,962] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_202105151437593516800572895012479_0004_m_000135_423: needsTaskCommit() Task attempt_202105151437593516800572895012479_0004_m_000135_423: duration 0:00.002s
[2021-05-15 11:39:51,963] {docker.py:276} INFO - 21/05/15 14:39:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593516800572895012479_0004_m_000135_423
[2021-05-15 11:39:51,964] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 4544 bytes result sent to driver
[2021-05-15 11:39:51,965] {docker.py:276} INFO - 21/05/15 14:39:51 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (bbe2d2545a9d, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:51,967] {docker.py:276} INFO - 21/05/15 14:39:51 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
21/05/15 14:39:51 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 3341 ms on bbe2d2545a9d (executor driver) (135/200)
[2021-05-15 11:39:51,976] {docker.py:276} INFO - 21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:51,978] {docker.py:276} INFO - 21/05/15 14:39:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759308840002690459406_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759308840002690459406_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759308840002690459406_0004}; taskId=attempt_20210515143759308840002690459406_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@253c98f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:51 INFO StagingCommitter: Starting: Task committer attempt_20210515143759308840002690459406_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759308840002690459406_0004_m_000138_426
[2021-05-15 11:39:51,980] {docker.py:276} INFO - 21/05/15 14:39:51 INFO StagingCommitter: Task committer attempt_20210515143759308840002690459406_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759308840002690459406_0004_m_000138_426 : duration 0:00.002s
[2021-05-15 11:39:52,141] {docker.py:276} INFO - 21/05/15 14:39:52 INFO StagingCommitter: Starting: Task committer attempt_202105151437591681739135975102658_0004_m_000133_421: needsTaskCommit() Task attempt_202105151437591681739135975102658_0004_m_000133_421
[2021-05-15 11:39:52,142] {docker.py:276} INFO - 21/05/15 14:39:52 INFO StagingCommitter: Task committer attempt_202105151437591681739135975102658_0004_m_000133_421: needsTaskCommit() Task attempt_202105151437591681739135975102658_0004_m_000133_421: duration 0:00.000s
21/05/15 14:39:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591681739135975102658_0004_m_000133_421
[2021-05-15 11:39:52,144] {docker.py:276} INFO - 21/05/15 14:39:52 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-15 11:39:52,145] {docker.py:276} INFO - 21/05/15 14:39:52 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (bbe2d2545a9d, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:52,146] {docker.py:276} INFO - 21/05/15 14:39:52 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
21/05/15 14:39:52 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 3784 ms on bbe2d2545a9d (executor driver) (136/200)
[2021-05-15 11:39:52,155] {docker.py:276} INFO - 21/05/15 14:39:52 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:52,157] {docker.py:276} INFO - 21/05/15 14:39:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:52,157] {docker.py:276} INFO - 21/05/15 14:39:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597204491241590798550_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597204491241590798550_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597204491241590798550_0004}; taskId=attempt_202105151437597204491241590798550_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@306cea66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:52 INFO StagingCommitter: Starting: Task committer attempt_202105151437597204491241590798550_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597204491241590798550_0004_m_000139_427
[2021-05-15 11:39:52,160] {docker.py:276} INFO - 21/05/15 14:39:52 INFO StagingCommitter: Task committer attempt_202105151437597204491241590798550_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597204491241590798550_0004_m_000139_427 : duration 0:00.003s
[2021-05-15 11:39:54,198] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_20210515143759971151647847938945_0004_m_000136_424: needsTaskCommit() Task attempt_20210515143759971151647847938945_0004_m_000136_424
[2021-05-15 11:39:54,198] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_20210515143759971151647847938945_0004_m_000136_424: needsTaskCommit() Task attempt_20210515143759971151647847938945_0004_m_000136_424: duration 0:00.001s
21/05/15 14:39:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759971151647847938945_0004_m_000136_424
[2021-05-15 11:39:54,203] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 4544 bytes result sent to driver
[2021-05-15 11:39:54,204] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (bbe2d2545a9d, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:54,205] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 2580 ms on bbe2d2545a9d (executor driver) (137/200)
[2021-05-15 11:39:54,206] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-15 11:39:54,214] {docker.py:276} INFO - 21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:54,216] {docker.py:276} INFO - 21/05/15 14:39:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594576919651153748624_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594576919651153748624_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594576919651153748624_0004}; taskId=attempt_202105151437594576919651153748624_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e94de34}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_202105151437594576919651153748624_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594576919651153748624_0004_m_000140_428
[2021-05-15 11:39:54,219] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_202105151437594576919651153748624_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594576919651153748624_0004_m_000140_428 : duration 0:00.003s
[2021-05-15 11:39:54,327] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_20210515143759308840002690459406_0004_m_000138_426: needsTaskCommit() Task attempt_20210515143759308840002690459406_0004_m_000138_426
21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_20210515143759308840002690459406_0004_m_000138_426: needsTaskCommit() Task attempt_20210515143759308840002690459406_0004_m_000138_426: duration 0:00.000s
21/05/15 14:39:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759308840002690459406_0004_m_000138_426
[2021-05-15 11:39:54,328] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 4544 bytes result sent to driver
[2021-05-15 11:39:54,329] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (bbe2d2545a9d, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:54,330] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
[2021-05-15 11:39:54,330] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 2368 ms on bbe2d2545a9d (executor driver) (138/200)
[2021-05-15 11:39:54,339] {docker.py:276} INFO - 21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:54,340] {docker.py:276} INFO - 21/05/15 14:39:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594758541420091196310_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594758541420091196310_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594758541420091196310_0004}; taskId=attempt_202105151437594758541420091196310_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71e4589f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:54,341] {docker.py:276} INFO - 21/05/15 14:39:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_202105151437594758541420091196310_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594758541420091196310_0004_m_000141_429
[2021-05-15 11:39:54,343] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_202105151437594758541420091196310_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594758541420091196310_0004_m_000141_429 : duration 0:00.003s
[2021-05-15 11:39:54,660] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_202105151437592516338175432059771_0004_m_000137_425: needsTaskCommit() Task attempt_202105151437592516338175432059771_0004_m_000137_425
[2021-05-15 11:39:54,662] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_202105151437592516338175432059771_0004_m_000137_425: needsTaskCommit() Task attempt_202105151437592516338175432059771_0004_m_000137_425: duration 0:00.001s
21/05/15 14:39:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592516338175432059771_0004_m_000137_425
[2021-05-15 11:39:54,662] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 4544 bytes result sent to driver
[2021-05-15 11:39:54,663] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (bbe2d2545a9d, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:54,665] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
21/05/15 14:39:54 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 2867 ms on bbe2d2545a9d (executor driver) (139/200)
[2021-05-15 11:39:54,675] {docker.py:276} INFO - 21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:54,675] {docker.py:276} INFO - 21/05/15 14:39:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:54,677] {docker.py:276} INFO - 21/05/15 14:39:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:39:54,677] {docker.py:276} INFO - 21/05/15 14:39:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:39:54,678] {docker.py:276} INFO - 21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:54,678] {docker.py:276} INFO - 21/05/15 14:39:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593197345069115619566_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593197345069115619566_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593197345069115619566_0004}; taskId=attempt_202105151437593197345069115619566_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3109aa11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:54,678] {docker.py:276} INFO - 21/05/15 14:39:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:54,679] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_202105151437593197345069115619566_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593197345069115619566_0004_m_000142_430
[2021-05-15 11:39:54,681] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_202105151437593197345069115619566_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593197345069115619566_0004_m_000142_430 : duration 0:00.003s
[2021-05-15 11:39:54,988] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Starting: Task committer attempt_202105151437597204491241590798550_0004_m_000139_427: needsTaskCommit() Task attempt_202105151437597204491241590798550_0004_m_000139_427
[2021-05-15 11:39:54,990] {docker.py:276} INFO - 21/05/15 14:39:54 INFO StagingCommitter: Task committer attempt_202105151437597204491241590798550_0004_m_000139_427: needsTaskCommit() Task attempt_202105151437597204491241590798550_0004_m_000139_427: duration 0:00.002s
21/05/15 14:39:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597204491241590798550_0004_m_000139_427
[2021-05-15 11:39:54,991] {docker.py:276} INFO - 21/05/15 14:39:54 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 4544 bytes result sent to driver
[2021-05-15 11:39:54,993] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (bbe2d2545a9d, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:54,995] {docker.py:276} INFO - 21/05/15 14:39:54 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 2853 ms on bbe2d2545a9d (executor driver) (140/200)
[2021-05-15 11:39:54,995] {docker.py:276} INFO - 21/05/15 14:39:55 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
[2021-05-15 11:39:55,004] {docker.py:276} INFO - 21/05/15 14:39:55 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:55,006] {docker.py:276} INFO - 21/05/15 14:39:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:55,007] {docker.py:276} INFO - 21/05/15 14:39:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593107317989926035302_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593107317989926035302_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593107317989926035302_0004}; taskId=attempt_202105151437593107317989926035302_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c0a57b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:55,007] {docker.py:276} INFO - 21/05/15 14:39:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:55 INFO StagingCommitter: Starting: Task committer attempt_202105151437593107317989926035302_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593107317989926035302_0004_m_000143_431
[2021-05-15 11:39:55,010] {docker.py:276} INFO - 21/05/15 14:39:55 INFO StagingCommitter: Task committer attempt_202105151437593107317989926035302_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593107317989926035302_0004_m_000143_431 : duration 0:00.003s
[2021-05-15 11:39:56,924] {docker.py:276} INFO - 21/05/15 14:39:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437594576919651153748624_0004_m_000140_428: needsTaskCommit() Task attempt_202105151437594576919651153748624_0004_m_000140_428
[2021-05-15 11:39:56,925] {docker.py:276} INFO - 21/05/15 14:39:56 INFO StagingCommitter: Task committer attempt_202105151437594576919651153748624_0004_m_000140_428: needsTaskCommit() Task attempt_202105151437594576919651153748624_0004_m_000140_428: duration 0:00.001s
21/05/15 14:39:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594576919651153748624_0004_m_000140_428
[2021-05-15 11:39:56,926] {docker.py:276} INFO - 21/05/15 14:39:56 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 4544 bytes result sent to driver
[2021-05-15 11:39:56,927] {docker.py:276} INFO - 21/05/15 14:39:56 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (bbe2d2545a9d, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:56,929] {docker.py:276} INFO - 21/05/15 14:39:56 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-15 11:39:56,929] {docker.py:276} INFO - 21/05/15 14:39:56 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 2729 ms on bbe2d2545a9d (executor driver) (141/200)
[2021-05-15 11:39:56,938] {docker.py:276} INFO - 21/05/15 14:39:56 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:56,940] {docker.py:276} INFO - 21/05/15 14:39:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594240662193137820579_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594240662193137820579_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594240662193137820579_0004}; taskId=attempt_202105151437594240662193137820579_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55ea4626}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:56 INFO StagingCommitter: Starting: Task committer attempt_202105151437594240662193137820579_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594240662193137820579_0004_m_000144_432
[2021-05-15 11:39:56,942] {docker.py:276} INFO - 21/05/15 14:39:56 INFO StagingCommitter: Task committer attempt_202105151437594240662193137820579_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594240662193137820579_0004_m_000144_432 : duration 0:00.002s
[2021-05-15 11:39:57,064] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437594758541420091196310_0004_m_000141_429: needsTaskCommit() Task attempt_202105151437594758541420091196310_0004_m_000141_429
[2021-05-15 11:39:57,066] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437594758541420091196310_0004_m_000141_429: needsTaskCommit() Task attempt_202105151437594758541420091196310_0004_m_000141_429: duration 0:00.001s
[2021-05-15 11:39:57,066] {docker.py:276} INFO - 21/05/15 14:39:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594758541420091196310_0004_m_000141_429
[2021-05-15 11:39:57,068] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 4544 bytes result sent to driver
[2021-05-15 11:39:57,069] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (bbe2d2545a9d, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:57,070] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-15 11:39:57,071] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 2744 ms on bbe2d2545a9d (executor driver) (142/200)
[2021-05-15 11:39:57,080] {docker.py:276} INFO - 21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:57,081] {docker.py:276} INFO - 21/05/15 14:39:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591740603540860900872_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591740603540860900872_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591740603540860900872_0004}; taskId=attempt_202105151437591740603540860900872_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d8f24ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437591740603540860900872_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591740603540860900872_0004_m_000145_433
[2021-05-15 11:39:57,084] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437591740603540860900872_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591740603540860900872_0004_m_000145_433 : duration 0:00.003s
[2021-05-15 11:39:57,261] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437593197345069115619566_0004_m_000142_430: needsTaskCommit() Task attempt_202105151437593197345069115619566_0004_m_000142_430
[2021-05-15 11:39:57,262] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437593197345069115619566_0004_m_000142_430: needsTaskCommit() Task attempt_202105151437593197345069115619566_0004_m_000142_430: duration 0:00.001s
[2021-05-15 11:39:57,263] {docker.py:276} INFO - 21/05/15 14:39:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593197345069115619566_0004_m_000142_430
[2021-05-15 11:39:57,265] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 4587 bytes result sent to driver
[2021-05-15 11:39:57,266] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (bbe2d2545a9d, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:57,267] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 2607 ms on bbe2d2545a9d (executor driver) (143/200)
[2021-05-15 11:39:57,267] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-15 11:39:57,277] {docker.py:276} INFO - 21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:57,278] {docker.py:276} INFO - 21/05/15 14:39:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593688746768619277922_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593688746768619277922_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593688746768619277922_0004}; taskId=attempt_202105151437593688746768619277922_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ae1850}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437593688746768619277922_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593688746768619277922_0004_m_000146_434
[2021-05-15 11:39:57,281] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437593688746768619277922_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593688746768619277922_0004_m_000146_434 : duration 0:00.003s
[2021-05-15 11:39:57,799] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437593107317989926035302_0004_m_000143_431: needsTaskCommit() Task attempt_202105151437593107317989926035302_0004_m_000143_431
[2021-05-15 11:39:57,800] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437593107317989926035302_0004_m_000143_431: needsTaskCommit() Task attempt_202105151437593107317989926035302_0004_m_000143_431: duration 0:00.000s
21/05/15 14:39:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593107317989926035302_0004_m_000143_431
[2021-05-15 11:39:57,801] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 4587 bytes result sent to driver
[2021-05-15 11:39:57,801] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (bbe2d2545a9d, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:57,802] {docker.py:276} INFO - 21/05/15 14:39:57 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-15 11:39:57,802] {docker.py:276} INFO - 21/05/15 14:39:57 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 2814 ms on bbe2d2545a9d (executor driver) (144/200)
[2021-05-15 11:39:57,809] {docker.py:276} INFO - 21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:57,811] {docker.py:276} INFO - 21/05/15 14:39:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591262294614951102395_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591262294614951102395_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591262294614951102395_0004}; taskId=attempt_202105151437591262294614951102395_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@148109e2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:39:57,811] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Starting: Task committer attempt_202105151437591262294614951102395_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591262294614951102395_0004_m_000147_435
[2021-05-15 11:39:57,814] {docker.py:276} INFO - 21/05/15 14:39:57 INFO StagingCommitter: Task committer attempt_202105151437591262294614951102395_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591262294614951102395_0004_m_000147_435 : duration 0:00.003s
[2021-05-15 11:39:59,563] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437594240662193137820579_0004_m_000144_432: needsTaskCommit() Task attempt_202105151437594240662193137820579_0004_m_000144_432
[2021-05-15 11:39:59,564] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_202105151437594240662193137820579_0004_m_000144_432: needsTaskCommit() Task attempt_202105151437594240662193137820579_0004_m_000144_432: duration 0:00.000s
[2021-05-15 11:39:59,565] {docker.py:276} INFO - 21/05/15 14:39:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594240662193137820579_0004_m_000144_432
[2021-05-15 11:39:59,566] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 4587 bytes result sent to driver
[2021-05-15 11:39:59,566] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (bbe2d2545a9d, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:59,567] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-15 11:39:59,568] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 2644 ms on bbe2d2545a9d (executor driver) (145/200)
[2021-05-15 11:39:59,577] {docker.py:276} INFO - 21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:59,578] {docker.py:276} INFO - 21/05/15 14:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596776178553627819365_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596776178553627819365_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596776178553627819365_0004}; taskId=attempt_202105151437596776178553627819365_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6515571a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437596776178553627819365_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596776178553627819365_0004_m_000148_436
[2021-05-15 11:39:59,582] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_202105151437596776178553627819365_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596776178553627819365_0004_m_000148_436 : duration 0:00.003s
[2021-05-15 11:39:59,895] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437591740603540860900872_0004_m_000145_433: needsTaskCommit() Task attempt_202105151437591740603540860900872_0004_m_000145_433
[2021-05-15 11:39:59,896] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_202105151437591740603540860900872_0004_m_000145_433: needsTaskCommit() Task attempt_202105151437591740603540860900872_0004_m_000145_433: duration 0:00.001s
21/05/15 14:39:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591740603540860900872_0004_m_000145_433
[2021-05-15 11:39:59,898] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 4587 bytes result sent to driver
[2021-05-15 11:39:59,898] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (bbe2d2545a9d, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:59,899] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-15 11:39:59,900] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 2834 ms on bbe2d2545a9d (executor driver) (146/200)
[2021-05-15 11:39:59,907] {docker.py:276} INFO - 21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:59,908] {docker.py:276} INFO - 21/05/15 14:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:59,909] {docker.py:276} INFO - 21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599185463116589375471_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599185463116589375471_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599185463116589375471_0004}; taskId=attempt_202105151437599185463116589375471_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@786f62f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437599185463116589375471_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599185463116589375471_0004_m_000149_437
[2021-05-15 11:39:59,912] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_202105151437599185463116589375471_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599185463116589375471_0004_m_000149_437 : duration 0:00.003s
[2021-05-15 11:39:59,917] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_202105151437593688746768619277922_0004_m_000146_434: needsTaskCommit() Task attempt_202105151437593688746768619277922_0004_m_000146_434
[2021-05-15 11:39:59,918] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_202105151437593688746768619277922_0004_m_000146_434: needsTaskCommit() Task attempt_202105151437593688746768619277922_0004_m_000146_434: duration 0:00.001s
21/05/15 14:39:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593688746768619277922_0004_m_000146_434
[2021-05-15 11:39:59,918] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 4544 bytes result sent to driver
[2021-05-15 11:39:59,919] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (bbe2d2545a9d, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:39:59,920] {docker.py:276} INFO - 21/05/15 14:39:59 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 2659 ms on bbe2d2545a9d (executor driver) (147/200)
[2021-05-15 11:39:59,920] {docker.py:276} INFO - 21/05/15 14:39:59 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-15 11:39:59,929] {docker.py:276} INFO - 21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:39:59,930] {docker.py:276} INFO - 21/05/15 14:39:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:39:59,931] {docker.py:276} INFO - 21/05/15 14:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:39:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759397341803830649968_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759397341803830649968_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759397341803830649968_0004}; taskId=attempt_20210515143759397341803830649968_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ac1afd4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:39:59,932] {docker.py:276} INFO - 21/05/15 14:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:39:59 INFO StagingCommitter: Starting: Task committer attempt_20210515143759397341803830649968_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759397341803830649968_0004_m_000150_438
[2021-05-15 11:39:59,934] {docker.py:276} INFO - 21/05/15 14:39:59 INFO StagingCommitter: Task committer attempt_20210515143759397341803830649968_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759397341803830649968_0004_m_000150_438 : duration 0:00.002s
[2021-05-15 11:40:00,534] {docker.py:276} INFO - 21/05/15 14:40:00 INFO StagingCommitter: Starting: Task committer attempt_202105151437591262294614951102395_0004_m_000147_435: needsTaskCommit() Task attempt_202105151437591262294614951102395_0004_m_000147_435
[2021-05-15 11:40:00,535] {docker.py:276} INFO - 21/05/15 14:40:00 INFO StagingCommitter: Task committer attempt_202105151437591262294614951102395_0004_m_000147_435: needsTaskCommit() Task attempt_202105151437591262294614951102395_0004_m_000147_435: duration 0:00.000s
21/05/15 14:40:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591262294614951102395_0004_m_000147_435
[2021-05-15 11:40:00,536] {docker.py:276} INFO - 21/05/15 14:40:00 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 4544 bytes result sent to driver
[2021-05-15 11:40:00,537] {docker.py:276} INFO - 21/05/15 14:40:00 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (bbe2d2545a9d, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:00,538] {docker.py:276} INFO - 21/05/15 14:40:00 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-15 11:40:00,539] {docker.py:276} INFO - 21/05/15 14:40:00 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 2740 ms on bbe2d2545a9d (executor driver) (148/200)
[2021-05-15 11:40:00,548] {docker.py:276} INFO - 21/05/15 14:40:00 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:00,550] {docker.py:276} INFO - 21/05/15 14:40:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598059958070418132756_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598059958070418132756_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598059958070418132756_0004}; taskId=attempt_202105151437598059958070418132756_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@acc4381}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:00,550] {docker.py:276} INFO - 21/05/15 14:40:00 INFO StagingCommitter: Starting: Task committer attempt_202105151437598059958070418132756_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598059958070418132756_0004_m_000151_439
[2021-05-15 11:40:00,553] {docker.py:276} INFO - 21/05/15 14:40:00 INFO StagingCommitter: Task committer attempt_202105151437598059958070418132756_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598059958070418132756_0004_m_000151_439 : duration 0:00.003s
[2021-05-15 11:40:02,467] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_202105151437596776178553627819365_0004_m_000148_436: needsTaskCommit() Task attempt_202105151437596776178553627819365_0004_m_000148_436
[2021-05-15 11:40:02,468] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_202105151437596776178553627819365_0004_m_000148_436: needsTaskCommit() Task attempt_202105151437596776178553627819365_0004_m_000148_436: duration 0:00.000s
21/05/15 14:40:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596776178553627819365_0004_m_000148_436
[2021-05-15 11:40:02,469] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 4544 bytes result sent to driver
[2021-05-15 11:40:02,471] {docker.py:276} INFO - 21/05/15 14:40:02 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (bbe2d2545a9d, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:02,472] {docker.py:276} INFO - 21/05/15 14:40:02 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 2909 ms on bbe2d2545a9d (executor driver) (149/200)
[2021-05-15 11:40:02,473] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-15 11:40:02,482] {docker.py:276} INFO - 21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Getting 5 (25.7 KiB) non-empty blocks including 5 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:02,484] {docker.py:276} INFO - 21/05/15 14:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593152025120800239140_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593152025120800239140_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593152025120800239140_0004}; taskId=attempt_202105151437593152025120800239140_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dc5af41}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_202105151437593152025120800239140_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593152025120800239140_0004_m_000152_440
[2021-05-15 11:40:02,487] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_202105151437593152025120800239140_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593152025120800239140_0004_m_000152_440 : duration 0:00.003s
[2021-05-15 11:40:02,513] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_202105151437599185463116589375471_0004_m_000149_437: needsTaskCommit() Task attempt_202105151437599185463116589375471_0004_m_000149_437
[2021-05-15 11:40:02,514] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_202105151437599185463116589375471_0004_m_000149_437: needsTaskCommit() Task attempt_202105151437599185463116589375471_0004_m_000149_437: duration 0:00.001s
21/05/15 14:40:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599185463116589375471_0004_m_000149_437
[2021-05-15 11:40:02,515] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 4544 bytes result sent to driver
[2021-05-15 11:40:02,516] {docker.py:276} INFO - 21/05/15 14:40:02 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (bbe2d2545a9d, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:02,518] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-15 11:40:02,518] {docker.py:276} INFO - 21/05/15 14:40:02 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 2623 ms on bbe2d2545a9d (executor driver) (150/200)
[2021-05-15 11:40:02,527] {docker.py:276} INFO - 21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:02,529] {docker.py:276} INFO - 21/05/15 14:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591219899991874291062_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591219899991874291062_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591219899991874291062_0004}; taskId=attempt_202105151437591219899991874291062_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b313109}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_202105151437591219899991874291062_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591219899991874291062_0004_m_000153_441
[2021-05-15 11:40:02,532] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_202105151437591219899991874291062_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591219899991874291062_0004_m_000153_441 : duration 0:00.003s
[2021-05-15 11:40:02,665] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_20210515143759397341803830649968_0004_m_000150_438: needsTaskCommit() Task attempt_20210515143759397341803830649968_0004_m_000150_438
[2021-05-15 11:40:02,666] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_20210515143759397341803830649968_0004_m_000150_438: needsTaskCommit() Task attempt_20210515143759397341803830649968_0004_m_000150_438: duration 0:00.001s
21/05/15 14:40:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759397341803830649968_0004_m_000150_438
[2021-05-15 11:40:02,667] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 4544 bytes result sent to driver
[2021-05-15 11:40:02,668] {docker.py:276} INFO - 21/05/15 14:40:02 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (bbe2d2545a9d, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:02,670] {docker.py:276} INFO - 21/05/15 14:40:02 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
21/05/15 14:40:02 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 2754 ms on bbe2d2545a9d (executor driver) (151/200)
[2021-05-15 11:40:02,680] {docker.py:276} INFO - 21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:02,682] {docker.py:276} INFO - 21/05/15 14:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:02,683] {docker.py:276} INFO - 21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:02,683] {docker.py:276} INFO - 21/05/15 14:40:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594540485047244472309_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594540485047244472309_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594540485047244472309_0004}; taskId=attempt_202105151437594540485047244472309_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3528c45e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:02 INFO StagingCommitter: Starting: Task committer attempt_202105151437594540485047244472309_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594540485047244472309_0004_m_000154_442
[2021-05-15 11:40:02,686] {docker.py:276} INFO - 21/05/15 14:40:02 INFO StagingCommitter: Task committer attempt_202105151437594540485047244472309_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594540485047244472309_0004_m_000154_442 : duration 0:00.003s
[2021-05-15 11:40:03,424] {docker.py:276} INFO - 21/05/15 14:40:03 INFO StagingCommitter: Starting: Task committer attempt_202105151437598059958070418132756_0004_m_000151_439: needsTaskCommit() Task attempt_202105151437598059958070418132756_0004_m_000151_439
21/05/15 14:40:03 INFO StagingCommitter: Task committer attempt_202105151437598059958070418132756_0004_m_000151_439: needsTaskCommit() Task attempt_202105151437598059958070418132756_0004_m_000151_439: duration 0:00.001s
21/05/15 14:40:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598059958070418132756_0004_m_000151_439
[2021-05-15 11:40:03,427] {docker.py:276} INFO - 21/05/15 14:40:03 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 4544 bytes result sent to driver
[2021-05-15 11:40:03,429] {docker.py:276} INFO - 21/05/15 14:40:03 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (bbe2d2545a9d, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:03,430] {docker.py:276} INFO - 21/05/15 14:40:03 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
21/05/15 14:40:03 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 2897 ms on bbe2d2545a9d (executor driver) (152/200)
[2021-05-15 11:40:03,440] {docker.py:276} INFO - 21/05/15 14:40:03 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:03,442] {docker.py:276} INFO - 21/05/15 14:40:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593335681572865269323_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593335681572865269323_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593335681572865269323_0004}; taskId=attempt_202105151437593335681572865269323_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16566305}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:03 INFO StagingCommitter: Starting: Task committer attempt_202105151437593335681572865269323_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593335681572865269323_0004_m_000155_443
[2021-05-15 11:40:03,445] {docker.py:276} INFO - 21/05/15 14:40:03 INFO StagingCommitter: Task committer attempt_202105151437593335681572865269323_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593335681572865269323_0004_m_000155_443 : duration 0:00.003s
[2021-05-15 11:40:05,163] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_202105151437593152025120800239140_0004_m_000152_440: needsTaskCommit() Task attempt_202105151437593152025120800239140_0004_m_000152_440
[2021-05-15 11:40:05,164] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_202105151437593152025120800239140_0004_m_000152_440: needsTaskCommit() Task attempt_202105151437593152025120800239140_0004_m_000152_440: duration 0:00.001s
21/05/15 14:40:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593152025120800239140_0004_m_000152_440
[2021-05-15 11:40:05,166] {docker.py:276} INFO - 21/05/15 14:40:05 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 4544 bytes result sent to driver
[2021-05-15 11:40:05,167] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (bbe2d2545a9d, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:05,168] {docker.py:276} INFO - 21/05/15 14:40:05 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-15 11:40:05,169] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 2702 ms on bbe2d2545a9d (executor driver) (153/200)
[2021-05-15 11:40:05,178] {docker.py:276} INFO - 21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:05,180] {docker.py:276} INFO - 21/05/15 14:40:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:05,180] {docker.py:276} INFO - 21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759156989156610844784_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759156989156610844784_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759156989156610844784_0004}; taskId=attempt_20210515143759156989156610844784_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@672cdc5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_20210515143759156989156610844784_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759156989156610844784_0004_m_000156_444
[2021-05-15 11:40:05,183] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_20210515143759156989156610844784_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759156989156610844784_0004_m_000156_444 : duration 0:00.003s
[2021-05-15 11:40:05,309] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_202105151437591219899991874291062_0004_m_000153_441: needsTaskCommit() Task attempt_202105151437591219899991874291062_0004_m_000153_441
[2021-05-15 11:40:05,310] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_202105151437591219899991874291062_0004_m_000153_441: needsTaskCommit() Task attempt_202105151437591219899991874291062_0004_m_000153_441: duration 0:00.002s
[2021-05-15 11:40:05,311] {docker.py:276} INFO - 21/05/15 14:40:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591219899991874291062_0004_m_000153_441
[2021-05-15 11:40:05,313] {docker.py:276} INFO - 21/05/15 14:40:05 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 4544 bytes result sent to driver
[2021-05-15 11:40:05,315] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (bbe2d2545a9d, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:05,316] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 2802 ms on bbe2d2545a9d (executor driver) (154/200)
21/05/15 14:40:05 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-15 11:40:05,325] {docker.py:276} INFO - 21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Getting 5 (24.6 KiB) non-empty blocks including 5 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:05,326] {docker.py:276} INFO - 21/05/15 14:40:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595785586511915355278_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595785586511915355278_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595785586511915355278_0004}; taskId=attempt_202105151437595785586511915355278_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53c2dc04}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_202105151437595785586511915355278_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595785586511915355278_0004_m_000157_445
[2021-05-15 11:40:05,329] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_202105151437595785586511915355278_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595785586511915355278_0004_m_000157_445 : duration 0:00.002s
[2021-05-15 11:40:05,424] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_202105151437594540485047244472309_0004_m_000154_442: needsTaskCommit() Task attempt_202105151437594540485047244472309_0004_m_000154_442
[2021-05-15 11:40:05,425] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_202105151437594540485047244472309_0004_m_000154_442: needsTaskCommit() Task attempt_202105151437594540485047244472309_0004_m_000154_442: duration 0:00.001s
21/05/15 14:40:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594540485047244472309_0004_m_000154_442
[2021-05-15 11:40:05,427] {docker.py:276} INFO - 21/05/15 14:40:05 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 4544 bytes result sent to driver
[2021-05-15 11:40:05,428] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (bbe2d2545a9d, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:05,429] {docker.py:276} INFO - 21/05/15 14:40:05 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-15 11:40:05,430] {docker.py:276} INFO - 21/05/15 14:40:05 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 2764 ms on bbe2d2545a9d (executor driver) (155/200)
[2021-05-15 11:40:05,438] {docker.py:276} INFO - 21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:05,439] {docker.py:276} INFO - 21/05/15 14:40:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593158089954704378730_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593158089954704378730_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593158089954704378730_0004}; taskId=attempt_202105151437593158089954704378730_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72b89e83}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:05 INFO StagingCommitter: Starting: Task committer attempt_202105151437593158089954704378730_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593158089954704378730_0004_m_000158_446
[2021-05-15 11:40:05,442] {docker.py:276} INFO - 21/05/15 14:40:05 INFO StagingCommitter: Task committer attempt_202105151437593158089954704378730_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593158089954704378730_0004_m_000158_446 : duration 0:00.003s
[2021-05-15 11:40:06,098] {docker.py:276} INFO - 21/05/15 14:40:06 INFO StagingCommitter: Starting: Task committer attempt_202105151437593335681572865269323_0004_m_000155_443: needsTaskCommit() Task attempt_202105151437593335681572865269323_0004_m_000155_443
[2021-05-15 11:40:06,099] {docker.py:276} INFO - 21/05/15 14:40:06 INFO StagingCommitter: Task committer attempt_202105151437593335681572865269323_0004_m_000155_443: needsTaskCommit() Task attempt_202105151437593335681572865269323_0004_m_000155_443: duration 0:00.001s
[2021-05-15 11:40:06,100] {docker.py:276} INFO - 21/05/15 14:40:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593335681572865269323_0004_m_000155_443
[2021-05-15 11:40:06,101] {docker.py:276} INFO - 21/05/15 14:40:06 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 4544 bytes result sent to driver
[2021-05-15 11:40:06,102] {docker.py:276} INFO - 21/05/15 14:40:06 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (bbe2d2545a9d, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:06,103] {docker.py:276} INFO - 21/05/15 14:40:06 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-15 11:40:06,104] {docker.py:276} INFO - 21/05/15 14:40:06 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 2677 ms on bbe2d2545a9d (executor driver) (156/200)
[2021-05-15 11:40:06,122] {docker.py:276} INFO - 21/05/15 14:40:06 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:06,124] {docker.py:276} INFO - 21/05/15 14:40:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592072249301214345629_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592072249301214345629_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592072249301214345629_0004}; taskId=attempt_202105151437592072249301214345629_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@251fcd23}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:06,124] {docker.py:276} INFO - 21/05/15 14:40:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:06 INFO StagingCommitter: Starting: Task committer attempt_202105151437592072249301214345629_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592072249301214345629_0004_m_000159_447
[2021-05-15 11:40:06,126] {docker.py:276} INFO - 21/05/15 14:40:06 INFO StagingCommitter: Task committer attempt_202105151437592072249301214345629_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592072249301214345629_0004_m_000159_447 : duration 0:00.003s
[2021-05-15 11:40:07,982] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_20210515143759156989156610844784_0004_m_000156_444: needsTaskCommit() Task attempt_20210515143759156989156610844784_0004_m_000156_444
[2021-05-15 11:40:07,983] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_20210515143759156989156610844784_0004_m_000156_444: needsTaskCommit() Task attempt_20210515143759156989156610844784_0004_m_000156_444: duration 0:00.001s
21/05/15 14:40:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759156989156610844784_0004_m_000156_444
[2021-05-15 11:40:07,984] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437595785586511915355278_0004_m_000157_445: needsTaskCommit() Task attempt_202105151437595785586511915355278_0004_m_000157_445
[2021-05-15 11:40:07,985] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437595785586511915355278_0004_m_000157_445: needsTaskCommit() Task attempt_202105151437595785586511915355278_0004_m_000157_445: duration 0:00.001s
21/05/15 14:40:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595785586511915355278_0004_m_000157_445
[2021-05-15 11:40:07,986] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 4587 bytes result sent to driver
21/05/15 14:40:08 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 4587 bytes result sent to driver
[2021-05-15 11:40:07,986] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (bbe2d2545a9d, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:07,987] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (bbe2d2545a9d, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:07,988] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
[2021-05-15 11:40:07,988] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 2678 ms on bbe2d2545a9d (executor driver) (157/200)
[2021-05-15 11:40:07,989] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 2825 ms on bbe2d2545a9d (executor driver) (158/200)
[2021-05-15 11:40:07,990] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-15 11:40:07,998] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:08,000] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:08,000] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/15 14:40:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:08,001] {docker.py:276} INFO - 21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591361235069101946523_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591361235069101946523_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591361235069101946523_0004}; taskId=attempt_202105151437591361235069101946523_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ae5a905}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:08,002] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437591361235069101946523_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591361235069101946523_0004_m_000160_448
[2021-05-15 11:40:08,002] {docker.py:276} INFO - 21/05/15 14:40:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:08,003] {docker.py:276} INFO - 21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598128493527942754709_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598128493527942754709_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598128493527942754709_0004}; taskId=attempt_202105151437598128493527942754709_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f84acd0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:08,003] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437598128493527942754709_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598128493527942754709_0004_m_000161_449
[2021-05-15 11:40:08,003] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437591361235069101946523_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591361235069101946523_0004_m_000160_448 : duration 0:00.004s
[2021-05-15 11:40:08,006] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437598128493527942754709_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598128493527942754709_0004_m_000161_449 : duration 0:00.005s
[2021-05-15 11:40:08,103] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437593158089954704378730_0004_m_000158_446: needsTaskCommit() Task attempt_202105151437593158089954704378730_0004_m_000158_446
[2021-05-15 11:40:08,104] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437593158089954704378730_0004_m_000158_446: needsTaskCommit() Task attempt_202105151437593158089954704378730_0004_m_000158_446: duration 0:00.002s
[2021-05-15 11:40:08,105] {docker.py:276} INFO - 21/05/15 14:40:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593158089954704378730_0004_m_000158_446
[2021-05-15 11:40:08,106] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 4587 bytes result sent to driver
[2021-05-15 11:40:08,108] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (bbe2d2545a9d, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:08,108] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 2684 ms on bbe2d2545a9d (executor driver) (159/200)
[2021-05-15 11:40:08,109] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-15 11:40:08,118] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Getting 5 (24.6 KiB) non-empty blocks including 5 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:08,119] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:08,120] {docker.py:276} INFO - 21/05/15 14:40:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:08,121] {docker.py:276} INFO - 21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759543926193906476656_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759543926193906476656_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759543926193906476656_0004}; taskId=attempt_20210515143759543926193906476656_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@303f450e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:08,121] {docker.py:276} INFO - 21/05/15 14:40:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:08,121] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_20210515143759543926193906476656_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759543926193906476656_0004_m_000162_450
[2021-05-15 11:40:08,124] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_20210515143759543926193906476656_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759543926193906476656_0004_m_000162_450 : duration 0:00.002s
[2021-05-15 11:40:08,839] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437592072249301214345629_0004_m_000159_447: needsTaskCommit() Task attempt_202105151437592072249301214345629_0004_m_000159_447
[2021-05-15 11:40:08,839] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437592072249301214345629_0004_m_000159_447: needsTaskCommit() Task attempt_202105151437592072249301214345629_0004_m_000159_447: duration 0:00.000s
21/05/15 14:40:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592072249301214345629_0004_m_000159_447
[2021-05-15 11:40:08,842] {docker.py:276} INFO - 21/05/15 14:40:08 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 4587 bytes result sent to driver
21/05/15 14:40:08 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (bbe2d2545a9d, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:08,842] {docker.py:276} INFO - 21/05/15 14:40:08 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 2744 ms on bbe2d2545a9d (executor driver) (160/200)
21/05/15 14:40:08 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-15 11:40:08,852] {docker.py:276} INFO - 21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:08,853] {docker.py:276} INFO - 21/05/15 14:40:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:08,854] {docker.py:276} INFO - 21/05/15 14:40:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591493523637568997015_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591493523637568997015_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591493523637568997015_0004}; taskId=attempt_202105151437591493523637568997015_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a62a233}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:08,854] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Starting: Task committer attempt_202105151437591493523637568997015_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591493523637568997015_0004_m_000163_451
[2021-05-15 11:40:08,857] {docker.py:276} INFO - 21/05/15 14:40:08 INFO StagingCommitter: Task committer attempt_202105151437591493523637568997015_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591493523637568997015_0004_m_000163_451 : duration 0:00.003s
[2021-05-15 11:40:10,713] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_20210515143759543926193906476656_0004_m_000162_450: needsTaskCommit() Task attempt_20210515143759543926193906476656_0004_m_000162_450
[2021-05-15 11:40:10,714] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_20210515143759543926193906476656_0004_m_000162_450: needsTaskCommit() Task attempt_20210515143759543926193906476656_0004_m_000162_450: duration 0:00.001s
21/05/15 14:40:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759543926193906476656_0004_m_000162_450
[2021-05-15 11:40:10,715] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 4544 bytes result sent to driver
[2021-05-15 11:40:10,716] {docker.py:276} INFO - 21/05/15 14:40:10 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (bbe2d2545a9d, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:10,718] {docker.py:276} INFO - 21/05/15 14:40:10 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 2613 ms on bbe2d2545a9d (executor driver) (161/200)
[2021-05-15 11:40:10,719] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-15 11:40:10,728] {docker.py:276} INFO - 21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:10,730] {docker.py:276} INFO - 21/05/15 14:40:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:10,731] {docker.py:276} INFO - 21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591357020736437755830_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591357020736437755830_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591357020736437755830_0004}; taskId=attempt_202105151437591357020736437755830_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@223920bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:10,731] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_202105151437591357020736437755830_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591357020736437755830_0004_m_000164_452
[2021-05-15 11:40:10,732] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_202105151437591361235069101946523_0004_m_000160_448: needsTaskCommit() Task attempt_202105151437591361235069101946523_0004_m_000160_448
[2021-05-15 11:40:10,732] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_202105151437591361235069101946523_0004_m_000160_448: needsTaskCommit() Task attempt_202105151437591361235069101946523_0004_m_000160_448: duration 0:00.000s
21/05/15 14:40:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591361235069101946523_0004_m_000160_448
[2021-05-15 11:40:10,732] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 4544 bytes result sent to driver
[2021-05-15 11:40:10,733] {docker.py:276} INFO - 21/05/15 14:40:10 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (bbe2d2545a9d, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:10,734] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
21/05/15 14:40:10 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 2752 ms on bbe2d2545a9d (executor driver) (162/200)
[2021-05-15 11:40:10,736] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_202105151437591357020736437755830_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591357020736437755830_0004_m_000164_452 : duration 0:00.005s
[2021-05-15 11:40:10,742] {docker.py:276} INFO - 21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:10,744] {docker.py:276} INFO - 21/05/15 14:40:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596555356056867547845_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596555356056867547845_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596555356056867547845_0004}; taskId=attempt_202105151437596555356056867547845_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11b5dd55}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_202105151437596555356056867547845_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596555356056867547845_0004_m_000165_453
[2021-05-15 11:40:10,747] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_202105151437596555356056867547845_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596555356056867547845_0004_m_000165_453 : duration 0:00.003s
[2021-05-15 11:40:10,807] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_202105151437598128493527942754709_0004_m_000161_449: needsTaskCommit() Task attempt_202105151437598128493527942754709_0004_m_000161_449
[2021-05-15 11:40:10,808] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_202105151437598128493527942754709_0004_m_000161_449: needsTaskCommit() Task attempt_202105151437598128493527942754709_0004_m_000161_449: duration 0:00.001s
[2021-05-15 11:40:10,808] {docker.py:276} INFO - 21/05/15 14:40:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598128493527942754709_0004_m_000161_449
[2021-05-15 11:40:10,809] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 4544 bytes result sent to driver
[2021-05-15 11:40:10,811] {docker.py:276} INFO - 21/05/15 14:40:10 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (bbe2d2545a9d, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:10,813] {docker.py:276} INFO - 21/05/15 14:40:10 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
[2021-05-15 11:40:10,813] {docker.py:276} INFO - 21/05/15 14:40:10 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 2829 ms on bbe2d2545a9d (executor driver) (163/200)
[2021-05-15 11:40:10,822] {docker.py:276} INFO - 21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:10,824] {docker.py:276} INFO - 21/05/15 14:40:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597955370175508698648_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597955370175508698648_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597955370175508698648_0004}; taskId=attempt_202105151437597955370175508698648_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79d3f4cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:10,824] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Starting: Task committer attempt_202105151437597955370175508698648_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597955370175508698648_0004_m_000166_454
[2021-05-15 11:40:10,827] {docker.py:276} INFO - 21/05/15 14:40:10 INFO StagingCommitter: Task committer attempt_202105151437597955370175508698648_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597955370175508698648_0004_m_000166_454 : duration 0:00.003s
[2021-05-15 11:40:11,456] {docker.py:276} INFO - 21/05/15 14:40:11 INFO StagingCommitter: Starting: Task committer attempt_202105151437591493523637568997015_0004_m_000163_451: needsTaskCommit() Task attempt_202105151437591493523637568997015_0004_m_000163_451
[2021-05-15 11:40:11,457] {docker.py:276} INFO - 21/05/15 14:40:11 INFO StagingCommitter: Task committer attempt_202105151437591493523637568997015_0004_m_000163_451: needsTaskCommit() Task attempt_202105151437591493523637568997015_0004_m_000163_451: duration 0:00.001s
[2021-05-15 11:40:11,457] {docker.py:276} INFO - 21/05/15 14:40:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591493523637568997015_0004_m_000163_451
[2021-05-15 11:40:11,458] {docker.py:276} INFO - 21/05/15 14:40:11 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 4544 bytes result sent to driver
[2021-05-15 11:40:11,459] {docker.py:276} INFO - 21/05/15 14:40:11 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (bbe2d2545a9d, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:11,460] {docker.py:276} INFO - 21/05/15 14:40:11 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-15 11:40:11,461] {docker.py:276} INFO - 21/05/15 14:40:11 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 2623 ms on bbe2d2545a9d (executor driver) (164/200)
[2021-05-15 11:40:11,470] {docker.py:276} INFO - 21/05/15 14:40:11 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:11,472] {docker.py:276} INFO - 21/05/15 14:40:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:11,473] {docker.py:276} INFO - 21/05/15 14:40:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593171550467329339812_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593171550467329339812_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593171550467329339812_0004}; taskId=attempt_202105151437593171550467329339812_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7301a5a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:11 INFO StagingCommitter: Starting: Task committer attempt_202105151437593171550467329339812_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593171550467329339812_0004_m_000167_455
[2021-05-15 11:40:11,476] {docker.py:276} INFO - 21/05/15 14:40:11 INFO StagingCommitter: Task committer attempt_202105151437593171550467329339812_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593171550467329339812_0004_m_000167_455 : duration 0:00.003s
[2021-05-15 11:40:13,348] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437591357020736437755830_0004_m_000164_452: needsTaskCommit() Task attempt_202105151437591357020736437755830_0004_m_000164_452
21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437591357020736437755830_0004_m_000164_452: needsTaskCommit() Task attempt_202105151437591357020736437755830_0004_m_000164_452: duration 0:00.001s
21/05/15 14:40:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591357020736437755830_0004_m_000164_452
[2021-05-15 11:40:13,351] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 4544 bytes result sent to driver
[2021-05-15 11:40:13,352] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (bbe2d2545a9d, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:13,353] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 2640 ms on bbe2d2545a9d (executor driver) (165/200)
[2021-05-15 11:40:13,354] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-15 11:40:13,363] {docker.py:276} INFO - 21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:13,364] {docker.py:276} INFO - 21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:13,365] {docker.py:276} INFO - 21/05/15 14:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:13,366] {docker.py:276} INFO - 21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437596915123014259019190_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596915123014259019190_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437596915123014259019190_0004}; taskId=attempt_202105151437596915123014259019190_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@331eb3c7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:13,366] {docker.py:276} INFO - 21/05/15 14:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:13,367] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437596915123014259019190_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596915123014259019190_0004_m_000168_456
[2021-05-15 11:40:13,369] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437596915123014259019190_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437596915123014259019190_0004_m_000168_456 : duration 0:00.003s
[2021-05-15 11:40:13,545] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437596555356056867547845_0004_m_000165_453: needsTaskCommit() Task attempt_202105151437596555356056867547845_0004_m_000165_453
[2021-05-15 11:40:13,546] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437596555356056867547845_0004_m_000165_453: needsTaskCommit() Task attempt_202105151437596555356056867547845_0004_m_000165_453: duration 0:00.001s
21/05/15 14:40:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596555356056867547845_0004_m_000165_453
[2021-05-15 11:40:13,548] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 4544 bytes result sent to driver
[2021-05-15 11:40:13,549] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (bbe2d2545a9d, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:13,550] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 2819 ms on bbe2d2545a9d (executor driver) (166/200)
[2021-05-15 11:40:13,551] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-15 11:40:13,559] {docker.py:276} INFO - 21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Getting 5 (23.3 KiB) non-empty blocks including 5 (23.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:13,561] {docker.py:276} INFO - 21/05/15 14:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594676413653098039219_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594676413653098039219_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594676413653098039219_0004}; taskId=attempt_202105151437594676413653098039219_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4edf2ab1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437594676413653098039219_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594676413653098039219_0004_m_000169_457
[2021-05-15 11:40:13,563] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437594676413653098039219_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594676413653098039219_0004_m_000169_457 : duration 0:00.003s
[2021-05-15 11:40:13,643] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437597955370175508698648_0004_m_000166_454: needsTaskCommit() Task attempt_202105151437597955370175508698648_0004_m_000166_454
[2021-05-15 11:40:13,644] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437597955370175508698648_0004_m_000166_454: needsTaskCommit() Task attempt_202105151437597955370175508698648_0004_m_000166_454: duration 0:00.000s
21/05/15 14:40:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597955370175508698648_0004_m_000166_454
[2021-05-15 11:40:13,646] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 4544 bytes result sent to driver
[2021-05-15 11:40:13,647] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (bbe2d2545a9d, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:13,648] {docker.py:276} INFO - 21/05/15 14:40:13 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
[2021-05-15 11:40:13,649] {docker.py:276} INFO - 21/05/15 14:40:13 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 2842 ms on bbe2d2545a9d (executor driver) (167/200)
[2021-05-15 11:40:13,656] {docker.py:276} INFO - 21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Getting 5 (25.7 KiB) non-empty blocks including 5 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:13,658] {docker.py:276} INFO - 21/05/15 14:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594975502245257641276_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594975502245257641276_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594975502245257641276_0004}; taskId=attempt_202105151437594975502245257641276_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3647a435}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:13,658] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Starting: Task committer attempt_202105151437594975502245257641276_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594975502245257641276_0004_m_000170_458
[2021-05-15 11:40:13,661] {docker.py:276} INFO - 21/05/15 14:40:13 INFO StagingCommitter: Task committer attempt_202105151437594975502245257641276_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594975502245257641276_0004_m_000170_458 : duration 0:00.003s
[2021-05-15 11:40:14,127] {docker.py:276} INFO - 21/05/15 14:40:14 INFO StagingCommitter: Starting: Task committer attempt_202105151437593171550467329339812_0004_m_000167_455: needsTaskCommit() Task attempt_202105151437593171550467329339812_0004_m_000167_455
[2021-05-15 11:40:14,128] {docker.py:276} INFO - 21/05/15 14:40:14 INFO StagingCommitter: Task committer attempt_202105151437593171550467329339812_0004_m_000167_455: needsTaskCommit() Task attempt_202105151437593171550467329339812_0004_m_000167_455: duration 0:00.001s
21/05/15 14:40:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593171550467329339812_0004_m_000167_455
[2021-05-15 11:40:14,130] {docker.py:276} INFO - 21/05/15 14:40:14 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 4544 bytes result sent to driver
[2021-05-15 11:40:14,132] {docker.py:276} INFO - 21/05/15 14:40:14 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (bbe2d2545a9d, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:14,133] {docker.py:276} INFO - 21/05/15 14:40:14 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
21/05/15 14:40:14 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 2677 ms on bbe2d2545a9d (executor driver) (168/200)
[2021-05-15 11:40:14,143] {docker.py:276} INFO - 21/05/15 14:40:14 INFO ShuffleBlockFetcherIterator: Getting 5 (26.4 KiB) non-empty blocks including 5 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:14,145] {docker.py:276} INFO - 21/05/15 14:40:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597464404970814916964_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597464404970814916964_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597464404970814916964_0004}; taskId=attempt_202105151437597464404970814916964_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68cef285}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:14,145] {docker.py:276} INFO - 21/05/15 14:40:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:14 INFO StagingCommitter: Starting: Task committer attempt_202105151437597464404970814916964_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597464404970814916964_0004_m_000171_459
[2021-05-15 11:40:14,149] {docker.py:276} INFO - 21/05/15 14:40:14 INFO StagingCommitter: Task committer attempt_202105151437597464404970814916964_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597464404970814916964_0004_m_000171_459 : duration 0:00.003s
[2021-05-15 11:40:16,031] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437596915123014259019190_0004_m_000168_456: needsTaskCommit() Task attempt_202105151437596915123014259019190_0004_m_000168_456
[2021-05-15 11:40:16,032] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437596915123014259019190_0004_m_000168_456: needsTaskCommit() Task attempt_202105151437596915123014259019190_0004_m_000168_456: duration 0:00.001s
[2021-05-15 11:40:16,033] {docker.py:276} INFO - 21/05/15 14:40:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437596915123014259019190_0004_m_000168_456
[2021-05-15 11:40:16,033] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 4544 bytes result sent to driver
[2021-05-15 11:40:16,035] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (bbe2d2545a9d, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:16,036] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
[2021-05-15 11:40:16,037] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 2687 ms on bbe2d2545a9d (executor driver) (169/200)
[2021-05-15 11:40:16,046] {docker.py:276} INFO - 21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Getting 5 (23.7 KiB) non-empty blocks including 5 (23.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:16,047] {docker.py:276} INFO - 21/05/15 14:40:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:16,048] {docker.py:276} INFO - 21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598381944432713808959_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598381944432713808959_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598381944432713808959_0004}; taskId=attempt_202105151437598381944432713808959_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b5cdcfe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437598381944432713808959_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598381944432713808959_0004_m_000172_460
[2021-05-15 11:40:16,063] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437598381944432713808959_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598381944432713808959_0004_m_000172_460 : duration 0:00.015s
[2021-05-15 11:40:16,220] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437594676413653098039219_0004_m_000169_457: needsTaskCommit() Task attempt_202105151437594676413653098039219_0004_m_000169_457
[2021-05-15 11:40:16,221] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437594676413653098039219_0004_m_000169_457: needsTaskCommit() Task attempt_202105151437594676413653098039219_0004_m_000169_457: duration 0:00.002s
21/05/15 14:40:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594676413653098039219_0004_m_000169_457
[2021-05-15 11:40:16,223] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 4587 bytes result sent to driver
[2021-05-15 11:40:16,224] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (bbe2d2545a9d, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:16,226] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 2681 ms on bbe2d2545a9d (executor driver) (170/200)
[2021-05-15 11:40:16,227] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-15 11:40:16,240] {docker.py:276} INFO - 21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Getting 5 (24.2 KiB) non-empty blocks including 5 (24.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:16,242] {docker.py:276} INFO - 21/05/15 14:40:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437597275853129417867474_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597275853129417867474_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437597275853129417867474_0004}; taskId=attempt_202105151437597275853129417867474_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e3b5143}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:16,242] {docker.py:276} INFO - 21/05/15 14:40:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437597275853129417867474_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597275853129417867474_0004_m_000173_461
[2021-05-15 11:40:16,245] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437597275853129417867474_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437597275853129417867474_0004_m_000173_461 : duration 0:00.003s
[2021-05-15 11:40:16,401] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437594975502245257641276_0004_m_000170_458: needsTaskCommit() Task attempt_202105151437594975502245257641276_0004_m_000170_458
[2021-05-15 11:40:16,401] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437594975502245257641276_0004_m_000170_458: needsTaskCommit() Task attempt_202105151437594975502245257641276_0004_m_000170_458: duration 0:00.001s
21/05/15 14:40:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594975502245257641276_0004_m_000170_458
[2021-05-15 11:40:16,403] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 4587 bytes result sent to driver
[2021-05-15 11:40:16,404] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (bbe2d2545a9d, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:16,405] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-15 11:40:16,406] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 2762 ms on bbe2d2545a9d (executor driver) (171/200)
[2021-05-15 11:40:16,416] {docker.py:276} INFO - 21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Getting 5 (25.1 KiB) non-empty blocks including 5 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:16,417] {docker.py:276} INFO - 21/05/15 14:40:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595366124861308697108_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595366124861308697108_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595366124861308697108_0004}; taskId=attempt_202105151437595366124861308697108_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a393ef9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:16,417] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437595366124861308697108_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595366124861308697108_0004_m_000174_462
[2021-05-15 11:40:16,420] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437595366124861308697108_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595366124861308697108_0004_m_000174_462 : duration 0:00.004s
[2021-05-15 11:40:16,825] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437597464404970814916964_0004_m_000171_459: needsTaskCommit() Task attempt_202105151437597464404970814916964_0004_m_000171_459
21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437597464404970814916964_0004_m_000171_459: needsTaskCommit() Task attempt_202105151437597464404970814916964_0004_m_000171_459: duration 0:00.001s
[2021-05-15 11:40:16,826] {docker.py:276} INFO - 21/05/15 14:40:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597464404970814916964_0004_m_000171_459
[2021-05-15 11:40:16,827] {docker.py:276} INFO - 21/05/15 14:40:16 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 4587 bytes result sent to driver
[2021-05-15 11:40:16,829] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (bbe2d2545a9d, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:16,830] {docker.py:276} INFO - 21/05/15 14:40:16 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 2701 ms on bbe2d2545a9d (executor driver) (172/200)
21/05/15 14:40:16 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-15 11:40:16,840] {docker.py:276} INFO - 21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:16,842] {docker.py:276} INFO - 21/05/15 14:40:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599008569843739934120_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599008569843739934120_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599008569843739934120_0004}; taskId=attempt_202105151437599008569843739934120_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@580aecf8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:16,842] {docker.py:276} INFO - 21/05/15 14:40:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:16 INFO StagingCommitter: Starting: Task committer attempt_202105151437599008569843739934120_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599008569843739934120_0004_m_000175_463
[2021-05-15 11:40:16,845] {docker.py:276} INFO - 21/05/15 14:40:16 INFO StagingCommitter: Task committer attempt_202105151437599008569843739934120_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599008569843739934120_0004_m_000175_463 : duration 0:00.003s
[2021-05-15 11:40:18,649] {docker.py:276} INFO - 21/05/15 14:40:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437598381944432713808959_0004_m_000172_460: needsTaskCommit() Task attempt_202105151437598381944432713808959_0004_m_000172_460
[2021-05-15 11:40:18,650] {docker.py:276} INFO - 21/05/15 14:40:18 INFO StagingCommitter: Task committer attempt_202105151437598381944432713808959_0004_m_000172_460: needsTaskCommit() Task attempt_202105151437598381944432713808959_0004_m_000172_460: duration 0:00.001s
21/05/15 14:40:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598381944432713808959_0004_m_000172_460
[2021-05-15 11:40:18,651] {docker.py:276} INFO - 21/05/15 14:40:18 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 4587 bytes result sent to driver
[2021-05-15 11:40:18,653] {docker.py:276} INFO - 21/05/15 14:40:18 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (bbe2d2545a9d, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:18,654] {docker.py:276} INFO - 21/05/15 14:40:18 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 2622 ms on bbe2d2545a9d (executor driver) (173/200)
[2021-05-15 11:40:18,655] {docker.py:276} INFO - 21/05/15 14:40:18 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-15 11:40:18,664] {docker.py:276} INFO - 21/05/15 14:40:18 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:18,666] {docker.py:276} INFO - 21/05/15 14:40:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595981163275078529673_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595981163275078529673_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595981163275078529673_0004}; taskId=attempt_202105151437595981163275078529673_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@767f9c74}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:18 INFO StagingCommitter: Starting: Task committer attempt_202105151437595981163275078529673_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595981163275078529673_0004_m_000176_464
[2021-05-15 11:40:18,669] {docker.py:276} INFO - 21/05/15 14:40:18 INFO StagingCommitter: Task committer attempt_202105151437595981163275078529673_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595981163275078529673_0004_m_000176_464 : duration 0:00.003s
[2021-05-15 11:40:18,998] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437597275853129417867474_0004_m_000173_461: needsTaskCommit() Task attempt_202105151437597275853129417867474_0004_m_000173_461
[2021-05-15 11:40:18,999] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437597275853129417867474_0004_m_000173_461: needsTaskCommit() Task attempt_202105151437597275853129417867474_0004_m_000173_461: duration 0:00.001s
[2021-05-15 11:40:19,000] {docker.py:276} INFO - 21/05/15 14:40:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437597275853129417867474_0004_m_000173_461
[2021-05-15 11:40:19,001] {docker.py:276} INFO - 21/05/15 14:40:19 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 4544 bytes result sent to driver
[2021-05-15 11:40:19,002] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (bbe2d2545a9d, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:19,003] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 2782 ms on bbe2d2545a9d (executor driver) (174/200)
21/05/15 14:40:19 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
[2021-05-15 11:40:19,012] {docker.py:276} INFO - 21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:19,014] {docker.py:276} INFO - 21/05/15 14:40:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:19,014] {docker.py:276} INFO - 21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591423445059167958768_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591423445059167958768_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591423445059167958768_0004}; taskId=attempt_202105151437591423445059167958768_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b31c082}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:19,015] {docker.py:276} INFO - 21/05/15 14:40:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:19,015] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437591423445059167958768_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591423445059167958768_0004_m_000177_465
[2021-05-15 11:40:19,017] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437591423445059167958768_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591423445059167958768_0004_m_000177_465 : duration 0:00.004s
[2021-05-15 11:40:19,020] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437595366124861308697108_0004_m_000174_462: needsTaskCommit() Task attempt_202105151437595366124861308697108_0004_m_000174_462
[2021-05-15 11:40:19,021] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437595366124861308697108_0004_m_000174_462: needsTaskCommit() Task attempt_202105151437595366124861308697108_0004_m_000174_462: duration 0:00.000s
21/05/15 14:40:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595366124861308697108_0004_m_000174_462
[2021-05-15 11:40:19,021] {docker.py:276} INFO - 21/05/15 14:40:19 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 4544 bytes result sent to driver
[2021-05-15 11:40:19,022] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (bbe2d2545a9d, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:19,023] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 2622 ms on bbe2d2545a9d (executor driver) (175/200)
[2021-05-15 11:40:19,023] {docker.py:276} INFO - 21/05/15 14:40:19 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-15 11:40:19,031] {docker.py:276} INFO - 21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:19,031] {docker.py:276} INFO - 21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:19,033] {docker.py:276} INFO - 21/05/15 14:40:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:40:19,033] {docker.py:276} INFO - 21/05/15 14:40:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:19,034] {docker.py:276} INFO - 21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:19,034] {docker.py:276} INFO - 21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437599195512980014767806_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599195512980014767806_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437599195512980014767806_0004}; taskId=attempt_202105151437599195512980014767806_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d73b000}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:19,034] {docker.py:276} INFO - 21/05/15 14:40:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:19,035] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437599195512980014767806_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599195512980014767806_0004_m_000178_466
[2021-05-15 11:40:19,037] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437599195512980014767806_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437599195512980014767806_0004_m_000178_466 : duration 0:00.003s
[2021-05-15 11:40:19,737] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437599008569843739934120_0004_m_000175_463: needsTaskCommit() Task attempt_202105151437599008569843739934120_0004_m_000175_463
[2021-05-15 11:40:19,738] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437599008569843739934120_0004_m_000175_463: needsTaskCommit() Task attempt_202105151437599008569843739934120_0004_m_000175_463: duration 0:00.000s
21/05/15 14:40:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599008569843739934120_0004_m_000175_463
[2021-05-15 11:40:19,740] {docker.py:276} INFO - 21/05/15 14:40:19 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 4544 bytes result sent to driver
[2021-05-15 11:40:19,742] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (bbe2d2545a9d, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:19,743] {docker.py:276} INFO - 21/05/15 14:40:19 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
[2021-05-15 11:40:19,743] {docker.py:276} INFO - 21/05/15 14:40:19 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 2882 ms on bbe2d2545a9d (executor driver) (176/200)
[2021-05-15 11:40:19,753] {docker.py:276} INFO - 21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:19,753] {docker.py:276} INFO - 21/05/15 14:40:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:19,755] {docker.py:276} INFO - 21/05/15 14:40:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:40:19,755] {docker.py:276} INFO - 21/05/15 14:40:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:19,756] {docker.py:276} INFO - 21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:19,756] {docker.py:276} INFO - 21/05/15 14:40:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591198622200212426676_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591198622200212426676_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591198622200212426676_0004}; taskId=attempt_202105151437591198622200212426676_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@225021dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:19,756] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Starting: Task committer attempt_202105151437591198622200212426676_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591198622200212426676_0004_m_000179_467
[2021-05-15 11:40:19,759] {docker.py:276} INFO - 21/05/15 14:40:19 INFO StagingCommitter: Task committer attempt_202105151437591198622200212426676_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591198622200212426676_0004_m_000179_467 : duration 0:00.003s
[2021-05-15 11:40:21,451] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437595981163275078529673_0004_m_000176_464: needsTaskCommit() Task attempt_202105151437595981163275078529673_0004_m_000176_464
[2021-05-15 11:40:21,452] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437595981163275078529673_0004_m_000176_464: needsTaskCommit() Task attempt_202105151437595981163275078529673_0004_m_000176_464: duration 0:00.001s
21/05/15 14:40:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595981163275078529673_0004_m_000176_464
[2021-05-15 11:40:21,454] {docker.py:276} INFO - 21/05/15 14:40:21 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 4544 bytes result sent to driver
[2021-05-15 11:40:21,455] {docker.py:276} INFO - 21/05/15 14:40:21 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (bbe2d2545a9d, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:21,456] {docker.py:276} INFO - 21/05/15 14:40:21 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 2773 ms on bbe2d2545a9d (executor driver) (177/200)
21/05/15 14:40:21 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
[2021-05-15 11:40:21,466] {docker.py:276} INFO - 21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:21,467] {docker.py:276} INFO - 21/05/15 14:40:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595759718843289126063_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595759718843289126063_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595759718843289126063_0004}; taskId=attempt_202105151437595759718843289126063_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26d9a66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437595759718843289126063_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595759718843289126063_0004_m_000180_468
[2021-05-15 11:40:21,471] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437595759718843289126063_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595759718843289126063_0004_m_000180_468 : duration 0:00.003s
[2021-05-15 11:40:21,682] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437591423445059167958768_0004_m_000177_465: needsTaskCommit() Task attempt_202105151437591423445059167958768_0004_m_000177_465
[2021-05-15 11:40:21,683] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437591423445059167958768_0004_m_000177_465: needsTaskCommit() Task attempt_202105151437591423445059167958768_0004_m_000177_465: duration 0:00.000s
21/05/15 14:40:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591423445059167958768_0004_m_000177_465
[2021-05-15 11:40:21,685] {docker.py:276} INFO - 21/05/15 14:40:21 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 4544 bytes result sent to driver
[2021-05-15 11:40:21,686] {docker.py:276} INFO - 21/05/15 14:40:21 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (bbe2d2545a9d, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:21,686] {docker.py:276} INFO - 21/05/15 14:40:21 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
21/05/15 14:40:21 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 2653 ms on bbe2d2545a9d (executor driver) (178/200)
[2021-05-15 11:40:21,696] {docker.py:276} INFO - 21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Getting 5 (24.5 KiB) non-empty blocks including 5 (24.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:21,698] {docker.py:276} INFO - 21/05/15 14:40:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:21,699] {docker.py:276} INFO - 21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592983140312150564170_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592983140312150564170_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592983140312150564170_0004}; taskId=attempt_202105151437592983140312150564170_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36f8a9fd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:21,699] {docker.py:276} INFO - 21/05/15 14:40:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437592983140312150564170_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592983140312150564170_0004_m_000181_469
[2021-05-15 11:40:21,702] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437592983140312150564170_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592983140312150564170_0004_m_000181_469 : duration 0:00.002s
[2021-05-15 11:40:21,730] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437599195512980014767806_0004_m_000178_466: needsTaskCommit() Task attempt_202105151437599195512980014767806_0004_m_000178_466
[2021-05-15 11:40:21,731] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437599195512980014767806_0004_m_000178_466: needsTaskCommit() Task attempt_202105151437599195512980014767806_0004_m_000178_466: duration 0:00.001s
[2021-05-15 11:40:21,732] {docker.py:276} INFO - 21/05/15 14:40:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437599195512980014767806_0004_m_000178_466
[2021-05-15 11:40:21,733] {docker.py:276} INFO - 21/05/15 14:40:21 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 4544 bytes result sent to driver
[2021-05-15 11:40:21,734] {docker.py:276} INFO - 21/05/15 14:40:21 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (bbe2d2545a9d, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:21,735] {docker.py:276} INFO - 21/05/15 14:40:21 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 2681 ms on bbe2d2545a9d (executor driver) (179/200)
21/05/15 14:40:21 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-15 11:40:21,750] {docker.py:276} INFO - 21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Getting 5 (24.9 KiB) non-empty blocks including 5 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:21,752] {docker.py:276} INFO - 21/05/15 14:40:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:21,752] {docker.py:276} INFO - 21/05/15 14:40:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437593951147765008290391_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593951147765008290391_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437593951147765008290391_0004}; taskId=attempt_202105151437593951147765008290391_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d9e3ac4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:21 INFO StagingCommitter: Starting: Task committer attempt_202105151437593951147765008290391_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593951147765008290391_0004_m_000182_470
[2021-05-15 11:40:21,755] {docker.py:276} INFO - 21/05/15 14:40:21 INFO StagingCommitter: Task committer attempt_202105151437593951147765008290391_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437593951147765008290391_0004_m_000182_470 : duration 0:00.004s
[2021-05-15 11:40:22,589] {docker.py:276} INFO - 21/05/15 14:40:22 INFO StagingCommitter: Starting: Task committer attempt_202105151437591198622200212426676_0004_m_000179_467: needsTaskCommit() Task attempt_202105151437591198622200212426676_0004_m_000179_467
[2021-05-15 11:40:22,591] {docker.py:276} INFO - 21/05/15 14:40:22 INFO StagingCommitter: Task committer attempt_202105151437591198622200212426676_0004_m_000179_467: needsTaskCommit() Task attempt_202105151437591198622200212426676_0004_m_000179_467: duration 0:00.001s
21/05/15 14:40:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591198622200212426676_0004_m_000179_467
[2021-05-15 11:40:22,593] {docker.py:276} INFO - 21/05/15 14:40:22 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 4544 bytes result sent to driver
[2021-05-15 11:40:22,593] {docker.py:276} INFO - 21/05/15 14:40:22 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (bbe2d2545a9d, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:22,594] {docker.py:276} INFO - 21/05/15 14:40:22 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-15 11:40:22,596] {docker.py:276} INFO - 21/05/15 14:40:22 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 2858 ms on bbe2d2545a9d (executor driver) (180/200)
[2021-05-15 11:40:22,605] {docker.py:276} INFO - 21/05/15 14:40:22 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:22,607] {docker.py:276} INFO - 21/05/15 14:40:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595996742590516420440_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595996742590516420440_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595996742590516420440_0004}; taskId=attempt_202105151437595996742590516420440_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@489109b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:22 INFO StagingCommitter: Starting: Task committer attempt_202105151437595996742590516420440_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595996742590516420440_0004_m_000183_471
[2021-05-15 11:40:22,610] {docker.py:276} INFO - 21/05/15 14:40:22 INFO StagingCommitter: Task committer attempt_202105151437595996742590516420440_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595996742590516420440_0004_m_000183_471 : duration 0:00.003s
[2021-05-15 11:40:23,846] {docker.py:276} INFO - 21/05/15 14:40:23 INFO StagingCommitter: Starting: Task committer attempt_202105151437595759718843289126063_0004_m_000180_468: needsTaskCommit() Task attempt_202105151437595759718843289126063_0004_m_000180_468
21/05/15 14:40:23 INFO StagingCommitter: Task committer attempt_202105151437595759718843289126063_0004_m_000180_468: needsTaskCommit() Task attempt_202105151437595759718843289126063_0004_m_000180_468: duration 0:00.000s
21/05/15 14:40:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595759718843289126063_0004_m_000180_468
[2021-05-15 11:40:23,849] {docker.py:276} INFO - 21/05/15 14:40:23 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 4544 bytes result sent to driver
[2021-05-15 11:40:23,850] {docker.py:276} INFO - 21/05/15 14:40:23 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (bbe2d2545a9d, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:23,851] {docker.py:276} INFO - 21/05/15 14:40:23 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
[2021-05-15 11:40:23,852] {docker.py:276} INFO - 21/05/15 14:40:23 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 2401 ms on bbe2d2545a9d (executor driver) (181/200)
[2021-05-15 11:40:23,860] {docker.py:276} INFO - 21/05/15 14:40:23 INFO ShuffleBlockFetcherIterator: Getting 5 (26.4 KiB) non-empty blocks including 5 (26.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:23,862] {docker.py:276} INFO - 21/05/15 14:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:23,862] {docker.py:276} INFO - 21/05/15 14:40:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759887598197524523991_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759887598197524523991_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759887598197524523991_0004}; taskId=attempt_20210515143759887598197524523991_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@b40769f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:23 INFO StagingCommitter: Starting: Task committer attempt_20210515143759887598197524523991_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759887598197524523991_0004_m_000184_472
[2021-05-15 11:40:23,865] {docker.py:276} INFO - 21/05/15 14:40:23 INFO StagingCommitter: Task committer attempt_20210515143759887598197524523991_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759887598197524523991_0004_m_000184_472 : duration 0:00.003s
[2021-05-15 11:40:24,105] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437593951147765008290391_0004_m_000182_470: needsTaskCommit() Task attempt_202105151437593951147765008290391_0004_m_000182_470
[2021-05-15 11:40:24,105] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437593951147765008290391_0004_m_000182_470: needsTaskCommit() Task attempt_202105151437593951147765008290391_0004_m_000182_470: duration 0:00.001s
21/05/15 14:40:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437593951147765008290391_0004_m_000182_470
[2021-05-15 11:40:24,107] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 4544 bytes result sent to driver
[2021-05-15 11:40:24,108] {docker.py:276} INFO - 21/05/15 14:40:24 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (bbe2d2545a9d, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:24,110] {docker.py:276} INFO - 21/05/15 14:40:24 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 2379 ms on bbe2d2545a9d (executor driver) (182/200)
[2021-05-15 11:40:24,111] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-15 11:40:24,119] {docker.py:276} INFO - 21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Getting 5 (25.1 KiB) non-empty blocks including 5 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:24,121] {docker.py:276} INFO - 21/05/15 14:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594510067313836215253_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594510067313836215253_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594510067313836215253_0004}; taskId=attempt_202105151437594510067313836215253_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d54cc29}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:24,121] {docker.py:276} INFO - 21/05/15 14:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437594510067313836215253_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594510067313836215253_0004_m_000185_473
[2021-05-15 11:40:24,124] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437594510067313836215253_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594510067313836215253_0004_m_000185_473 : duration 0:00.004s
[2021-05-15 11:40:24,359] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437592983140312150564170_0004_m_000181_469: needsTaskCommit() Task attempt_202105151437592983140312150564170_0004_m_000181_469
[2021-05-15 11:40:24,361] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437592983140312150564170_0004_m_000181_469: needsTaskCommit() Task attempt_202105151437592983140312150564170_0004_m_000181_469: duration 0:00.001s
21/05/15 14:40:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592983140312150564170_0004_m_000181_469
[2021-05-15 11:40:24,362] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-15 11:40:24,363] {docker.py:276} INFO - 21/05/15 14:40:24 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (bbe2d2545a9d, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:24,363] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
21/05/15 14:40:24 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 2681 ms on bbe2d2545a9d (executor driver) (183/200)
[2021-05-15 11:40:24,373] {docker.py:276} INFO - 21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Getting 5 (25.6 KiB) non-empty blocks including 5 (25.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:24,374] {docker.py:276} INFO - 21/05/15 14:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437592999103496922570709_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592999103496922570709_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437592999103496922570709_0004}; taskId=attempt_202105151437592999103496922570709_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a8ede0e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:24,375] {docker.py:276} INFO - 21/05/15 14:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437592999103496922570709_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592999103496922570709_0004_m_000186_474
[2021-05-15 11:40:24,377] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437592999103496922570709_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437592999103496922570709_0004_m_000186_474 : duration 0:00.003s
[2021-05-15 11:40:24,724] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437595996742590516420440_0004_m_000183_471: needsTaskCommit() Task attempt_202105151437595996742590516420440_0004_m_000183_471
[2021-05-15 11:40:24,724] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437595996742590516420440_0004_m_000183_471: needsTaskCommit() Task attempt_202105151437595996742590516420440_0004_m_000183_471: duration 0:00.001s
21/05/15 14:40:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595996742590516420440_0004_m_000183_471
[2021-05-15 11:40:24,727] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 4587 bytes result sent to driver
[2021-05-15 11:40:24,728] {docker.py:276} INFO - 21/05/15 14:40:24 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (bbe2d2545a9d, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:24,730] {docker.py:276} INFO - 21/05/15 14:40:24 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
21/05/15 14:40:24 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 2139 ms on bbe2d2545a9d (executor driver) (184/200)
[2021-05-15 11:40:24,739] {docker.py:276} INFO - 21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:24,741] {docker.py:276} INFO - 21/05/15 14:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591248422770518077936_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591248422770518077936_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591248422770518077936_0004}; taskId=attempt_202105151437591248422770518077936_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43e61e51}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:24 INFO StagingCommitter: Starting: Task committer attempt_202105151437591248422770518077936_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591248422770518077936_0004_m_000187_475
[2021-05-15 11:40:24,744] {docker.py:276} INFO - 21/05/15 14:40:24 INFO StagingCommitter: Task committer attempt_202105151437591248422770518077936_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591248422770518077936_0004_m_000187_475 : duration 0:00.003s
[2021-05-15 11:40:26,565] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_20210515143759887598197524523991_0004_m_000184_472: needsTaskCommit() Task attempt_20210515143759887598197524523991_0004_m_000184_472
[2021-05-15 11:40:26,566] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_20210515143759887598197524523991_0004_m_000184_472: needsTaskCommit() Task attempt_20210515143759887598197524523991_0004_m_000184_472: duration 0:00.000s
21/05/15 14:40:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759887598197524523991_0004_m_000184_472
[2021-05-15 11:40:26,568] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 4587 bytes result sent to driver
[2021-05-15 11:40:26,569] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (bbe2d2545a9d, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:26,570] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
21/05/15 14:40:26 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 2723 ms on bbe2d2545a9d (executor driver) (185/200)
[2021-05-15 11:40:26,580] {docker.py:276} INFO - 21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Getting 5 (25.3 KiB) non-empty blocks including 5 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:26,581] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591781423426893348140_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591781423426893348140_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591781423426893348140_0004}; taskId=attempt_202105151437591781423426893348140_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27502ff3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:26,582] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437591781423426893348140_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591781423426893348140_0004_m_000188_476
[2021-05-15 11:40:26,585] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437591781423426893348140_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591781423426893348140_0004_m_000188_476 : duration 0:00.003s
[2021-05-15 11:40:26,738] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437592999103496922570709_0004_m_000186_474: needsTaskCommit() Task attempt_202105151437592999103496922570709_0004_m_000186_474
[2021-05-15 11:40:26,739] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437592999103496922570709_0004_m_000186_474: needsTaskCommit() Task attempt_202105151437592999103496922570709_0004_m_000186_474: duration 0:00.000s
21/05/15 14:40:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437592999103496922570709_0004_m_000186_474
[2021-05-15 11:40:26,741] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 4587 bytes result sent to driver
[2021-05-15 11:40:26,742] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (bbe2d2545a9d, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:26,743] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 2384 ms on bbe2d2545a9d (executor driver) (186/200)
[2021-05-15 11:40:26,744] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-15 11:40:26,754] {docker.py:276} INFO - 21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Getting 5 (24.8 KiB) non-empty blocks including 5 (24.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:26,754] {docker.py:276} INFO - 21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:26,756] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:40:26,756] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:26,757] {docker.py:276} INFO - 21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:26,757] {docker.py:276} INFO - 21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598254234366435926575_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598254234366435926575_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598254234366435926575_0004}; taskId=attempt_202105151437598254234366435926575_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e538ba4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:26,758] {docker.py:276} INFO - 21/05/15 14:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:26,758] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437598254234366435926575_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598254234366435926575_0004_m_000189_477
[2021-05-15 11:40:26,760] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437598254234366435926575_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598254234366435926575_0004_m_000189_477 : duration 0:00.003s
[2021-05-15 11:40:26,885] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437591248422770518077936_0004_m_000187_475: needsTaskCommit() Task attempt_202105151437591248422770518077936_0004_m_000187_475
[2021-05-15 11:40:26,886] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437591248422770518077936_0004_m_000187_475: needsTaskCommit() Task attempt_202105151437591248422770518077936_0004_m_000187_475: duration 0:00.001s
21/05/15 14:40:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591248422770518077936_0004_m_000187_475
[2021-05-15 11:40:26,887] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 4544 bytes result sent to driver
[2021-05-15 11:40:26,888] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (bbe2d2545a9d, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:26,889] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-15 11:40:26,890] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 2165 ms on bbe2d2545a9d (executor driver) (187/200)
[2021-05-15 11:40:26,899] {docker.py:276} INFO - 21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Getting 5 (25.9 KiB) non-empty blocks including 5 (25.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:26,901] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 11:40:26,902] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:26,902] {docker.py:276} INFO - 21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598105236720847342247_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598105236720847342247_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598105236720847342247_0004}; taskId=attempt_202105151437598105236720847342247_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@563f5885}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437598105236720847342247_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598105236720847342247_0004_m_000190_478
[2021-05-15 11:40:26,906] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437598105236720847342247_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598105236720847342247_0004_m_000190_478 : duration 0:00.004s
[2021-05-15 11:40:26,939] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437594510067313836215253_0004_m_000185_473: needsTaskCommit() Task attempt_202105151437594510067313836215253_0004_m_000185_473
21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437594510067313836215253_0004_m_000185_473: needsTaskCommit() Task attempt_202105151437594510067313836215253_0004_m_000185_473: duration 0:00.000s
21/05/15 14:40:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594510067313836215253_0004_m_000185_473
[2021-05-15 11:40:26,939] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 4587 bytes result sent to driver
[2021-05-15 11:40:26,940] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (bbe2d2545a9d, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:26,941] {docker.py:276} INFO - 21/05/15 14:40:26 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-15 11:40:26,941] {docker.py:276} INFO - 21/05/15 14:40:26 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 2837 ms on bbe2d2545a9d (executor driver) (188/200)
[2021-05-15 11:40:26,949] {docker.py:276} INFO - 21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:26,951] {docker.py:276} INFO - 21/05/15 14:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:26,952] {docker.py:276} INFO - 21/05/15 14:40:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598689379723943273661_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598689379723943273661_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598689379723943273661_0004}; taskId=attempt_202105151437598689379723943273661_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62cccf38}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:26 INFO StagingCommitter: Starting: Task committer attempt_202105151437598689379723943273661_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598689379723943273661_0004_m_000191_479
[2021-05-15 11:40:26,954] {docker.py:276} INFO - 21/05/15 14:40:26 INFO StagingCommitter: Task committer attempt_202105151437598689379723943273661_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598689379723943273661_0004_m_000191_479 : duration 0:00.003s
[2021-05-15 11:40:29,407] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437598254234366435926575_0004_m_000189_477: needsTaskCommit() Task attempt_202105151437598254234366435926575_0004_m_000189_477
[2021-05-15 11:40:29,408] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437598254234366435926575_0004_m_000189_477: needsTaskCommit() Task attempt_202105151437598254234366435926575_0004_m_000189_477: duration 0:00.001s
21/05/15 14:40:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598254234366435926575_0004_m_000189_477
[2021-05-15 11:40:29,410] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 4544 bytes result sent to driver
[2021-05-15 11:40:29,413] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (bbe2d2545a9d, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:29,414] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 2676 ms on bbe2d2545a9d (executor driver) (189/200)
[2021-05-15 11:40:29,415] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-15 11:40:29,426] {docker.py:276} INFO - 21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Getting 5 (24.4 KiB) non-empty blocks including 5 (24.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:29,428] {docker.py:276} INFO - 21/05/15 14:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595545398024376328743_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595545398024376328743_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595545398024376328743_0004}; taskId=attempt_202105151437595545398024376328743_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27c71890}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:29,429] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437595545398024376328743_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595545398024376328743_0004_m_000192_480
[2021-05-15 11:40:29,432] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437595545398024376328743_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595545398024376328743_0004_m_000192_480 : duration 0:00.003s
[2021-05-15 11:40:29,694] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437598105236720847342247_0004_m_000190_478: needsTaskCommit() Task attempt_202105151437598105236720847342247_0004_m_000190_478
[2021-05-15 11:40:29,695] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437598105236720847342247_0004_m_000190_478: needsTaskCommit() Task attempt_202105151437598105236720847342247_0004_m_000190_478: duration 0:00.000s
21/05/15 14:40:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598105236720847342247_0004_m_000190_478
[2021-05-15 11:40:29,696] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 4544 bytes result sent to driver
[2021-05-15 11:40:29,698] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (bbe2d2545a9d, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:29,699] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-15 11:40:29,700] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 2815 ms on bbe2d2545a9d (executor driver) (190/200)
[2021-05-15 11:40:29,709] {docker.py:276} INFO - 21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:29,710] {docker.py:276} INFO - 21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:29,711] {docker.py:276} INFO - 21/05/15 14:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:29,711] {docker.py:276} INFO - 21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591671014710658029054_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591671014710658029054_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591671014710658029054_0004}; taskId=attempt_202105151437591671014710658029054_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23fa7aa5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:29,712] {docker.py:276} INFO - 21/05/15 14:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:29,712] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437591671014710658029054_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591671014710658029054_0004_m_000193_481
[2021-05-15 11:40:29,714] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437591671014710658029054_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591671014710658029054_0004_m_000193_481 : duration 0:00.003s
[2021-05-15 11:40:29,749] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437598689379723943273661_0004_m_000191_479: needsTaskCommit() Task attempt_202105151437598689379723943273661_0004_m_000191_479
[2021-05-15 11:40:29,750] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437598689379723943273661_0004_m_000191_479: needsTaskCommit() Task attempt_202105151437598689379723943273661_0004_m_000191_479: duration 0:00.000s
21/05/15 14:40:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598689379723943273661_0004_m_000191_479
[2021-05-15 11:40:29,751] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 4544 bytes result sent to driver
[2021-05-15 11:40:29,753] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (bbe2d2545a9d, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:29,754] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-15 11:40:29,755] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 2818 ms on bbe2d2545a9d (executor driver) (191/200)
[2021-05-15 11:40:29,764] {docker.py:276} INFO - 21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Getting 5 (25.5 KiB) non-empty blocks including 5 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:29,766] {docker.py:276} INFO - 21/05/15 14:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:29,766] {docker.py:276} INFO - 21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759262262125786777792_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759262262125786777792_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759262262125786777792_0004}; taskId=attempt_20210515143759262262125786777792_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9dfc076}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_20210515143759262262125786777792_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759262262125786777792_0004_m_000194_482
[2021-05-15 11:40:29,769] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_20210515143759262262125786777792_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759262262125786777792_0004_m_000194_482 : duration 0:00.003s
[2021-05-15 11:40:29,906] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437591781423426893348140_0004_m_000188_476: needsTaskCommit() Task attempt_202105151437591781423426893348140_0004_m_000188_476
[2021-05-15 11:40:29,907] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437591781423426893348140_0004_m_000188_476: needsTaskCommit() Task attempt_202105151437591781423426893348140_0004_m_000188_476: duration 0:00.002s
21/05/15 14:40:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591781423426893348140_0004_m_000188_476
[2021-05-15 11:40:29,909] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 4544 bytes result sent to driver
[2021-05-15 11:40:29,910] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (bbe2d2545a9d, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:29,912] {docker.py:276} INFO - 21/05/15 14:40:29 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-15 11:40:29,912] {docker.py:276} INFO - 21/05/15 14:40:29 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 3347 ms on bbe2d2545a9d (executor driver) (192/200)
[2021-05-15 11:40:29,921] {docker.py:276} INFO - 21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Getting 5 (23.8 KiB) non-empty blocks including 5 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:29,923] {docker.py:276} INFO - 21/05/15 14:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437591022236297805947869_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591022236297805947869_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437591022236297805947869_0004}; taskId=attempt_202105151437591022236297805947869_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ebc3440}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:29 INFO StagingCommitter: Starting: Task committer attempt_202105151437591022236297805947869_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591022236297805947869_0004_m_000195_483
[2021-05-15 11:40:29,926] {docker.py:276} INFO - 21/05/15 14:40:29 INFO StagingCommitter: Task committer attempt_202105151437591022236297805947869_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437591022236297805947869_0004_m_000195_483 : duration 0:00.003s
[2021-05-15 11:40:32,365] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437591022236297805947869_0004_m_000195_483: needsTaskCommit() Task attempt_202105151437591022236297805947869_0004_m_000195_483
21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437591022236297805947869_0004_m_000195_483: needsTaskCommit() Task attempt_202105151437591022236297805947869_0004_m_000195_483: duration 0:00.000s
21/05/15 14:40:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591022236297805947869_0004_m_000195_483
[2021-05-15 11:40:32,368] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 4544 bytes result sent to driver
[2021-05-15 11:40:32,369] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (bbe2d2545a9d, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:32,370] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
21/05/15 14:40:32 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 2463 ms on bbe2d2545a9d (executor driver) (193/200)
[2021-05-15 11:40:32,380] {docker.py:276} INFO - 21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Getting 5 (26.0 KiB) non-empty blocks including 5 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:32,381] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437595545398024376328743_0004_m_000192_480: needsTaskCommit() Task attempt_202105151437595545398024376328743_0004_m_000192_480
[2021-05-15 11:40:32,382] {docker.py:276} INFO - 21/05/15 14:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:32,382] {docker.py:276} INFO - 21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:32,382] {docker.py:276} INFO - 21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437598659940226286862447_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598659940226286862447_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437598659940226286862447_0004}; taskId=attempt_202105151437598659940226286862447_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d9b9740}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:32,383] {docker.py:276} INFO - 21/05/15 14:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437598659940226286862447_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598659940226286862447_0004_m_000196_484
[2021-05-15 11:40:32,383] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437595545398024376328743_0004_m_000192_480: needsTaskCommit() Task attempt_202105151437595545398024376328743_0004_m_000192_480: duration 0:00.002s
[2021-05-15 11:40:32,384] {docker.py:276} INFO - 21/05/15 14:40:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595545398024376328743_0004_m_000192_480
[2021-05-15 11:40:32,384] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 4544 bytes result sent to driver
[2021-05-15 11:40:32,385] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (bbe2d2545a9d, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:32,386] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
[2021-05-15 11:40:32,387] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 2978 ms on bbe2d2545a9d (executor driver) (194/200)
[2021-05-15 11:40:32,389] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437598659940226286862447_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437598659940226286862447_0004_m_000196_484 : duration 0:00.006s
[2021-05-15 11:40:32,395] {docker.py:276} INFO - 21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Getting 5 (24.0 KiB) non-empty blocks including 5 (24.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 11:40:32,396] {docker.py:276} INFO - 21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:32,398] {docker.py:276} INFO - 21/05/15 14:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 11:40:32,398] {docker.py:276} INFO - 21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:32,399] {docker.py:276} INFO - 21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437595190954745507152196_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595190954745507152196_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437595190954745507152196_0004}; taskId=attempt_202105151437595190954745507152196_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6cac1050}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:32,399] {docker.py:276} INFO - 21/05/15 14:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 11:40:32,399] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437595190954745507152196_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595190954745507152196_0004_m_000197_485
[2021-05-15 11:40:32,403] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437595190954745507152196_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437595190954745507152196_0004_m_000197_485 : duration 0:00.003s
[2021-05-15 11:40:32,434] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_20210515143759262262125786777792_0004_m_000194_482: needsTaskCommit() Task attempt_20210515143759262262125786777792_0004_m_000194_482
21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_20210515143759262262125786777792_0004_m_000194_482: needsTaskCommit() Task attempt_20210515143759262262125786777792_0004_m_000194_482: duration 0:00.000s
21/05/15 14:40:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759262262125786777792_0004_m_000194_482
[2021-05-15 11:40:32,435] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 4544 bytes result sent to driver
[2021-05-15 11:40:32,437] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (bbe2d2545a9d, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:32,438] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-15 11:40:32,439] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 2689 ms on bbe2d2545a9d (executor driver) (195/200)
[2021-05-15 11:40:32,447] {docker.py:276} INFO - 21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Getting 5 (24.7 KiB) non-empty blocks including 5 (24.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:32,449] {docker.py:276} INFO - 21/05/15 14:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515143759437840496802701827_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759437840496802701827_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515143759437840496802701827_0004}; taskId=attempt_20210515143759437840496802701827_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76771a2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_20210515143759437840496802701827_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759437840496802701827_0004_m_000198_486
[2021-05-15 11:40:32,452] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_20210515143759437840496802701827_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_20210515143759437840496802701827_0004_m_000198_486 : duration 0:00.002s
[2021-05-15 11:40:32,641] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437591671014710658029054_0004_m_000193_481: needsTaskCommit() Task attempt_202105151437591671014710658029054_0004_m_000193_481
[2021-05-15 11:40:32,642] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437591671014710658029054_0004_m_000193_481: needsTaskCommit() Task attempt_202105151437591671014710658029054_0004_m_000193_481: duration 0:00.002s
21/05/15 14:40:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437591671014710658029054_0004_m_000193_481
[2021-05-15 11:40:32,644] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 4544 bytes result sent to driver
[2021-05-15 11:40:32,645] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (bbe2d2545a9d, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 11:40:32,645] {docker.py:276} INFO - 21/05/15 14:40:32 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-15 11:40:32,646] {docker.py:276} INFO - 21/05/15 14:40:32 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 2952 ms on bbe2d2545a9d (executor driver) (196/200)
[2021-05-15 11:40:32,656] {docker.py:276} INFO - 21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Getting 5 (25.2 KiB) non-empty blocks including 5 (25.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 14:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 11:40:32,658] {docker.py:276} INFO - 21/05/15 14:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 14:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 14:40:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151437594048666389564885655_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594048666389564885655_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151437594048666389564885655_0004}; taskId=attempt_202105151437594048666389564885655_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ab17b59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/4a7e5142-b127-458d-b245-d28b43e8528a/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 14:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 14:40:32 INFO StagingCommitter: Starting: Task committer attempt_202105151437594048666389564885655_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594048666389564885655_0004_m_000199_487
[2021-05-15 11:40:32,661] {docker.py:276} INFO - 21/05/15 14:40:32 INFO StagingCommitter: Task committer attempt_202105151437594048666389564885655_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/4a7e5142-b127-458d-b245-d28b43e8528a/_temporary/0/_temporary/attempt_202105151437594048666389564885655_0004_m_000199_487 : duration 0:00.002s
[2021-05-15 11:40:34,762] {docker.py:276} INFO - 21/05/15 14:40:34 INFO StagingCommitter: Starting: Task committer attempt_202105151437598659940226286862447_0004_m_000196_484: needsTaskCommit() Task attempt_202105151437598659940226286862447_0004_m_000196_484
[2021-05-15 11:40:34,764] {docker.py:276} INFO - 21/05/15 14:40:34 INFO StagingCommitter: Task committer attempt_202105151437598659940226286862447_0004_m_000196_484: needsTaskCommit() Task attempt_202105151437598659940226286862447_0004_m_000196_484: duration 0:00.001s
21/05/15 14:40:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437598659940226286862447_0004_m_000196_484
[2021-05-15 11:40:34,766] {docker.py:276} INFO - 21/05/15 14:40:34 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 4587 bytes result sent to driver
[2021-05-15 11:40:34,768] {docker.py:276} INFO - 21/05/15 14:40:34 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 2402 ms on bbe2d2545a9d (executor driver) (197/200)
[2021-05-15 11:40:35,011] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Starting: Task committer attempt_202105151437594048666389564885655_0004_m_000199_487: needsTaskCommit() Task attempt_202105151437594048666389564885655_0004_m_000199_487
[2021-05-15 11:40:35,012] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Task committer attempt_202105151437594048666389564885655_0004_m_000199_487: needsTaskCommit() Task attempt_202105151437594048666389564885655_0004_m_000199_487: duration 0:00.000s
[2021-05-15 11:40:35,012] {docker.py:276} INFO - 21/05/15 14:40:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437594048666389564885655_0004_m_000199_487
[2021-05-15 11:40:35,014] {docker.py:276} INFO - 21/05/15 14:40:35 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 4587 bytes result sent to driver
[2021-05-15 11:40:35,015] {docker.py:276} INFO - 21/05/15 14:40:35 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 2374 ms on bbe2d2545a9d (executor driver) (198/200)
[2021-05-15 11:40:35,035] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Starting: Task committer attempt_202105151437595190954745507152196_0004_m_000197_485: needsTaskCommit() Task attempt_202105151437595190954745507152196_0004_m_000197_485
[2021-05-15 11:40:35,036] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Task committer attempt_202105151437595190954745507152196_0004_m_000197_485: needsTaskCommit() Task attempt_202105151437595190954745507152196_0004_m_000197_485: duration 0:00.001s
21/05/15 14:40:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151437595190954745507152196_0004_m_000197_485
[2021-05-15 11:40:35,039] {docker.py:276} INFO - 21/05/15 14:40:35 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 4587 bytes result sent to driver
[2021-05-15 11:40:35,041] {docker.py:276} INFO - 21/05/15 14:40:35 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 2658 ms on bbe2d2545a9d (executor driver) (199/200)
[2021-05-15 11:40:35,217] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Starting: Task committer attempt_20210515143759437840496802701827_0004_m_000198_486: needsTaskCommit() Task attempt_20210515143759437840496802701827_0004_m_000198_486
[2021-05-15 11:40:35,218] {docker.py:276} INFO - 21/05/15 14:40:35 INFO StagingCommitter: Task committer attempt_20210515143759437840496802701827_0004_m_000198_486: needsTaskCommit() Task attempt_20210515143759437840496802701827_0004_m_000198_486: duration 0:00.001s
21/05/15 14:40:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515143759437840496802701827_0004_m_000198_486
[2021-05-15 11:40:35,220] {docker.py:276} INFO - 21/05/15 14:40:35 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 4587 bytes result sent to driver
[2021-05-15 11:40:35,221] {docker.py:276} INFO - 21/05/15 14:40:35 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 2788 ms on bbe2d2545a9d (executor driver) (200/200)
[2021-05-15 11:40:35,222] {docker.py:276} INFO - 21/05/15 14:40:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-15 11:40:35,223] {docker.py:276} INFO - 21/05/15 14:40:35 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 137.094 s
[2021-05-15 11:40:35,224] {docker.py:276} INFO - 21/05/15 14:40:35 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 14:40:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-15 11:40:35,225] {docker.py:276} INFO - 21/05/15 14:40:35 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 156.297313 s
[2021-05-15 11:40:35,228] {docker.py:276} INFO - 21/05/15 14:40:35 INFO AbstractS3ACommitter: Starting: Task committer attempt_20210515143758484892628465264288_0000_m_000000_0: commitJob((no job ID))
[2021-05-15 11:40:35,245] {docker.py:276} INFO - 21/05/15 14:40:35 WARN AbstractS3ACommitter: Task committer attempt_20210515143758484892628465264288_0000_m_000000_0: No pending uploads to commit
[2021-05-15 11:40:35,759] {docker.py:276} INFO - 21/05/15 14:40:35 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/15 14:40:35 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-15 11:40:35,941] {docker.py:276} INFO - 21/05/15 14:40:35 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.182s
21/05/15 14:40:35 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.182s
[2021-05-15 11:40:35,942] {docker.py:276} INFO - 21/05/15 14:40:35 INFO AbstractS3ACommitter: Task committer attempt_20210515143758484892628465264288_0000_m_000000_0: commitJob((no job ID)): duration 0:00.715s
[2021-05-15 11:40:36,484] {docker.py:276} INFO - 21/05/15 14:40:36 INFO FileFormatWriter: Write Job 4a7e5142-b127-458d-b245-d28b43e8528a committed.
[2021-05-15 11:40:36,494] {docker.py:276} INFO - 21/05/15 14:40:36 INFO FileFormatWriter: Finished processing stats for write job 4a7e5142-b127-458d-b245-d28b43e8528a.
[2021-05-15 11:40:36,597] {docker.py:276} INFO - 21/05/15 14:40:36 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-15 11:40:36,612] {docker.py:276} INFO - 21/05/15 14:40:36 INFO SparkUI: Stopped Spark web UI at http://bbe2d2545a9d:4040
[2021-05-15 11:40:36,634] {docker.py:276} INFO - 21/05/15 14:40:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-15 11:40:36,651] {docker.py:276} INFO - 21/05/15 14:40:36 INFO MemoryStore: MemoryStore cleared
[2021-05-15 11:40:36,651] {docker.py:276} INFO - 21/05/15 14:40:36 INFO BlockManager: BlockManager stopped
[2021-05-15 11:40:36,654] {docker.py:276} INFO - 21/05/15 14:40:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-15 11:40:36,658] {docker.py:276} INFO - 21/05/15 14:40:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-15 11:40:36,667] {docker.py:276} INFO - 21/05/15 14:40:36 INFO SparkContext: Successfully stopped SparkContext
[2021-05-15 11:40:36,667] {docker.py:276} INFO - 21/05/15 14:40:36 INFO ShutdownHookManager: Shutdown hook called
[2021-05-15 11:40:36,668] {docker.py:276} INFO - 21/05/15 14:40:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b70c15a-eb3c-4a27-9954-453c043bc7ee/pyspark-e453275a-17d9-4bb8-ad29-ed09ed79240b
[2021-05-15 11:40:36,670] {docker.py:276} INFO - 21/05/15 14:40:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-841a0f3e-d5c8-40c8-912f-bd6cf0d09353
[2021-05-15 11:40:36,673] {docker.py:276} INFO - 21/05/15 14:40:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b70c15a-eb3c-4a27-9954-453c043bc7ee
[2021-05-15 11:40:36,679] {docker.py:276} INFO - 21/05/15 14:40:36 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-15 11:40:36,679] {docker.py:276} INFO - 21/05/15 14:40:36 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-15 11:40:36,680] {docker.py:276} INFO - 21/05/15 14:40:36 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-15 11:40:36,898] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T143630, start_date=20210515T143721, end_date=20210515T144036
[2021-05-15 11:40:36,948] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-15 11:40:36,967] {local_task_job.py:146} INFO - Task exited with return code 0
