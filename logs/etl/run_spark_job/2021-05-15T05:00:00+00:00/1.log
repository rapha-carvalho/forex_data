[2021-05-15 10:05:00,770] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T05:00:00+00:00 [queued]>
[2021-05-15 10:05:00,778] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T05:00:00+00:00 [queued]>
[2021-05-15 10:05:00,779] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 10:05:00,779] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-15 10:05:00,779] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-15 10:05:00,785] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T05:00:00+00:00
[2021-05-15 10:05:00,789] {standard_task_runner.py:52} INFO - Started process 27068 to run task
[2021-05-15 10:05:00,797] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T05:00:00+00:00', '--job-id', '649', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmp0uhwn9yx', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpkj919ak1']
[2021-05-15 10:05:00,799] {standard_task_runner.py:77} INFO - Job 649: Subtask run_spark_job
[2021-05-15 10:05:00,843] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T05:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-15 10:05:00,883] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T05:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-15T05:00:00+00:00
[2021-05-15 10:05:00,887] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-15 10:05:03,773] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-15 10:05:03,777] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-15 10:05:03,778] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-15 10:05:03,783] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-15 10:05:06,436] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-15 10:05:07,211] {docker.py:276} INFO - 21/05/15 13:05:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-15 10:05:10,058] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-15 10:05:10,073] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkContext: Running Spark version 3.1.1
[2021-05-15 10:05:10,145] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceUtils: ==============================================================
[2021-05-15 10:05:10,146] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-15 10:05:10,147] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceUtils: ==============================================================
[2021-05-15 10:05:10,148] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkContext: Submitted application: spark.py
[2021-05-15 10:05:10,185] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-15 10:05:10,204] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceProfile: Limiting resource is cpu
[2021-05-15 10:05:10,205] {docker.py:276} INFO - 21/05/15 13:05:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-15 10:05:10,312] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-15 10:05:10,312] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-15 10:05:10,312] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SecurityManager: Changing view acls groups to:
[2021-05-15 10:05:10,313] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SecurityManager: Changing modify acls groups to:
[2021-05-15 10:05:10,314] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-15 10:05:10,709] {docker.py:276} INFO - 21/05/15 13:05:10 INFO Utils: Successfully started service 'sparkDriver' on port 34905.
[2021-05-15 10:05:10,758] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkEnv: Registering MapOutputTracker
[2021-05-15 10:05:10,814] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-15 10:05:10,855] {docker.py:276} INFO - 21/05/15 13:05:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-15 10:05:10,857] {docker.py:276} INFO - 21/05/15 13:05:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-15 10:05:10,865] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-15 10:05:10,885] {docker.py:276} INFO - 21/05/15 13:05:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9b8d258f-3f6e-436e-84f4-d61719352034
[2021-05-15 10:05:10,917] {docker.py:276} INFO - 21/05/15 13:05:10 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-15 10:05:10,943] {docker.py:276} INFO - 21/05/15 13:05:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-15 10:05:11,268] {docker.py:276} INFO - 21/05/15 13:05:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-15 10:05:11,370] {docker.py:276} INFO - 21/05/15 13:05:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://4703d70ea67c:4040
[2021-05-15 10:05:11,646] {docker.py:276} INFO - 21/05/15 13:05:11 INFO Executor: Starting executor ID driver on host 4703d70ea67c
[2021-05-15 10:05:11,693] {docker.py:276} INFO - 21/05/15 13:05:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46535.
21/05/15 13:05:11 INFO NettyBlockTransferService: Server created on 4703d70ea67c:46535
[2021-05-15 10:05:11,696] {docker.py:276} INFO - 21/05/15 13:05:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-15 10:05:11,709] {docker.py:276} INFO - 21/05/15 13:05:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4703d70ea67c, 46535, None)
[2021-05-15 10:05:11,718] {docker.py:276} INFO - 21/05/15 13:05:11 INFO BlockManagerMasterEndpoint: Registering block manager 4703d70ea67c:46535 with 934.4 MiB RAM, BlockManagerId(driver, 4703d70ea67c, 46535, None)
[2021-05-15 10:05:11,722] {docker.py:276} INFO - 21/05/15 13:05:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4703d70ea67c, 46535, None)
[2021-05-15 10:05:11,724] {docker.py:276} INFO - 21/05/15 13:05:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4703d70ea67c, 46535, None)
[2021-05-15 10:05:12,396] {docker.py:276} INFO - 21/05/15 13:05:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-15 10:05:12,397] {docker.py:276} INFO - 21/05/15 13:05:12 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-15 10:05:13,573] {docker.py:276} INFO - 21/05/15 13:05:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-15 10:05:13,627] {docker.py:276} INFO - 21/05/15 13:05:13 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/15 13:05:13 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-15 10:05:20,073] {docker.py:276} INFO - 21/05/15 13:05:20 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 87 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621031676_to_1621033476.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621033476_to_1621035276.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621035276_to_1621037076.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621037076_to_1621038876.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621038876_to_1621040676.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621040676_to_1621042476.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621042476_to_1621044276.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621044276_to_1621046076.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621046076_to_1621047876.csv.
[2021-05-15 10:05:20,607] {docker.py:276} INFO - 21/05/15 13:05:20 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:20,633] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 87 output partitions
[2021-05-15 10:05:20,634] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:05:20,635] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 10:05:20,639] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:05:20,659] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:05:20,777] {docker.py:276} INFO - 21/05/15 13:05:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-15 10:05:20,836] {docker.py:276} INFO - 21/05/15 13:05:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-15 10:05:20,840] {docker.py:276} INFO - 21/05/15 13:05:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4703d70ea67c:46535 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:05:20,847] {docker.py:276} INFO - 21/05/15 13:05:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:05:20,872] {docker.py:276} INFO - 21/05/15 13:05:20 INFO DAGScheduler: Submitting 87 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 10:05:20,874] {docker.py:276} INFO - 21/05/15 13:05:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 87 tasks resource profile 0
[2021-05-15 10:05:20,982] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4703d70ea67c, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:20,987] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (4703d70ea67c, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:20,992] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (4703d70ea67c, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:20,994] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (4703d70ea67c, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:21,021] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-15 10:05:21,021] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-15 10:05:21,022] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-15 10:05:21,025] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-15 10:05:21,475] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1929 bytes result sent to driver
[2021-05-15 10:05:21,480] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (4703d70ea67c, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:21,482] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-15 10:05:21,493] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 548 ms on 4703d70ea67c (executor driver) (1/87)
[2021-05-15 10:05:21,671] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-15 10:05:21,673] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (4703d70ea67c, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:21,674] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-15 10:05:21,675] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 195 ms on 4703d70ea67c (executor driver) (2/87)
[2021-05-15 10:05:21,855] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-15 10:05:21,858] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (4703d70ea67c, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:21,860] {docker.py:276} INFO - 21/05/15 13:05:21 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-15 10:05:21,861] {docker.py:276} INFO - 21/05/15 13:05:21 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 189 ms on 4703d70ea67c (executor driver) (3/87)
[2021-05-15 10:05:21,990] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-15 10:05:21,993] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (4703d70ea67c, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:21,996] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1004 ms on 4703d70ea67c (executor driver) (4/87)
[2021-05-15 10:05:21,997] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-15 10:05:22,001] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-15 10:05:22,002] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-15 10:05:22,003] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (4703d70ea67c, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,004] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1018 ms on 4703d70ea67c (executor driver) (5/87)
[2021-05-15 10:05:22,007] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (4703d70ea67c, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,010] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-15 10:05:22,012] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1027 ms on 4703d70ea67c (executor driver) (6/87)
[2021-05-15 10:05:22,016] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-15 10:05:22,050] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1843 bytes result sent to driver
[2021-05-15 10:05:22,052] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (4703d70ea67c, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,053] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-15 10:05:22,053] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 196 ms on 4703d70ea67c (executor driver) (7/87)
[2021-05-15 10:05:22,194] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-15 10:05:22,196] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-15 10:05:22,198] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (4703d70ea67c, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,200] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (4703d70ea67c, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,201] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-15 10:05:22,202] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-15 10:05:22,203] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-15 10:05:22,203] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 210 ms on 4703d70ea67c (executor driver) (8/87)
[2021-05-15 10:05:22,205] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (4703d70ea67c, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,205] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 198 ms on 4703d70ea67c (executor driver) (9/87)
[2021-05-15 10:05:22,206] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-15 10:05:22,207] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 204 ms on 4703d70ea67c (executor driver) (10/87)
[2021-05-15 10:05:22,242] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-15 10:05:22,243] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (4703d70ea67c, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,244] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 193 ms on 4703d70ea67c (executor driver) (11/87)
[2021-05-15 10:05:22,245] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-15 10:05:22,387] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-15 10:05:22,389] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (4703d70ea67c, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,391] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 194 ms on 4703d70ea67c (executor driver) (12/87)
[2021-05-15 10:05:22,392] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-15 10:05:22,395] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-15 10:05:22,397] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
21/05/15 13:05:22 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (4703d70ea67c, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,399] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 199 ms on 4703d70ea67c (executor driver) (13/87)
[2021-05-15 10:05:22,399] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-15 10:05:22,400] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (4703d70ea67c, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,402] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 198 ms on 4703d70ea67c (executor driver) (14/87)
[2021-05-15 10:05:22,403] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-15 10:05:22,416] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-15 10:05:22,418] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (4703d70ea67c, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,419] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-15 10:05:22,419] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 176 ms on 4703d70ea67c (executor driver) (15/87)
[2021-05-15 10:05:22,580] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-15 10:05:22,583] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (4703d70ea67c, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,584] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1843 bytes result sent to driver
[2021-05-15 10:05:22,585] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
21/05/15 13:05:22 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
21/05/15 13:05:22 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 185 ms on 4703d70ea67c (executor driver) (16/87)
[2021-05-15 10:05:22,586] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 198 ms on 4703d70ea67c (executor driver) (17/87)
[2021-05-15 10:05:22,588] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (4703d70ea67c, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,592] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-15 10:05:22,593] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1843 bytes result sent to driver
[2021-05-15 10:05:22,604] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (4703d70ea67c, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,605] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-15 10:05:22,607] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (4703d70ea67c, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,608] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 211 ms on 4703d70ea67c (executor driver) (18/87)
[2021-05-15 10:05:22,608] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-15 10:05:22,609] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 191 ms on 4703d70ea67c (executor driver) (19/87)
[2021-05-15 10:05:22,791] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1886 bytes result sent to driver
[2021-05-15 10:05:22,795] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1886 bytes result sent to driver
[2021-05-15 10:05:22,796] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (4703d70ea67c, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,798] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-15 10:05:22,799] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (4703d70ea67c, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,800] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 213 ms on 4703d70ea67c (executor driver) (20/87)
[2021-05-15 10:05:22,801] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
21/05/15 13:05:22 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-15 10:05:22,802] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 218 ms on 4703d70ea67c (executor driver) (21/87)
[2021-05-15 10:05:22,803] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (4703d70ea67c, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,804] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
21/05/15 13:05:22 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 198 ms on 4703d70ea67c (executor driver) (22/87)
[2021-05-15 10:05:22,819] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1843 bytes result sent to driver
[2021-05-15 10:05:22,820] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (4703d70ea67c, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,821] {docker.py:276} INFO - 21/05/15 13:05:22 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-15 10:05:22,822] {docker.py:276} INFO - 21/05/15 13:05:22 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 228 ms on 4703d70ea67c (executor driver) (23/87)
[2021-05-15 10:05:22,981] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-15 10:05:22,982] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-15 10:05:22,984] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-15 10:05:22,985] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (4703d70ea67c, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,987] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (4703d70ea67c, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,988] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 193 ms on 4703d70ea67c (executor driver) (24/87)
[2021-05-15 10:05:22,989] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 186 ms on 4703d70ea67c (executor driver) (25/87)
[2021-05-15 10:05:22,990] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-15 10:05:22,991] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
21/05/15 13:05:23 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (4703d70ea67c, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:22,992] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 195 ms on 4703d70ea67c (executor driver) (26/87)
[2021-05-15 10:05:22,993] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-15 10:05:22,998] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-15 10:05:23,000] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (4703d70ea67c, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,001] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 181 ms on 4703d70ea67c (executor driver) (27/87)
[2021-05-15 10:05:23,002] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-15 10:05:23,171] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-15 10:05:23,173] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
[2021-05-15 10:05:23,174] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (4703d70ea67c, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,175] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 189 ms on 4703d70ea67c (executor driver) (28/87)
[2021-05-15 10:05:23,177] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 193 ms on 4703d70ea67c (executor driver) (29/87)
[2021-05-15 10:05:23,177] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-15 10:05:23,179] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (4703d70ea67c, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,180] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-15 10:05:23,181] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-15 10:05:23,181] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 191 ms on 4703d70ea67c (executor driver) (30/87)
[2021-05-15 10:05:23,182] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-15 10:05:23,182] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (4703d70ea67c, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,183] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-15 10:05:23,184] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (4703d70ea67c, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,184] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 186 ms on 4703d70ea67c (executor driver) (31/87)
[2021-05-15 10:05:23,185] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-15 10:05:23,364] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-15 10:05:23,365] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-15 10:05:23,366] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (4703d70ea67c, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,369] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (4703d70ea67c, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,370] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 197 ms on 4703d70ea67c (executor driver) (32/87)
[2021-05-15 10:05:23,371] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-15 10:05:23,372] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 189 ms on 4703d70ea67c (executor driver) (33/87)
[2021-05-15 10:05:23,373] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-15 10:05:23,374] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-15 10:05:23,376] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (4703d70ea67c, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,377] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 200 ms on 4703d70ea67c (executor driver) (34/87)
[2021-05-15 10:05:23,378] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-15 10:05:23,379] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-15 10:05:23,381] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (4703d70ea67c, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,382] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 200 ms on 4703d70ea67c (executor driver) (35/87)
[2021-05-15 10:05:23,382] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-15 10:05:23,552] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1843 bytes result sent to driver
[2021-05-15 10:05:23,553] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (4703d70ea67c, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,554] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 179 ms on 4703d70ea67c (executor driver) (36/87)
[2021-05-15 10:05:23,555] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1843 bytes result sent to driver
[2021-05-15 10:05:23,557] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (4703d70ea67c, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,558] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 192 ms on 4703d70ea67c (executor driver) (37/87)
[2021-05-15 10:05:23,559] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-15 10:05:23,560] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1843 bytes result sent to driver
[2021-05-15 10:05:23,561] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (4703d70ea67c, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,562] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-15 10:05:23,563] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 195 ms on 4703d70ea67c (executor driver) (38/87)
[2021-05-15 10:05:23,563] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
[2021-05-15 10:05:23,564] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-15 10:05:23,565] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (4703d70ea67c, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,566] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-15 10:05:23,566] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 185 ms on 4703d70ea67c (executor driver) (39/87)
[2021-05-15 10:05:23,736] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1886 bytes result sent to driver
[2021-05-15 10:05:23,739] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (4703d70ea67c, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,739] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 183 ms on 4703d70ea67c (executor driver) (40/87)
[2021-05-15 10:05:23,741] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-15 10:05:23,748] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1886 bytes result sent to driver
[2021-05-15 10:05:23,750] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (4703d70ea67c, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,751] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 198 ms on 4703d70ea67c (executor driver) (41/87)
[2021-05-15 10:05:23,752] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-15 10:05:23,754] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1886 bytes result sent to driver
[2021-05-15 10:05:23,756] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (4703d70ea67c, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,757] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1886 bytes result sent to driver
[2021-05-15 10:05:23,757] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 192 ms on 4703d70ea67c (executor driver) (42/87)
[2021-05-15 10:05:23,758] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (4703d70ea67c, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,759] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-15 10:05:23,760] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 198 ms on 4703d70ea67c (executor driver) (43/87)
[2021-05-15 10:05:23,760] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-15 10:05:23,934] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-15 10:05:23,936] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-15 10:05:23,937] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (4703d70ea67c, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,938] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 180 ms on 4703d70ea67c (executor driver) (44/87)
[2021-05-15 10:05:23,940] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (4703d70ea67c, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,941] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-15 10:05:23,942] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 192 ms on 4703d70ea67c (executor driver) (45/87)
[2021-05-15 10:05:23,945] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-15 10:05:23,946] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (4703d70ea67c, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,947] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 192 ms on 4703d70ea67c (executor driver) (46/87)
[2021-05-15 10:05:23,949] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-15 10:05:23,950] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-15 10:05:23,951] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-15 10:05:23,952] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (4703d70ea67c, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:23,952] {docker.py:276} INFO - 21/05/15 13:05:23 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 214 ms on 4703d70ea67c (executor driver) (47/87)
[2021-05-15 10:05:23,954] {docker.py:276} INFO - 21/05/15 13:05:23 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-15 10:05:24,123] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-15 10:05:24,125] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (4703d70ea67c, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,125] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
21/05/15 13:05:24 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 186 ms on 4703d70ea67c (executor driver) (48/87)
[2021-05-15 10:05:24,131] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-15 10:05:24,132] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (4703d70ea67c, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,133] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 182 ms on 4703d70ea67c (executor driver) (49/87)
[2021-05-15 10:05:24,134] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-15 10:05:24,141] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-15 10:05:24,142] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (4703d70ea67c, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,143] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 197 ms on 4703d70ea67c (executor driver) (50/87)
[2021-05-15 10:05:24,144] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-15 10:05:24,148] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-15 10:05:24,149] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (4703d70ea67c, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,150] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 215 ms on 4703d70ea67c (executor driver) (51/87)
[2021-05-15 10:05:24,151] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-15 10:05:24,319] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1843 bytes result sent to driver
[2021-05-15 10:05:24,320] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1843 bytes result sent to driver
[2021-05-15 10:05:24,321] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (4703d70ea67c, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,322] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 197 ms on 4703d70ea67c (executor driver) (52/87)
[2021-05-15 10:05:24,323] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (4703d70ea67c, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,323] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-15 10:05:24,324] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 182 ms on 4703d70ea67c (executor driver) (53/87)
[2021-05-15 10:05:24,325] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-15 10:05:24,328] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1843 bytes result sent to driver
[2021-05-15 10:05:24,330] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (4703d70ea67c, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,331] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 200 ms on 4703d70ea67c (executor driver) (54/87)
[2021-05-15 10:05:24,332] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-15 10:05:24,340] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1843 bytes result sent to driver
[2021-05-15 10:05:24,341] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (4703d70ea67c, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,342] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 193 ms on 4703d70ea67c (executor driver) (55/87)
[2021-05-15 10:05:24,344] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-15 10:05:24,498] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-15 10:05:24,499] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (4703d70ea67c, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,501] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 181 ms on 4703d70ea67c (executor driver) (56/87)
[2021-05-15 10:05:24,502] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-15 10:05:24,507] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-15 10:05:24,508] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-15 10:05:24,508] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (4703d70ea67c, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,509] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (4703d70ea67c, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:05:24 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-15 10:05:24,510] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-15 10:05:24,511] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 188 ms on 4703d70ea67c (executor driver) (57/87)
[2021-05-15 10:05:24,512] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 182 ms on 4703d70ea67c (executor driver) (58/87)
[2021-05-15 10:05:24,515] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-15 10:05:24,516] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (4703d70ea67c, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,516] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 176 ms on 4703d70ea67c (executor driver) (59/87)
[2021-05-15 10:05:24,517] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-15 10:05:24,683] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-15 10:05:24,685] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (4703d70ea67c, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,687] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-15 10:05:24,688] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 188 ms on 4703d70ea67c (executor driver) (60/87)
[2021-05-15 10:05:24,710] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-15 10:05:24,711] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (4703d70ea67c, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,712] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-15 10:05:24,713] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 206 ms on 4703d70ea67c (executor driver) (61/87)
[2021-05-15 10:05:24,714] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-15 10:05:24,715] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (4703d70ea67c, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,716] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-15 10:05:24,717] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 207 ms on 4703d70ea67c (executor driver) (62/87)
[2021-05-15 10:05:24,718] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-15 10:05:24,719] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (4703d70ea67c, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,720] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-15 10:05:24,723] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 208 ms on 4703d70ea67c (executor driver) (63/87)
[2021-05-15 10:05:24,869] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1886 bytes result sent to driver
[2021-05-15 10:05:24,870] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (4703d70ea67c, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,872] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 188 ms on 4703d70ea67c (executor driver) (64/87)
[2021-05-15 10:05:24,872] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-15 10:05:24,886] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1886 bytes result sent to driver
[2021-05-15 10:05:24,887] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (4703d70ea67c, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,888] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-15 10:05:24,888] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 177 ms on 4703d70ea67c (executor driver) (65/87)
[2021-05-15 10:05:24,891] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1886 bytes result sent to driver
[2021-05-15 10:05:24,892] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (4703d70ea67c, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,894] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-15 10:05:24,894] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 180 ms on 4703d70ea67c (executor driver) (66/87)
[2021-05-15 10:05:24,901] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1886 bytes result sent to driver
[2021-05-15 10:05:24,903] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (4703d70ea67c, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:24,903] {docker.py:276} INFO - 21/05/15 13:05:24 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 184 ms on 4703d70ea67c (executor driver) (67/87)
[2021-05-15 10:05:24,904] {docker.py:276} INFO - 21/05/15 13:05:24 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-15 10:05:25,052] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-15 10:05:25,054] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (4703d70ea67c, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,056] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 186 ms on 4703d70ea67c (executor driver) (68/87)
[2021-05-15 10:05:25,057] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-15 10:05:25,072] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1843 bytes result sent to driver
[2021-05-15 10:05:25,074] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1843 bytes result sent to driver
[2021-05-15 10:05:25,074] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (4703d70ea67c, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,076] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 184 ms on 4703d70ea67c (executor driver) (69/87)
[2021-05-15 10:05:25,077] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-15 10:05:25,077] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 175 ms on 4703d70ea67c (executor driver) (70/87)
[2021-05-15 10:05:25,078] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1843 bytes result sent to driver
[2021-05-15 10:05:25,080] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (4703d70ea67c, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,081] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-15 10:05:25,082] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (4703d70ea67c, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,083] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 197 ms on 4703d70ea67c (executor driver) (71/87)
[2021-05-15 10:05:25,084] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-15 10:05:25,249] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1843 bytes result sent to driver
[2021-05-15 10:05:25,250] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (4703d70ea67c, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,252] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 198 ms on 4703d70ea67c (executor driver) (72/87)
[2021-05-15 10:05:25,253] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-15 10:05:25,254] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (4703d70ea67c, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,256] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
[2021-05-15 10:05:25,257] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-15 10:05:25,257] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 177 ms on 4703d70ea67c (executor driver) (73/87)
[2021-05-15 10:05:25,258] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 177 ms on 4703d70ea67c (executor driver) (74/87)
[2021-05-15 10:05:25,259] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-15 10:05:25,260] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-15 10:05:25,261] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (4703d70ea67c, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,263] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-15 10:05:25,263] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (4703d70ea67c, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,264] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 192 ms on 4703d70ea67c (executor driver) (75/87)
[2021-05-15 10:05:25,266] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-15 10:05:25,437] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-15 10:05:25,438] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
21/05/15 13:05:25 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-15 10:05:25,440] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (4703d70ea67c, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,442] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-15 10:05:25,443] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (4703d70ea67c, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,444] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (4703d70ea67c, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,445] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-15 10:05:25,446] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 157 ms on 4703d70ea67c (executor driver) (76/87)
[2021-05-15 10:05:25,447] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 162 ms on 4703d70ea67c (executor driver) (77/87)
[2021-05-15 10:05:25,448] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 152 ms on 4703d70ea67c (executor driver) (78/87)
[2021-05-15 10:05:25,450] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-15 10:05:25,451] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (4703d70ea67c, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,452] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-15 10:05:25,452] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 155 ms on 4703d70ea67c (executor driver) (79/87)
[2021-05-15 10:05:25,455] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-15 10:05:25,624] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-15 10:05:25,625] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (4703d70ea67c, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,626] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-15 10:05:25,626] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 187 ms on 4703d70ea67c (executor driver) (80/87)
[2021-05-15 10:05:25,627] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-15 10:05:25,628] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (4703d70ea67c, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,629] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 178 ms on 4703d70ea67c (executor driver) (81/87)
[2021-05-15 10:05:25,635] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-15 10:05:25,642] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-15 10:05:25,643] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (4703d70ea67c, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,644] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 202 ms on 4703d70ea67c (executor driver) (82/87)
[2021-05-15 10:05:25,645] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-15 10:05:25,680] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-15 10:05:25,681] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (4703d70ea67c, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:25,682] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 241 ms on 4703d70ea67c (executor driver) (83/87)
[2021-05-15 10:05:25,683] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-15 10:05:25,803] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-15 10:05:25,809] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 184 ms on 4703d70ea67c (executor driver) (84/87)
[2021-05-15 10:05:25,812] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1843 bytes result sent to driver
[2021-05-15 10:05:25,813] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 185 ms on 4703d70ea67c (executor driver) (85/87)
[2021-05-15 10:05:25,820] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1843 bytes result sent to driver
[2021-05-15 10:05:25,821] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 178 ms on 4703d70ea67c (executor driver) (86/87)
[2021-05-15 10:05:25,851] {docker.py:276} INFO - 21/05/15 13:05:25 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
[2021-05-15 10:05:25,853] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 172 ms on 4703d70ea67c (executor driver) (87/87)
[2021-05-15 10:05:25,857] {docker.py:276} INFO - 21/05/15 13:05:25 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 5.133 s
[2021-05-15 10:05:25,858] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-15 10:05:25,864] {docker.py:276} INFO - 21/05/15 13:05:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 10:05:25,864] {docker.py:276} INFO - 21/05/15 13:05:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-15 10:05:25,868] {docker.py:276} INFO - 21/05/15 13:05:25 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 5.266723 s
[2021-05-15 10:05:25,904] {docker.py:276} INFO - 21/05/15 13:05:25 INFO InMemoryFileIndex: It took 5850 ms to list leaf files for 87 paths.
[2021-05-15 10:05:26,018] {docker.py:276} INFO - 21/05/15 13:05:26 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 87 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621031676_to_1621033476.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621033476_to_1621035276.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621035276_to_1621037076.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621037076_to_1621038876.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621038876_to_1621040676.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621040676_to_1621042476.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621042476_to_1621044276.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621044276_to_1621046076.csv, s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621046076_to_1621047876.csv.
[2021-05-15 10:05:26,065] {docker.py:276} INFO - 21/05/15 13:05:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:26,067] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 87 output partitions
[2021-05-15 10:05:26,067] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:05:26,068] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 10:05:26,068] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:05:26,069] {docker.py:276} INFO - 21/05/15 13:05:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4703d70ea67c:46535 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:05:26,070] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:05:26,084] {docker.py:276} INFO - 21/05/15 13:05:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.3 MiB)
[2021-05-15 10:05:26,087] {docker.py:276} INFO - 21/05/15 13:05:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-15 10:05:26,088] {docker.py:276} INFO - 21/05/15 13:05:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4703d70ea67c:46535 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:05:26,089] {docker.py:276} INFO - 21/05/15 13:05:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:05:26,091] {docker.py:276} INFO - 21/05/15 13:05:26 INFO DAGScheduler: Submitting 87 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 10:05:26,091] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 87 tasks resource profile 0
[2021-05-15 10:05:26,093] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 87) (4703d70ea67c, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,094] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 88) (4703d70ea67c, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,095] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 89) (4703d70ea67c, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,096] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 90) (4703d70ea67c, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,096] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 87)
[2021-05-15 10:05:26,097] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 2.0 in stage 1.0 (TID 89)
[2021-05-15 10:05:26,098] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 1.0 in stage 1.0 (TID 88)
[2021-05-15 10:05:26,099] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 3.0 in stage 1.0 (TID 90)
[2021-05-15 10:05:26,277] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 1.0 in stage 1.0 (TID 88). 1843 bytes result sent to driver
[2021-05-15 10:05:26,279] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 91) (4703d70ea67c, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,279] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 88) in 186 ms on 4703d70ea67c (executor driver) (1/87)
[2021-05-15 10:05:26,280] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 4.0 in stage 1.0 (TID 91)
[2021-05-15 10:05:26,289] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 87). 1843 bytes result sent to driver
[2021-05-15 10:05:26,296] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 3.0 in stage 1.0 (TID 90). 1843 bytes result sent to driver
[2021-05-15 10:05:26,297] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 92) (4703d70ea67c, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,298] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 5.0 in stage 1.0 (TID 92)
[2021-05-15 10:05:26,299] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 2.0 in stage 1.0 (TID 89). 1843 bytes result sent to driver
[2021-05-15 10:05:26,299] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 93) (4703d70ea67c, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,300] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 6.0 in stage 1.0 (TID 93)
[2021-05-15 10:05:26,301] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 87) in 207 ms on 4703d70ea67c (executor driver) (2/87)
[2021-05-15 10:05:26,301] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 90) in 205 ms on 4703d70ea67c (executor driver) (3/87)
[2021-05-15 10:05:26,303] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 94) (4703d70ea67c, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,303] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 89) in 209 ms on 4703d70ea67c (executor driver) (4/87)
[2021-05-15 10:05:26,304] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 7.0 in stage 1.0 (TID 94)
[2021-05-15 10:05:26,459] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 4.0 in stage 1.0 (TID 91). 1843 bytes result sent to driver
[2021-05-15 10:05:26,461] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 95) (4703d70ea67c, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,463] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 91) in 184 ms on 4703d70ea67c (executor driver) (5/87)
[2021-05-15 10:05:26,464] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 8.0 in stage 1.0 (TID 95)
[2021-05-15 10:05:26,480] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 6.0 in stage 1.0 (TID 93). 1843 bytes result sent to driver
21/05/15 13:05:26 INFO Executor: Finished task 5.0 in stage 1.0 (TID 92). 1843 bytes result sent to driver
[2021-05-15 10:05:26,482] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 96) (4703d70ea67c, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,482] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 93) in 185 ms on 4703d70ea67c (executor driver) (6/87)
21/05/15 13:05:26 INFO Executor: Running task 9.0 in stage 1.0 (TID 96)
[2021-05-15 10:05:26,484] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 97) (4703d70ea67c, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,484] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 92) in 188 ms on 4703d70ea67c (executor driver) (7/87)
[2021-05-15 10:05:26,485] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 10.0 in stage 1.0 (TID 97)
[2021-05-15 10:05:26,507] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 7.0 in stage 1.0 (TID 94). 1843 bytes result sent to driver
[2021-05-15 10:05:26,508] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 98) (4703d70ea67c, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,509] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 94) in 207 ms on 4703d70ea67c (executor driver) (8/87)
[2021-05-15 10:05:26,511] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 11.0 in stage 1.0 (TID 98)
[2021-05-15 10:05:26,634] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 8.0 in stage 1.0 (TID 95). 1843 bytes result sent to driver
[2021-05-15 10:05:26,635] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 99) (4703d70ea67c, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,636] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 95) in 176 ms on 4703d70ea67c (executor driver) (9/87)
[2021-05-15 10:05:26,637] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 12.0 in stage 1.0 (TID 99)
[2021-05-15 10:05:26,654] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 9.0 in stage 1.0 (TID 96). 1843 bytes result sent to driver
[2021-05-15 10:05:26,656] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 100) (4703d70ea67c, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,657] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 96) in 175 ms on 4703d70ea67c (executor driver) (10/87)
[2021-05-15 10:05:26,657] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 13.0 in stage 1.0 (TID 100)
[2021-05-15 10:05:26,658] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 10.0 in stage 1.0 (TID 97). 1843 bytes result sent to driver
[2021-05-15 10:05:26,659] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 101) (4703d70ea67c, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,661] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 97) in 178 ms on 4703d70ea67c (executor driver) (11/87)
[2021-05-15 10:05:26,661] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 14.0 in stage 1.0 (TID 101)
[2021-05-15 10:05:26,677] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 11.0 in stage 1.0 (TID 98). 1843 bytes result sent to driver
[2021-05-15 10:05:26,678] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 102) (4703d70ea67c, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,679] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 98) in 171 ms on 4703d70ea67c (executor driver) (12/87)
[2021-05-15 10:05:26,681] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 15.0 in stage 1.0 (TID 102)
[2021-05-15 10:05:26,804] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 12.0 in stage 1.0 (TID 99). 1843 bytes result sent to driver
[2021-05-15 10:05:26,806] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 103) (4703d70ea67c, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,807] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 99) in 173 ms on 4703d70ea67c (executor driver) (13/87)
[2021-05-15 10:05:26,808] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 16.0 in stage 1.0 (TID 103)
[2021-05-15 10:05:26,838] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 13.0 in stage 1.0 (TID 100). 1843 bytes result sent to driver
[2021-05-15 10:05:26,839] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 104) (4703d70ea67c, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,840] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 14.0 in stage 1.0 (TID 101). 1843 bytes result sent to driver
[2021-05-15 10:05:26,841] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 101) in 182 ms on 4703d70ea67c (executor driver) (14/87)
[2021-05-15 10:05:26,842] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 17.0 in stage 1.0 (TID 104)
[2021-05-15 10:05:26,843] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 105) (4703d70ea67c, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,844] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 100) in 189 ms on 4703d70ea67c (executor driver) (15/87)
[2021-05-15 10:05:26,844] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 18.0 in stage 1.0 (TID 105)
[2021-05-15 10:05:26,847] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 15.0 in stage 1.0 (TID 102). 1843 bytes result sent to driver
[2021-05-15 10:05:26,849] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 106) (4703d70ea67c, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,850] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 102) in 172 ms on 4703d70ea67c (executor driver) (16/87)
[2021-05-15 10:05:26,851] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 19.0 in stage 1.0 (TID 106)
[2021-05-15 10:05:26,974] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Finished task 16.0 in stage 1.0 (TID 103). 1886 bytes result sent to driver
[2021-05-15 10:05:26,975] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 103) in 170 ms on 4703d70ea67c (executor driver) (17/87)
[2021-05-15 10:05:26,976] {docker.py:276} INFO - 21/05/15 13:05:26 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 107) (4703d70ea67c, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:26,978] {docker.py:276} INFO - 21/05/15 13:05:26 INFO Executor: Running task 20.0 in stage 1.0 (TID 107)
[2021-05-15 10:05:27,014] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 17.0 in stage 1.0 (TID 104). 1886 bytes result sent to driver
[2021-05-15 10:05:27,016] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 108) (4703d70ea67c, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,017] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 104) in 178 ms on 4703d70ea67c (executor driver) (18/87)
[2021-05-15 10:05:27,018] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 21.0 in stage 1.0 (TID 108)
[2021-05-15 10:05:27,027] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 19.0 in stage 1.0 (TID 106). 1886 bytes result sent to driver
[2021-05-15 10:05:27,028] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 109) (4703d70ea67c, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,029] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 106) in 180 ms on 4703d70ea67c (executor driver) (19/87)
[2021-05-15 10:05:27,029] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 22.0 in stage 1.0 (TID 109)
[2021-05-15 10:05:27,038] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 18.0 in stage 1.0 (TID 105). 1886 bytes result sent to driver
[2021-05-15 10:05:27,039] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 110) (4703d70ea67c, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,040] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 105) in 198 ms on 4703d70ea67c (executor driver) (20/87)
[2021-05-15 10:05:27,041] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 23.0 in stage 1.0 (TID 110)
[2021-05-15 10:05:27,149] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 20.0 in stage 1.0 (TID 107). 1843 bytes result sent to driver
[2021-05-15 10:05:27,151] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 111) (4703d70ea67c, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,152] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 24.0 in stage 1.0 (TID 111)
21/05/15 13:05:27 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 107) in 176 ms on 4703d70ea67c (executor driver) (21/87)
[2021-05-15 10:05:27,193] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 21.0 in stage 1.0 (TID 108). 1843 bytes result sent to driver
[2021-05-15 10:05:27,195] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 112) (4703d70ea67c, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,197] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 25.0 in stage 1.0 (TID 112)
21/05/15 13:05:27 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 108) in 182 ms on 4703d70ea67c (executor driver) (22/87)
[2021-05-15 10:05:27,203] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 22.0 in stage 1.0 (TID 109). 1843 bytes result sent to driver
[2021-05-15 10:05:27,204] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 113) (4703d70ea67c, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,206] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 26.0 in stage 1.0 (TID 113)
[2021-05-15 10:05:27,206] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 109) in 178 ms on 4703d70ea67c (executor driver) (23/87)
[2021-05-15 10:05:27,211] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 23.0 in stage 1.0 (TID 110). 1843 bytes result sent to driver
[2021-05-15 10:05:27,213] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 114) (4703d70ea67c, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,213] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 110) in 175 ms on 4703d70ea67c (executor driver) (24/87)
[2021-05-15 10:05:27,214] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 27.0 in stage 1.0 (TID 114)
[2021-05-15 10:05:27,326] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 24.0 in stage 1.0 (TID 111). 1843 bytes result sent to driver
[2021-05-15 10:05:27,328] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 111) in 178 ms on 4703d70ea67c (executor driver) (25/87)
[2021-05-15 10:05:27,329] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 115) (4703d70ea67c, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,330] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 28.0 in stage 1.0 (TID 115)
[2021-05-15 10:05:27,375] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 25.0 in stage 1.0 (TID 112). 1843 bytes result sent to driver
[2021-05-15 10:05:27,377] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 116) (4703d70ea67c, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,378] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 29.0 in stage 1.0 (TID 116)
[2021-05-15 10:05:27,379] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 112) in 185 ms on 4703d70ea67c (executor driver) (26/87)
[2021-05-15 10:05:27,383] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 26.0 in stage 1.0 (TID 113). 1843 bytes result sent to driver
[2021-05-15 10:05:27,384] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 117) (4703d70ea67c, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,385] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 113) in 181 ms on 4703d70ea67c (executor driver) (27/87)
[2021-05-15 10:05:27,386] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 27.0 in stage 1.0 (TID 114). 1843 bytes result sent to driver
[2021-05-15 10:05:27,387] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 30.0 in stage 1.0 (TID 117)
[2021-05-15 10:05:27,388] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 118) (4703d70ea67c, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,388] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 114) in 176 ms on 4703d70ea67c (executor driver) (28/87)
[2021-05-15 10:05:27,389] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 31.0 in stage 1.0 (TID 118)
[2021-05-15 10:05:27,502] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 28.0 in stage 1.0 (TID 115). 1843 bytes result sent to driver
[2021-05-15 10:05:27,504] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 119) (4703d70ea67c, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,505] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 115) in 176 ms on 4703d70ea67c (executor driver) (29/87)
21/05/15 13:05:27 INFO Executor: Running task 32.0 in stage 1.0 (TID 119)
[2021-05-15 10:05:27,550] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 29.0 in stage 1.0 (TID 116). 1843 bytes result sent to driver
[2021-05-15 10:05:27,551] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 120) (4703d70ea67c, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,553] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 116) in 177 ms on 4703d70ea67c (executor driver) (30/87)
[2021-05-15 10:05:27,553] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 33.0 in stage 1.0 (TID 120)
[2021-05-15 10:05:27,560] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 30.0 in stage 1.0 (TID 117). 1843 bytes result sent to driver
[2021-05-15 10:05:27,562] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 121) (4703d70ea67c, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,562] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 34.0 in stage 1.0 (TID 121)
21/05/15 13:05:27 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 117) in 179 ms on 4703d70ea67c (executor driver) (31/87)
[2021-05-15 10:05:27,564] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 31.0 in stage 1.0 (TID 118). 1843 bytes result sent to driver
[2021-05-15 10:05:27,565] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 122) (4703d70ea67c, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,566] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 35.0 in stage 1.0 (TID 122)
[2021-05-15 10:05:27,567] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 118) in 180 ms on 4703d70ea67c (executor driver) (32/87)
[2021-05-15 10:05:27,678] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 32.0 in stage 1.0 (TID 119). 1843 bytes result sent to driver
[2021-05-15 10:05:27,680] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 123) (4703d70ea67c, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,681] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 119) in 178 ms on 4703d70ea67c (executor driver) (33/87)
[2021-05-15 10:05:27,682] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 36.0 in stage 1.0 (TID 123)
[2021-05-15 10:05:27,723] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 33.0 in stage 1.0 (TID 120). 1843 bytes result sent to driver
[2021-05-15 10:05:27,725] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 124) (4703d70ea67c, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,726] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 120) in 175 ms on 4703d70ea67c (executor driver) (34/87)
[2021-05-15 10:05:27,728] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 37.0 in stage 1.0 (TID 124)
[2021-05-15 10:05:27,736] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 35.0 in stage 1.0 (TID 122). 1843 bytes result sent to driver
[2021-05-15 10:05:27,737] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 125) (4703d70ea67c, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,738] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 122) in 173 ms on 4703d70ea67c (executor driver) (35/87)
21/05/15 13:05:27 INFO Executor: Finished task 34.0 in stage 1.0 (TID 121). 1843 bytes result sent to driver
[2021-05-15 10:05:27,739] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 38.0 in stage 1.0 (TID 125)
[2021-05-15 10:05:27,740] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 126) (4703d70ea67c, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,741] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 121) in 180 ms on 4703d70ea67c (executor driver) (36/87)
[2021-05-15 10:05:27,742] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 39.0 in stage 1.0 (TID 126)
[2021-05-15 10:05:27,861] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 36.0 in stage 1.0 (TID 123). 1843 bytes result sent to driver
[2021-05-15 10:05:27,863] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 127) (4703d70ea67c, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,865] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 123) in 186 ms on 4703d70ea67c (executor driver) (37/87)
[2021-05-15 10:05:27,866] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 40.0 in stage 1.0 (TID 127)
[2021-05-15 10:05:27,904] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 37.0 in stage 1.0 (TID 124). 1843 bytes result sent to driver
[2021-05-15 10:05:27,906] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 128) (4703d70ea67c, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,907] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 124) in 184 ms on 4703d70ea67c (executor driver) (38/87)
[2021-05-15 10:05:27,908] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 41.0 in stage 1.0 (TID 128)
[2021-05-15 10:05:27,914] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 38.0 in stage 1.0 (TID 125). 1843 bytes result sent to driver
[2021-05-15 10:05:27,915] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Finished task 39.0 in stage 1.0 (TID 126). 1843 bytes result sent to driver
[2021-05-15 10:05:27,915] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 125) in 178 ms on 4703d70ea67c (executor driver) (39/87)
[2021-05-15 10:05:27,917] {docker.py:276} INFO - 21/05/15 13:05:27 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 129) (4703d70ea67c, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,919] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 42.0 in stage 1.0 (TID 129)
21/05/15 13:05:27 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 130) (4703d70ea67c, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:27,920] {docker.py:276} INFO - 21/05/15 13:05:27 INFO Executor: Running task 43.0 in stage 1.0 (TID 130)
21/05/15 13:05:27 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 126) in 181 ms on 4703d70ea67c (executor driver) (40/87)
[2021-05-15 10:05:28,037] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 40.0 in stage 1.0 (TID 127). 1843 bytes result sent to driver
[2021-05-15 10:05:28,038] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 131) (4703d70ea67c, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,039] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 44.0 in stage 1.0 (TID 131)
[2021-05-15 10:05:28,039] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 127) in 177 ms on 4703d70ea67c (executor driver) (41/87)
[2021-05-15 10:05:28,085] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 41.0 in stage 1.0 (TID 128). 1886 bytes result sent to driver
[2021-05-15 10:05:28,087] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 132) (4703d70ea67c, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,088] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 128) in 183 ms on 4703d70ea67c (executor driver) (42/87)
[2021-05-15 10:05:28,089] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 45.0 in stage 1.0 (TID 132)
[2021-05-15 10:05:28,093] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 43.0 in stage 1.0 (TID 130). 1886 bytes result sent to driver
[2021-05-15 10:05:28,094] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 42.0 in stage 1.0 (TID 129). 1886 bytes result sent to driver
[2021-05-15 10:05:28,094] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 133) (4703d70ea67c, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,095] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 46.0 in stage 1.0 (TID 133)
[2021-05-15 10:05:28,096] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 134) (4703d70ea67c, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,096] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 129) in 180 ms on 4703d70ea67c (executor driver) (43/87)
[2021-05-15 10:05:28,097] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 47.0 in stage 1.0 (TID 134)
[2021-05-15 10:05:28,098] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 130) in 178 ms on 4703d70ea67c (executor driver) (44/87)
[2021-05-15 10:05:28,216] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 44.0 in stage 1.0 (TID 131). 1886 bytes result sent to driver
[2021-05-15 10:05:28,218] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 135) (4703d70ea67c, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,219] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 48.0 in stage 1.0 (TID 135)
[2021-05-15 10:05:28,220] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 131) in 181 ms on 4703d70ea67c (executor driver) (45/87)
[2021-05-15 10:05:28,269] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 46.0 in stage 1.0 (TID 133). 1843 bytes result sent to driver
21/05/15 13:05:28 INFO Executor: Finished task 47.0 in stage 1.0 (TID 134). 1843 bytes result sent to driver
[2021-05-15 10:05:28,270] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 45.0 in stage 1.0 (TID 132). 1843 bytes result sent to driver
[2021-05-15 10:05:28,271] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 136) (4703d70ea67c, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,273] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 49.0 in stage 1.0 (TID 136)
21/05/15 13:05:28 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 137) (4703d70ea67c, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,274] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 133) in 181 ms on 4703d70ea67c (executor driver) (46/87)
[2021-05-15 10:05:28,275] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 50.0 in stage 1.0 (TID 137)
[2021-05-15 10:05:28,276] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 138) (4703d70ea67c, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,277] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 134) in 181 ms on 4703d70ea67c (executor driver) (47/87)
[2021-05-15 10:05:28,278] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 132) in 191 ms on 4703d70ea67c (executor driver) (48/87)
[2021-05-15 10:05:28,279] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 51.0 in stage 1.0 (TID 138)
[2021-05-15 10:05:28,395] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 48.0 in stage 1.0 (TID 135). 1843 bytes result sent to driver
[2021-05-15 10:05:28,396] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 139) (4703d70ea67c, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,398] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 135) in 181 ms on 4703d70ea67c (executor driver) (49/87)
[2021-05-15 10:05:28,399] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 52.0 in stage 1.0 (TID 139)
[2021-05-15 10:05:28,445] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 51.0 in stage 1.0 (TID 138). 1843 bytes result sent to driver
[2021-05-15 10:05:28,446] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 140) (4703d70ea67c, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,447] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 138) in 172 ms on 4703d70ea67c (executor driver) (50/87)
[2021-05-15 10:05:28,448] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 53.0 in stage 1.0 (TID 140)
[2021-05-15 10:05:28,451] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 50.0 in stage 1.0 (TID 137). 1843 bytes result sent to driver
[2021-05-15 10:05:28,452] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 141) (4703d70ea67c, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,453] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 137) in 181 ms on 4703d70ea67c (executor driver) (51/87)
[2021-05-15 10:05:28,454] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 54.0 in stage 1.0 (TID 141)
[2021-05-15 10:05:28,456] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 49.0 in stage 1.0 (TID 136). 1843 bytes result sent to driver
[2021-05-15 10:05:28,456] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 136) in 186 ms on 4703d70ea67c (executor driver) (52/87)
[2021-05-15 10:05:28,457] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 142) (4703d70ea67c, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,458] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 55.0 in stage 1.0 (TID 142)
[2021-05-15 10:05:28,566] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 52.0 in stage 1.0 (TID 139). 1843 bytes result sent to driver
[2021-05-15 10:05:28,567] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 143) (4703d70ea67c, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:05:28 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 139) in 172 ms on 4703d70ea67c (executor driver) (53/87)
[2021-05-15 10:05:28,568] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 56.0 in stage 1.0 (TID 143)
[2021-05-15 10:05:28,618] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 53.0 in stage 1.0 (TID 140). 1843 bytes result sent to driver
[2021-05-15 10:05:28,619] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 144) (4703d70ea67c, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,620] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 140) in 174 ms on 4703d70ea67c (executor driver) (54/87)
[2021-05-15 10:05:28,621] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 57.0 in stage 1.0 (TID 144)
[2021-05-15 10:05:28,630] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 55.0 in stage 1.0 (TID 142). 1843 bytes result sent to driver
21/05/15 13:05:28 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 145) (4703d70ea67c, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:05:28 INFO Executor: Running task 58.0 in stage 1.0 (TID 145)
21/05/15 13:05:28 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 142) in 172 ms on 4703d70ea67c (executor driver) (55/87)
[2021-05-15 10:05:28,679] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 54.0 in stage 1.0 (TID 141). 1843 bytes result sent to driver
[2021-05-15 10:05:28,682] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 146) (4703d70ea67c, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/15 13:05:28 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 141) in 229 ms on 4703d70ea67c (executor driver) (56/87)
[2021-05-15 10:05:28,682] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 59.0 in stage 1.0 (TID 146)
[2021-05-15 10:05:28,734] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 56.0 in stage 1.0 (TID 143). 1843 bytes result sent to driver
[2021-05-15 10:05:28,735] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 147) (4703d70ea67c, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,736] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 143) in 170 ms on 4703d70ea67c (executor driver) (57/87)
21/05/15 13:05:28 INFO Executor: Running task 60.0 in stage 1.0 (TID 147)
[2021-05-15 10:05:28,789] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 57.0 in stage 1.0 (TID 144). 1843 bytes result sent to driver
[2021-05-15 10:05:28,790] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 148) (4703d70ea67c, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,793] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 144) in 172 ms on 4703d70ea67c (executor driver) (58/87)
21/05/15 13:05:28 INFO Executor: Running task 61.0 in stage 1.0 (TID 148)
[2021-05-15 10:05:28,801] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 58.0 in stage 1.0 (TID 145). 1843 bytes result sent to driver
[2021-05-15 10:05:28,802] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 149) (4703d70ea67c, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,802] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 145) in 174 ms on 4703d70ea67c (executor driver) (59/87)
[2021-05-15 10:05:28,803] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 62.0 in stage 1.0 (TID 149)
[2021-05-15 10:05:28,863] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 59.0 in stage 1.0 (TID 146). 1843 bytes result sent to driver
[2021-05-15 10:05:28,865] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 150) (4703d70ea67c, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,867] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 146) in 187 ms on 4703d70ea67c (executor driver) (60/87)
21/05/15 13:05:28 INFO Executor: Running task 63.0 in stage 1.0 (TID 150)
[2021-05-15 10:05:28,903] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 60.0 in stage 1.0 (TID 147). 1843 bytes result sent to driver
[2021-05-15 10:05:28,905] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 151) (4703d70ea67c, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,906] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 147) in 172 ms on 4703d70ea67c (executor driver) (61/87)
[2021-05-15 10:05:28,907] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 64.0 in stage 1.0 (TID 151)
[2021-05-15 10:05:28,969] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 61.0 in stage 1.0 (TID 148). 1843 bytes result sent to driver
[2021-05-15 10:05:28,970] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 152) (4703d70ea67c, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,972] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 148) in 182 ms on 4703d70ea67c (executor driver) (62/87)
[2021-05-15 10:05:28,974] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Finished task 62.0 in stage 1.0 (TID 149). 1843 bytes result sent to driver
[2021-05-15 10:05:28,974] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 65.0 in stage 1.0 (TID 152)
[2021-05-15 10:05:28,975] {docker.py:276} INFO - 21/05/15 13:05:28 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 153) (4703d70ea67c, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:28,977] {docker.py:276} INFO - 21/05/15 13:05:28 INFO Executor: Running task 66.0 in stage 1.0 (TID 153)
21/05/15 13:05:28 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 149) in 175 ms on 4703d70ea67c (executor driver) (63/87)
[2021-05-15 10:05:29,049] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 63.0 in stage 1.0 (TID 150). 1843 bytes result sent to driver
[2021-05-15 10:05:29,052] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 154) (4703d70ea67c, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,053] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 150) in 188 ms on 4703d70ea67c (executor driver) (64/87)
[2021-05-15 10:05:29,054] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 67.0 in stage 1.0 (TID 154)
[2021-05-15 10:05:29,080] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 64.0 in stage 1.0 (TID 151). 1843 bytes result sent to driver
[2021-05-15 10:05:29,081] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 155) (4703d70ea67c, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,082] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 151) in 177 ms on 4703d70ea67c (executor driver) (65/87)
[2021-05-15 10:05:29,084] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 68.0 in stage 1.0 (TID 155)
[2021-05-15 10:05:29,144] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 65.0 in stage 1.0 (TID 152). 1843 bytes result sent to driver
[2021-05-15 10:05:29,157] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 156) (4703d70ea67c, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,158] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 152) in 189 ms on 4703d70ea67c (executor driver) (66/87)
[2021-05-15 10:05:29,159] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 69.0 in stage 1.0 (TID 156)
[2021-05-15 10:05:29,160] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 66.0 in stage 1.0 (TID 153). 1886 bytes result sent to driver
[2021-05-15 10:05:29,161] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 157) (4703d70ea67c, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,162] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 153) in 188 ms on 4703d70ea67c (executor driver) (67/87)
21/05/15 13:05:29 INFO Executor: Running task 70.0 in stage 1.0 (TID 157)
[2021-05-15 10:05:29,230] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 67.0 in stage 1.0 (TID 154). 1886 bytes result sent to driver
[2021-05-15 10:05:29,232] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 158) (4703d70ea67c, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,234] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 154) in 184 ms on 4703d70ea67c (executor driver) (68/87)
21/05/15 13:05:29 INFO Executor: Running task 71.0 in stage 1.0 (TID 158)
[2021-05-15 10:05:29,330] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 70.0 in stage 1.0 (TID 157). 1843 bytes result sent to driver
[2021-05-15 10:05:29,332] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 159) (4703d70ea67c, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,334] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 72.0 in stage 1.0 (TID 159)
[2021-05-15 10:05:29,334] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 157) in 172 ms on 4703d70ea67c (executor driver) (69/87)
[2021-05-15 10:05:29,335] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 69.0 in stage 1.0 (TID 156). 1843 bytes result sent to driver
[2021-05-15 10:05:29,337] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 160) (4703d70ea67c, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,338] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 156) in 193 ms on 4703d70ea67c (executor driver) (70/87)
[2021-05-15 10:05:29,339] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 73.0 in stage 1.0 (TID 160)
[2021-05-15 10:05:29,342] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 68.0 in stage 1.0 (TID 155). 1886 bytes result sent to driver
[2021-05-15 10:05:29,343] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 161) (4703d70ea67c, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,344] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 74.0 in stage 1.0 (TID 161)
21/05/15 13:05:29 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 155) in 264 ms on 4703d70ea67c (executor driver) (71/87)
[2021-05-15 10:05:29,418] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 71.0 in stage 1.0 (TID 158). 1843 bytes result sent to driver
[2021-05-15 10:05:29,420] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 162) (4703d70ea67c, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,421] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 158) in 190 ms on 4703d70ea67c (executor driver) (72/87)
[2021-05-15 10:05:29,422] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 75.0 in stage 1.0 (TID 162)
[2021-05-15 10:05:29,509] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 72.0 in stage 1.0 (TID 159). 1843 bytes result sent to driver
[2021-05-15 10:05:29,511] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 163) (4703d70ea67c, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,512] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 159) in 181 ms on 4703d70ea67c (executor driver) (73/87)
[2021-05-15 10:05:29,513] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 76.0 in stage 1.0 (TID 163)
[2021-05-15 10:05:29,514] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 74.0 in stage 1.0 (TID 161). 1843 bytes result sent to driver
[2021-05-15 10:05:29,515] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 164) (4703d70ea67c, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,516] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 161) in 173 ms on 4703d70ea67c (executor driver) (74/87)
21/05/15 13:05:29 INFO Executor: Running task 77.0 in stage 1.0 (TID 164)
[2021-05-15 10:05:29,517] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 73.0 in stage 1.0 (TID 160). 1843 bytes result sent to driver
[2021-05-15 10:05:29,519] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 165) (4703d70ea67c, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,520] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 160) in 185 ms on 4703d70ea67c (executor driver) (75/87)
[2021-05-15 10:05:29,522] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 78.0 in stage 1.0 (TID 165)
[2021-05-15 10:05:29,602] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 75.0 in stage 1.0 (TID 162). 1843 bytes result sent to driver
[2021-05-15 10:05:29,605] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 166) (4703d70ea67c, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,607] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 162) in 186 ms on 4703d70ea67c (executor driver) (76/87)
21/05/15 13:05:29 INFO Executor: Running task 79.0 in stage 1.0 (TID 166)
[2021-05-15 10:05:29,686] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 76.0 in stage 1.0 (TID 163). 1843 bytes result sent to driver
[2021-05-15 10:05:29,688] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 167) (4703d70ea67c, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,689] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 163) in 179 ms on 4703d70ea67c (executor driver) (77/87)
21/05/15 13:05:29 INFO Executor: Running task 80.0 in stage 1.0 (TID 167)
[2021-05-15 10:05:29,692] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 78.0 in stage 1.0 (TID 165). 1843 bytes result sent to driver
[2021-05-15 10:05:29,693] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 168) (4703d70ea67c, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,694] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 165) in 175 ms on 4703d70ea67c (executor driver) (78/87)
[2021-05-15 10:05:29,695] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 81.0 in stage 1.0 (TID 168)
[2021-05-15 10:05:29,697] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 77.0 in stage 1.0 (TID 164). 1886 bytes result sent to driver
[2021-05-15 10:05:29,699] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 169) (4703d70ea67c, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,700] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 164) in 185 ms on 4703d70ea67c (executor driver) (79/87)
[2021-05-15 10:05:29,701] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 82.0 in stage 1.0 (TID 169)
[2021-05-15 10:05:29,791] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 79.0 in stage 1.0 (TID 166). 1843 bytes result sent to driver
[2021-05-15 10:05:29,793] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 170) (4703d70ea67c, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,794] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 166) in 191 ms on 4703d70ea67c (executor driver) (80/87)
[2021-05-15 10:05:29,795] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 83.0 in stage 1.0 (TID 170)
[2021-05-15 10:05:29,868] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 81.0 in stage 1.0 (TID 168). 1843 bytes result sent to driver
[2021-05-15 10:05:29,870] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 171) (4703d70ea67c, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,871] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 168) in 179 ms on 4703d70ea67c (executor driver) (81/87)
[2021-05-15 10:05:29,873] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 84.0 in stage 1.0 (TID 171)
[2021-05-15 10:05:29,874] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 82.0 in stage 1.0 (TID 169). 1843 bytes result sent to driver
[2021-05-15 10:05:29,874] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 80.0 in stage 1.0 (TID 167). 1843 bytes result sent to driver
[2021-05-15 10:05:29,875] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 172) (4703d70ea67c, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,876] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Running task 85.0 in stage 1.0 (TID 172)
[2021-05-15 10:05:29,877] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 173) (4703d70ea67c, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:29,878] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 169) in 179 ms on 4703d70ea67c (executor driver) (82/87)
21/05/15 13:05:29 INFO Executor: Running task 86.0 in stage 1.0 (TID 173)
[2021-05-15 10:05:29,879] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 167) in 192 ms on 4703d70ea67c (executor driver) (83/87)
[2021-05-15 10:05:29,971] {docker.py:276} INFO - 21/05/15 13:05:29 INFO Executor: Finished task 83.0 in stage 1.0 (TID 170). 1843 bytes result sent to driver
[2021-05-15 10:05:29,973] {docker.py:276} INFO - 21/05/15 13:05:29 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 170) in 180 ms on 4703d70ea67c (executor driver) (84/87)
[2021-05-15 10:05:30,052] {docker.py:276} INFO - 21/05/15 13:05:30 INFO Executor: Finished task 85.0 in stage 1.0 (TID 172). 1843 bytes result sent to driver
21/05/15 13:05:30 INFO Executor: Finished task 86.0 in stage 1.0 (TID 173). 1843 bytes result sent to driver
[2021-05-15 10:05:30,053] {docker.py:276} INFO - 21/05/15 13:05:30 INFO Executor: Finished task 84.0 in stage 1.0 (TID 171). 1843 bytes result sent to driver
[2021-05-15 10:05:30,054] {docker.py:276} INFO - 21/05/15 13:05:30 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 172) in 179 ms on 4703d70ea67c (executor driver) (85/87)
[2021-05-15 10:05:30,056] {docker.py:276} INFO - 21/05/15 13:05:30 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 171) in 186 ms on 4703d70ea67c (executor driver) (86/87)
[2021-05-15 10:05:30,056] {docker.py:276} INFO - 21/05/15 13:05:30 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 173) in 180 ms on 4703d70ea67c (executor driver) (87/87)
[2021-05-15 10:05:30,057] {docker.py:276} INFO - 21/05/15 13:05:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-15 10:05:30,058] {docker.py:276} INFO - 21/05/15 13:05:30 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 3.985 s
[2021-05-15 10:05:30,059] {docker.py:276} INFO - 21/05/15 13:05:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-15 10:05:30,060] {docker.py:276} INFO - 21/05/15 13:05:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-15 10:05:30,060] {docker.py:276} INFO - 21/05/15 13:05:30 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 3.998764 s
[2021-05-15 10:05:30,074] {docker.py:276} INFO - 21/05/15 13:05:30 INFO InMemoryFileIndex: It took 4061 ms to list leaf files for 87 paths.
[2021-05-15 10:05:30,319] {docker.py:276} INFO - 21/05/15 13:05:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4703d70ea67c:46535 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-15 10:05:32,609] {docker.py:276} INFO - 21/05/15 13:05:32 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 10:05:32,615] {docker.py:276} INFO - 21/05/15 13:05:32 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-15 10:05:32,620] {docker.py:276} INFO - 21/05/15 13:05:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 10:05:33,125] {docker.py:276} INFO - 21/05/15 13:05:33 INFO CodeGenerator: Code generated in 277.6341 ms
[2021-05-15 10:05:33,140] {docker.py:276} INFO - 21/05/15 13:05:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-15 10:05:33,150] {docker.py:276} INFO - 21/05/15 13:05:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-15 10:05:33,152] {docker.py:276} INFO - 21/05/15 13:05:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4703d70ea67c:46535 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 10:05:33,153] {docker.py:276} INFO - 21/05/15 13:05:33 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:33,171] {docker.py:276} INFO - 21/05/15 13:05:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:05:33,266] {docker.py:276} INFO - 21/05/15 13:05:33 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:33,268] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/15 13:05:33 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-15 10:05:33,268] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Parents of final stage: List()
[2021-05-15 10:05:33,269] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Missing parents: List()
[2021-05-15 10:05:33,270] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:05:33,299] {docker.py:276} INFO - 21/05/15 13:05:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-15 10:05:33,318] {docker.py:276} INFO - 21/05/15 13:05:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-15 10:05:33,318] {docker.py:276} INFO - 21/05/15 13:05:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4703d70ea67c:46535 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 10:05:33,319] {docker.py:276} INFO - 21/05/15 13:05:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:05:33,320] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/15 13:05:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-15 10:05:33,325] {docker.py:276} INFO - 21/05/15 13:05:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 174) (4703d70ea67c, executor driver, partition 0, PROCESS_LOCAL, 7215 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:33,326] {docker.py:276} INFO - 21/05/15 13:05:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 174)
[2021-05-15 10:05:33,437] {docker.py:276} INFO - 21/05/15 13:05:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:33,468] {docker.py:276} INFO - 21/05/15 13:05:33 INFO CodeGenerator: Code generated in 21.507 ms
[2021-05-15 10:05:33,848] {docker.py:276} INFO - 21/05/15 13:05:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 174). 1564 bytes result sent to driver
[2021-05-15 10:05:33,850] {docker.py:276} INFO - 21/05/15 13:05:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 174) in 528 ms on 4703d70ea67c (executor driver) (1/1)
21/05/15 13:05:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-15 10:05:33,851] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.576 s
[2021-05-15 10:05:33,851] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 13:05:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-15 10:05:33,852] {docker.py:276} INFO - 21/05/15 13:05:33 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.586250 s
[2021-05-15 10:05:33,887] {docker.py:276} INFO - 21/05/15 13:05:33 INFO CodeGenerator: Code generated in 16.8863 ms
[2021-05-15 10:05:33,979] {docker.py:276} INFO - 21/05/15 13:05:33 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 10:05:33,979] {docker.py:276} INFO - 21/05/15 13:05:33 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 10:05:33,980] {docker.py:276} INFO - 21/05/15 13:05:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-15 10:05:33,987] {docker.py:276} INFO - 21/05/15 13:05:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-15 10:05:34,018] {docker.py:276} INFO - 21/05/15 13:05:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4703d70ea67c:46535 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-15 10:05:34,023] {docker.py:276} INFO - 21/05/15 13:05:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-15 10:05:34,024] {docker.py:276} INFO - 21/05/15 13:05:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4703d70ea67c:46535 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:05:34,025] {docker.py:276} INFO - 21/05/15 13:05:34 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:34,030] {docker.py:276} INFO - 21/05/15 13:05:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:05:34,686] {docker.py:276} INFO - 21/05/15 13:05:34 INFO FileSourceStrategy: Pushed Filters:
[2021-05-15 10:05:34,687] {docker.py:276} INFO - 21/05/15 13:05:34 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-15 10:05:34,688] {docker.py:276} INFO - 21/05/15 13:05:34 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-15 10:05:38,389] {docker.py:276} INFO - 21/05/15 13:05:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:38,393] {docker.py:276} INFO - 21/05/15 13:05:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538415247702313742323_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538415247702313742323_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538415247702313742323_0000}; taskId=attempt_20210515130538415247702313742323_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58f40927}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:38,394] {docker.py:276} INFO - 21/05/15 13:05:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:38,425] {docker.py:276} INFO - 21/05/15 13:05:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 10:05:38,513] {docker.py:276} INFO - 21/05/15 13:05:38 INFO CodeGenerator: Code generated in 57.7998 ms
[2021-05-15 10:05:38,516] {docker.py:276} INFO - 21/05/15 13:05:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-15 10:05:38,561] {docker.py:276} INFO - 21/05/15 13:05:38 INFO CodeGenerator: Code generated in 38.1552 ms
[2021-05-15 10:05:38,565] {docker.py:276} INFO - 21/05/15 13:05:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 933.8 MiB)
[2021-05-15 10:05:38,601] {docker.py:276} INFO - 21/05/15 13:05:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 933.8 MiB)
[2021-05-15 10:05:38,602] {docker.py:276} INFO - 21/05/15 13:05:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4703d70ea67c:46535 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:05:38,604] {docker.py:276} INFO - 21/05/15 13:05:38 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:38,611] {docker.py:276} INFO - 21/05/15 13:05:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 93551346 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-15 10:05:38,677] {docker.py:276} INFO - 21/05/15 13:05:38 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4703d70ea67c:46535 in memory (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-15 10:05:38,741] {docker.py:276} INFO - 21/05/15 13:05:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-15 10:05:38,745] {docker.py:276} INFO - 21/05/15 13:05:38 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-15 10:05:38,748] {docker.py:276} INFO - 21/05/15 13:05:38 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/15 13:05:38 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
21/05/15 13:05:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-15 10:05:38,750] {docker.py:276} INFO - 21/05/15 13:05:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-15 10:05:38,752] {docker.py:276} INFO - 21/05/15 13:05:38 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:05:38,766] {docker.py:276} INFO - 21/05/15 13:05:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-15 10:05:38,786] {docker.py:276} INFO - 21/05/15 13:05:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-15 10:05:38,787] {docker.py:276} INFO - 21/05/15 13:05:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4703d70ea67c:46535 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-15 10:05:38,788] {docker.py:276} INFO - 21/05/15 13:05:38 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:05:38,790] {docker.py:276} INFO - 21/05/15 13:05:38 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21/05/15 13:05:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0
[2021-05-15 10:05:38,793] {docker.py:276} INFO - 21/05/15 13:05:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 175) (4703d70ea67c, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:38,793] {docker.py:276} INFO - 21/05/15 13:05:38 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 176) (4703d70ea67c, executor driver, partition 1, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:38,794] {docker.py:276} INFO - 21/05/15 13:05:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 177) (4703d70ea67c, executor driver, partition 2, PROCESS_LOCAL, 7204 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:38,794] {docker.py:276} INFO - 21/05/15 13:05:38 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 178) (4703d70ea67c, executor driver, partition 3, PROCESS_LOCAL, 7094 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:38,795] {docker.py:276} INFO - 21/05/15 13:05:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 175)
[2021-05-15 10:05:38,797] {docker.py:276} INFO - 21/05/15 13:05:38 INFO Executor: Running task 1.0 in stage 3.0 (TID 176)
[2021-05-15 10:05:38,797] {docker.py:276} INFO - 21/05/15 13:05:38 INFO Executor: Running task 3.0 in stage 3.0 (TID 178)
[2021-05-15 10:05:38,797] {docker.py:276} INFO - 21/05/15 13:05:38 INFO Executor: Running task 2.0 in stage 3.0 (TID 177)
[2021-05-15 10:05:38,878] {docker.py:276} INFO - 21/05/15 13:05:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4703d70ea67c:46535 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-15 10:05:38,922] {docker.py:276} INFO - 21/05/15 13:05:38 INFO CodeGenerator: Code generated in 28.8038 ms
[2021-05-15 10:05:38,951] {docker.py:276} INFO - 21/05/15 13:05:38 INFO CodeGenerator: Code generated in 10.7562 ms
[2021-05-15 10:05:38,976] {docker.py:276} INFO - 21/05/15 13:05:38 INFO CodeGenerator: Code generated in 16.9713 ms
[2021-05-15 10:05:38,996] {docker.py:276} INFO - 21/05/15 13:05:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:38,998] {docker.py:276} INFO - 21/05/15 13:05:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621069476_to_1621071276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:38,998] {docker.py:276} INFO - 21/05/15 13:05:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621044276_to_1621046076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:39,001] {docker.py:276} INFO - 21/05/15 13:05:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621056876_to_1621058676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:40,335] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621031676_to_1621033476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:40,347] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621046076_to_1621047876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:40,354] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621071276_to_1621073076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:40,368] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621058676_to_1621060476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:40,738] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621060476_to_1621062276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:40,816] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621033476_to_1621035276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:40,871] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621047876_to_1621049676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:40,880] {docker.py:276} INFO - 21/05/15 13:05:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621073076_to_1621074876.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:41,093] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621062276_to_1621064076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:41,171] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621035276_to_1621037076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:41,261] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621074876_to_1621076676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:41,262] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621049676_to_1621051476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:41,452] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621064076_to_1621065876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:41,520] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621037076_to_1621038876.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:41,633] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621051476_to_1621053276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:41,775] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621076676_to_1621078476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:41,826] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621065876_to_1621067676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:41,897] {docker.py:276} INFO - 21/05/15 13:05:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621038876_to_1621040676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:42,013] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621053276_to_1621055076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,140] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621078476_to_1621080276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:42,182] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621067676_to_1621069476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,241] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621040676_to_1621042476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:42,370] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621055076_to_1621056876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,503] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621080276_to_1621082076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:42,547] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621069476_to_1621071276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,591] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621042476_to_1621044276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:42,748] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621056876_to_1621058676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,860] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,908] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621071276_to_1621073076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:42,928] {docker.py:276} INFO - 21/05/15 13:05:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621044276_to_1621046076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:43,103] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621058676_to_1621060476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,223] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621031676_to_1621033476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,267] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621073076_to_1621074876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,273] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621046076_to_1621047876.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:43,490] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621060476_to_1621062276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,576] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621033476_to_1621035276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,609] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621047876_to_1621049676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:43,668] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621074876_to_1621076676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,849] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621062276_to_1621064076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:43,938] {docker.py:276} INFO - 21/05/15 13:05:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621035276_to_1621037076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,025] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621049676_to_1621051476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:44,068] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621076676_to_1621078476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,211] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621064076_to_1621065876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,294] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621037076_to_1621038876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,382] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621051476_to_1621053276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:44,419] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621078476_to_1621080276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,560] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621065876_to_1621067676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,660] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621038876_to_1621040676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,725] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621053276_to_1621055076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:44,772] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621080276_to_1621082076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:44,915] {docker.py:276} INFO - 21/05/15 13:05:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621067676_to_1621069476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,035] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621040676_to_1621042476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,080] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621055076_to_1621056876.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:45,131] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621029876_to_1621031676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,262] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621069476_to_1621071276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,390] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621042476_to_1621044276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,434] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621056876_to_1621058676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:45,485] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621031676_to_1621033476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,602] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621071276_to_1621073076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,761] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621044276_to_1621046076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,781] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621058676_to_1621060476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:45,842] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621033476_to_1621035276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:45,952] {docker.py:276} INFO - 21/05/15 13:05:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621073076_to_1621074876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,128] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621046076_to_1621047876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,133] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621060476_to_1621062276.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:46,203] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621035276_to_1621037076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,301] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621074876_to_1621076676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,479] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621062276_to_1621064076.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:46,484] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621047876_to_1621049676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,560] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621037076_to_1621038876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,661] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621076676_to_1621078476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,843] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621049676_to_1621051476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:46,845] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621064076_to_1621065876.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:46,912] {docker.py:276} INFO - 21/05/15 13:05:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621038876_to_1621040676.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,023] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621078476_to_1621080276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,201] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621051476_to_1621053276.csv, range: 0-104506, partition values: [empty row]
21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621065876_to_1621067676.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:47,264] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621040676_to_1621042476.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,382] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621080276_to_1621082076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,550] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-15_10_04_36/from_1621067676_to_1621069476.csv, range: 0-111710, partition values: [empty row]
[2021-05-15 10:05:47,585] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621053276_to_1621055076.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,618] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-15_10_04_36/from_1621042476_to_1621044276.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:47,935] {docker.py:276} INFO - 21/05/15 13:05:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-15_10_04_36/from_1621055076_to_1621056876.csv, range: 0-104506, partition values: [empty row]
[2021-05-15 10:05:48,070] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Finished task 3.0 in stage 3.0 (TID 178). 2722 bytes result sent to driver
[2021-05-15 10:05:48,077] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 178) in 9294 ms on 4703d70ea67c (executor driver) (1/4)
[2021-05-15 10:05:48,247] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Finished task 0.0 in stage 3.0 (TID 175). 2679 bytes result sent to driver
[2021-05-15 10:05:48,248] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 175) in 9468 ms on 4703d70ea67c (executor driver) (2/4)
[2021-05-15 10:05:48,280] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Finished task 2.0 in stage 3.0 (TID 177). 2679 bytes result sent to driver
[2021-05-15 10:05:48,281] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 177) in 9499 ms on 4703d70ea67c (executor driver) (3/4)
[2021-05-15 10:05:48,490] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Finished task 1.0 in stage 3.0 (TID 176). 2679 bytes result sent to driver
[2021-05-15 10:05:48,490] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 176) in 9709 ms on 4703d70ea67c (executor driver) (4/4)
21/05/15 13:05:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-15 10:05:48,491] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 9.745 s
[2021-05-15 10:05:48,492] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: looking for newly runnable stages
[2021-05-15 10:05:48,493] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: running: Set()
[2021-05-15 10:05:48,493] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-15 10:05:48,494] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: failed: Set()
[2021-05-15 10:05:48,498] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-15 10:05:48,544] {docker.py:276} INFO - 21/05/15 13:05:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-15 10:05:48,547] {docker.py:276} INFO - 21/05/15 13:05:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-15 10:05:48,548] {docker.py:276} INFO - 21/05/15 13:05:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4703d70ea67c:46535 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-15 10:05:48,549] {docker.py:276} INFO - 21/05/15 13:05:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-15 10:05:48,550] {docker.py:276} INFO - 21/05/15 13:05:48 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-15 10:05:48,551] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-15 10:05:48,559] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 179) (4703d70ea67c, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:48,559] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 180) (4703d70ea67c, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:48,560] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 181) (4703d70ea67c, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:48,561] {docker.py:276} INFO - 21/05/15 13:05:48 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 182) (4703d70ea67c, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:48,562] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Running task 2.0 in stage 4.0 (TID 181)
[2021-05-15 10:05:48,562] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Running task 1.0 in stage 4.0 (TID 180)
[2021-05-15 10:05:48,567] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Running task 3.0 in stage 4.0 (TID 182)
[2021-05-15 10:05:48,571] {docker.py:276} INFO - 21/05/15 13:05:48 INFO Executor: Running task 0.0 in stage 4.0 (TID 179)
[2021-05-15 10:05:48,645] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:48,646] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:48,646] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:48,647] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:48,649] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-15 10:05:48,649] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-15 10:05:48,649] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-15 10:05:48,650] {docker.py:276} INFO - 21/05/15 13:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2021-05-15 10:05:48,666] {docker.py:276} INFO - 21/05/15 13:05:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:05:48,667] {docker.py:276} INFO - 21/05/15 13:05:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:48,667] {docker.py:276} INFO - 21/05/15 13:05:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:48,672] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,672] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,673] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,673] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387054384947066069715_0004_m_000001_180, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054384947066069715_0004_m_000001_180}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387054384947066069715_0004}; taskId=attempt_202105151305387054384947066069715_0004_m_000001_180, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@303e1af3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,674] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538944783109184322613_0004_m_000000_179, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538944783109184322613_0004_m_000000_179}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538944783109184322613_0004}; taskId=attempt_20210515130538944783109184322613_0004_m_000000_179, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4610318}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,675] {docker.py:276} INFO - 21/05/15 13:05:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:48,676] {docker.py:276} INFO - 21/05/15 13:05:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:48,678] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305387054384947066069715_0004_m_000001_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054384947066069715_0004_m_000001_180
[2021-05-15 10:05:48,678] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,679] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383862961194972364678_0004_m_000002_181, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383862961194972364678_0004_m_000002_181}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383862961194972364678_0004}; taskId=attempt_202105151305383862961194972364678_0004_m_000002_181, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e705d05}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:48,680] {docker.py:276} INFO - 21/05/15 13:05:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386111389246267274760_0004_m_000003_182, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386111389246267274760_0004_m_000003_182}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386111389246267274760_0004}; taskId=attempt_202105151305386111389246267274760_0004_m_000003_182, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c6c7dde}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:48,681] {docker.py:276} INFO - 21/05/15 13:05:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:48,682] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305383862961194972364678_0004_m_000002_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383862961194972364678_0004_m_000002_181 
21/05/15 13:05:48 INFO StagingCommitter: Starting: Task committer attempt_20210515130538944783109184322613_0004_m_000000_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538944783109184322613_0004_m_000000_179
[2021-05-15 10:05:48,682] {docker.py:276} INFO - 21/05/15 13:05:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:48,683] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305386111389246267274760_0004_m_000003_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386111389246267274760_0004_m_000003_182
[2021-05-15 10:05:48,709] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Task committer attempt_202105151305386111389246267274760_0004_m_000003_182: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386111389246267274760_0004_m_000003_182 : duration 0:00.026s
[2021-05-15 10:05:48,711] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Task committer attempt_202105151305383862961194972364678_0004_m_000002_181: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383862961194972364678_0004_m_000002_181 : duration 0:00.032s
21/05/15 13:05:48 INFO StagingCommitter: Task committer attempt_20210515130538944783109184322613_0004_m_000000_179: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538944783109184322613_0004_m_000000_179 : duration 0:00.030s
[2021-05-15 10:05:48,721] {docker.py:276} INFO - 21/05/15 13:05:48 INFO StagingCommitter: Task committer attempt_202105151305387054384947066069715_0004_m_000001_180: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054384947066069715_0004_m_000001_180 : duration 0:00.043s
[2021-05-15 10:05:50,560] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_20210515130538944783109184322613_0004_m_000000_179: needsTaskCommit() Task attempt_20210515130538944783109184322613_0004_m_000000_179
[2021-05-15 10:05:50,561] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_20210515130538944783109184322613_0004_m_000000_179: needsTaskCommit() Task attempt_20210515130538944783109184322613_0004_m_000000_179: duration 0:00.001s
[2021-05-15 10:05:50,562] {docker.py:276} INFO - 21/05/15 13:05:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538944783109184322613_0004_m_000000_179
[2021-05-15 10:05:50,566] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 179). 4630 bytes result sent to driver
[2021-05-15 10:05:50,567] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 183) (4703d70ea67c, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:50,569] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 179) in 2015 ms on 4703d70ea67c (executor driver) (1/200)
21/05/15 13:05:50 INFO Executor: Running task 4.0 in stage 4.0 (TID 183)
[2021-05-15 10:05:50,579] {docker.py:276} INFO - 21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:50,582] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305383862961194972364678_0004_m_000002_181: needsTaskCommit() Task attempt_202105151305383862961194972364678_0004_m_000002_181
21/05/15 13:05:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:50,582] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_202105151305383862961194972364678_0004_m_000002_181: needsTaskCommit() Task attempt_202105151305383862961194972364678_0004_m_000002_181: duration 0:00.000s
[2021-05-15 10:05:50,584] {docker.py:276} INFO - 21/05/15 13:05:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383862961194972364678_0004_m_000002_181
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538162135670242325007_0004_m_000004_183, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538162135670242325007_0004_m_000004_183}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538162135670242325007_0004}; taskId=attempt_20210515130538162135670242325007_0004_m_000004_183, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3524060c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:50,584] {docker.py:276} INFO - 21/05/15 13:05:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_20210515130538162135670242325007_0004_m_000004_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538162135670242325007_0004_m_000004_183
[2021-05-15 10:05:50,586] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Finished task 2.0 in stage 4.0 (TID 181). 4587 bytes result sent to driver
[2021-05-15 10:05:50,586] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305387054384947066069715_0004_m_000001_180: needsTaskCommit() Task attempt_202105151305387054384947066069715_0004_m_000001_180
21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305386111389246267274760_0004_m_000003_182: needsTaskCommit() Task attempt_202105151305386111389246267274760_0004_m_000003_182
[2021-05-15 10:05:50,587] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 184) (4703d70ea67c, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:50,587] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_202105151305386111389246267274760_0004_m_000003_182: needsTaskCommit() Task attempt_202105151305386111389246267274760_0004_m_000003_182: duration 0:00.003s
21/05/15 13:05:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386111389246267274760_0004_m_000003_182
[2021-05-15 10:05:50,587] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Running task 5.0 in stage 4.0 (TID 184)
[2021-05-15 10:05:50,588] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 181) in 2031 ms on 4703d70ea67c (executor driver) (2/200)
[2021-05-15 10:05:50,589] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Finished task 3.0 in stage 4.0 (TID 182). 4630 bytes result sent to driver
[2021-05-15 10:05:50,590] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_202105151305387054384947066069715_0004_m_000001_180: needsTaskCommit() Task attempt_202105151305387054384947066069715_0004_m_000001_180: duration 0:00.006s
21/05/15 13:05:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387054384947066069715_0004_m_000001_180
[2021-05-15 10:05:50,591] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 185) (4703d70ea67c, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:50,592] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 182) in 2033 ms on 4703d70ea67c (executor driver) (3/200)
[2021-05-15 10:05:50,592] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Running task 6.0 in stage 4.0 (TID 185)
[2021-05-15 10:05:50,603] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Finished task 1.0 in stage 4.0 (TID 180). 4587 bytes result sent to driver
[2021-05-15 10:05:50,604] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 186) (4703d70ea67c, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:50,606] {docker.py:276} INFO - 21/05/15 13:05:50 INFO Executor: Running task 7.0 in stage 4.0 (TID 186)
[2021-05-15 10:05:50,607] {docker.py:276} INFO - 21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:50,609] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_20210515130538162135670242325007_0004_m_000004_183: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538162135670242325007_0004_m_000004_183 : duration 0:00.024s
[2021-05-15 10:05:50,609] {docker.py:276} INFO - 21/05/15 13:05:50 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 180) in 2052 ms on 4703d70ea67c (executor driver) (4/200)
[2021-05-15 10:05:50,618] {docker.py:276} INFO - 21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:50,631] {docker.py:276} INFO - 21/05/15 13:05:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:50,632] {docker.py:276} INFO - 21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382643193169368930661_0004_m_000005_184, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382643193169368930661_0004_m_000005_184}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382643193169368930661_0004}; taskId=attempt_202105151305382643193169368930661_0004_m_000005_184, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27362053}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:50,632] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305382643193169368930661_0004_m_000005_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382643193169368930661_0004_m_000005_184
[2021-05-15 10:05:50,635] {docker.py:276} INFO - 21/05/15 13:05:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:50,635] {docker.py:276} INFO - 21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387454388071844132488_0004_m_000006_185, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387454388071844132488_0004_m_000006_185}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387454388071844132488_0004}; taskId=attempt_202105151305387454388071844132488_0004_m_000006_185, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63e239f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305387454388071844132488_0004_m_000006_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387454388071844132488_0004_m_000006_185
[2021-05-15 10:05:50,638] {docker.py:276} INFO - 21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.9 KiB) non-empty blocks including 4 (12.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:05:50,646] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_202105151305382643193169368930661_0004_m_000005_184: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382643193169368930661_0004_m_000005_184 : duration 0:00.014s
[2021-05-15 10:05:50,648] {docker.py:276} INFO - 21/05/15 13:05:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538471186271220926181_0004_m_000007_186, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538471186271220926181_0004_m_000007_186}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538471186271220926181_0004}; taskId=attempt_20210515130538471186271220926181_0004_m_000007_186, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2212b3c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:50,649] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Starting: Task committer attempt_20210515130538471186271220926181_0004_m_000007_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538471186271220926181_0004_m_000007_186
[2021-05-15 10:05:50,665] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_202105151305387454388071844132488_0004_m_000006_185: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387454388071844132488_0004_m_000006_185 : duration 0:00.027s
[2021-05-15 10:05:50,665] {docker.py:276} INFO - 21/05/15 13:05:50 INFO StagingCommitter: Task committer attempt_20210515130538471186271220926181_0004_m_000007_186: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538471186271220926181_0004_m_000007_186 : duration 0:00.016s
[2021-05-15 10:05:53,055] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_20210515130538162135670242325007_0004_m_000004_183: needsTaskCommit() Task attempt_20210515130538162135670242325007_0004_m_000004_183
[2021-05-15 10:05:53,056] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_20210515130538162135670242325007_0004_m_000004_183: needsTaskCommit() Task attempt_20210515130538162135670242325007_0004_m_000004_183: duration 0:00.002s
21/05/15 13:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538162135670242325007_0004_m_000004_183
[2021-05-15 10:05:53,060] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Finished task 4.0 in stage 4.0 (TID 183). 4587 bytes result sent to driver
[2021-05-15 10:05:53,062] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 187) (4703d70ea67c, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:53,063] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Running task 8.0 in stage 4.0 (TID 187)
[2021-05-15 10:05:53,064] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 183) in 2499 ms on 4703d70ea67c (executor driver) (5/200)
[2021-05-15 10:05:53,076] {docker.py:276} INFO - 21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:53,078] {docker.py:276} INFO - 21/05/15 13:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:53,079] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386424433707462705932_0004_m_000008_187, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386424433707462705932_0004_m_000008_187}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386424433707462705932_0004}; taskId=attempt_202105151305386424433707462705932_0004_m_000008_187, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1dbbc1ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,079] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:53,079] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305386424433707462705932_0004_m_000008_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386424433707462705932_0004_m_000008_187
[2021-05-15 10:05:53,083] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305386424433707462705932_0004_m_000008_187: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386424433707462705932_0004_m_000008_187 : duration 0:00.004s
[2021-05-15 10:05:53,141] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_20210515130538471186271220926181_0004_m_000007_186: needsTaskCommit() Task attempt_20210515130538471186271220926181_0004_m_000007_186
[2021-05-15 10:05:53,142] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_20210515130538471186271220926181_0004_m_000007_186: needsTaskCommit() Task attempt_20210515130538471186271220926181_0004_m_000007_186: duration 0:00.001s
[2021-05-15 10:05:53,142] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538471186271220926181_0004_m_000007_186
[2021-05-15 10:05:53,144] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Finished task 7.0 in stage 4.0 (TID 186). 4587 bytes result sent to driver
[2021-05-15 10:05:53,145] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 188) (4703d70ea67c, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:53,146] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Running task 9.0 in stage 4.0 (TID 188)
[2021-05-15 10:05:53,147] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 186) in 2545 ms on 4703d70ea67c (executor driver) (6/200)
[2021-05-15 10:05:53,156] {docker.py:276} INFO - 21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:53,159] {docker.py:276} INFO - 21/05/15 13:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:53,159] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,160] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386633173899955547871_0004_m_000009_188, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386633173899955547871_0004_m_000009_188}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386633173899955547871_0004}; taskId=attempt_202105151305386633173899955547871_0004_m_000009_188, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ba1032d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,160] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:53,160] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305386633173899955547871_0004_m_000009_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386633173899955547871_0004_m_000009_188
[2021-05-15 10:05:53,164] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305386633173899955547871_0004_m_000009_188: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386633173899955547871_0004_m_000009_188 : duration 0:00.004s
[2021-05-15 10:05:53,181] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305387454388071844132488_0004_m_000006_185: needsTaskCommit() Task attempt_202105151305387454388071844132488_0004_m_000006_185
[2021-05-15 10:05:53,182] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305387454388071844132488_0004_m_000006_185: needsTaskCommit() Task attempt_202105151305387454388071844132488_0004_m_000006_185: duration 0:00.001s
21/05/15 13:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387454388071844132488_0004_m_000006_185
[2021-05-15 10:05:53,184] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Finished task 6.0 in stage 4.0 (TID 185). 4587 bytes result sent to driver
[2021-05-15 10:05:53,185] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 189) (4703d70ea67c, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:53,185] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 185) in 2599 ms on 4703d70ea67c (executor driver) (7/200)
[2021-05-15 10:05:53,186] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Running task 10.0 in stage 4.0 (TID 189)
[2021-05-15 10:05:53,192] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305382643193169368930661_0004_m_000005_184: needsTaskCommit() Task attempt_202105151305382643193169368930661_0004_m_000005_184
[2021-05-15 10:05:53,192] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305382643193169368930661_0004_m_000005_184: needsTaskCommit() Task attempt_202105151305382643193169368930661_0004_m_000005_184: duration 0:00.001s
[2021-05-15 10:05:53,193] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382643193169368930661_0004_m_000005_184
[2021-05-15 10:05:53,197] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Finished task 5.0 in stage 4.0 (TID 184). 4587 bytes result sent to driver
[2021-05-15 10:05:53,198] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 190) (4703d70ea67c, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:53,199] {docker.py:276} INFO - 21/05/15 13:05:53 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 184) in 2616 ms on 4703d70ea67c (executor driver) (8/200)
[2021-05-15 10:05:53,200] {docker.py:276} INFO - 21/05/15 13:05:53 INFO Executor: Running task 11.0 in stage 4.0 (TID 190)
[2021-05-15 10:05:53,204] {docker.py:276} INFO - 21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:53,204] {docker.py:276} INFO - 21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:53,207] {docker.py:276} INFO - 21/05/15 13:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:53,208] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,209] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381569219216782474985_0004_m_000010_189, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381569219216782474985_0004_m_000010_189}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381569219216782474985_0004}; taskId=attempt_202105151305381569219216782474985_0004_m_000010_189, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c7fab30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,209] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305381569219216782474985_0004_m_000010_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381569219216782474985_0004_m_000010_189
[2021-05-15 10:05:53,212] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305381569219216782474985_0004_m_000010_189: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381569219216782474985_0004_m_000010_189 : duration 0:00.005s
[2021-05-15 10:05:53,218] {docker.py:276} INFO - 21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:05:53,220] {docker.py:276} INFO - 21/05/15 13:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:53,221] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,221] {docker.py:276} INFO - 21/05/15 13:05:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382346988228927069427_0004_m_000011_190, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382346988228927069427_0004_m_000011_190}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382346988228927069427_0004}; taskId=attempt_202105151305382346988228927069427_0004_m_000011_190, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@155e9b86}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:53,222] {docker.py:276} INFO - 21/05/15 13:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:53,222] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305382346988228927069427_0004_m_000011_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382346988228927069427_0004_m_000011_190
[2021-05-15 10:05:53,227] {docker.py:276} INFO - 21/05/15 13:05:53 INFO StagingCommitter: Task committer attempt_202105151305382346988228927069427_0004_m_000011_190: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382346988228927069427_0004_m_000011_190 : duration 0:00.005s
[2021-05-15 10:05:55,588] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305382346988228927069427_0004_m_000011_190: needsTaskCommit() Task attempt_202105151305382346988228927069427_0004_m_000011_190
[2021-05-15 10:05:55,589] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305382346988228927069427_0004_m_000011_190: needsTaskCommit() Task attempt_202105151305382346988228927069427_0004_m_000011_190: duration 0:00.001s
[2021-05-15 10:05:55,590] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382346988228927069427_0004_m_000011_190
[2021-05-15 10:05:55,592] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Finished task 11.0 in stage 4.0 (TID 190). 4544 bytes result sent to driver
[2021-05-15 10:05:55,595] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 191) (4703d70ea67c, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:55,596] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Running task 12.0 in stage 4.0 (TID 191)
[2021-05-15 10:05:55,597] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 190) in 2366 ms on 4703d70ea67c (executor driver) (9/200)
[2021-05-15 10:05:55,609] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:55,609] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:55,611] {docker.py:276} INFO - 21/05/15 13:05:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:55,612] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,613] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383255692381898490430_0004_m_000012_191, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383255692381898490430_0004_m_000012_191}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383255692381898490430_0004}; taskId=attempt_202105151305383255692381898490430_0004_m_000012_191, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d2b782d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,613] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:55,613] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305383255692381898490430_0004_m_000012_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383255692381898490430_0004_m_000012_191
[2021-05-15 10:05:55,617] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305383255692381898490430_0004_m_000012_191: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383255692381898490430_0004_m_000012_191 : duration 0:00.004s
[2021-05-15 10:05:55,622] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305386424433707462705932_0004_m_000008_187: needsTaskCommit() Task attempt_202105151305386424433707462705932_0004_m_000008_187
[2021-05-15 10:05:55,622] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305386424433707462705932_0004_m_000008_187: needsTaskCommit() Task attempt_202105151305386424433707462705932_0004_m_000008_187: duration 0:00.002s
21/05/15 13:05:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386424433707462705932_0004_m_000008_187
[2021-05-15 10:05:55,623] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Finished task 8.0 in stage 4.0 (TID 187). 4544 bytes result sent to driver
[2021-05-15 10:05:55,624] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 192) (4703d70ea67c, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:55,625] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Running task 13.0 in stage 4.0 (TID 192)
[2021-05-15 10:05:55,626] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 187) in 2534 ms on 4703d70ea67c (executor driver) (10/200)
[2021-05-15 10:05:55,637] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:55,640] {docker.py:276} INFO - 21/05/15 13:05:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:55,640] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385768333465268938244_0004_m_000013_192, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768333465268938244_0004_m_000013_192}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385768333465268938244_0004}; taskId=attempt_202105151305385768333465268938244_0004_m_000013_192, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@59f01f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,641] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305385768333465268938244_0004_m_000013_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768333465268938244_0004_m_000013_192
[2021-05-15 10:05:55,644] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305385768333465268938244_0004_m_000013_192: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768333465268938244_0004_m_000013_192 : duration 0:00.004s
[2021-05-15 10:05:55,701] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305381569219216782474985_0004_m_000010_189: needsTaskCommit() Task attempt_202105151305381569219216782474985_0004_m_000010_189
[2021-05-15 10:05:55,702] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305381569219216782474985_0004_m_000010_189: needsTaskCommit() Task attempt_202105151305381569219216782474985_0004_m_000010_189: duration 0:00.001s
[2021-05-15 10:05:55,702] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381569219216782474985_0004_m_000010_189
[2021-05-15 10:05:55,703] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Finished task 10.0 in stage 4.0 (TID 189). 4544 bytes result sent to driver
[2021-05-15 10:05:55,704] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 193) (4703d70ea67c, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:55,705] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Running task 14.0 in stage 4.0 (TID 193)
21/05/15 13:05:55 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 189) in 2489 ms on 4703d70ea67c (executor driver) (11/200)
[2021-05-15 10:05:55,708] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305386633173899955547871_0004_m_000009_188: needsTaskCommit() Task attempt_202105151305386633173899955547871_0004_m_000009_188
[2021-05-15 10:05:55,709] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305386633173899955547871_0004_m_000009_188: needsTaskCommit() Task attempt_202105151305386633173899955547871_0004_m_000009_188: duration 0:00.001s
21/05/15 13:05:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386633173899955547871_0004_m_000009_188
[2021-05-15 10:05:55,710] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Finished task 9.0 in stage 4.0 (TID 188). 4544 bytes result sent to driver
[2021-05-15 10:05:55,711] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 194) (4703d70ea67c, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:55,712] {docker.py:276} INFO - 21/05/15 13:05:55 INFO Executor: Running task 15.0 in stage 4.0 (TID 194)
[2021-05-15 10:05:55,712] {docker.py:276} INFO - 21/05/15 13:05:55 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 188) in 2535 ms on 4703d70ea67c (executor driver) (12/200)
[2021-05-15 10:05:55,720] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:55,720] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:55,723] {docker.py:276} INFO - 21/05/15 13:05:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:55,723] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,724] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305389038830188755935459_0004_m_000014_193, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389038830188755935459_0004_m_000014_193}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305389038830188755935459_0004}; taskId=attempt_202105151305389038830188755935459_0004_m_000014_193, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@287ee153}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,724] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:55,724] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305389038830188755935459_0004_m_000014_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389038830188755935459_0004_m_000014_193
[2021-05-15 10:05:55,727] {docker.py:276} INFO - 21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:55,729] {docker.py:276} INFO - 21/05/15 13:05:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:55,730] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,730] {docker.py:276} INFO - 21/05/15 13:05:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385561005669844983254_0004_m_000015_194, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385561005669844983254_0004_m_000015_194}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385561005669844983254_0004}; taskId=attempt_202105151305385561005669844983254_0004_m_000015_194, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd74cef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:55,730] {docker.py:276} INFO - 21/05/15 13:05:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305385561005669844983254_0004_m_000015_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385561005669844983254_0004_m_000015_194
[2021-05-15 10:05:55,735] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305389038830188755935459_0004_m_000014_193: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389038830188755935459_0004_m_000014_193 : duration 0:00.009s
[2021-05-15 10:05:55,742] {docker.py:276} INFO - 21/05/15 13:05:55 INFO StagingCommitter: Task committer attempt_202105151305385561005669844983254_0004_m_000015_194: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385561005669844983254_0004_m_000015_194 : duration 0:00.006s
[2021-05-15 10:05:57,494] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Starting: Task committer attempt_202105151305385768333465268938244_0004_m_000013_192: needsTaskCommit() Task attempt_202105151305385768333465268938244_0004_m_000013_192
[2021-05-15 10:05:57,494] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Task committer attempt_202105151305385768333465268938244_0004_m_000013_192: needsTaskCommit() Task attempt_202105151305385768333465268938244_0004_m_000013_192: duration 0:00.000s
21/05/15 13:05:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385768333465268938244_0004_m_000013_192
[2021-05-15 10:05:57,496] {docker.py:276} INFO - 21/05/15 13:05:57 INFO Executor: Finished task 13.0 in stage 4.0 (TID 192). 4544 bytes result sent to driver
[2021-05-15 10:05:57,497] {docker.py:276} INFO - 21/05/15 13:05:57 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 195) (4703d70ea67c, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:57,498] {docker.py:276} INFO - 21/05/15 13:05:57 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 192) in 1876 ms on 4703d70ea67c (executor driver) (13/200)
[2021-05-15 10:05:57,498] {docker.py:276} INFO - 21/05/15 13:05:57 INFO Executor: Running task 16.0 in stage 4.0 (TID 195)
[2021-05-15 10:05:57,508] {docker.py:276} INFO - 21/05/15 13:05:57 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:57,524] {docker.py:276} INFO - 21/05/15 13:05:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:57,525] {docker.py:276} INFO - 21/05/15 13:05:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:05:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385611046493481613907_0004_m_000016_195, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385611046493481613907_0004_m_000016_195}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385611046493481613907_0004}; taskId=attempt_202105151305385611046493481613907_0004_m_000016_195, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41173ce6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:57,525] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Starting: Task committer attempt_202105151305385611046493481613907_0004_m_000016_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385611046493481613907_0004_m_000016_195
[2021-05-15 10:05:57,530] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Task committer attempt_202105151305385611046493481613907_0004_m_000016_195: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385611046493481613907_0004_m_000016_195 : duration 0:00.005s
[2021-05-15 10:05:57,698] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Starting: Task committer attempt_202105151305389038830188755935459_0004_m_000014_193: needsTaskCommit() Task attempt_202105151305389038830188755935459_0004_m_000014_193
[2021-05-15 10:05:57,700] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Task committer attempt_202105151305389038830188755935459_0004_m_000014_193: needsTaskCommit() Task attempt_202105151305389038830188755935459_0004_m_000014_193: duration 0:00.003s
21/05/15 13:05:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305389038830188755935459_0004_m_000014_193
[2021-05-15 10:05:57,702] {docker.py:276} INFO - 21/05/15 13:05:57 INFO Executor: Finished task 14.0 in stage 4.0 (TID 193). 4587 bytes result sent to driver
[2021-05-15 10:05:57,705] {docker.py:276} INFO - 21/05/15 13:05:57 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 196) (4703d70ea67c, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:57,707] {docker.py:276} INFO - 21/05/15 13:05:57 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 193) in 2005 ms on 4703d70ea67c (executor driver) (14/200)
[2021-05-15 10:05:57,709] {docker.py:276} INFO - 21/05/15 13:05:57 INFO Executor: Running task 17.0 in stage 4.0 (TID 196)
[2021-05-15 10:05:57,720] {docker.py:276} INFO - 21/05/15 13:05:57 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:05:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:57,722] {docker.py:276} INFO - 21/05/15 13:05:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:57,722] {docker.py:276} INFO - 21/05/15 13:05:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388354340275392651296_0004_m_000017_196, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388354340275392651296_0004_m_000017_196}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388354340275392651296_0004}; taskId=attempt_202105151305388354340275392651296_0004_m_000017_196, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16f440ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:05:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:57 INFO StagingCommitter: Starting: Task committer attempt_202105151305388354340275392651296_0004_m_000017_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388354340275392651296_0004_m_000017_196
[2021-05-15 10:05:57,726] {docker.py:276} INFO - 21/05/15 13:05:57 INFO StagingCommitter: Task committer attempt_202105151305388354340275392651296_0004_m_000017_196: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388354340275392651296_0004_m_000017_196 : duration 0:00.003s
[2021-05-15 10:05:58,022] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305383255692381898490430_0004_m_000012_191: needsTaskCommit() Task attempt_202105151305383255692381898490430_0004_m_000012_191
[2021-05-15 10:05:58,023] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Task committer attempt_202105151305383255692381898490430_0004_m_000012_191: needsTaskCommit() Task attempt_202105151305383255692381898490430_0004_m_000012_191: duration 0:00.004s
[2021-05-15 10:05:58,024] {docker.py:276} INFO - 21/05/15 13:05:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383255692381898490430_0004_m_000012_191
[2021-05-15 10:05:58,026] {docker.py:276} INFO - 21/05/15 13:05:58 INFO Executor: Finished task 12.0 in stage 4.0 (TID 191). 4587 bytes result sent to driver
[2021-05-15 10:05:58,028] {docker.py:276} INFO - 21/05/15 13:05:58 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 197) (4703d70ea67c, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:58,029] {docker.py:276} INFO - 21/05/15 13:05:58 INFO Executor: Running task 18.0 in stage 4.0 (TID 197)
21/05/15 13:05:58 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 191) in 2438 ms on 4703d70ea67c (executor driver) (15/200)
[2021-05-15 10:05:58,040] {docker.py:276} INFO - 21/05/15 13:05:58 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:58,040] {docker.py:276} INFO - 21/05/15 13:05:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:05:58,043] {docker.py:276} INFO - 21/05/15 13:05:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:05:58,043] {docker.py:276} INFO - 21/05/15 13:05:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:58,043] {docker.py:276} INFO - 21/05/15 13:05:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:58,044] {docker.py:276} INFO - 21/05/15 13:05:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386806615314081333776_0004_m_000018_197, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386806615314081333776_0004_m_000018_197}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386806615314081333776_0004}; taskId=attempt_202105151305386806615314081333776_0004_m_000018_197, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43d8f7d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:58,044] {docker.py:276} INFO - 21/05/15 13:05:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:58,045] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305386806615314081333776_0004_m_000018_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386806615314081333776_0004_m_000018_197
[2021-05-15 10:05:58,051] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Task committer attempt_202105151305386806615314081333776_0004_m_000018_197: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386806615314081333776_0004_m_000018_197 : duration 0:00.006s
[2021-05-15 10:05:58,615] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305385561005669844983254_0004_m_000015_194: needsTaskCommit() Task attempt_202105151305385561005669844983254_0004_m_000015_194
[2021-05-15 10:05:58,616] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Task committer attempt_202105151305385561005669844983254_0004_m_000015_194: needsTaskCommit() Task attempt_202105151305385561005669844983254_0004_m_000015_194: duration 0:00.001s
[2021-05-15 10:05:58,616] {docker.py:276} INFO - 21/05/15 13:05:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385561005669844983254_0004_m_000015_194
[2021-05-15 10:05:58,617] {docker.py:276} INFO - 21/05/15 13:05:58 INFO Executor: Finished task 15.0 in stage 4.0 (TID 194). 4587 bytes result sent to driver
[2021-05-15 10:05:58,619] {docker.py:276} INFO - 21/05/15 13:05:58 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 198) (4703d70ea67c, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:58,620] {docker.py:276} INFO - 21/05/15 13:05:58 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 194) in 2913 ms on 4703d70ea67c (executor driver) (16/200)
21/05/15 13:05:58 INFO Executor: Running task 19.0 in stage 4.0 (TID 198)
[2021-05-15 10:05:58,629] {docker.py:276} INFO - 21/05/15 13:05:58 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:58,630] {docker.py:276} INFO - 21/05/15 13:05:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:58,632] {docker.py:276} INFO - 21/05/15 13:05:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:05:58,632] {docker.py:276} INFO - 21/05/15 13:05:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:58,633] {docker.py:276} INFO - 21/05/15 13:05:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382090504324265348684_0004_m_000019_198, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382090504324265348684_0004_m_000019_198}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382090504324265348684_0004}; taskId=attempt_202105151305382090504324265348684_0004_m_000019_198, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6709286e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:58,633] {docker.py:276} INFO - 21/05/15 13:05:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:05:58,633] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305382090504324265348684_0004_m_000019_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382090504324265348684_0004_m_000019_198
[2021-05-15 10:05:58,638] {docker.py:276} INFO - 21/05/15 13:05:58 INFO StagingCommitter: Task committer attempt_202105151305382090504324265348684_0004_m_000019_198: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382090504324265348684_0004_m_000019_198 : duration 0:00.005s
[2021-05-15 10:05:59,766] {docker.py:276} INFO - 21/05/15 13:05:59 INFO StagingCommitter: Starting: Task committer attempt_202105151305388354340275392651296_0004_m_000017_196: needsTaskCommit() Task attempt_202105151305388354340275392651296_0004_m_000017_196
[2021-05-15 10:05:59,767] {docker.py:276} INFO - 21/05/15 13:05:59 INFO StagingCommitter: Task committer attempt_202105151305388354340275392651296_0004_m_000017_196: needsTaskCommit() Task attempt_202105151305388354340275392651296_0004_m_000017_196: duration 0:00.003s
21/05/15 13:05:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388354340275392651296_0004_m_000017_196
[2021-05-15 10:05:59,769] {docker.py:276} INFO - 21/05/15 13:05:59 INFO Executor: Finished task 17.0 in stage 4.0 (TID 196). 4544 bytes result sent to driver
[2021-05-15 10:05:59,770] {docker.py:276} INFO - 21/05/15 13:05:59 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 199) (4703d70ea67c, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:05:59,771] {docker.py:276} INFO - 21/05/15 13:05:59 INFO Executor: Running task 20.0 in stage 4.0 (TID 199)
[2021-05-15 10:05:59,772] {docker.py:276} INFO - 21/05/15 13:05:59 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 196) in 2070 ms on 4703d70ea67c (executor driver) (17/200)
[2021-05-15 10:05:59,782] {docker.py:276} INFO - 21/05/15 13:05:59 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:05:59,783] {docker.py:276} INFO - 21/05/15 13:05:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:05:59,785] {docker.py:276} INFO - 21/05/15 13:05:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:05:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:05:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:59,785] {docker.py:276} INFO - 21/05/15 13:05:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385875472889494665540_0004_m_000020_199, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385875472889494665540_0004_m_000020_199}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385875472889494665540_0004}; taskId=attempt_202105151305385875472889494665540_0004_m_000020_199, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a404e37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:05:59,786] {docker.py:276} INFO - 21/05/15 13:05:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:05:59 INFO StagingCommitter: Starting: Task committer attempt_202105151305385875472889494665540_0004_m_000020_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385875472889494665540_0004_m_000020_199
[2021-05-15 10:05:59,790] {docker.py:276} INFO - 21/05/15 13:05:59 INFO StagingCommitter: Task committer attempt_202105151305385875472889494665540_0004_m_000020_199: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385875472889494665540_0004_m_000020_199 : duration 0:00.005s
[2021-05-15 10:06:00,362] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305386806615314081333776_0004_m_000018_197: needsTaskCommit() Task attempt_202105151305386806615314081333776_0004_m_000018_197
[2021-05-15 10:06:00,363] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Task committer attempt_202105151305386806615314081333776_0004_m_000018_197: needsTaskCommit() Task attempt_202105151305386806615314081333776_0004_m_000018_197: duration 0:00.004s
21/05/15 13:06:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386806615314081333776_0004_m_000018_197
[2021-05-15 10:06:00,364] {docker.py:276} INFO - 21/05/15 13:06:00 INFO Executor: Finished task 18.0 in stage 4.0 (TID 197). 4544 bytes result sent to driver
[2021-05-15 10:06:00,366] {docker.py:276} INFO - 21/05/15 13:06:00 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 200) (4703d70ea67c, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:00,367] {docker.py:276} INFO - 21/05/15 13:06:00 INFO Executor: Running task 21.0 in stage 4.0 (TID 200)
21/05/15 13:06:00 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 197) in 2342 ms on 4703d70ea67c (executor driver) (18/200)
[2021-05-15 10:06:00,377] {docker.py:276} INFO - 21/05/15 13:06:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:00,380] {docker.py:276} INFO - 21/05/15 13:06:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382389085775906377698_0004_m_000021_200, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382389085775906377698_0004_m_000021_200}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382389085775906377698_0004}; taskId=attempt_202105151305382389085775906377698_0004_m_000021_200, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f147129}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:00,380] {docker.py:276} INFO - 21/05/15 13:06:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:00,381] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305382389085775906377698_0004_m_000021_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382389085775906377698_0004_m_000021_200
[2021-05-15 10:06:00,386] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Task committer attempt_202105151305382389085775906377698_0004_m_000021_200: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382389085775906377698_0004_m_000021_200 : duration 0:00.006s
[2021-05-15 10:06:00,549] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305385611046493481613907_0004_m_000016_195: needsTaskCommit() Task attempt_202105151305385611046493481613907_0004_m_000016_195
[2021-05-15 10:06:00,550] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Task committer attempt_202105151305385611046493481613907_0004_m_000016_195: needsTaskCommit() Task attempt_202105151305385611046493481613907_0004_m_000016_195: duration 0:00.003s
[2021-05-15 10:06:00,551] {docker.py:276} INFO - 21/05/15 13:06:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385611046493481613907_0004_m_000016_195
[2021-05-15 10:06:00,552] {docker.py:276} INFO - 21/05/15 13:06:00 INFO Executor: Finished task 16.0 in stage 4.0 (TID 195). 4587 bytes result sent to driver
[2021-05-15 10:06:00,554] {docker.py:276} INFO - 21/05/15 13:06:00 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 201) (4703d70ea67c, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:00,555] {docker.py:276} INFO - 21/05/15 13:06:00 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 195) in 3063 ms on 4703d70ea67c (executor driver) (19/200)
[2021-05-15 10:06:00,556] {docker.py:276} INFO - 21/05/15 13:06:00 INFO Executor: Running task 22.0 in stage 4.0 (TID 201)
[2021-05-15 10:06:00,567] {docker.py:276} INFO - 21/05/15 13:06:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:00,569] {docker.py:276} INFO - 21/05/15 13:06:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:00,570] {docker.py:276} INFO - 21/05/15 13:06:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387182344317243163020_0004_m_000022_201, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387182344317243163020_0004_m_000022_201}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387182344317243163020_0004}; taskId=attempt_202105151305387182344317243163020_0004_m_000022_201, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a932040}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:00,570] {docker.py:276} INFO - 21/05/15 13:06:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305387182344317243163020_0004_m_000022_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387182344317243163020_0004_m_000022_201
[2021-05-15 10:06:00,575] {docker.py:276} INFO - 21/05/15 13:06:00 INFO StagingCommitter: Task committer attempt_202105151305387182344317243163020_0004_m_000022_201: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387182344317243163020_0004_m_000022_201 : duration 0:00.005s
[2021-05-15 10:06:01,105] {docker.py:276} INFO - 21/05/15 13:06:01 INFO StagingCommitter: Starting: Task committer attempt_202105151305382090504324265348684_0004_m_000019_198: needsTaskCommit() Task attempt_202105151305382090504324265348684_0004_m_000019_198
[2021-05-15 10:06:01,105] {docker.py:276} INFO - 21/05/15 13:06:01 INFO StagingCommitter: Task committer attempt_202105151305382090504324265348684_0004_m_000019_198: needsTaskCommit() Task attempt_202105151305382090504324265348684_0004_m_000019_198: duration 0:00.001s
[2021-05-15 10:06:01,106] {docker.py:276} INFO - 21/05/15 13:06:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382090504324265348684_0004_m_000019_198
[2021-05-15 10:06:01,107] {docker.py:276} INFO - 21/05/15 13:06:01 INFO Executor: Finished task 19.0 in stage 4.0 (TID 198). 4544 bytes result sent to driver
[2021-05-15 10:06:01,108] {docker.py:276} INFO - 21/05/15 13:06:01 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 202) (4703d70ea67c, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:01,109] {docker.py:276} INFO - 21/05/15 13:06:01 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 198) in 2493 ms on 4703d70ea67c (executor driver) (20/200)
[2021-05-15 10:06:01,109] {docker.py:276} INFO - 21/05/15 13:06:01 INFO Executor: Running task 23.0 in stage 4.0 (TID 202)
[2021-05-15 10:06:01,117] {docker.py:276} INFO - 21/05/15 13:06:01 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:01,119] {docker.py:276} INFO - 21/05/15 13:06:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:01,120] {docker.py:276} INFO - 21/05/15 13:06:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386737029900835607070_0004_m_000023_202, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386737029900835607070_0004_m_000023_202}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386737029900835607070_0004}; taskId=attempt_202105151305386737029900835607070_0004_m_000023_202, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11531190}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:01,120] {docker.py:276} INFO - 21/05/15 13:06:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:01,120] {docker.py:276} INFO - 21/05/15 13:06:01 INFO StagingCommitter: Starting: Task committer attempt_202105151305386737029900835607070_0004_m_000023_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386737029900835607070_0004_m_000023_202
[2021-05-15 10:06:01,124] {docker.py:276} INFO - 21/05/15 13:06:01 INFO StagingCommitter: Task committer attempt_202105151305386737029900835607070_0004_m_000023_202: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386737029900835607070_0004_m_000023_202 : duration 0:00.004s
[2021-05-15 10:06:02,243] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305385875472889494665540_0004_m_000020_199: needsTaskCommit() Task attempt_202105151305385875472889494665540_0004_m_000020_199
[2021-05-15 10:06:02,243] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305385875472889494665540_0004_m_000020_199: needsTaskCommit() Task attempt_202105151305385875472889494665540_0004_m_000020_199: duration 0:00.001s
[2021-05-15 10:06:02,244] {docker.py:276} INFO - 21/05/15 13:06:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385875472889494665540_0004_m_000020_199
[2021-05-15 10:06:02,244] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Finished task 20.0 in stage 4.0 (TID 199). 4544 bytes result sent to driver
[2021-05-15 10:06:02,246] {docker.py:276} INFO - 21/05/15 13:06:02 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 203) (4703d70ea67c, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:02,247] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Running task 24.0 in stage 4.0 (TID 203)
[2021-05-15 10:06:02,247] {docker.py:276} INFO - 21/05/15 13:06:02 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 199) in 2482 ms on 4703d70ea67c (executor driver) (21/200)
[2021-05-15 10:06:02,261] {docker.py:276} INFO - 21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:02,262] {docker.py:276} INFO - 21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:02,264] {docker.py:276} INFO - 21/05/15 13:06:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:02,264] {docker.py:276} INFO - 21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:02,264] {docker.py:276} INFO - 21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383577467232109973708_0004_m_000024_203, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383577467232109973708_0004_m_000024_203}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383577467232109973708_0004}; taskId=attempt_202105151305383577467232109973708_0004_m_000024_203, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@669f4d9a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:02,265] {docker.py:276} INFO - 21/05/15 13:06:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:02,265] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305383577467232109973708_0004_m_000024_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383577467232109973708_0004_m_000024_203
[2021-05-15 10:06:02,269] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305383577467232109973708_0004_m_000024_203: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383577467232109973708_0004_m_000024_203 : duration 0:00.005s
[2021-05-15 10:06:02,428] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305387182344317243163020_0004_m_000022_201: needsTaskCommit() Task attempt_202105151305387182344317243163020_0004_m_000022_201
[2021-05-15 10:06:02,428] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305387182344317243163020_0004_m_000022_201: needsTaskCommit() Task attempt_202105151305387182344317243163020_0004_m_000022_201: duration 0:00.001s
[2021-05-15 10:06:02,428] {docker.py:276} INFO - 21/05/15 13:06:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387182344317243163020_0004_m_000022_201
[2021-05-15 10:06:02,429] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Finished task 22.0 in stage 4.0 (TID 201). 4544 bytes result sent to driver
[2021-05-15 10:06:02,431] {docker.py:276} INFO - 21/05/15 13:06:02 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 204) (4703d70ea67c, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:02,431] {docker.py:276} INFO - 21/05/15 13:06:02 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 201) in 1880 ms on 4703d70ea67c (executor driver) (22/200)
[2021-05-15 10:06:02,432] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Running task 25.0 in stage 4.0 (TID 204)
[2021-05-15 10:06:02,440] {docker.py:276} INFO - 21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:02,442] {docker.py:276} INFO - 21/05/15 13:06:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:02,443] {docker.py:276} INFO - 21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387105566900699021792_0004_m_000025_204, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387105566900699021792_0004_m_000025_204}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387105566900699021792_0004}; taskId=attempt_202105151305387105566900699021792_0004_m_000025_204, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@351db945}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:02,443] {docker.py:276} INFO - 21/05/15 13:06:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:02,443] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305387105566900699021792_0004_m_000025_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387105566900699021792_0004_m_000025_204
[2021-05-15 10:06:02,447] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305387105566900699021792_0004_m_000025_204: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387105566900699021792_0004_m_000025_204 : duration 0:00.004s
[2021-05-15 10:06:02,735] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305382389085775906377698_0004_m_000021_200: needsTaskCommit() Task attempt_202105151305382389085775906377698_0004_m_000021_200
[2021-05-15 10:06:02,735] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305382389085775906377698_0004_m_000021_200: needsTaskCommit() Task attempt_202105151305382389085775906377698_0004_m_000021_200: duration 0:00.002s
21/05/15 13:06:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382389085775906377698_0004_m_000021_200
[2021-05-15 10:06:02,736] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Finished task 21.0 in stage 4.0 (TID 200). 4544 bytes result sent to driver
[2021-05-15 10:06:02,737] {docker.py:276} INFO - 21/05/15 13:06:02 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 205) (4703d70ea67c, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:02,738] {docker.py:276} INFO - 21/05/15 13:06:02 INFO Executor: Running task 26.0 in stage 4.0 (TID 205)
21/05/15 13:06:02 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 200) in 2376 ms on 4703d70ea67c (executor driver) (23/200)
[2021-05-15 10:06:02,746] {docker.py:276} INFO - 21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:02,749] {docker.py:276} INFO - 21/05/15 13:06:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:02,749] {docker.py:276} INFO - 21/05/15 13:06:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382767288250544448071_0004_m_000026_205, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382767288250544448071_0004_m_000026_205}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382767288250544448071_0004}; taskId=attempt_202105151305382767288250544448071_0004_m_000026_205, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18b3f01e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:02,749] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305382767288250544448071_0004_m_000026_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382767288250544448071_0004_m_000026_205
[2021-05-15 10:06:02,753] {docker.py:276} INFO - 21/05/15 13:06:02 INFO StagingCommitter: Task committer attempt_202105151305382767288250544448071_0004_m_000026_205: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382767288250544448071_0004_m_000026_205 : duration 0:00.004s
[2021-05-15 10:06:03,508] {docker.py:276} INFO - 21/05/15 13:06:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305386737029900835607070_0004_m_000023_202: needsTaskCommit() Task attempt_202105151305386737029900835607070_0004_m_000023_202
[2021-05-15 10:06:03,510] {docker.py:276} INFO - 21/05/15 13:06:03 INFO StagingCommitter: Task committer attempt_202105151305386737029900835607070_0004_m_000023_202: needsTaskCommit() Task attempt_202105151305386737029900835607070_0004_m_000023_202: duration 0:00.004s
21/05/15 13:06:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386737029900835607070_0004_m_000023_202
[2021-05-15 10:06:03,513] {docker.py:276} INFO - 21/05/15 13:06:03 INFO Executor: Finished task 23.0 in stage 4.0 (TID 202). 4544 bytes result sent to driver
[2021-05-15 10:06:03,515] {docker.py:276} INFO - 21/05/15 13:06:03 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 206) (4703d70ea67c, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:03 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 202) in 2410 ms on 4703d70ea67c (executor driver) (24/200)
[2021-05-15 10:06:03,516] {docker.py:276} INFO - 21/05/15 13:06:03 INFO Executor: Running task 27.0 in stage 4.0 (TID 206)
[2021-05-15 10:06:03,525] {docker.py:276} INFO - 21/05/15 13:06:03 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:03,526] {docker.py:276} INFO - 21/05/15 13:06:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:03,528] {docker.py:276} INFO - 21/05/15 13:06:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:03,528] {docker.py:276} INFO - 21/05/15 13:06:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:03,529] {docker.py:276} INFO - 21/05/15 13:06:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:03,529] {docker.py:276} INFO - 21/05/15 13:06:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388855573566673456331_0004_m_000027_206, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388855573566673456331_0004_m_000027_206}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388855573566673456331_0004}; taskId=attempt_202105151305388855573566673456331_0004_m_000027_206, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@cac8668}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305388855573566673456331_0004_m_000027_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388855573566673456331_0004_m_000027_206
[2021-05-15 10:06:03,532] {docker.py:276} INFO - 21/05/15 13:06:03 INFO StagingCommitter: Task committer attempt_202105151305388855573566673456331_0004_m_000027_206: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388855573566673456331_0004_m_000027_206 : duration 0:00.003s
[2021-05-15 10:06:04,982] {docker.py:276} INFO - 21/05/15 13:06:04 INFO StagingCommitter: Starting: Task committer attempt_202105151305387105566900699021792_0004_m_000025_204: needsTaskCommit() Task attempt_202105151305387105566900699021792_0004_m_000025_204
[2021-05-15 10:06:04,983] {docker.py:276} INFO - 21/05/15 13:06:04 INFO StagingCommitter: Task committer attempt_202105151305387105566900699021792_0004_m_000025_204: needsTaskCommit() Task attempt_202105151305387105566900699021792_0004_m_000025_204: duration 0:00.001s
21/05/15 13:06:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387105566900699021792_0004_m_000025_204
[2021-05-15 10:06:04,985] {docker.py:276} INFO - 21/05/15 13:06:04 INFO Executor: Finished task 25.0 in stage 4.0 (TID 204). 4544 bytes result sent to driver
[2021-05-15 10:06:04,986] {docker.py:276} INFO - 21/05/15 13:06:04 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 207) (4703d70ea67c, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:04,987] {docker.py:276} INFO - 21/05/15 13:06:04 INFO Executor: Running task 28.0 in stage 4.0 (TID 207)
21/05/15 13:06:04 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 204) in 2559 ms on 4703d70ea67c (executor driver) (25/200)
[2021-05-15 10:06:05,016] {docker.py:276} INFO - 21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:05,017] {docker.py:276} INFO - 21/05/15 13:06:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386753149150033405037_0004_m_000028_207, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386753149150033405037_0004_m_000028_207}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386753149150033405037_0004}; taskId=attempt_202105151305386753149150033405037_0004_m_000028_207, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f38ad1f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:05,018] {docker.py:276} INFO - 21/05/15 13:06:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:05,018] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305386753149150033405037_0004_m_000028_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386753149150033405037_0004_m_000028_207
[2021-05-15 10:06:05,020] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Task committer attempt_202105151305386753149150033405037_0004_m_000028_207: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386753149150033405037_0004_m_000028_207 : duration 0:00.003s
[2021-05-15 10:06:05,159] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305383577467232109973708_0004_m_000024_203: needsTaskCommit() Task attempt_202105151305383577467232109973708_0004_m_000024_203
[2021-05-15 10:06:05,159] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Task committer attempt_202105151305383577467232109973708_0004_m_000024_203: needsTaskCommit() Task attempt_202105151305383577467232109973708_0004_m_000024_203: duration 0:00.001s
21/05/15 13:06:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383577467232109973708_0004_m_000024_203
[2021-05-15 10:06:05,162] {docker.py:276} INFO - 21/05/15 13:06:05 INFO Executor: Finished task 24.0 in stage 4.0 (TID 203). 4587 bytes result sent to driver
[2021-05-15 10:06:05,163] {docker.py:276} INFO - 21/05/15 13:06:05 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 208) (4703d70ea67c, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:05,164] {docker.py:276} INFO - 21/05/15 13:06:05 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 203) in 2922 ms on 4703d70ea67c (executor driver) (26/200)
21/05/15 13:06:05 INFO Executor: Running task 29.0 in stage 4.0 (TID 208)
[2021-05-15 10:06:05,172] {docker.py:276} INFO - 21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:05,174] {docker.py:276} INFO - 21/05/15 13:06:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387031845547439551761_0004_m_000029_208, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387031845547439551761_0004_m_000029_208}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387031845547439551761_0004}; taskId=attempt_202105151305387031845547439551761_0004_m_000029_208, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52244ea4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:05,174] {docker.py:276} INFO - 21/05/15 13:06:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305387031845547439551761_0004_m_000029_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387031845547439551761_0004_m_000029_208
[2021-05-15 10:06:05,177] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Task committer attempt_202105151305387031845547439551761_0004_m_000029_208: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387031845547439551761_0004_m_000029_208 : duration 0:00.003s
[2021-05-15 10:06:05,287] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305382767288250544448071_0004_m_000026_205: needsTaskCommit() Task attempt_202105151305382767288250544448071_0004_m_000026_205
[2021-05-15 10:06:05,288] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Task committer attempt_202105151305382767288250544448071_0004_m_000026_205: needsTaskCommit() Task attempt_202105151305382767288250544448071_0004_m_000026_205: duration 0:00.002s
21/05/15 13:06:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382767288250544448071_0004_m_000026_205
[2021-05-15 10:06:05,289] {docker.py:276} INFO - 21/05/15 13:06:05 INFO Executor: Finished task 26.0 in stage 4.0 (TID 205). 4587 bytes result sent to driver
[2021-05-15 10:06:05,290] {docker.py:276} INFO - 21/05/15 13:06:05 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 209) (4703d70ea67c, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:05,290] {docker.py:276} INFO - 21/05/15 13:06:05 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 205) in 2556 ms on 4703d70ea67c (executor driver) (27/200)
[2021-05-15 10:06:05,291] {docker.py:276} INFO - 21/05/15 13:06:05 INFO Executor: Running task 30.0 in stage 4.0 (TID 209)
[2021-05-15 10:06:05,299] {docker.py:276} INFO - 21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:05,299] {docker.py:276} INFO - 21/05/15 13:06:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:05,302] {docker.py:276} INFO - 21/05/15 13:06:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:05,303] {docker.py:276} INFO - 21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:05,303] {docker.py:276} INFO - 21/05/15 13:06:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382113681905392150660_0004_m_000030_209, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382113681905392150660_0004_m_000030_209}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382113681905392150660_0004}; taskId=attempt_202105151305382113681905392150660_0004_m_000030_209, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65cc89ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:05,303] {docker.py:276} INFO - 21/05/15 13:06:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:05,303] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305382113681905392150660_0004_m_000030_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382113681905392150660_0004_m_000030_209
[2021-05-15 10:06:05,306] {docker.py:276} INFO - 21/05/15 13:06:05 INFO StagingCommitter: Task committer attempt_202105151305382113681905392150660_0004_m_000030_209: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382113681905392150660_0004_m_000030_209 : duration 0:00.002s
[2021-05-15 10:06:06,262] {docker.py:276} INFO - 21/05/15 13:06:06 INFO StagingCommitter: Starting: Task committer attempt_202105151305388855573566673456331_0004_m_000027_206: needsTaskCommit() Task attempt_202105151305388855573566673456331_0004_m_000027_206
21/05/15 13:06:06 INFO StagingCommitter: Task committer attempt_202105151305388855573566673456331_0004_m_000027_206: needsTaskCommit() Task attempt_202105151305388855573566673456331_0004_m_000027_206: duration 0:00.002s
21/05/15 13:06:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388855573566673456331_0004_m_000027_206
[2021-05-15 10:06:06,264] {docker.py:276} INFO - 21/05/15 13:06:06 INFO Executor: Finished task 27.0 in stage 4.0 (TID 206). 4587 bytes result sent to driver
[2021-05-15 10:06:06,265] {docker.py:276} INFO - 21/05/15 13:06:06 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 210) (4703d70ea67c, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:06,267] {docker.py:276} INFO - 21/05/15 13:06:06 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 206) in 2756 ms on 4703d70ea67c (executor driver) (28/200)
[2021-05-15 10:06:06,268] {docker.py:276} INFO - 21/05/15 13:06:06 INFO Executor: Running task 31.0 in stage 4.0 (TID 210)
[2021-05-15 10:06:06,279] {docker.py:276} INFO - 21/05/15 13:06:06 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:06,281] {docker.py:276} INFO - 21/05/15 13:06:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:06,282] {docker.py:276} INFO - 21/05/15 13:06:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387537191750905732075_0004_m_000031_210, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387537191750905732075_0004_m_000031_210}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387537191750905732075_0004}; taskId=attempt_202105151305387537191750905732075_0004_m_000031_210, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bcab6b3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:06 INFO StagingCommitter: Starting: Task committer attempt_202105151305387537191750905732075_0004_m_000031_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387537191750905732075_0004_m_000031_210
[2021-05-15 10:06:06,284] {docker.py:276} INFO - 21/05/15 13:06:06 INFO StagingCommitter: Task committer attempt_202105151305387537191750905732075_0004_m_000031_210: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387537191750905732075_0004_m_000031_210 : duration 0:00.003s
[2021-05-15 10:06:08,357] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305382113681905392150660_0004_m_000030_209: needsTaskCommit() Task attempt_202105151305382113681905392150660_0004_m_000030_209
[2021-05-15 10:06:08,358] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_202105151305382113681905392150660_0004_m_000030_209: needsTaskCommit() Task attempt_202105151305382113681905392150660_0004_m_000030_209: duration 0:00.003s
21/05/15 13:06:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382113681905392150660_0004_m_000030_209
[2021-05-15 10:06:08,360] {docker.py:276} INFO - 21/05/15 13:06:08 INFO Executor: Finished task 30.0 in stage 4.0 (TID 209). 4544 bytes result sent to driver
[2021-05-15 10:06:08,360] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 211) (4703d70ea67c, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:08,361] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 209) in 3075 ms on 4703d70ea67c (executor driver) (29/200)
[2021-05-15 10:06:08,362] {docker.py:276} INFO - 21/05/15 13:06:08 INFO Executor: Running task 32.0 in stage 4.0 (TID 211)
[2021-05-15 10:06:08,371] {docker.py:276} INFO - 21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:08,371] {docker.py:276} INFO - 21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:08,374] {docker.py:276} INFO - 21/05/15 13:06:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:08,374] {docker.py:276} INFO - 21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:08,375] {docker.py:276} INFO - 21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381363109454440639191_0004_m_000032_211, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381363109454440639191_0004_m_000032_211}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381363109454440639191_0004}; taskId=attempt_202105151305381363109454440639191_0004_m_000032_211, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bd223f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:08,375] {docker.py:276} INFO - 21/05/15 13:06:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:08,375] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305381363109454440639191_0004_m_000032_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381363109454440639191_0004_m_000032_211
[2021-05-15 10:06:08,379] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_202105151305381363109454440639191_0004_m_000032_211: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381363109454440639191_0004_m_000032_211 : duration 0:00.003s
[2021-05-15 10:06:08,580] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305386753149150033405037_0004_m_000028_207: needsTaskCommit() Task attempt_202105151305386753149150033405037_0004_m_000028_207
[2021-05-15 10:06:08,582] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305387031845547439551761_0004_m_000029_208: needsTaskCommit() Task attempt_202105151305387031845547439551761_0004_m_000029_208
21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_202105151305386753149150033405037_0004_m_000028_207: needsTaskCommit() Task attempt_202105151305386753149150033405037_0004_m_000028_207: duration 0:00.003s
21/05/15 13:06:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386753149150033405037_0004_m_000028_207
[2021-05-15 10:06:08,582] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_202105151305387031845547439551761_0004_m_000029_208: needsTaskCommit() Task attempt_202105151305387031845547439551761_0004_m_000029_208: duration 0:00.004s
21/05/15 13:06:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387031845547439551761_0004_m_000029_208
[2021-05-15 10:06:08,583] {docker.py:276} INFO - 21/05/15 13:06:08 INFO Executor: Finished task 28.0 in stage 4.0 (TID 207). 4587 bytes result sent to driver
[2021-05-15 10:06:08,584] {docker.py:276} INFO - 21/05/15 13:06:08 INFO Executor: Finished task 29.0 in stage 4.0 (TID 208). 4544 bytes result sent to driver
[2021-05-15 10:06:08,585] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 212) (4703d70ea67c, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:08,587] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 213) (4703d70ea67c, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:08,588] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 208) in 3429 ms on 4703d70ea67c (executor driver) (30/200)
21/05/15 13:06:08 INFO Executor: Running task 33.0 in stage 4.0 (TID 212)
[2021-05-15 10:06:08,589] {docker.py:276} INFO - 21/05/15 13:06:08 INFO Executor: Running task 34.0 in stage 4.0 (TID 213)
[2021-05-15 10:06:08,590] {docker.py:276} INFO - 21/05/15 13:06:08 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 207) in 3607 ms on 4703d70ea67c (executor driver) (31/200)
[2021-05-15 10:06:08,598] {docker.py:276} INFO - 21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:08,599] {docker.py:276} INFO - 21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:08,600] {docker.py:276} INFO - 21/05/15 13:06:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:08,601] {docker.py:276} INFO - 21/05/15 13:06:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:08,601] {docker.py:276} INFO - 21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386879622765251099823_0004_m_000033_212, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386879622765251099823_0004_m_000033_212}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386879622765251099823_0004}; taskId=attempt_202105151305386879622765251099823_0004_m_000033_212, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65e9d914}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305386879622765251099823_0004_m_000033_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386879622765251099823_0004_m_000033_212
[2021-05-15 10:06:08,603] {docker.py:276} INFO - 21/05/15 13:06:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:08,604] {docker.py:276} INFO - 21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:08,604] {docker.py:276} INFO - 21/05/15 13:06:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538827274869654709085_0004_m_000034_213, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538827274869654709085_0004_m_000034_213}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538827274869654709085_0004}; taskId=attempt_20210515130538827274869654709085_0004_m_000034_213, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1118f321}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_202105151305386879622765251099823_0004_m_000033_212: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386879622765251099823_0004_m_000033_212 : duration 0:00.005s
[2021-05-15 10:06:08,607] {docker.py:276} INFO - 21/05/15 13:06:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:08 INFO StagingCommitter: Starting: Task committer attempt_20210515130538827274869654709085_0004_m_000034_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538827274869654709085_0004_m_000034_213
[2021-05-15 10:06:08,610] {docker.py:276} INFO - 21/05/15 13:06:08 INFO StagingCommitter: Task committer attempt_20210515130538827274869654709085_0004_m_000034_213: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538827274869654709085_0004_m_000034_213 : duration 0:00.003s
[2021-05-15 10:06:10,033] {docker.py:276} INFO - 21/05/15 13:06:10 INFO StagingCommitter: Starting: Task committer attempt_202105151305387537191750905732075_0004_m_000031_210: needsTaskCommit() Task attempt_202105151305387537191750905732075_0004_m_000031_210
[2021-05-15 10:06:10,035] {docker.py:276} INFO - 21/05/15 13:06:10 INFO StagingCommitter: Task committer attempt_202105151305387537191750905732075_0004_m_000031_210: needsTaskCommit() Task attempt_202105151305387537191750905732075_0004_m_000031_210: duration 0:00.001s
21/05/15 13:06:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387537191750905732075_0004_m_000031_210
[2021-05-15 10:06:10,037] {docker.py:276} INFO - 21/05/15 13:06:10 INFO Executor: Finished task 31.0 in stage 4.0 (TID 210). 4544 bytes result sent to driver
[2021-05-15 10:06:10,039] {docker.py:276} INFO - 21/05/15 13:06:10 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 214) (4703d70ea67c, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:10,039] {docker.py:276} INFO - 21/05/15 13:06:10 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 210) in 3779 ms on 4703d70ea67c (executor driver) (32/200)
[2021-05-15 10:06:10,040] {docker.py:276} INFO - 21/05/15 13:06:10 INFO Executor: Running task 35.0 in stage 4.0 (TID 214)
[2021-05-15 10:06:10,051] {docker.py:276} INFO - 21/05/15 13:06:10 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:10,054] {docker.py:276} INFO - 21/05/15 13:06:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:10,054] {docker.py:276} INFO - 21/05/15 13:06:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386934375162672283783_0004_m_000035_214, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386934375162672283783_0004_m_000035_214}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386934375162672283783_0004}; taskId=attempt_202105151305386934375162672283783_0004_m_000035_214, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63e9f877}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:10,054] {docker.py:276} INFO - 21/05/15 13:06:10 INFO StagingCommitter: Starting: Task committer attempt_202105151305386934375162672283783_0004_m_000035_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386934375162672283783_0004_m_000035_214
[2021-05-15 10:06:10,057] {docker.py:276} INFO - 21/05/15 13:06:10 INFO StagingCommitter: Task committer attempt_202105151305386934375162672283783_0004_m_000035_214: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386934375162672283783_0004_m_000035_214 : duration 0:00.003s
[2021-05-15 10:06:11,436] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305381363109454440639191_0004_m_000032_211: needsTaskCommit() Task attempt_202105151305381363109454440639191_0004_m_000032_211
[2021-05-15 10:06:11,437] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Task committer attempt_202105151305381363109454440639191_0004_m_000032_211: needsTaskCommit() Task attempt_202105151305381363109454440639191_0004_m_000032_211: duration 0:00.001s
21/05/15 13:06:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381363109454440639191_0004_m_000032_211
[2021-05-15 10:06:11,439] {docker.py:276} INFO - 21/05/15 13:06:11 INFO Executor: Finished task 32.0 in stage 4.0 (TID 211). 4544 bytes result sent to driver
[2021-05-15 10:06:11,440] {docker.py:276} INFO - 21/05/15 13:06:11 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 215) (4703d70ea67c, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:11,442] {docker.py:276} INFO - 21/05/15 13:06:11 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 211) in 3085 ms on 4703d70ea67c (executor driver) (33/200)
[2021-05-15 10:06:11,442] {docker.py:276} INFO - 21/05/15 13:06:11 INFO Executor: Running task 36.0 in stage 4.0 (TID 215)
[2021-05-15 10:06:11,452] {docker.py:276} INFO - 21/05/15 13:06:11 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:11,452] {docker.py:276} INFO - 21/05/15 13:06:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:11,454] {docker.py:276} INFO - 21/05/15 13:06:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:11,454] {docker.py:276} INFO - 21/05/15 13:06:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:11,455] {docker.py:276} INFO - 21/05/15 13:06:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381182142819725762206_0004_m_000036_215, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381182142819725762206_0004_m_000036_215}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381182142819725762206_0004}; taskId=attempt_202105151305381182142819725762206_0004_m_000036_215, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d4ac2ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:11,455] {docker.py:276} INFO - 21/05/15 13:06:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:11,455] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305381182142819725762206_0004_m_000036_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381182142819725762206_0004_m_000036_215
[2021-05-15 10:06:11,458] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Task committer attempt_202105151305381182142819725762206_0004_m_000036_215: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381182142819725762206_0004_m_000036_215 : duration 0:00.003s
[2021-05-15 10:06:11,969] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Starting: Task committer attempt_20210515130538827274869654709085_0004_m_000034_213: needsTaskCommit() Task attempt_20210515130538827274869654709085_0004_m_000034_213
[2021-05-15 10:06:11,970] {docker.py:276} INFO - 21/05/15 13:06:11 INFO StagingCommitter: Task committer attempt_20210515130538827274869654709085_0004_m_000034_213: needsTaskCommit() Task attempt_20210515130538827274869654709085_0004_m_000034_213: duration 0:00.001s
[2021-05-15 10:06:11,971] {docker.py:276} INFO - 21/05/15 13:06:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538827274869654709085_0004_m_000034_213
[2021-05-15 10:06:11,972] {docker.py:276} INFO - 21/05/15 13:06:11 INFO Executor: Finished task 34.0 in stage 4.0 (TID 213). 4544 bytes result sent to driver
[2021-05-15 10:06:11,975] {docker.py:276} INFO - 21/05/15 13:06:11 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 216) (4703d70ea67c, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:11,976] {docker.py:276} INFO - 21/05/15 13:06:11 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 213) in 3392 ms on 4703d70ea67c (executor driver) (34/200)
[2021-05-15 10:06:11,977] {docker.py:276} INFO - 21/05/15 13:06:11 INFO Executor: Running task 37.0 in stage 4.0 (TID 216)
[2021-05-15 10:06:11,985] {docker.py:276} INFO - 21/05/15 13:06:12 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:11,987] {docker.py:276} INFO - 21/05/15 13:06:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:11,987] {docker.py:276} INFO - 21/05/15 13:06:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385819807477590369824_0004_m_000037_216, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385819807477590369824_0004_m_000037_216}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385819807477590369824_0004}; taskId=attempt_202105151305385819807477590369824_0004_m_000037_216, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@467e3973}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:11,987] {docker.py:276} INFO - 21/05/15 13:06:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305385819807477590369824_0004_m_000037_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385819807477590369824_0004_m_000037_216
[2021-05-15 10:06:11,990] {docker.py:276} INFO - 21/05/15 13:06:12 INFO StagingCommitter: Task committer attempt_202105151305385819807477590369824_0004_m_000037_216: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385819807477590369824_0004_m_000037_216 : duration 0:00.003s
[2021-05-15 10:06:12,657] {docker.py:276} INFO - 21/05/15 13:06:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305386879622765251099823_0004_m_000033_212: needsTaskCommit() Task attempt_202105151305386879622765251099823_0004_m_000033_212
[2021-05-15 10:06:12,659] {docker.py:276} INFO - 21/05/15 13:06:12 INFO StagingCommitter: Task committer attempt_202105151305386879622765251099823_0004_m_000033_212: needsTaskCommit() Task attempt_202105151305386879622765251099823_0004_m_000033_212: duration 0:00.002s
21/05/15 13:06:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386879622765251099823_0004_m_000033_212
[2021-05-15 10:06:12,661] {docker.py:276} INFO - 21/05/15 13:06:12 INFO Executor: Finished task 33.0 in stage 4.0 (TID 212). 4544 bytes result sent to driver
[2021-05-15 10:06:12,662] {docker.py:276} INFO - 21/05/15 13:06:12 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 217) (4703d70ea67c, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:12,663] {docker.py:276} INFO - 21/05/15 13:06:12 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 212) in 4083 ms on 4703d70ea67c (executor driver) (35/200)
[2021-05-15 10:06:12,665] {docker.py:276} INFO - 21/05/15 13:06:12 INFO Executor: Running task 38.0 in stage 4.0 (TID 217)
[2021-05-15 10:06:12,674] {docker.py:276} INFO - 21/05/15 13:06:12 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:12,676] {docker.py:276} INFO - 21/05/15 13:06:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:12,677] {docker.py:276} INFO - 21/05/15 13:06:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:12,677] {docker.py:276} INFO - 21/05/15 13:06:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305389168027966677570027_0004_m_000038_217, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389168027966677570027_0004_m_000038_217}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305389168027966677570027_0004}; taskId=attempt_202105151305389168027966677570027_0004_m_000038_217, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c048d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:12,677] {docker.py:276} INFO - 21/05/15 13:06:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305389168027966677570027_0004_m_000038_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389168027966677570027_0004_m_000038_217
[2021-05-15 10:06:12,680] {docker.py:276} INFO - 21/05/15 13:06:12 INFO StagingCommitter: Task committer attempt_202105151305389168027966677570027_0004_m_000038_217: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389168027966677570027_0004_m_000038_217 : duration 0:00.003s
[2021-05-15 10:06:13,176] {docker.py:276} INFO - 21/05/15 13:06:13 INFO StagingCommitter: Starting: Task committer attempt_202105151305386934375162672283783_0004_m_000035_214: needsTaskCommit() Task attempt_202105151305386934375162672283783_0004_m_000035_214
[2021-05-15 10:06:13,176] {docker.py:276} INFO - 21/05/15 13:06:13 INFO StagingCommitter: Task committer attempt_202105151305386934375162672283783_0004_m_000035_214: needsTaskCommit() Task attempt_202105151305386934375162672283783_0004_m_000035_214: duration 0:00.000s
21/05/15 13:06:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386934375162672283783_0004_m_000035_214
[2021-05-15 10:06:13,178] {docker.py:276} INFO - 21/05/15 13:06:13 INFO Executor: Finished task 35.0 in stage 4.0 (TID 214). 4544 bytes result sent to driver
[2021-05-15 10:06:13,179] {docker.py:276} INFO - 21/05/15 13:06:13 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 218) (4703d70ea67c, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:13,180] {docker.py:276} INFO - 21/05/15 13:06:13 INFO Executor: Running task 39.0 in stage 4.0 (TID 218)
21/05/15 13:06:13 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 214) in 3145 ms on 4703d70ea67c (executor driver) (36/200)
[2021-05-15 10:06:13,188] {docker.py:276} INFO - 21/05/15 13:06:13 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:13,190] {docker.py:276} INFO - 21/05/15 13:06:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383837013625228291099_0004_m_000039_218, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383837013625228291099_0004_m_000039_218}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383837013625228291099_0004}; taskId=attempt_202105151305383837013625228291099_0004_m_000039_218, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5abfd8d0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:13,190] {docker.py:276} INFO - 21/05/15 13:06:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:13 INFO StagingCommitter: Starting: Task committer attempt_202105151305383837013625228291099_0004_m_000039_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383837013625228291099_0004_m_000039_218
[2021-05-15 10:06:13,194] {docker.py:276} INFO - 21/05/15 13:06:13 INFO StagingCommitter: Task committer attempt_202105151305383837013625228291099_0004_m_000039_218: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383837013625228291099_0004_m_000039_218 : duration 0:00.003s
[2021-05-15 10:06:14,245] {docker.py:276} INFO - 21/05/15 13:06:14 INFO StagingCommitter: Starting: Task committer attempt_202105151305385819807477590369824_0004_m_000037_216: needsTaskCommit() Task attempt_202105151305385819807477590369824_0004_m_000037_216
[2021-05-15 10:06:14,246] {docker.py:276} INFO - 21/05/15 13:06:14 INFO StagingCommitter: Task committer attempt_202105151305385819807477590369824_0004_m_000037_216: needsTaskCommit() Task attempt_202105151305385819807477590369824_0004_m_000037_216: duration 0:00.000s
[2021-05-15 10:06:14,246] {docker.py:276} INFO - 21/05/15 13:06:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385819807477590369824_0004_m_000037_216
[2021-05-15 10:06:14,247] {docker.py:276} INFO - 21/05/15 13:06:14 INFO Executor: Finished task 37.0 in stage 4.0 (TID 216). 4544 bytes result sent to driver
[2021-05-15 10:06:14,248] {docker.py:276} INFO - 21/05/15 13:06:14 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 219) (4703d70ea67c, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:14,249] {docker.py:276} INFO - 21/05/15 13:06:14 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 216) in 2280 ms on 4703d70ea67c (executor driver) (37/200)
[2021-05-15 10:06:14,250] {docker.py:276} INFO - 21/05/15 13:06:14 INFO Executor: Running task 40.0 in stage 4.0 (TID 219)
[2021-05-15 10:06:14,258] {docker.py:276} INFO - 21/05/15 13:06:14 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:14,260] {docker.py:276} INFO - 21/05/15 13:06:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:14,260] {docker.py:276} INFO - 21/05/15 13:06:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387924076693927919273_0004_m_000040_219, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387924076693927919273_0004_m_000040_219}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387924076693927919273_0004}; taskId=attempt_202105151305387924076693927919273_0004_m_000040_219, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51ed1a52}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:14 INFO StagingCommitter: Starting: Task committer attempt_202105151305387924076693927919273_0004_m_000040_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387924076693927919273_0004_m_000040_219
[2021-05-15 10:06:14,263] {docker.py:276} INFO - 21/05/15 13:06:14 INFO StagingCommitter: Task committer attempt_202105151305387924076693927919273_0004_m_000040_219: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387924076693927919273_0004_m_000040_219 : duration 0:00.003s
[2021-05-15 10:06:15,498] {docker.py:276} INFO - 21/05/15 13:06:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305389168027966677570027_0004_m_000038_217: needsTaskCommit() Task attempt_202105151305389168027966677570027_0004_m_000038_217
[2021-05-15 10:06:15,499] {docker.py:276} INFO - 21/05/15 13:06:15 INFO StagingCommitter: Task committer attempt_202105151305389168027966677570027_0004_m_000038_217: needsTaskCommit() Task attempt_202105151305389168027966677570027_0004_m_000038_217: duration 0:00.000s
[2021-05-15 10:06:15,500] {docker.py:276} INFO - 21/05/15 13:06:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305389168027966677570027_0004_m_000038_217
[2021-05-15 10:06:15,502] {docker.py:276} INFO - 21/05/15 13:06:15 INFO Executor: Finished task 38.0 in stage 4.0 (TID 217). 4544 bytes result sent to driver
[2021-05-15 10:06:15,504] {docker.py:276} INFO - 21/05/15 13:06:15 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 220) (4703d70ea67c, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:15,505] {docker.py:276} INFO - 21/05/15 13:06:15 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 217) in 2847 ms on 4703d70ea67c (executor driver) (38/200)
[2021-05-15 10:06:15,506] {docker.py:276} INFO - 21/05/15 13:06:15 INFO Executor: Running task 41.0 in stage 4.0 (TID 220)
[2021-05-15 10:06:15,515] {docker.py:276} INFO - 21/05/15 13:06:15 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:15,516] {docker.py:276} INFO - 21/05/15 13:06:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:15,518] {docker.py:276} INFO - 21/05/15 13:06:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:15,518] {docker.py:276} INFO - 21/05/15 13:06:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:15,519] {docker.py:276} INFO - 21/05/15 13:06:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:15,519] {docker.py:276} INFO - 21/05/15 13:06:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383120419584062194147_0004_m_000041_220, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383120419584062194147_0004_m_000041_220}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383120419584062194147_0004}; taskId=attempt_202105151305383120419584062194147_0004_m_000041_220, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e208ed2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:15,519] {docker.py:276} INFO - 21/05/15 13:06:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:15,520] {docker.py:276} INFO - 21/05/15 13:06:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305383120419584062194147_0004_m_000041_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383120419584062194147_0004_m_000041_220
[2021-05-15 10:06:15,523] {docker.py:276} INFO - 21/05/15 13:06:15 INFO StagingCommitter: Task committer attempt_202105151305383120419584062194147_0004_m_000041_220: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383120419584062194147_0004_m_000041_220 : duration 0:00.004s
[2021-05-15 10:06:16,153] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305381182142819725762206_0004_m_000036_215: needsTaskCommit() Task attempt_202105151305381182142819725762206_0004_m_000036_215
[2021-05-15 10:06:16,155] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_202105151305381182142819725762206_0004_m_000036_215: needsTaskCommit() Task attempt_202105151305381182142819725762206_0004_m_000036_215: duration 0:00.002s
21/05/15 13:06:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381182142819725762206_0004_m_000036_215
[2021-05-15 10:06:16,158] {docker.py:276} INFO - 21/05/15 13:06:16 INFO Executor: Finished task 36.0 in stage 4.0 (TID 215). 4544 bytes result sent to driver
[2021-05-15 10:06:16,161] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 221) (4703d70ea67c, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:16,162] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 215) in 4727 ms on 4703d70ea67c (executor driver) (39/200)
[2021-05-15 10:06:16,163] {docker.py:276} INFO - 21/05/15 13:06:16 INFO Executor: Running task 42.0 in stage 4.0 (TID 221)
[2021-05-15 10:06:16,186] {docker.py:276} INFO - 21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:16,188] {docker.py:276} INFO - 21/05/15 13:06:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:16,189] {docker.py:276} INFO - 21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386895282493073984566_0004_m_000042_221, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386895282493073984566_0004_m_000042_221}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386895282493073984566_0004}; taskId=attempt_202105151305386895282493073984566_0004_m_000042_221, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74fb2dc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305386895282493073984566_0004_m_000042_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386895282493073984566_0004_m_000042_221
[2021-05-15 10:06:16,191] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_202105151305386895282493073984566_0004_m_000042_221: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386895282493073984566_0004_m_000042_221 : duration 0:00.002s
[2021-05-15 10:06:16,330] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305383837013625228291099_0004_m_000039_218: needsTaskCommit() Task attempt_202105151305383837013625228291099_0004_m_000039_218
[2021-05-15 10:06:16,331] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_202105151305383837013625228291099_0004_m_000039_218: needsTaskCommit() Task attempt_202105151305383837013625228291099_0004_m_000039_218: duration 0:00.002s
[2021-05-15 10:06:16,332] {docker.py:276} INFO - 21/05/15 13:06:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383837013625228291099_0004_m_000039_218
[2021-05-15 10:06:16,334] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305387924076693927919273_0004_m_000040_219: needsTaskCommit() Task attempt_202105151305387924076693927919273_0004_m_000040_219
21/05/15 13:06:16 INFO Executor: Finished task 39.0 in stage 4.0 (TID 218). 4587 bytes result sent to driver
[2021-05-15 10:06:16,335] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 222) (4703d70ea67c, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:16,336] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 218) in 3161 ms on 4703d70ea67c (executor driver) (40/200)
[2021-05-15 10:06:16,337] {docker.py:276} INFO - 21/05/15 13:06:16 INFO Executor: Running task 43.0 in stage 4.0 (TID 222)
[2021-05-15 10:06:16,338] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_202105151305387924076693927919273_0004_m_000040_219: needsTaskCommit() Task attempt_202105151305387924076693927919273_0004_m_000040_219: duration 0:00.004s
[2021-05-15 10:06:16,339] {docker.py:276} INFO - 21/05/15 13:06:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387924076693927919273_0004_m_000040_219
[2021-05-15 10:06:16,339] {docker.py:276} INFO - 21/05/15 13:06:16 INFO Executor: Finished task 40.0 in stage 4.0 (TID 219). 4587 bytes result sent to driver
[2021-05-15 10:06:16,340] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 223) (4703d70ea67c, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:16,341] {docker.py:276} INFO - 21/05/15 13:06:16 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 219) in 2095 ms on 4703d70ea67c (executor driver) (41/200)
[2021-05-15 10:06:16,343] {docker.py:276} INFO - 21/05/15 13:06:16 INFO Executor: Running task 44.0 in stage 4.0 (TID 223)
[2021-05-15 10:06:16,349] {docker.py:276} INFO - 21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:16,351] {docker.py:276} INFO - 21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Getting 4 (11.6 KiB) non-empty blocks including 4 (11.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:16,351] {docker.py:276} INFO - 21/05/15 13:06:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:16,352] {docker.py:276} INFO - 21/05/15 13:06:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:16,352] {docker.py:276} INFO - 21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538548772656484602561_0004_m_000043_222, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538548772656484602561_0004_m_000043_222}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538548772656484602561_0004}; taskId=attempt_20210515130538548772656484602561_0004_m_000043_222, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@454103a1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:16,352] {docker.py:276} INFO - 21/05/15 13:06:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:16,353] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_20210515130538548772656484602561_0004_m_000043_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538548772656484602561_0004_m_000043_222
[2021-05-15 10:06:16,353] {docker.py:276} INFO - 21/05/15 13:06:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:16,354] {docker.py:276} INFO - 21/05/15 13:06:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382430445581029640715_0004_m_000044_223, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382430445581029640715_0004_m_000044_223}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382430445581029640715_0004}; taskId=attempt_202105151305382430445581029640715_0004_m_000044_223, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d5faa4c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305382430445581029640715_0004_m_000044_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382430445581029640715_0004_m_000044_223
[2021-05-15 10:06:16,359] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_20210515130538548772656484602561_0004_m_000043_222: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538548772656484602561_0004_m_000043_222 : duration 0:00.007s
[2021-05-15 10:06:16,359] {docker.py:276} INFO - 21/05/15 13:06:16 INFO StagingCommitter: Task committer attempt_202105151305382430445581029640715_0004_m_000044_223: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382430445581029640715_0004_m_000044_223 : duration 0:00.005s
[2021-05-15 10:06:18,510] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305383120419584062194147_0004_m_000041_220: needsTaskCommit() Task attempt_202105151305383120419584062194147_0004_m_000041_220
[2021-05-15 10:06:18,511] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Task committer attempt_202105151305383120419584062194147_0004_m_000041_220: needsTaskCommit() Task attempt_202105151305383120419584062194147_0004_m_000041_220: duration 0:00.001s
[2021-05-15 10:06:18,512] {docker.py:276} INFO - 21/05/15 13:06:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383120419584062194147_0004_m_000041_220
[2021-05-15 10:06:18,513] {docker.py:276} INFO - 21/05/15 13:06:18 INFO Executor: Finished task 41.0 in stage 4.0 (TID 220). 4587 bytes result sent to driver
[2021-05-15 10:06:18,515] {docker.py:276} INFO - 21/05/15 13:06:18 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 224) (4703d70ea67c, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:18,516] {docker.py:276} INFO - 21/05/15 13:06:18 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 220) in 3016 ms on 4703d70ea67c (executor driver) (42/200)
[2021-05-15 10:06:18,517] {docker.py:276} INFO - 21/05/15 13:06:18 INFO Executor: Running task 45.0 in stage 4.0 (TID 224)
[2021-05-15 10:06:18,527] {docker.py:276} INFO - 21/05/15 13:06:18 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:18,527] {docker.py:276} INFO - 21/05/15 13:06:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:18,529] {docker.py:276} INFO - 21/05/15 13:06:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:18,530] {docker.py:276} INFO - 21/05/15 13:06:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385851266574334871326_0004_m_000045_224, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385851266574334871326_0004_m_000045_224}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385851266574334871326_0004}; taskId=attempt_202105151305385851266574334871326_0004_m_000045_224, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b5751b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:18,530] {docker.py:276} INFO - 21/05/15 13:06:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305385851266574334871326_0004_m_000045_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385851266574334871326_0004_m_000045_224
[2021-05-15 10:06:18,533] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Task committer attempt_202105151305385851266574334871326_0004_m_000045_224: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385851266574334871326_0004_m_000045_224 : duration 0:00.004s
[2021-05-15 10:06:18,742] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305382430445581029640715_0004_m_000044_223: needsTaskCommit() Task attempt_202105151305382430445581029640715_0004_m_000044_223
[2021-05-15 10:06:18,743] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Task committer attempt_202105151305382430445581029640715_0004_m_000044_223: needsTaskCommit() Task attempt_202105151305382430445581029640715_0004_m_000044_223: duration 0:00.001s
21/05/15 13:06:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382430445581029640715_0004_m_000044_223
[2021-05-15 10:06:18,744] {docker.py:276} INFO - 21/05/15 13:06:18 INFO Executor: Finished task 44.0 in stage 4.0 (TID 223). 4544 bytes result sent to driver
[2021-05-15 10:06:18,746] {docker.py:276} INFO - 21/05/15 13:06:18 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 225) (4703d70ea67c, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:18,748] {docker.py:276} INFO - 21/05/15 13:06:18 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 223) in 2410 ms on 4703d70ea67c (executor driver) (43/200)
[2021-05-15 10:06:18,749] {docker.py:276} INFO - 21/05/15 13:06:18 INFO Executor: Running task 46.0 in stage 4.0 (TID 225)
[2021-05-15 10:06:18,758] {docker.py:276} INFO - 21/05/15 13:06:18 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:18,761] {docker.py:276} INFO - 21/05/15 13:06:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386765624801749704787_0004_m_000046_225, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386765624801749704787_0004_m_000046_225}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386765624801749704787_0004}; taskId=attempt_202105151305386765624801749704787_0004_m_000046_225, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e6994cc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305386765624801749704787_0004_m_000046_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386765624801749704787_0004_m_000046_225
[2021-05-15 10:06:18,764] {docker.py:276} INFO - 21/05/15 13:06:18 INFO StagingCommitter: Task committer attempt_202105151305386765624801749704787_0004_m_000046_225: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386765624801749704787_0004_m_000046_225 : duration 0:00.003s
[2021-05-15 10:06:18,997] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Starting: Task committer attempt_20210515130538548772656484602561_0004_m_000043_222: needsTaskCommit() Task attempt_20210515130538548772656484602561_0004_m_000043_222
[2021-05-15 10:06:18,998] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Task committer attempt_20210515130538548772656484602561_0004_m_000043_222: needsTaskCommit() Task attempt_20210515130538548772656484602561_0004_m_000043_222: duration 0:00.001s
[2021-05-15 10:06:18,998] {docker.py:276} INFO - 21/05/15 13:06:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538548772656484602561_0004_m_000043_222
[2021-05-15 10:06:19,000] {docker.py:276} INFO - 21/05/15 13:06:19 INFO Executor: Finished task 43.0 in stage 4.0 (TID 222). 4544 bytes result sent to driver
[2021-05-15 10:06:19,002] {docker.py:276} INFO - 21/05/15 13:06:19 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 226) (4703d70ea67c, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:19,003] {docker.py:276} INFO - 21/05/15 13:06:19 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 222) in 2670 ms on 4703d70ea67c (executor driver) (44/200)
[2021-05-15 10:06:19,004] {docker.py:276} INFO - 21/05/15 13:06:19 INFO Executor: Running task 47.0 in stage 4.0 (TID 226)
[2021-05-15 10:06:19,013] {docker.py:276} INFO - 21/05/15 13:06:19 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:19,015] {docker.py:276} INFO - 21/05/15 13:06:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:19,016] {docker.py:276} INFO - 21/05/15 13:06:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388305002864947518673_0004_m_000047_226, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388305002864947518673_0004_m_000047_226}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388305002864947518673_0004}; taskId=attempt_202105151305388305002864947518673_0004_m_000047_226, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38b277d2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:19,016] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305388305002864947518673_0004_m_000047_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388305002864947518673_0004_m_000047_226
[2021-05-15 10:06:19,019] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Task committer attempt_202105151305388305002864947518673_0004_m_000047_226: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388305002864947518673_0004_m_000047_226 : duration 0:00.004s
[2021-05-15 10:06:19,058] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305386895282493073984566_0004_m_000042_221: needsTaskCommit() Task attempt_202105151305386895282493073984566_0004_m_000042_221
[2021-05-15 10:06:19,059] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Task committer attempt_202105151305386895282493073984566_0004_m_000042_221: needsTaskCommit() Task attempt_202105151305386895282493073984566_0004_m_000042_221: duration 0:00.001s
21/05/15 13:06:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386895282493073984566_0004_m_000042_221
[2021-05-15 10:06:19,060] {docker.py:276} INFO - 21/05/15 13:06:19 INFO Executor: Finished task 42.0 in stage 4.0 (TID 221). 4587 bytes result sent to driver
[2021-05-15 10:06:19,061] {docker.py:276} INFO - 21/05/15 13:06:19 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 227) (4703d70ea67c, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:19,062] {docker.py:276} INFO - 21/05/15 13:06:19 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 221) in 2906 ms on 4703d70ea67c (executor driver) (45/200)
[2021-05-15 10:06:19,064] {docker.py:276} INFO - 21/05/15 13:06:19 INFO Executor: Running task 48.0 in stage 4.0 (TID 227)
[2021-05-15 10:06:19,072] {docker.py:276} INFO - 21/05/15 13:06:19 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:19,074] {docker.py:276} INFO - 21/05/15 13:06:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384018189171077938746_0004_m_000048_227, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384018189171077938746_0004_m_000048_227}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384018189171077938746_0004}; taskId=attempt_202105151305384018189171077938746_0004_m_000048_227, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53cf9064}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305384018189171077938746_0004_m_000048_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384018189171077938746_0004_m_000048_227
[2021-05-15 10:06:19,077] {docker.py:276} INFO - 21/05/15 13:06:19 INFO StagingCommitter: Task committer attempt_202105151305384018189171077938746_0004_m_000048_227: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384018189171077938746_0004_m_000048_227 : duration 0:00.002s
[2021-05-15 10:06:20,996] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305385851266574334871326_0004_m_000045_224: needsTaskCommit() Task attempt_202105151305385851266574334871326_0004_m_000045_224
[2021-05-15 10:06:20,997] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305385851266574334871326_0004_m_000045_224: needsTaskCommit() Task attempt_202105151305385851266574334871326_0004_m_000045_224: duration 0:00.001s
[2021-05-15 10:06:20,997] {docker.py:276} INFO - 21/05/15 13:06:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385851266574334871326_0004_m_000045_224
[2021-05-15 10:06:20,999] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Finished task 45.0 in stage 4.0 (TID 224). 4544 bytes result sent to driver
[2021-05-15 10:06:21,000] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 228) (4703d70ea67c, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:21,002] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 224) in 2490 ms on 4703d70ea67c (executor driver) (46/200)
21/05/15 13:06:21 INFO Executor: Running task 49.0 in stage 4.0 (TID 228)
[2021-05-15 10:06:21,012] {docker.py:276} INFO - 21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:21,014] {docker.py:276} INFO - 21/05/15 13:06:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:21,014] {docker.py:276} INFO - 21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384146487165464635064_0004_m_000049_228, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384146487165464635064_0004_m_000049_228}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384146487165464635064_0004}; taskId=attempt_202105151305384146487165464635064_0004_m_000049_228, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd82a21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:21,015] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305384146487165464635064_0004_m_000049_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384146487165464635064_0004_m_000049_228
[2021-05-15 10:06:21,018] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305384146487165464635064_0004_m_000049_228: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384146487165464635064_0004_m_000049_228 : duration 0:00.004s
[2021-05-15 10:06:21,282] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305386765624801749704787_0004_m_000046_225: needsTaskCommit() Task attempt_202105151305386765624801749704787_0004_m_000046_225
[2021-05-15 10:06:21,283] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305386765624801749704787_0004_m_000046_225: needsTaskCommit() Task attempt_202105151305386765624801749704787_0004_m_000046_225: duration 0:00.000s
21/05/15 13:06:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386765624801749704787_0004_m_000046_225
[2021-05-15 10:06:21,285] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Finished task 46.0 in stage 4.0 (TID 225). 4544 bytes result sent to driver
[2021-05-15 10:06:21,288] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 229) (4703d70ea67c, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:21 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 225) in 2545 ms on 4703d70ea67c (executor driver) (47/200)
21/05/15 13:06:21 INFO Executor: Running task 50.0 in stage 4.0 (TID 229)
[2021-05-15 10:06:21,300] {docker.py:276} INFO - 21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:21,302] {docker.py:276} INFO - 21/05/15 13:06:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382963407359068889636_0004_m_000050_229, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382963407359068889636_0004_m_000050_229}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382963407359068889636_0004}; taskId=attempt_202105151305382963407359068889636_0004_m_000050_229, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30c8c5e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305382963407359068889636_0004_m_000050_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382963407359068889636_0004_m_000050_229
[2021-05-15 10:06:21,306] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305382963407359068889636_0004_m_000050_229: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382963407359068889636_0004_m_000050_229 : duration 0:00.003s
[2021-05-15 10:06:21,439] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305388305002864947518673_0004_m_000047_226: needsTaskCommit() Task attempt_202105151305388305002864947518673_0004_m_000047_226
[2021-05-15 10:06:21,440] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305388305002864947518673_0004_m_000047_226: needsTaskCommit() Task attempt_202105151305388305002864947518673_0004_m_000047_226: duration 0:00.000s
21/05/15 13:06:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388305002864947518673_0004_m_000047_226
[2021-05-15 10:06:21,441] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Finished task 47.0 in stage 4.0 (TID 226). 4544 bytes result sent to driver
[2021-05-15 10:06:21,442] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 230) (4703d70ea67c, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:21,443] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Running task 51.0 in stage 4.0 (TID 230)
21/05/15 13:06:21 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 226) in 2445 ms on 4703d70ea67c (executor driver) (48/200)
[2021-05-15 10:06:21,451] {docker.py:276} INFO - 21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:21,453] {docker.py:276} INFO - 21/05/15 13:06:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384094950399002485794_0004_m_000051_230, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384094950399002485794_0004_m_000051_230}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384094950399002485794_0004}; taskId=attempt_202105151305384094950399002485794_0004_m_000051_230, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b6ef4ea}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:21,454] {docker.py:276} INFO - 21/05/15 13:06:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305384094950399002485794_0004_m_000051_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384094950399002485794_0004_m_000051_230
[2021-05-15 10:06:21,456] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305384094950399002485794_0004_m_000051_230: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384094950399002485794_0004_m_000051_230 : duration 0:00.002s
[2021-05-15 10:06:21,579] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305384018189171077938746_0004_m_000048_227: needsTaskCommit() Task attempt_202105151305384018189171077938746_0004_m_000048_227
[2021-05-15 10:06:21,580] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305384018189171077938746_0004_m_000048_227: needsTaskCommit() Task attempt_202105151305384018189171077938746_0004_m_000048_227: duration 0:00.001s
[2021-05-15 10:06:21,581] {docker.py:276} INFO - 21/05/15 13:06:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384018189171077938746_0004_m_000048_227
[2021-05-15 10:06:21,582] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Finished task 48.0 in stage 4.0 (TID 227). 4544 bytes result sent to driver
[2021-05-15 10:06:21,583] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 231) (4703d70ea67c, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:21,584] {docker.py:276} INFO - 21/05/15 13:06:21 INFO Executor: Running task 52.0 in stage 4.0 (TID 231)
[2021-05-15 10:06:21,585] {docker.py:276} INFO - 21/05/15 13:06:21 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 227) in 2526 ms on 4703d70ea67c (executor driver) (49/200)
[2021-05-15 10:06:21,594] {docker.py:276} INFO - 21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:21,596] {docker.py:276} INFO - 21/05/15 13:06:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388839603537992336530_0004_m_000052_231, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388839603537992336530_0004_m_000052_231}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388839603537992336530_0004}; taskId=attempt_202105151305388839603537992336530_0004_m_000052_231, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cc99ada}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305388839603537992336530_0004_m_000052_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388839603537992336530_0004_m_000052_231
[2021-05-15 10:06:21,599] {docker.py:276} INFO - 21/05/15 13:06:21 INFO StagingCommitter: Task committer attempt_202105151305388839603537992336530_0004_m_000052_231: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388839603537992336530_0004_m_000052_231 : duration 0:00.003s
[2021-05-15 10:06:23,571] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305384146487165464635064_0004_m_000049_228: needsTaskCommit() Task attempt_202105151305384146487165464635064_0004_m_000049_228
21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_202105151305384146487165464635064_0004_m_000049_228: needsTaskCommit() Task attempt_202105151305384146487165464635064_0004_m_000049_228: duration 0:00.000s
21/05/15 13:06:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384146487165464635064_0004_m_000049_228
[2021-05-15 10:06:23,574] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Finished task 49.0 in stage 4.0 (TID 228). 4544 bytes result sent to driver
[2021-05-15 10:06:23,576] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 232) (4703d70ea67c, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:23,577] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Running task 53.0 in stage 4.0 (TID 232)
[2021-05-15 10:06:23,578] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 228) in 2580 ms on 4703d70ea67c (executor driver) (50/200)
[2021-05-15 10:06:23,586] {docker.py:276} INFO - 21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:23,588] {docker.py:276} INFO - 21/05/15 13:06:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382882844811518005991_0004_m_000053_232, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382882844811518005991_0004_m_000053_232}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382882844811518005991_0004}; taskId=attempt_202105151305382882844811518005991_0004_m_000053_232, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ebfbfac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:23,588] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305382882844811518005991_0004_m_000053_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382882844811518005991_0004_m_000053_232
[2021-05-15 10:06:23,591] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_202105151305382882844811518005991_0004_m_000053_232: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382882844811518005991_0004_m_000053_232 : duration 0:00.003s
[2021-05-15 10:06:23,729] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305382963407359068889636_0004_m_000050_229: needsTaskCommit() Task attempt_202105151305382963407359068889636_0004_m_000050_229
[2021-05-15 10:06:23,729] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_202105151305382963407359068889636_0004_m_000050_229: needsTaskCommit() Task attempt_202105151305382963407359068889636_0004_m_000050_229: duration 0:00.001s
21/05/15 13:06:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382963407359068889636_0004_m_000050_229
[2021-05-15 10:06:23,730] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Finished task 50.0 in stage 4.0 (TID 229). 4544 bytes result sent to driver
[2021-05-15 10:06:23,732] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 233) (4703d70ea67c, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:23,732] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Running task 54.0 in stage 4.0 (TID 233)
[2021-05-15 10:06:23,733] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 229) in 2449 ms on 4703d70ea67c (executor driver) (51/200)
[2021-05-15 10:06:23,740] {docker.py:276} INFO - 21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:23,742] {docker.py:276} INFO - 21/05/15 13:06:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538655249159406600813_0004_m_000054_233, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538655249159406600813_0004_m_000054_233}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538655249159406600813_0004}; taskId=attempt_20210515130538655249159406600813_0004_m_000054_233, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ed54b63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:23,742] {docker.py:276} INFO - 21/05/15 13:06:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_20210515130538655249159406600813_0004_m_000054_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538655249159406600813_0004_m_000054_233
[2021-05-15 10:06:23,745] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_20210515130538655249159406600813_0004_m_000054_233: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538655249159406600813_0004_m_000054_233 : duration 0:00.003s
[2021-05-15 10:06:23,789] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305384094950399002485794_0004_m_000051_230: needsTaskCommit() Task attempt_202105151305384094950399002485794_0004_m_000051_230
[2021-05-15 10:06:23,790] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_202105151305384094950399002485794_0004_m_000051_230: needsTaskCommit() Task attempt_202105151305384094950399002485794_0004_m_000051_230: duration 0:00.001s
21/05/15 13:06:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384094950399002485794_0004_m_000051_230
[2021-05-15 10:06:23,793] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Finished task 51.0 in stage 4.0 (TID 230). 4544 bytes result sent to driver
[2021-05-15 10:06:23,796] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 234) (4703d70ea67c, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:23,797] {docker.py:276} INFO - 21/05/15 13:06:23 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 230) in 2357 ms on 4703d70ea67c (executor driver) (52/200)
[2021-05-15 10:06:23,797] {docker.py:276} INFO - 21/05/15 13:06:23 INFO Executor: Running task 55.0 in stage 4.0 (TID 234)
[2021-05-15 10:06:23,808] {docker.py:276} INFO - 21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:23,810] {docker.py:276} INFO - 21/05/15 13:06:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381506965824180425497_0004_m_000055_234, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381506965824180425497_0004_m_000055_234}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381506965824180425497_0004}; taskId=attempt_202105151305381506965824180425497_0004_m_000055_234, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b0482f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305381506965824180425497_0004_m_000055_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381506965824180425497_0004_m_000055_234
[2021-05-15 10:06:23,813] {docker.py:276} INFO - 21/05/15 13:06:23 INFO StagingCommitter: Task committer attempt_202105151305381506965824180425497_0004_m_000055_234: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381506965824180425497_0004_m_000055_234 : duration 0:00.003s
[2021-05-15 10:06:23,998] {docker.py:276} INFO - 21/05/15 13:06:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305388839603537992336530_0004_m_000052_231: needsTaskCommit() Task attempt_202105151305388839603537992336530_0004_m_000052_231
[2021-05-15 10:06:23,999] {docker.py:276} INFO - 21/05/15 13:06:24 INFO StagingCommitter: Task committer attempt_202105151305388839603537992336530_0004_m_000052_231: needsTaskCommit() Task attempt_202105151305388839603537992336530_0004_m_000052_231: duration 0:00.001s
21/05/15 13:06:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388839603537992336530_0004_m_000052_231
[2021-05-15 10:06:24,004] {docker.py:276} INFO - 21/05/15 13:06:24 INFO Executor: Finished task 52.0 in stage 4.0 (TID 231). 4544 bytes result sent to driver
21/05/15 13:06:24 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 235) (4703d70ea67c, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:24 INFO Executor: Running task 56.0 in stage 4.0 (TID 235)
[2021-05-15 10:06:24,005] {docker.py:276} INFO - 21/05/15 13:06:24 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 231) in 2426 ms on 4703d70ea67c (executor driver) (53/200)
[2021-05-15 10:06:24,018] {docker.py:276} INFO - 21/05/15 13:06:24 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:24,023] {docker.py:276} INFO - 21/05/15 13:06:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:24,023] {docker.py:276} INFO - 21/05/15 13:06:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382379858827068160191_0004_m_000056_235, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382379858827068160191_0004_m_000056_235}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382379858827068160191_0004}; taskId=attempt_202105151305382379858827068160191_0004_m_000056_235, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fc9eba5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:24,024] {docker.py:276} INFO - 21/05/15 13:06:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:24,024] {docker.py:276} INFO - 21/05/15 13:06:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305382379858827068160191_0004_m_000056_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382379858827068160191_0004_m_000056_235
[2021-05-15 10:06:24,027] {docker.py:276} INFO - 21/05/15 13:06:24 INFO StagingCommitter: Task committer attempt_202105151305382379858827068160191_0004_m_000056_235: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382379858827068160191_0004_m_000056_235 : duration 0:00.003s
[2021-05-15 10:06:26,041] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305382882844811518005991_0004_m_000053_232: needsTaskCommit() Task attempt_202105151305382882844811518005991_0004_m_000053_232
[2021-05-15 10:06:26,042] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305382882844811518005991_0004_m_000053_232: needsTaskCommit() Task attempt_202105151305382882844811518005991_0004_m_000053_232: duration 0:00.001s
[2021-05-15 10:06:26,043] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382882844811518005991_0004_m_000053_232
[2021-05-15 10:06:26,044] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Finished task 53.0 in stage 4.0 (TID 232). 4544 bytes result sent to driver
[2021-05-15 10:06:26,045] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 236) (4703d70ea67c, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:26,045] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 232) in 2438 ms on 4703d70ea67c (executor driver) (54/200)
[2021-05-15 10:06:26,047] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Running task 57.0 in stage 4.0 (TID 236)
[2021-05-15 10:06:26,061] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:26,064] {docker.py:276} INFO - 21/05/15 13:06:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:26,065] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,065] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382906338056550249329_0004_m_000057_236, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382906338056550249329_0004_m_000057_236}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382906338056550249329_0004}; taskId=attempt_202105151305382906338056550249329_0004_m_000057_236, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47014712}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,066] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:26,066] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305382906338056550249329_0004_m_000057_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382906338056550249329_0004_m_000057_236
[2021-05-15 10:06:26,069] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305382906338056550249329_0004_m_000057_236: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382906338056550249329_0004_m_000057_236 : duration 0:00.004s
[2021-05-15 10:06:26,121] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305381506965824180425497_0004_m_000055_234: needsTaskCommit() Task attempt_202105151305381506965824180425497_0004_m_000055_234
[2021-05-15 10:06:26,122] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305381506965824180425497_0004_m_000055_234: needsTaskCommit() Task attempt_202105151305381506965824180425497_0004_m_000055_234: duration 0:00.000s
21/05/15 13:06:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381506965824180425497_0004_m_000055_234
[2021-05-15 10:06:26,123] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Finished task 55.0 in stage 4.0 (TID 234). 4544 bytes result sent to driver
[2021-05-15 10:06:26,125] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 237) (4703d70ea67c, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:26,125] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Running task 58.0 in stage 4.0 (TID 237)
[2021-05-15 10:06:26,126] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 234) in 2300 ms on 4703d70ea67c (executor driver) (55/200)
[2021-05-15 10:06:26,136] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:26,138] {docker.py:276} INFO - 21/05/15 13:06:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:26,139] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,139] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383483012223348160040_0004_m_000058_237, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383483012223348160040_0004_m_000058_237}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383483012223348160040_0004}; taskId=attempt_202105151305383483012223348160040_0004_m_000058_237, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c66e69e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,139] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:26,140] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305383483012223348160040_0004_m_000058_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383483012223348160040_0004_m_000058_237
[2021-05-15 10:06:26,164] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305383483012223348160040_0004_m_000058_237: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383483012223348160040_0004_m_000058_237 : duration 0:00.025s
[2021-05-15 10:06:26,241] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_20210515130538655249159406600813_0004_m_000054_233: needsTaskCommit() Task attempt_20210515130538655249159406600813_0004_m_000054_233
[2021-05-15 10:06:26,242] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_20210515130538655249159406600813_0004_m_000054_233: needsTaskCommit() Task attempt_20210515130538655249159406600813_0004_m_000054_233: duration 0:00.000s
[2021-05-15 10:06:26,242] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538655249159406600813_0004_m_000054_233
[2021-05-15 10:06:26,243] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Finished task 54.0 in stage 4.0 (TID 233). 4587 bytes result sent to driver
[2021-05-15 10:06:26,244] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 238) (4703d70ea67c, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:26,245] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Running task 59.0 in stage 4.0 (TID 238)
[2021-05-15 10:06:26,246] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 233) in 2482 ms on 4703d70ea67c (executor driver) (56/200)
[2021-05-15 10:06:26,254] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:26,255] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:06:26,257] {docker.py:276} INFO - 21/05/15 13:06:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:26,258] {docker.py:276} INFO - 21/05/15 13:06:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:26,258] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,258] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387928965012517600425_0004_m_000059_238, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387928965012517600425_0004_m_000059_238}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387928965012517600425_0004}; taskId=attempt_202105151305387928965012517600425_0004_m_000059_238, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d521209}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,259] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:26,259] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305387928965012517600425_0004_m_000059_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387928965012517600425_0004_m_000059_238
[2021-05-15 10:06:26,262] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305387928965012517600425_0004_m_000059_238: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387928965012517600425_0004_m_000059_238 : duration 0:00.003s
[2021-05-15 10:06:26,564] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305382379858827068160191_0004_m_000056_235: needsTaskCommit() Task attempt_202105151305382379858827068160191_0004_m_000056_235
[2021-05-15 10:06:26,565] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305382379858827068160191_0004_m_000056_235: needsTaskCommit() Task attempt_202105151305382379858827068160191_0004_m_000056_235: duration 0:00.001s
[2021-05-15 10:06:26,566] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382379858827068160191_0004_m_000056_235
[2021-05-15 10:06:26,567] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Finished task 56.0 in stage 4.0 (TID 235). 4587 bytes result sent to driver
[2021-05-15 10:06:26,569] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 239) (4703d70ea67c, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:26,570] {docker.py:276} INFO - 21/05/15 13:06:26 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 235) in 2534 ms on 4703d70ea67c (executor driver) (57/200)
[2021-05-15 10:06:26,570] {docker.py:276} INFO - 21/05/15 13:06:26 INFO Executor: Running task 60.0 in stage 4.0 (TID 239)
[2021-05-15 10:06:26,581] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:26,582] {docker.py:276} INFO - 21/05/15 13:06:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:26,584] {docker.py:276} INFO - 21/05/15 13:06:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:26,584] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,585] {docker.py:276} INFO - 21/05/15 13:06:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388730329687920520353_0004_m_000060_239, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388730329687920520353_0004_m_000060_239}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388730329687920520353_0004}; taskId=attempt_202105151305388730329687920520353_0004_m_000060_239, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41cf010}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:26,585] {docker.py:276} INFO - 21/05/15 13:06:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:26,585] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305388730329687920520353_0004_m_000060_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388730329687920520353_0004_m_000060_239
[2021-05-15 10:06:26,590] {docker.py:276} INFO - 21/05/15 13:06:26 INFO StagingCommitter: Task committer attempt_202105151305388730329687920520353_0004_m_000060_239: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388730329687920520353_0004_m_000060_239 : duration 0:00.005s
[2021-05-15 10:06:28,492] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305382906338056550249329_0004_m_000057_236: needsTaskCommit() Task attempt_202105151305382906338056550249329_0004_m_000057_236
[2021-05-15 10:06:28,493] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305382906338056550249329_0004_m_000057_236: needsTaskCommit() Task attempt_202105151305382906338056550249329_0004_m_000057_236: duration 0:00.000s
21/05/15 13:06:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382906338056550249329_0004_m_000057_236
[2021-05-15 10:06:28,495] {docker.py:276} INFO - 21/05/15 13:06:28 INFO Executor: Finished task 57.0 in stage 4.0 (TID 236). 4587 bytes result sent to driver
21/05/15 13:06:28 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 240) (4703d70ea67c, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:28,496] {docker.py:276} INFO - 21/05/15 13:06:28 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 236) in 2454 ms on 4703d70ea67c (executor driver) (58/200)
21/05/15 13:06:28 INFO Executor: Running task 61.0 in stage 4.0 (TID 240)
[2021-05-15 10:06:28,504] {docker.py:276} INFO - 21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:28,507] {docker.py:276} INFO - 21/05/15 13:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382266255392256344320_0004_m_000061_240, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382266255392256344320_0004_m_000061_240}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382266255392256344320_0004}; taskId=attempt_202105151305382266255392256344320_0004_m_000061_240, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c217235}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:28,507] {docker.py:276} INFO - 21/05/15 13:06:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305382266255392256344320_0004_m_000061_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382266255392256344320_0004_m_000061_240
[2021-05-15 10:06:28,510] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305382266255392256344320_0004_m_000061_240: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382266255392256344320_0004_m_000061_240 : duration 0:00.003s
[2021-05-15 10:06:28,677] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305387928965012517600425_0004_m_000059_238: needsTaskCommit() Task attempt_202105151305387928965012517600425_0004_m_000059_238
[2021-05-15 10:06:28,678] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305387928965012517600425_0004_m_000059_238: needsTaskCommit() Task attempt_202105151305387928965012517600425_0004_m_000059_238: duration 0:00.001s
21/05/15 13:06:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387928965012517600425_0004_m_000059_238
[2021-05-15 10:06:28,680] {docker.py:276} INFO - 21/05/15 13:06:28 INFO Executor: Finished task 59.0 in stage 4.0 (TID 238). 4544 bytes result sent to driver
[2021-05-15 10:06:28,682] {docker.py:276} INFO - 21/05/15 13:06:28 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 241) (4703d70ea67c, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:28,683] {docker.py:276} INFO - 21/05/15 13:06:28 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 238) in 2441 ms on 4703d70ea67c (executor driver) (59/200)
[2021-05-15 10:06:28,684] {docker.py:276} INFO - 21/05/15 13:06:28 INFO Executor: Running task 62.0 in stage 4.0 (TID 241)
[2021-05-15 10:06:28,693] {docker.py:276} INFO - 21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:28,696] {docker.py:276} INFO - 21/05/15 13:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:28,697] {docker.py:276} INFO - 21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388593495134828812338_0004_m_000062_241, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388593495134828812338_0004_m_000062_241}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388593495134828812338_0004}; taskId=attempt_202105151305388593495134828812338_0004_m_000062_241, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e44a101}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:28,697] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305388593495134828812338_0004_m_000062_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388593495134828812338_0004_m_000062_241
[2021-05-15 10:06:28,699] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305388593495134828812338_0004_m_000062_241: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388593495134828812338_0004_m_000062_241 : duration 0:00.003s
[2021-05-15 10:06:28,711] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305383483012223348160040_0004_m_000058_237: needsTaskCommit() Task attempt_202105151305383483012223348160040_0004_m_000058_237
[2021-05-15 10:06:28,711] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305383483012223348160040_0004_m_000058_237: needsTaskCommit() Task attempt_202105151305383483012223348160040_0004_m_000058_237: duration 0:00.000s
21/05/15 13:06:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383483012223348160040_0004_m_000058_237
[2021-05-15 10:06:28,712] {docker.py:276} INFO - 21/05/15 13:06:28 INFO Executor: Finished task 58.0 in stage 4.0 (TID 237). 4587 bytes result sent to driver
[2021-05-15 10:06:28,713] {docker.py:276} INFO - 21/05/15 13:06:28 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 242) (4703d70ea67c, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:28,714] {docker.py:276} INFO - 21/05/15 13:06:28 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 237) in 2592 ms on 4703d70ea67c (executor driver) (60/200)
[2021-05-15 10:06:28,714] {docker.py:276} INFO - 21/05/15 13:06:28 INFO Executor: Running task 63.0 in stage 4.0 (TID 242)
[2021-05-15 10:06:28,721] {docker.py:276} INFO - 21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:28,721] {docker.py:276} INFO - 21/05/15 13:06:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:28,723] {docker.py:276} INFO - 21/05/15 13:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:28,723] {docker.py:276} INFO - 21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386945028430581884435_0004_m_000063_242, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386945028430581884435_0004_m_000063_242}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386945028430581884435_0004}; taskId=attempt_202105151305386945028430581884435_0004_m_000063_242, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25c73cc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:28,723] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305386945028430581884435_0004_m_000063_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386945028430581884435_0004_m_000063_242
[2021-05-15 10:06:28,726] {docker.py:276} INFO - 21/05/15 13:06:28 INFO StagingCommitter: Task committer attempt_202105151305386945028430581884435_0004_m_000063_242: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386945028430581884435_0004_m_000063_242 : duration 0:00.003s
[2021-05-15 10:06:29,115] {docker.py:276} INFO - 21/05/15 13:06:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305388730329687920520353_0004_m_000060_239: needsTaskCommit() Task attempt_202105151305388730329687920520353_0004_m_000060_239
[2021-05-15 10:06:29,116] {docker.py:276} INFO - 21/05/15 13:06:29 INFO StagingCommitter: Task committer attempt_202105151305388730329687920520353_0004_m_000060_239: needsTaskCommit() Task attempt_202105151305388730329687920520353_0004_m_000060_239: duration 0:00.001s
21/05/15 13:06:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388730329687920520353_0004_m_000060_239
[2021-05-15 10:06:29,117] {docker.py:276} INFO - 21/05/15 13:06:29 INFO Executor: Finished task 60.0 in stage 4.0 (TID 239). 4544 bytes result sent to driver
[2021-05-15 10:06:29,119] {docker.py:276} INFO - 21/05/15 13:06:29 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 243) (4703d70ea67c, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:29,120] {docker.py:276} INFO - 21/05/15 13:06:29 INFO Executor: Running task 64.0 in stage 4.0 (TID 243)
21/05/15 13:06:29 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 239) in 2555 ms on 4703d70ea67c (executor driver) (61/200)
[2021-05-15 10:06:29,130] {docker.py:276} INFO - 21/05/15 13:06:29 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:29,132] {docker.py:276} INFO - 21/05/15 13:06:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:29,132] {docker.py:276} INFO - 21/05/15 13:06:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387201377244316488501_0004_m_000064_243, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201377244316488501_0004_m_000064_243}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387201377244316488501_0004}; taskId=attempt_202105151305387201377244316488501_0004_m_000064_243, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ff7bf3d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305387201377244316488501_0004_m_000064_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201377244316488501_0004_m_000064_243
[2021-05-15 10:06:29,136] {docker.py:276} INFO - 21/05/15 13:06:29 INFO StagingCommitter: Task committer attempt_202105151305387201377244316488501_0004_m_000064_243: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201377244316488501_0004_m_000064_243 : duration 0:00.003s
[2021-05-15 10:06:31,004] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305388593495134828812338_0004_m_000062_241: needsTaskCommit() Task attempt_202105151305388593495134828812338_0004_m_000062_241
[2021-05-15 10:06:31,006] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305388593495134828812338_0004_m_000062_241: needsTaskCommit() Task attempt_202105151305388593495134828812338_0004_m_000062_241: duration 0:00.003s
[2021-05-15 10:06:31,007] {docker.py:276} INFO - 21/05/15 13:06:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388593495134828812338_0004_m_000062_241
[2021-05-15 10:06:31,008] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Finished task 62.0 in stage 4.0 (TID 241). 4544 bytes result sent to driver
[2021-05-15 10:06:31,009] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 244) (4703d70ea67c, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:31,010] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Running task 65.0 in stage 4.0 (TID 244)
[2021-05-15 10:06:31,011] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 241) in 2332 ms on 4703d70ea67c (executor driver) (62/200)
[2021-05-15 10:06:31,019] {docker.py:276} INFO - 21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:31,021] {docker.py:276} INFO - 21/05/15 13:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:31,022] {docker.py:276} INFO - 21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388626551713986897777_0004_m_000065_244, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388626551713986897777_0004_m_000065_244}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388626551713986897777_0004}; taskId=attempt_202105151305388626551713986897777_0004_m_000065_244, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19e4b2ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305388626551713986897777_0004_m_000065_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388626551713986897777_0004_m_000065_244
[2021-05-15 10:06:31,025] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305388626551713986897777_0004_m_000065_244: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388626551713986897777_0004_m_000065_244 : duration 0:00.003s
[2021-05-15 10:06:31,054] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305382266255392256344320_0004_m_000061_240: needsTaskCommit() Task attempt_202105151305382266255392256344320_0004_m_000061_240
[2021-05-15 10:06:31,055] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305382266255392256344320_0004_m_000061_240: needsTaskCommit() Task attempt_202105151305382266255392256344320_0004_m_000061_240: duration 0:00.001s
21/05/15 13:06:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382266255392256344320_0004_m_000061_240
[2021-05-15 10:06:31,058] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Finished task 61.0 in stage 4.0 (TID 240). 4544 bytes result sent to driver
[2021-05-15 10:06:31,060] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 245) (4703d70ea67c, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:31,061] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Running task 66.0 in stage 4.0 (TID 245)
21/05/15 13:06:31 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 240) in 2570 ms on 4703d70ea67c (executor driver) (63/200)
[2021-05-15 10:06:31,071] {docker.py:276} INFO - 21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:31,073] {docker.py:276} INFO - 21/05/15 13:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385268697949605455052_0004_m_000066_245, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385268697949605455052_0004_m_000066_245}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385268697949605455052_0004}; taskId=attempt_202105151305385268697949605455052_0004_m_000066_245, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69f07339}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:31,073] {docker.py:276} INFO - 21/05/15 13:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305385268697949605455052_0004_m_000066_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385268697949605455052_0004_m_000066_245
[2021-05-15 10:06:31,076] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305385268697949605455052_0004_m_000066_245: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385268697949605455052_0004_m_000066_245 : duration 0:00.003s
[2021-05-15 10:06:31,268] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305386945028430581884435_0004_m_000063_242: needsTaskCommit() Task attempt_202105151305386945028430581884435_0004_m_000063_242
[2021-05-15 10:06:31,269] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305386945028430581884435_0004_m_000063_242: needsTaskCommit() Task attempt_202105151305386945028430581884435_0004_m_000063_242: duration 0:00.001s
21/05/15 13:06:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386945028430581884435_0004_m_000063_242
[2021-05-15 10:06:31,271] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Finished task 63.0 in stage 4.0 (TID 242). 4544 bytes result sent to driver
[2021-05-15 10:06:31,272] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 246) (4703d70ea67c, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:31,273] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Running task 67.0 in stage 4.0 (TID 246)
[2021-05-15 10:06:31,274] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 242) in 2564 ms on 4703d70ea67c (executor driver) (64/200)
[2021-05-15 10:06:31,283] {docker.py:276} INFO - 21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:31,285] {docker.py:276} INFO - 21/05/15 13:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:31,285] {docker.py:276} INFO - 21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388485145763744571925_0004_m_000067_246, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388485145763744571925_0004_m_000067_246}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388485145763744571925_0004}; taskId=attempt_202105151305388485145763744571925_0004_m_000067_246, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c58c37c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:31,286] {docker.py:276} INFO - 21/05/15 13:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:31,286] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305388485145763744571925_0004_m_000067_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388485145763744571925_0004_m_000067_246
[2021-05-15 10:06:31,289] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305388485145763744571925_0004_m_000067_246: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388485145763744571925_0004_m_000067_246 : duration 0:00.004s
[2021-05-15 10:06:31,612] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305387201377244316488501_0004_m_000064_243: needsTaskCommit() Task attempt_202105151305387201377244316488501_0004_m_000064_243
21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305387201377244316488501_0004_m_000064_243: needsTaskCommit() Task attempt_202105151305387201377244316488501_0004_m_000064_243: duration 0:00.000s
21/05/15 13:06:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387201377244316488501_0004_m_000064_243
[2021-05-15 10:06:31,614] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Finished task 64.0 in stage 4.0 (TID 243). 4544 bytes result sent to driver
[2021-05-15 10:06:31,615] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 247) (4703d70ea67c, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:31,616] {docker.py:276} INFO - 21/05/15 13:06:31 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 243) in 2501 ms on 4703d70ea67c (executor driver) (65/200)
[2021-05-15 10:06:31,616] {docker.py:276} INFO - 21/05/15 13:06:31 INFO Executor: Running task 68.0 in stage 4.0 (TID 247)
[2021-05-15 10:06:31,625] {docker.py:276} INFO - 21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:31,626] {docker.py:276} INFO - 21/05/15 13:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381588066958756633316_0004_m_000068_247, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381588066958756633316_0004_m_000068_247}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381588066958756633316_0004}; taskId=attempt_202105151305381588066958756633316_0004_m_000068_247, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@260f2278}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:31,627] {docker.py:276} INFO - 21/05/15 13:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305381588066958756633316_0004_m_000068_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381588066958756633316_0004_m_000068_247
[2021-05-15 10:06:31,630] {docker.py:276} INFO - 21/05/15 13:06:31 INFO StagingCommitter: Task committer attempt_202105151305381588066958756633316_0004_m_000068_247: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381588066958756633316_0004_m_000068_247 : duration 0:00.003s
[2021-05-15 10:06:33,536] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305388485145763744571925_0004_m_000067_246: needsTaskCommit() Task attempt_202105151305388485145763744571925_0004_m_000067_246
[2021-05-15 10:06:33,537] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_202105151305388485145763744571925_0004_m_000067_246: needsTaskCommit() Task attempt_202105151305388485145763744571925_0004_m_000067_246: duration 0:00.001s
21/05/15 13:06:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388485145763744571925_0004_m_000067_246
[2021-05-15 10:06:33,539] {docker.py:276} INFO - 21/05/15 13:06:33 INFO Executor: Finished task 67.0 in stage 4.0 (TID 246). 4544 bytes result sent to driver
[2021-05-15 10:06:33,541] {docker.py:276} INFO - 21/05/15 13:06:33 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 248) (4703d70ea67c, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:33 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 246) in 2272 ms on 4703d70ea67c (executor driver) (66/200)
21/05/15 13:06:33 INFO Executor: Running task 69.0 in stage 4.0 (TID 248)
[2021-05-15 10:06:33,548] {docker.py:276} INFO - 21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:33,550] {docker.py:276} INFO - 21/05/15 13:06:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:33,550] {docker.py:276} INFO - 21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385381869669970508414_0004_m_000069_248, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385381869669970508414_0004_m_000069_248}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385381869669970508414_0004}; taskId=attempt_202105151305385381869669970508414_0004_m_000069_248, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@164fb6b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305385381869669970508414_0004_m_000069_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385381869669970508414_0004_m_000069_248
[2021-05-15 10:06:33,553] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_202105151305385381869669970508414_0004_m_000069_248: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385381869669970508414_0004_m_000069_248 : duration 0:00.003s
[2021-05-15 10:06:33,561] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305388626551713986897777_0004_m_000065_244: needsTaskCommit() Task attempt_202105151305388626551713986897777_0004_m_000065_244
[2021-05-15 10:06:33,561] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_202105151305388626551713986897777_0004_m_000065_244: needsTaskCommit() Task attempt_202105151305388626551713986897777_0004_m_000065_244: duration 0:00.001s
21/05/15 13:06:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388626551713986897777_0004_m_000065_244
[2021-05-15 10:06:33,563] {docker.py:276} INFO - 21/05/15 13:06:33 INFO Executor: Finished task 65.0 in stage 4.0 (TID 244). 4544 bytes result sent to driver
[2021-05-15 10:06:33,564] {docker.py:276} INFO - 21/05/15 13:06:33 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 249) (4703d70ea67c, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:33,564] {docker.py:276} INFO - 21/05/15 13:06:33 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 244) in 2559 ms on 4703d70ea67c (executor driver) (67/200)
[2021-05-15 10:06:33,565] {docker.py:276} INFO - 21/05/15 13:06:33 INFO Executor: Running task 70.0 in stage 4.0 (TID 249)
[2021-05-15 10:06:33,572] {docker.py:276} INFO - 21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:33,573] {docker.py:276} INFO - 21/05/15 13:06:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381321270644684918310_0004_m_000070_249, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381321270644684918310_0004_m_000070_249}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381321270644684918310_0004}; taskId=attempt_202105151305381321270644684918310_0004_m_000070_249, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51349ed6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:33,574] {docker.py:276} INFO - 21/05/15 13:06:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305381321270644684918310_0004_m_000070_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381321270644684918310_0004_m_000070_249
[2021-05-15 10:06:33,577] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_202105151305381321270644684918310_0004_m_000070_249: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381321270644684918310_0004_m_000070_249 : duration 0:00.003s
[2021-05-15 10:06:33,642] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305385268697949605455052_0004_m_000066_245: needsTaskCommit() Task attempt_202105151305385268697949605455052_0004_m_000066_245
[2021-05-15 10:06:33,644] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_202105151305385268697949605455052_0004_m_000066_245: needsTaskCommit() Task attempt_202105151305385268697949605455052_0004_m_000066_245: duration 0:00.001s
21/05/15 13:06:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385268697949605455052_0004_m_000066_245
[2021-05-15 10:06:33,645] {docker.py:276} INFO - 21/05/15 13:06:33 INFO Executor: Finished task 66.0 in stage 4.0 (TID 245). 4544 bytes result sent to driver
[2021-05-15 10:06:33,647] {docker.py:276} INFO - 21/05/15 13:06:33 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 250) (4703d70ea67c, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:33,648] {docker.py:276} INFO - 21/05/15 13:06:33 INFO Executor: Running task 71.0 in stage 4.0 (TID 250)
21/05/15 13:06:33 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 245) in 2592 ms on 4703d70ea67c (executor driver) (68/200)
[2021-05-15 10:06:33,656] {docker.py:276} INFO - 21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:33,658] {docker.py:276} INFO - 21/05/15 13:06:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:33,658] {docker.py:276} INFO - 21/05/15 13:06:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538806129202650630367_0004_m_000071_250, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538806129202650630367_0004_m_000071_250}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538806129202650630367_0004}; taskId=attempt_20210515130538806129202650630367_0004_m_000071_250, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@670e9689}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:33 INFO StagingCommitter: Starting: Task committer attempt_20210515130538806129202650630367_0004_m_000071_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538806129202650630367_0004_m_000071_250
[2021-05-15 10:06:33,660] {docker.py:276} INFO - 21/05/15 13:06:33 INFO StagingCommitter: Task committer attempt_20210515130538806129202650630367_0004_m_000071_250: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538806129202650630367_0004_m_000071_250 : duration 0:00.003s
[2021-05-15 10:06:34,141] {docker.py:276} INFO - 21/05/15 13:06:34 INFO StagingCommitter: Starting: Task committer attempt_202105151305381588066958756633316_0004_m_000068_247: needsTaskCommit() Task attempt_202105151305381588066958756633316_0004_m_000068_247
[2021-05-15 10:06:34,142] {docker.py:276} INFO - 21/05/15 13:06:34 INFO StagingCommitter: Task committer attempt_202105151305381588066958756633316_0004_m_000068_247: needsTaskCommit() Task attempt_202105151305381588066958756633316_0004_m_000068_247: duration 0:00.000s
21/05/15 13:06:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381588066958756633316_0004_m_000068_247
[2021-05-15 10:06:34,143] {docker.py:276} INFO - 21/05/15 13:06:34 INFO Executor: Finished task 68.0 in stage 4.0 (TID 247). 4544 bytes result sent to driver
[2021-05-15 10:06:34,144] {docker.py:276} INFO - 21/05/15 13:06:34 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 251) (4703d70ea67c, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:34,145] {docker.py:276} INFO - 21/05/15 13:06:34 INFO Executor: Running task 72.0 in stage 4.0 (TID 251)
[2021-05-15 10:06:34,146] {docker.py:276} INFO - 21/05/15 13:06:34 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 247) in 2534 ms on 4703d70ea67c (executor driver) (69/200)
[2021-05-15 10:06:34,154] {docker.py:276} INFO - 21/05/15 13:06:34 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:34,157] {docker.py:276} INFO - 21/05/15 13:06:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381913897672285382478_0004_m_000072_251, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381913897672285382478_0004_m_000072_251}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381913897672285382478_0004}; taskId=attempt_202105151305381913897672285382478_0004_m_000072_251, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18dbd012}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:34,157] {docker.py:276} INFO - 21/05/15 13:06:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:34 INFO StagingCommitter: Starting: Task committer attempt_202105151305381913897672285382478_0004_m_000072_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381913897672285382478_0004_m_000072_251
[2021-05-15 10:06:34,159] {docker.py:276} INFO - 21/05/15 13:06:34 INFO StagingCommitter: Task committer attempt_202105151305381913897672285382478_0004_m_000072_251: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381913897672285382478_0004_m_000072_251 : duration 0:00.002s
[2021-05-15 10:06:35,887] {docker.py:276} INFO - 21/05/15 13:06:35 INFO StagingCommitter: Starting: Task committer attempt_202105151305385381869669970508414_0004_m_000069_248: needsTaskCommit() Task attempt_202105151305385381869669970508414_0004_m_000069_248
[2021-05-15 10:06:35,887] {docker.py:276} INFO - 21/05/15 13:06:35 INFO StagingCommitter: Task committer attempt_202105151305385381869669970508414_0004_m_000069_248: needsTaskCommit() Task attempt_202105151305385381869669970508414_0004_m_000069_248: duration 0:00.001s
21/05/15 13:06:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385381869669970508414_0004_m_000069_248
[2021-05-15 10:06:35,889] {docker.py:276} INFO - 21/05/15 13:06:35 INFO Executor: Finished task 69.0 in stage 4.0 (TID 248). 4544 bytes result sent to driver
[2021-05-15 10:06:35,890] {docker.py:276} INFO - 21/05/15 13:06:35 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 252) (4703d70ea67c, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:35,892] {docker.py:276} INFO - 21/05/15 13:06:35 INFO Executor: Running task 73.0 in stage 4.0 (TID 252)
[2021-05-15 10:06:35,893] {docker.py:276} INFO - 21/05/15 13:06:35 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 248) in 2356 ms on 4703d70ea67c (executor driver) (70/200)
[2021-05-15 10:06:35,905] {docker.py:276} INFO - 21/05/15 13:06:35 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:35,907] {docker.py:276} INFO - 21/05/15 13:06:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:35,907] {docker.py:276} INFO - 21/05/15 13:06:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384827343187990773522_0004_m_000073_252, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384827343187990773522_0004_m_000073_252}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384827343187990773522_0004}; taskId=attempt_202105151305384827343187990773522_0004_m_000073_252, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@798bfda1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:35 INFO StagingCommitter: Starting: Task committer attempt_202105151305384827343187990773522_0004_m_000073_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384827343187990773522_0004_m_000073_252
[2021-05-15 10:06:35,911] {docker.py:276} INFO - 21/05/15 13:06:35 INFO StagingCommitter: Task committer attempt_202105151305384827343187990773522_0004_m_000073_252: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384827343187990773522_0004_m_000073_252 : duration 0:00.004s
[2021-05-15 10:06:36,002] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305381321270644684918310_0004_m_000070_249: needsTaskCommit() Task attempt_202105151305381321270644684918310_0004_m_000070_249
[2021-05-15 10:06:36,003] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_202105151305381321270644684918310_0004_m_000070_249: needsTaskCommit() Task attempt_202105151305381321270644684918310_0004_m_000070_249: duration 0:00.001s
21/05/15 13:06:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381321270644684918310_0004_m_000070_249
[2021-05-15 10:06:36,005] {docker.py:276} INFO - 21/05/15 13:06:36 INFO Executor: Finished task 70.0 in stage 4.0 (TID 249). 4544 bytes result sent to driver
[2021-05-15 10:06:36,007] {docker.py:276} INFO - 21/05/15 13:06:36 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 253) (4703d70ea67c, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:36 INFO Executor: Running task 74.0 in stage 4.0 (TID 253)
21/05/15 13:06:36 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 249) in 2445 ms on 4703d70ea67c (executor driver) (71/200)
[2021-05-15 10:06:36,017] {docker.py:276} INFO - 21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:36,019] {docker.py:276} INFO - 21/05/15 13:06:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:36,019] {docker.py:276} INFO - 21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383529514471444400983_0004_m_000074_253, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383529514471444400983_0004_m_000074_253}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383529514471444400983_0004}; taskId=attempt_202105151305383529514471444400983_0004_m_000074_253, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7184a54c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305383529514471444400983_0004_m_000074_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383529514471444400983_0004_m_000074_253
[2021-05-15 10:06:36,022] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_202105151305383529514471444400983_0004_m_000074_253: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383529514471444400983_0004_m_000074_253 : duration 0:00.003s
[2021-05-15 10:06:36,061] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305381913897672285382478_0004_m_000072_251: needsTaskCommit() Task attempt_202105151305381913897672285382478_0004_m_000072_251
[2021-05-15 10:06:36,062] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_202105151305381913897672285382478_0004_m_000072_251: needsTaskCommit() Task attempt_202105151305381913897672285382478_0004_m_000072_251: duration 0:00.001s
21/05/15 13:06:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381913897672285382478_0004_m_000072_251
[2021-05-15 10:06:36,064] {docker.py:276} INFO - 21/05/15 13:06:36 INFO Executor: Finished task 72.0 in stage 4.0 (TID 251). 4544 bytes result sent to driver
[2021-05-15 10:06:36,065] {docker.py:276} INFO - 21/05/15 13:06:36 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 254) (4703d70ea67c, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:36,066] {docker.py:276} INFO - 21/05/15 13:06:36 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 251) in 1925 ms on 4703d70ea67c (executor driver) (72/200)
[2021-05-15 10:06:36,068] {docker.py:276} INFO - 21/05/15 13:06:36 INFO Executor: Running task 75.0 in stage 4.0 (TID 254)
[2021-05-15 10:06:36,077] {docker.py:276} INFO - 21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:36,079] {docker.py:276} INFO - 21/05/15 13:06:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:36,079] {docker.py:276} INFO - 21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384984156730034374630_0004_m_000075_254, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384984156730034374630_0004_m_000075_254}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384984156730034374630_0004}; taskId=attempt_202105151305384984156730034374630_0004_m_000075_254, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3abee464}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:36,079] {docker.py:276} INFO - 21/05/15 13:06:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:36,080] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305384984156730034374630_0004_m_000075_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384984156730034374630_0004_m_000075_254
[2021-05-15 10:06:36,081] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_202105151305384984156730034374630_0004_m_000075_254: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384984156730034374630_0004_m_000075_254 : duration 0:00.003s
[2021-05-15 10:06:36,117] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_20210515130538806129202650630367_0004_m_000071_250: needsTaskCommit() Task attempt_20210515130538806129202650630367_0004_m_000071_250
21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_20210515130538806129202650630367_0004_m_000071_250: needsTaskCommit() Task attempt_20210515130538806129202650630367_0004_m_000071_250: duration 0:00.001s
21/05/15 13:06:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538806129202650630367_0004_m_000071_250
[2021-05-15 10:06:36,119] {docker.py:276} INFO - 21/05/15 13:06:36 INFO Executor: Finished task 71.0 in stage 4.0 (TID 250). 4544 bytes result sent to driver
[2021-05-15 10:06:36,121] {docker.py:276} INFO - 21/05/15 13:06:36 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 255) (4703d70ea67c, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:36,122] {docker.py:276} INFO - 21/05/15 13:06:36 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 250) in 2479 ms on 4703d70ea67c (executor driver) (73/200)
21/05/15 13:06:36 INFO Executor: Running task 76.0 in stage 4.0 (TID 255)
[2021-05-15 10:06:36,142] {docker.py:276} INFO - 21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:36,144] {docker.py:276} INFO - 21/05/15 13:06:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:36,144] {docker.py:276} INFO - 21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:36,145] {docker.py:276} INFO - 21/05/15 13:06:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538785231786886421327_0004_m_000076_255, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538785231786886421327_0004_m_000076_255}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538785231786886421327_0004}; taskId=attempt_20210515130538785231786886421327_0004_m_000076_255, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f2ee6d1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:36,145] {docker.py:276} INFO - 21/05/15 13:06:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:36 INFO StagingCommitter: Starting: Task committer attempt_20210515130538785231786886421327_0004_m_000076_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538785231786886421327_0004_m_000076_255
[2021-05-15 10:06:36,148] {docker.py:276} INFO - 21/05/15 13:06:36 INFO StagingCommitter: Task committer attempt_20210515130538785231786886421327_0004_m_000076_255: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538785231786886421327_0004_m_000076_255 : duration 0:00.003s
[2021-05-15 10:06:38,316] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305384984156730034374630_0004_m_000075_254: needsTaskCommit() Task attempt_202105151305384984156730034374630_0004_m_000075_254
[2021-05-15 10:06:38,317] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305384984156730034374630_0004_m_000075_254: needsTaskCommit() Task attempt_202105151305384984156730034374630_0004_m_000075_254: duration 0:00.001s
21/05/15 13:06:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384984156730034374630_0004_m_000075_254
[2021-05-15 10:06:38,320] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Finished task 75.0 in stage 4.0 (TID 254). 4587 bytes result sent to driver
21/05/15 13:06:38 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 256) (4703d70ea67c, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:38,321] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Running task 77.0 in stage 4.0 (TID 256)
21/05/15 13:06:38 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 254) in 2259 ms on 4703d70ea67c (executor driver) (74/200)
[2021-05-15 10:06:38,332] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:38,333] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,334] {docker.py:276} INFO - 21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386672525900156008690_0004_m_000077_256, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386672525900156008690_0004_m_000077_256}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386672525900156008690_0004}; taskId=attempt_202105151305386672525900156008690_0004_m_000077_256, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@241f256c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305386672525900156008690_0004_m_000077_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386672525900156008690_0004_m_000077_256
[2021-05-15 10:06:38,336] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305386672525900156008690_0004_m_000077_256: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386672525900156008690_0004_m_000077_256 : duration 0:00.003s
[2021-05-15 10:06:38,593] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305383529514471444400983_0004_m_000074_253: needsTaskCommit() Task attempt_202105151305383529514471444400983_0004_m_000074_253
[2021-05-15 10:06:38,594] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305383529514471444400983_0004_m_000074_253: needsTaskCommit() Task attempt_202105151305383529514471444400983_0004_m_000074_253: duration 0:00.000s
21/05/15 13:06:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383529514471444400983_0004_m_000074_253
[2021-05-15 10:06:38,594] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Finished task 74.0 in stage 4.0 (TID 253). 4587 bytes result sent to driver
[2021-05-15 10:06:38,595] {docker.py:276} INFO - 21/05/15 13:06:38 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 257) (4703d70ea67c, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:38,596] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Running task 78.0 in stage 4.0 (TID 257)
21/05/15 13:06:38 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 253) in 2594 ms on 4703d70ea67c (executor driver) (75/200)
[2021-05-15 10:06:38,605] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:38,605] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:38,607] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:38,608] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:38,608] {docker.py:276} INFO - 21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,609] {docker.py:276} INFO - 21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387123309718645039136_0004_m_000078_257, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387123309718645039136_0004_m_000078_257}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387123309718645039136_0004}; taskId=attempt_202105151305387123309718645039136_0004_m_000078_257, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@543ddeb9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,609] {docker.py:276} INFO - 21/05/15 13:06:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:38,609] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305387123309718645039136_0004_m_000078_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387123309718645039136_0004_m_000078_257
[2021-05-15 10:06:38,612] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305387123309718645039136_0004_m_000078_257: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387123309718645039136_0004_m_000078_257 : duration 0:00.003s
[2021-05-15 10:06:38,613] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_20210515130538785231786886421327_0004_m_000076_255: needsTaskCommit() Task attempt_20210515130538785231786886421327_0004_m_000076_255
[2021-05-15 10:06:38,614] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_20210515130538785231786886421327_0004_m_000076_255: needsTaskCommit() Task attempt_20210515130538785231786886421327_0004_m_000076_255: duration 0:00.001s
[2021-05-15 10:06:38,614] {docker.py:276} INFO - 21/05/15 13:06:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538785231786886421327_0004_m_000076_255
[2021-05-15 10:06:38,615] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Finished task 76.0 in stage 4.0 (TID 255). 4587 bytes result sent to driver
[2021-05-15 10:06:38,616] {docker.py:276} INFO - 21/05/15 13:06:38 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 258) (4703d70ea67c, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:38,617] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Running task 79.0 in stage 4.0 (TID 258)
[2021-05-15 10:06:38,618] {docker.py:276} INFO - 21/05/15 13:06:38 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 255) in 2501 ms on 4703d70ea67c (executor driver) (76/200)
[2021-05-15 10:06:38,625] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:38,626] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:38,627] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:38,628] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:38,628] {docker.py:276} INFO - 21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,628] {docker.py:276} INFO - 21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382352036301905953250_0004_m_000079_258, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382352036301905953250_0004_m_000079_258}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382352036301905953250_0004}; taskId=attempt_202105151305382352036301905953250_0004_m_000079_258, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2deb3157}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,629] {docker.py:276} INFO - 21/05/15 13:06:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:38,629] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305382352036301905953250_0004_m_000079_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382352036301905953250_0004_m_000079_258
[2021-05-15 10:06:38,631] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305382352036301905953250_0004_m_000079_258: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382352036301905953250_0004_m_000079_258 : duration 0:00.002s
[2021-05-15 10:06:38,863] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305384827343187990773522_0004_m_000073_252: needsTaskCommit() Task attempt_202105151305384827343187990773522_0004_m_000073_252
[2021-05-15 10:06:38,864] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305384827343187990773522_0004_m_000073_252: needsTaskCommit() Task attempt_202105151305384827343187990773522_0004_m_000073_252: duration 0:00.001s
21/05/15 13:06:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384827343187990773522_0004_m_000073_252
[2021-05-15 10:06:38,867] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Finished task 73.0 in stage 4.0 (TID 252). 4587 bytes result sent to driver
[2021-05-15 10:06:38,868] {docker.py:276} INFO - 21/05/15 13:06:38 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 259) (4703d70ea67c, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:38,869] {docker.py:276} INFO - 21/05/15 13:06:38 INFO Executor: Running task 80.0 in stage 4.0 (TID 259)
[2021-05-15 10:06:38,869] {docker.py:276} INFO - 21/05/15 13:06:38 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 252) in 2983 ms on 4703d70ea67c (executor driver) (77/200)
[2021-05-15 10:06:38,879] {docker.py:276} INFO - 21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:38,880] {docker.py:276} INFO - 21/05/15 13:06:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387572958342507758721_0004_m_000080_259, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387572958342507758721_0004_m_000080_259}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387572958342507758721_0004}; taskId=attempt_202105151305387572958342507758721_0004_m_000080_259, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6bfbb677}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:38,881] {docker.py:276} INFO - 21/05/15 13:06:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305387572958342507758721_0004_m_000080_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387572958342507758721_0004_m_000080_259
[2021-05-15 10:06:38,883] {docker.py:276} INFO - 21/05/15 13:06:38 INFO StagingCommitter: Task committer attempt_202105151305387572958342507758721_0004_m_000080_259: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387572958342507758721_0004_m_000080_259 : duration 0:00.003s
[2021-05-15 10:06:40,486] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305387123309718645039136_0004_m_000078_257: needsTaskCommit() Task attempt_202105151305387123309718645039136_0004_m_000078_257
[2021-05-15 10:06:40,487] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Task committer attempt_202105151305387123309718645039136_0004_m_000078_257: needsTaskCommit() Task attempt_202105151305387123309718645039136_0004_m_000078_257: duration 0:00.001s
[2021-05-15 10:06:40,487] {docker.py:276} INFO - 21/05/15 13:06:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387123309718645039136_0004_m_000078_257
[2021-05-15 10:06:40,489] {docker.py:276} INFO - 21/05/15 13:06:40 INFO Executor: Finished task 78.0 in stage 4.0 (TID 257). 4544 bytes result sent to driver
[2021-05-15 10:06:40,490] {docker.py:276} INFO - 21/05/15 13:06:40 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 260) (4703d70ea67c, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:40,491] {docker.py:276} INFO - 21/05/15 13:06:40 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 257) in 1898 ms on 4703d70ea67c (executor driver) (78/200)
21/05/15 13:06:40 INFO Executor: Running task 81.0 in stage 4.0 (TID 260)
[2021-05-15 10:06:40,502] {docker.py:276} INFO - 21/05/15 13:06:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:40,504] {docker.py:276} INFO - 21/05/15 13:06:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388498835331769558101_0004_m_000081_260, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388498835331769558101_0004_m_000081_260}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388498835331769558101_0004}; taskId=attempt_202105151305388498835331769558101_0004_m_000081_260, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@400d5615}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:40,504] {docker.py:276} INFO - 21/05/15 13:06:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305388498835331769558101_0004_m_000081_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388498835331769558101_0004_m_000081_260
[2021-05-15 10:06:40,507] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Task committer attempt_202105151305388498835331769558101_0004_m_000081_260: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388498835331769558101_0004_m_000081_260 : duration 0:00.003s
[2021-05-15 10:06:40,760] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305386672525900156008690_0004_m_000077_256: needsTaskCommit() Task attempt_202105151305386672525900156008690_0004_m_000077_256
[2021-05-15 10:06:40,761] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Task committer attempt_202105151305386672525900156008690_0004_m_000077_256: needsTaskCommit() Task attempt_202105151305386672525900156008690_0004_m_000077_256: duration 0:00.000s
21/05/15 13:06:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386672525900156008690_0004_m_000077_256
[2021-05-15 10:06:40,763] {docker.py:276} INFO - 21/05/15 13:06:40 INFO Executor: Finished task 77.0 in stage 4.0 (TID 256). 4544 bytes result sent to driver
[2021-05-15 10:06:40,764] {docker.py:276} INFO - 21/05/15 13:06:40 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 261) (4703d70ea67c, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:40 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 256) in 2448 ms on 4703d70ea67c (executor driver) (79/200)
[2021-05-15 10:06:40,765] {docker.py:276} INFO - 21/05/15 13:06:40 INFO Executor: Running task 82.0 in stage 4.0 (TID 261)
[2021-05-15 10:06:40,775] {docker.py:276} INFO - 21/05/15 13:06:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:40,777] {docker.py:276} INFO - 21/05/15 13:06:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:40,778] {docker.py:276} INFO - 21/05/15 13:06:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388161772977107242284_0004_m_000082_261, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388161772977107242284_0004_m_000082_261}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388161772977107242284_0004}; taskId=attempt_202105151305388161772977107242284_0004_m_000082_261, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@639c884d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:40,779] {docker.py:276} INFO - 21/05/15 13:06:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305388161772977107242284_0004_m_000082_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388161772977107242284_0004_m_000082_261
[2021-05-15 10:06:40,782] {docker.py:276} INFO - 21/05/15 13:06:40 INFO StagingCommitter: Task committer attempt_202105151305388161772977107242284_0004_m_000082_261: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388161772977107242284_0004_m_000082_261 : duration 0:00.005s
[2021-05-15 10:06:41,097] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305382352036301905953250_0004_m_000079_258: needsTaskCommit() Task attempt_202105151305382352036301905953250_0004_m_000079_258
[2021-05-15 10:06:41,098] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Task committer attempt_202105151305382352036301905953250_0004_m_000079_258: needsTaskCommit() Task attempt_202105151305382352036301905953250_0004_m_000079_258: duration 0:00.001s
[2021-05-15 10:06:41,099] {docker.py:276} INFO - 21/05/15 13:06:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382352036301905953250_0004_m_000079_258
[2021-05-15 10:06:41,100] {docker.py:276} INFO - 21/05/15 13:06:41 INFO Executor: Finished task 79.0 in stage 4.0 (TID 258). 4544 bytes result sent to driver
[2021-05-15 10:06:41,101] {docker.py:276} INFO - 21/05/15 13:06:41 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 262) (4703d70ea67c, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:41,102] {docker.py:276} INFO - 21/05/15 13:06:41 INFO Executor: Running task 83.0 in stage 4.0 (TID 262)
[2021-05-15 10:06:41,103] {docker.py:276} INFO - 21/05/15 13:06:41 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 258) in 2490 ms on 4703d70ea67c (executor driver) (80/200)
[2021-05-15 10:06:41,112] {docker.py:276} INFO - 21/05/15 13:06:41 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:41,115] {docker.py:276} INFO - 21/05/15 13:06:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:41,115] {docker.py:276} INFO - 21/05/15 13:06:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:41,116] {docker.py:276} INFO - 21/05/15 13:06:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384612168053490614439_0004_m_000083_262, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384612168053490614439_0004_m_000083_262}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384612168053490614439_0004}; taskId=attempt_202105151305384612168053490614439_0004_m_000083_262, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c36c589}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:41,116] {docker.py:276} INFO - 21/05/15 13:06:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:41,116] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305384612168053490614439_0004_m_000083_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384612168053490614439_0004_m_000083_262
[2021-05-15 10:06:41,118] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Task committer attempt_202105151305384612168053490614439_0004_m_000083_262: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384612168053490614439_0004_m_000083_262 : duration 0:00.003s
[2021-05-15 10:06:41,428] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305387572958342507758721_0004_m_000080_259: needsTaskCommit() Task attempt_202105151305387572958342507758721_0004_m_000080_259
[2021-05-15 10:06:41,429] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Task committer attempt_202105151305387572958342507758721_0004_m_000080_259: needsTaskCommit() Task attempt_202105151305387572958342507758721_0004_m_000080_259: duration 0:00.001s
21/05/15 13:06:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387572958342507758721_0004_m_000080_259
[2021-05-15 10:06:41,431] {docker.py:276} INFO - 21/05/15 13:06:41 INFO Executor: Finished task 80.0 in stage 4.0 (TID 259). 4544 bytes result sent to driver
[2021-05-15 10:06:41,432] {docker.py:276} INFO - 21/05/15 13:06:41 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 263) (4703d70ea67c, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:41,433] {docker.py:276} INFO - 21/05/15 13:06:41 INFO Executor: Running task 84.0 in stage 4.0 (TID 263)
[2021-05-15 10:06:41,434] {docker.py:276} INFO - 21/05/15 13:06:41 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 259) in 2570 ms on 4703d70ea67c (executor driver) (81/200)
[2021-05-15 10:06:41,443] {docker.py:276} INFO - 21/05/15 13:06:41 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:41,445] {docker.py:276} INFO - 21/05/15 13:06:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381029895586565900616_0004_m_000084_263, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381029895586565900616_0004_m_000084_263}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381029895586565900616_0004}; taskId=attempt_202105151305381029895586565900616_0004_m_000084_263, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cd0d829}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:41,446] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305381029895586565900616_0004_m_000084_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381029895586565900616_0004_m_000084_263
[2021-05-15 10:06:41,449] {docker.py:276} INFO - 21/05/15 13:06:41 INFO StagingCommitter: Task committer attempt_202105151305381029895586565900616_0004_m_000084_263: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381029895586565900616_0004_m_000084_263 : duration 0:00.004s
[2021-05-15 10:06:42,534] {docker.py:276} INFO - 21/05/15 13:06:42 INFO StagingCommitter: Starting: Task committer attempt_202105151305388498835331769558101_0004_m_000081_260: needsTaskCommit() Task attempt_202105151305388498835331769558101_0004_m_000081_260
21/05/15 13:06:42 INFO StagingCommitter: Task committer attempt_202105151305388498835331769558101_0004_m_000081_260: needsTaskCommit() Task attempt_202105151305388498835331769558101_0004_m_000081_260: duration 0:00.001s
[2021-05-15 10:06:42,535] {docker.py:276} INFO - 21/05/15 13:06:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388498835331769558101_0004_m_000081_260
[2021-05-15 10:06:42,536] {docker.py:276} INFO - 21/05/15 13:06:42 INFO Executor: Finished task 81.0 in stage 4.0 (TID 260). 4544 bytes result sent to driver
[2021-05-15 10:06:42,537] {docker.py:276} INFO - 21/05/15 13:06:42 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 264) (4703d70ea67c, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:42,539] {docker.py:276} INFO - 21/05/15 13:06:42 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 260) in 2051 ms on 4703d70ea67c (executor driver) (82/200)
21/05/15 13:06:42 INFO Executor: Running task 85.0 in stage 4.0 (TID 264)
[2021-05-15 10:06:42,548] {docker.py:276} INFO - 21/05/15 13:06:42 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:42,549] {docker.py:276} INFO - 21/05/15 13:06:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:42,551] {docker.py:276} INFO - 21/05/15 13:06:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:42,551] {docker.py:276} INFO - 21/05/15 13:06:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382695041751593006126_0004_m_000085_264, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382695041751593006126_0004_m_000085_264}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382695041751593006126_0004}; taskId=attempt_202105151305382695041751593006126_0004_m_000085_264, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5efe98d7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:42,552] {docker.py:276} INFO - 21/05/15 13:06:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:42,552] {docker.py:276} INFO - 21/05/15 13:06:42 INFO StagingCommitter: Starting: Task committer attempt_202105151305382695041751593006126_0004_m_000085_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382695041751593006126_0004_m_000085_264
[2021-05-15 10:06:42,555] {docker.py:276} INFO - 21/05/15 13:06:42 INFO StagingCommitter: Task committer attempt_202105151305382695041751593006126_0004_m_000085_264: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382695041751593006126_0004_m_000085_264 : duration 0:00.003s
[2021-05-15 10:06:43,270] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305388161772977107242284_0004_m_000082_261: needsTaskCommit() Task attempt_202105151305388161772977107242284_0004_m_000082_261
[2021-05-15 10:06:43,270] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305388161772977107242284_0004_m_000082_261: needsTaskCommit() Task attempt_202105151305388161772977107242284_0004_m_000082_261: duration 0:00.000s
21/05/15 13:06:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388161772977107242284_0004_m_000082_261
[2021-05-15 10:06:43,272] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Finished task 82.0 in stage 4.0 (TID 261). 4544 bytes result sent to driver
[2021-05-15 10:06:43,273] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 265) (4703d70ea67c, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:43,274] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 261) in 2514 ms on 4703d70ea67c (executor driver) (83/200)
[2021-05-15 10:06:43,275] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Running task 86.0 in stage 4.0 (TID 265)
[2021-05-15 10:06:43,283] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:43,284] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:43,286] {docker.py:276} INFO - 21/05/15 13:06:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:43,287] {docker.py:276} INFO - 21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381558644469262091003_0004_m_000086_265, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381558644469262091003_0004_m_000086_265}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381558644469262091003_0004}; taskId=attempt_202105151305381558644469262091003_0004_m_000086_265, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20cd5bcd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305381558644469262091003_0004_m_000086_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381558644469262091003_0004_m_000086_265
[2021-05-15 10:06:43,290] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305381558644469262091003_0004_m_000086_265: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381558644469262091003_0004_m_000086_265 : duration 0:00.004s
[2021-05-15 10:06:43,600] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305384612168053490614439_0004_m_000083_262: needsTaskCommit() Task attempt_202105151305384612168053490614439_0004_m_000083_262
21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305384612168053490614439_0004_m_000083_262: needsTaskCommit() Task attempt_202105151305384612168053490614439_0004_m_000083_262: duration 0:00.001s
21/05/15 13:06:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384612168053490614439_0004_m_000083_262
[2021-05-15 10:06:43,602] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Finished task 83.0 in stage 4.0 (TID 262). 4544 bytes result sent to driver
[2021-05-15 10:06:43,604] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 266) (4703d70ea67c, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:43,605] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Running task 87.0 in stage 4.0 (TID 266)
[2021-05-15 10:06:43,606] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 262) in 2508 ms on 4703d70ea67c (executor driver) (84/200)
[2021-05-15 10:06:43,614] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:43,615] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:43,617] {docker.py:276} INFO - 21/05/15 13:06:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387146504847240991686_0004_m_000087_266, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387146504847240991686_0004_m_000087_266}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387146504847240991686_0004}; taskId=attempt_202105151305387146504847240991686_0004_m_000087_266, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5904d5b9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:43,617] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305387146504847240991686_0004_m_000087_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387146504847240991686_0004_m_000087_266
[2021-05-15 10:06:43,620] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305387146504847240991686_0004_m_000087_266: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387146504847240991686_0004_m_000087_266 : duration 0:00.003s
[2021-05-15 10:06:43,732] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305381029895586565900616_0004_m_000084_263: needsTaskCommit() Task attempt_202105151305381029895586565900616_0004_m_000084_263
[2021-05-15 10:06:43,733] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305381029895586565900616_0004_m_000084_263: needsTaskCommit() Task attempt_202105151305381029895586565900616_0004_m_000084_263: duration 0:00.002s
21/05/15 13:06:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381029895586565900616_0004_m_000084_263
[2021-05-15 10:06:43,736] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Finished task 84.0 in stage 4.0 (TID 263). 4544 bytes result sent to driver
[2021-05-15 10:06:43,738] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 267) (4703d70ea67c, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:43,739] {docker.py:276} INFO - 21/05/15 13:06:43 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 263) in 2310 ms on 4703d70ea67c (executor driver) (85/200)
[2021-05-15 10:06:43,740] {docker.py:276} INFO - 21/05/15 13:06:43 INFO Executor: Running task 88.0 in stage 4.0 (TID 267)
[2021-05-15 10:06:43,749] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:43,750] {docker.py:276} INFO - 21/05/15 13:06:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:43,751] {docker.py:276} INFO - 21/05/15 13:06:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:06:43,751] {docker.py:276} INFO - 21/05/15 13:06:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:43,752] {docker.py:276} INFO - 21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:43,752] {docker.py:276} INFO - 21/05/15 13:06:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387573866297483143290_0004_m_000088_267, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387573866297483143290_0004_m_000088_267}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387573866297483143290_0004}; taskId=attempt_202105151305387573866297483143290_0004_m_000088_267, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32268425}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:43,752] {docker.py:276} INFO - 21/05/15 13:06:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:43,753] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305387573866297483143290_0004_m_000088_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387573866297483143290_0004_m_000088_267
[2021-05-15 10:06:43,755] {docker.py:276} INFO - 21/05/15 13:06:43 INFO StagingCommitter: Task committer attempt_202105151305387573866297483143290_0004_m_000088_267: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387573866297483143290_0004_m_000088_267 : duration 0:00.003s
[2021-05-15 10:06:44,991] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305382695041751593006126_0004_m_000085_264: needsTaskCommit() Task attempt_202105151305382695041751593006126_0004_m_000085_264
21/05/15 13:06:45 INFO StagingCommitter: Task committer attempt_202105151305382695041751593006126_0004_m_000085_264: needsTaskCommit() Task attempt_202105151305382695041751593006126_0004_m_000085_264: duration 0:00.001s
21/05/15 13:06:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382695041751593006126_0004_m_000085_264
[2021-05-15 10:06:44,992] {docker.py:276} INFO - 21/05/15 13:06:45 INFO Executor: Finished task 85.0 in stage 4.0 (TID 264). 4544 bytes result sent to driver
[2021-05-15 10:06:44,993] {docker.py:276} INFO - 21/05/15 13:06:45 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 268) (4703d70ea67c, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:44,994] {docker.py:276} INFO - 21/05/15 13:06:45 INFO Executor: Running task 89.0 in stage 4.0 (TID 268)
[2021-05-15 10:06:44,995] {docker.py:276} INFO - 21/05/15 13:06:45 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 264) in 2460 ms on 4703d70ea67c (executor driver) (86/200)
[2021-05-15 10:06:45,005] {docker.py:276} INFO - 21/05/15 13:06:45 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:45,009] {docker.py:276} INFO - 21/05/15 13:06:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:45,009] {docker.py:276} INFO - 21/05/15 13:06:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386032596680618930509_0004_m_000089_268, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386032596680618930509_0004_m_000089_268}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386032596680618930509_0004}; taskId=attempt_202105151305386032596680618930509_0004_m_000089_268, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a9e0aba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305386032596680618930509_0004_m_000089_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386032596680618930509_0004_m_000089_268
[2021-05-15 10:06:45,011] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Task committer attempt_202105151305386032596680618930509_0004_m_000089_268: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386032596680618930509_0004_m_000089_268 : duration 0:00.003s
[2021-05-15 10:06:45,769] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305381558644469262091003_0004_m_000086_265: needsTaskCommit() Task attempt_202105151305381558644469262091003_0004_m_000086_265
[2021-05-15 10:06:45,770] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Task committer attempt_202105151305381558644469262091003_0004_m_000086_265: needsTaskCommit() Task attempt_202105151305381558644469262091003_0004_m_000086_265: duration 0:00.000s
21/05/15 13:06:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381558644469262091003_0004_m_000086_265
[2021-05-15 10:06:45,771] {docker.py:276} INFO - 21/05/15 13:06:45 INFO Executor: Finished task 86.0 in stage 4.0 (TID 265). 4544 bytes result sent to driver
[2021-05-15 10:06:45,773] {docker.py:276} INFO - 21/05/15 13:06:45 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 269) (4703d70ea67c, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:45,773] {docker.py:276} INFO - 21/05/15 13:06:45 INFO Executor: Running task 90.0 in stage 4.0 (TID 269)
[2021-05-15 10:06:45,775] {docker.py:276} INFO - 21/05/15 13:06:45 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 265) in 2505 ms on 4703d70ea67c (executor driver) (87/200)
[2021-05-15 10:06:45,784] {docker.py:276} INFO - 21/05/15 13:06:45 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:45,786] {docker.py:276} INFO - 21/05/15 13:06:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:45,787] {docker.py:276} INFO - 21/05/15 13:06:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384925492622580842380_0004_m_000090_269, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384925492622580842380_0004_m_000090_269}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384925492622580842380_0004}; taskId=attempt_202105151305384925492622580842380_0004_m_000090_269, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34df309e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:45,787] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305384925492622580842380_0004_m_000090_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384925492622580842380_0004_m_000090_269
[2021-05-15 10:06:45,790] {docker.py:276} INFO - 21/05/15 13:06:45 INFO StagingCommitter: Task committer attempt_202105151305384925492622580842380_0004_m_000090_269: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384925492622580842380_0004_m_000090_269 : duration 0:00.003s
[2021-05-15 10:06:45,980] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305387146504847240991686_0004_m_000087_266: needsTaskCommit() Task attempt_202105151305387146504847240991686_0004_m_000087_266
[2021-05-15 10:06:45,981] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Task committer attempt_202105151305387146504847240991686_0004_m_000087_266: needsTaskCommit() Task attempt_202105151305387146504847240991686_0004_m_000087_266: duration 0:00.001s
21/05/15 13:06:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387146504847240991686_0004_m_000087_266
[2021-05-15 10:06:45,982] {docker.py:276} INFO - 21/05/15 13:06:46 INFO Executor: Finished task 87.0 in stage 4.0 (TID 266). 4544 bytes result sent to driver
[2021-05-15 10:06:45,983] {docker.py:276} INFO - 21/05/15 13:06:46 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 270) (4703d70ea67c, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:45,984] {docker.py:276} INFO - 21/05/15 13:06:46 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 266) in 2384 ms on 4703d70ea67c (executor driver) (88/200)
[2021-05-15 10:06:45,985] {docker.py:276} INFO - 21/05/15 13:06:46 INFO Executor: Running task 91.0 in stage 4.0 (TID 270)
[2021-05-15 10:06:45,993] {docker.py:276} INFO - 21/05/15 13:06:46 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:45,994] {docker.py:276} INFO - 21/05/15 13:06:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381089388336796276881_0004_m_000091_270, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381089388336796276881_0004_m_000091_270}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381089388336796276881_0004}; taskId=attempt_202105151305381089388336796276881_0004_m_000091_270, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@465ccf30}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:45,995] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305381089388336796276881_0004_m_000091_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381089388336796276881_0004_m_000091_270
[2021-05-15 10:06:45,997] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Task committer attempt_202105151305381089388336796276881_0004_m_000091_270: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381089388336796276881_0004_m_000091_270 : duration 0:00.003s
[2021-05-15 10:06:46,061] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305387573866297483143290_0004_m_000088_267: needsTaskCommit() Task attempt_202105151305387573866297483143290_0004_m_000088_267
[2021-05-15 10:06:46,061] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Task committer attempt_202105151305387573866297483143290_0004_m_000088_267: needsTaskCommit() Task attempt_202105151305387573866297483143290_0004_m_000088_267: duration 0:00.000s
21/05/15 13:06:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387573866297483143290_0004_m_000088_267
[2021-05-15 10:06:46,063] {docker.py:276} INFO - 21/05/15 13:06:46 INFO Executor: Finished task 88.0 in stage 4.0 (TID 267). 4544 bytes result sent to driver
[2021-05-15 10:06:46,064] {docker.py:276} INFO - 21/05/15 13:06:46 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 271) (4703d70ea67c, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:46,065] {docker.py:276} INFO - 21/05/15 13:06:46 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 267) in 2331 ms on 4703d70ea67c (executor driver) (89/200)
[2021-05-15 10:06:46,066] {docker.py:276} INFO - 21/05/15 13:06:46 INFO Executor: Running task 92.0 in stage 4.0 (TID 271)
[2021-05-15 10:06:46,075] {docker.py:276} INFO - 21/05/15 13:06:46 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:46,077] {docker.py:276} INFO - 21/05/15 13:06:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388553007634291992137_0004_m_000092_271, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388553007634291992137_0004_m_000092_271}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388553007634291992137_0004}; taskId=attempt_202105151305388553007634291992137_0004_m_000092_271, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bf37aee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:46,077] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305388553007634291992137_0004_m_000092_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388553007634291992137_0004_m_000092_271
[2021-05-15 10:06:46,080] {docker.py:276} INFO - 21/05/15 13:06:46 INFO StagingCommitter: Task committer attempt_202105151305388553007634291992137_0004_m_000092_271: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388553007634291992137_0004_m_000092_271 : duration 0:00.003s
[2021-05-15 10:06:47,432] {docker.py:276} INFO - 21/05/15 13:06:47 INFO StagingCommitter: Starting: Task committer attempt_202105151305386032596680618930509_0004_m_000089_268: needsTaskCommit() Task attempt_202105151305386032596680618930509_0004_m_000089_268
21/05/15 13:06:47 INFO StagingCommitter: Task committer attempt_202105151305386032596680618930509_0004_m_000089_268: needsTaskCommit() Task attempt_202105151305386032596680618930509_0004_m_000089_268: duration 0:00.001s
21/05/15 13:06:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386032596680618930509_0004_m_000089_268
[2021-05-15 10:06:47,434] {docker.py:276} INFO - 21/05/15 13:06:47 INFO Executor: Finished task 89.0 in stage 4.0 (TID 268). 4544 bytes result sent to driver
[2021-05-15 10:06:47,435] {docker.py:276} INFO - 21/05/15 13:06:47 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 272) (4703d70ea67c, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:47,436] {docker.py:276} INFO - 21/05/15 13:06:47 INFO Executor: Running task 93.0 in stage 4.0 (TID 272)
[2021-05-15 10:06:47,437] {docker.py:276} INFO - 21/05/15 13:06:47 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 268) in 2447 ms on 4703d70ea67c (executor driver) (90/200)
[2021-05-15 10:06:47,447] {docker.py:276} INFO - 21/05/15 13:06:47 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:47,449] {docker.py:276} INFO - 21/05/15 13:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:47,450] {docker.py:276} INFO - 21/05/15 13:06:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538800724421388616639_0004_m_000093_272, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538800724421388616639_0004_m_000093_272}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538800724421388616639_0004}; taskId=attempt_20210515130538800724421388616639_0004_m_000093_272, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@10ae970e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:47,450] {docker.py:276} INFO - 21/05/15 13:06:47 INFO StagingCommitter: Starting: Task committer attempt_20210515130538800724421388616639_0004_m_000093_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538800724421388616639_0004_m_000093_272
[2021-05-15 10:06:47,452] {docker.py:276} INFO - 21/05/15 13:06:47 INFO StagingCommitter: Task committer attempt_20210515130538800724421388616639_0004_m_000093_272: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538800724421388616639_0004_m_000093_272 : duration 0:00.003s
[2021-05-15 10:06:48,175] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305384925492622580842380_0004_m_000090_269: needsTaskCommit() Task attempt_202105151305384925492622580842380_0004_m_000090_269
[2021-05-15 10:06:48,176] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305384925492622580842380_0004_m_000090_269: needsTaskCommit() Task attempt_202105151305384925492622580842380_0004_m_000090_269: duration 0:00.000s
21/05/15 13:06:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384925492622580842380_0004_m_000090_269
[2021-05-15 10:06:48,177] {docker.py:276} INFO - 21/05/15 13:06:48 INFO Executor: Finished task 90.0 in stage 4.0 (TID 269). 4587 bytes result sent to driver
[2021-05-15 10:06:48,179] {docker.py:276} INFO - 21/05/15 13:06:48 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 273) (4703d70ea67c, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:06:48 INFO Executor: Running task 94.0 in stage 4.0 (TID 273)
[2021-05-15 10:06:48,180] {docker.py:276} INFO - 21/05/15 13:06:48 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 269) in 2410 ms on 4703d70ea67c (executor driver) (91/200)
[2021-05-15 10:06:48,189] {docker.py:276} INFO - 21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:48,191] {docker.py:276} INFO - 21/05/15 13:06:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:48,192] {docker.py:276} INFO - 21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381332358948164084700_0004_m_000094_273, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381332358948164084700_0004_m_000094_273}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381332358948164084700_0004}; taskId=attempt_202105151305381332358948164084700_0004_m_000094_273, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dd07183}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305381332358948164084700_0004_m_000094_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381332358948164084700_0004_m_000094_273
[2021-05-15 10:06:48,195] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305381332358948164084700_0004_m_000094_273: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381332358948164084700_0004_m_000094_273 : duration 0:00.003s
[2021-05-15 10:06:48,288] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305381089388336796276881_0004_m_000091_270: needsTaskCommit() Task attempt_202105151305381089388336796276881_0004_m_000091_270
[2021-05-15 10:06:48,290] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305381089388336796276881_0004_m_000091_270: needsTaskCommit() Task attempt_202105151305381089388336796276881_0004_m_000091_270: duration 0:00.001s
[2021-05-15 10:06:48,290] {docker.py:276} INFO - 21/05/15 13:06:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381089388336796276881_0004_m_000091_270
[2021-05-15 10:06:48,292] {docker.py:276} INFO - 21/05/15 13:06:48 INFO Executor: Finished task 91.0 in stage 4.0 (TID 270). 4587 bytes result sent to driver
[2021-05-15 10:06:48,293] {docker.py:276} INFO - 21/05/15 13:06:48 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 274) (4703d70ea67c, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:48,294] {docker.py:276} INFO - 21/05/15 13:06:48 INFO Executor: Running task 95.0 in stage 4.0 (TID 274)
21/05/15 13:06:48 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 270) in 2313 ms on 4703d70ea67c (executor driver) (92/200)
[2021-05-15 10:06:48,309] {docker.py:276} INFO - 21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-15 10:06:48,312] {docker.py:276} INFO - 21/05/15 13:06:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383312780994858691000_0004_m_000095_274, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383312780994858691000_0004_m_000095_274}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383312780994858691000_0004}; taskId=attempt_202105151305383312780994858691000_0004_m_000095_274, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e81dde6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305383312780994858691000_0004_m_000095_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383312780994858691000_0004_m_000095_274
[2021-05-15 10:06:48,314] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305383312780994858691000_0004_m_000095_274: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383312780994858691000_0004_m_000095_274 : duration 0:00.003s
[2021-05-15 10:06:48,387] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305388553007634291992137_0004_m_000092_271: needsTaskCommit() Task attempt_202105151305388553007634291992137_0004_m_000092_271
[2021-05-15 10:06:48,388] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305388553007634291992137_0004_m_000092_271: needsTaskCommit() Task attempt_202105151305388553007634291992137_0004_m_000092_271: duration 0:00.000s
21/05/15 13:06:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388553007634291992137_0004_m_000092_271
[2021-05-15 10:06:48,389] {docker.py:276} INFO - 21/05/15 13:06:48 INFO Executor: Finished task 92.0 in stage 4.0 (TID 271). 4587 bytes result sent to driver
[2021-05-15 10:06:48,390] {docker.py:276} INFO - 21/05/15 13:06:48 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 275) (4703d70ea67c, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:48,391] {docker.py:276} INFO - 21/05/15 13:06:48 INFO Executor: Running task 96.0 in stage 4.0 (TID 275)
[2021-05-15 10:06:48,392] {docker.py:276} INFO - 21/05/15 13:06:48 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 271) in 2331 ms on 4703d70ea67c (executor driver) (93/200)
[2021-05-15 10:06:48,399] {docker.py:276} INFO - 21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:48,401] {docker.py:276} INFO - 21/05/15 13:06:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305389081497071705630932_0004_m_000096_275, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389081497071705630932_0004_m_000096_275}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305389081497071705630932_0004}; taskId=attempt_202105151305389081497071705630932_0004_m_000096_275, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bbbfec1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:48,402] {docker.py:276} INFO - 21/05/15 13:06:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305389081497071705630932_0004_m_000096_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389081497071705630932_0004_m_000096_275
[2021-05-15 10:06:48,405] {docker.py:276} INFO - 21/05/15 13:06:48 INFO StagingCommitter: Task committer attempt_202105151305389081497071705630932_0004_m_000096_275: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389081497071705630932_0004_m_000096_275 : duration 0:00.003s
[2021-05-15 10:06:49,763] {docker.py:276} INFO - 21/05/15 13:06:49 INFO StagingCommitter: Starting: Task committer attempt_20210515130538800724421388616639_0004_m_000093_272: needsTaskCommit() Task attempt_20210515130538800724421388616639_0004_m_000093_272
[2021-05-15 10:06:49,765] {docker.py:276} INFO - 21/05/15 13:06:49 INFO StagingCommitter: Task committer attempt_20210515130538800724421388616639_0004_m_000093_272: needsTaskCommit() Task attempt_20210515130538800724421388616639_0004_m_000093_272: duration 0:00.002s
21/05/15 13:06:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538800724421388616639_0004_m_000093_272
[2021-05-15 10:06:49,766] {docker.py:276} INFO - 21/05/15 13:06:49 INFO Executor: Finished task 93.0 in stage 4.0 (TID 272). 4587 bytes result sent to driver
[2021-05-15 10:06:49,768] {docker.py:276} INFO - 21/05/15 13:06:49 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 276) (4703d70ea67c, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:49,769] {docker.py:276} INFO - 21/05/15 13:06:49 INFO Executor: Running task 97.0 in stage 4.0 (TID 276)
[2021-05-15 10:06:49,769] {docker.py:276} INFO - 21/05/15 13:06:49 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 272) in 2337 ms on 4703d70ea67c (executor driver) (94/200)
[2021-05-15 10:06:49,778] {docker.py:276} INFO - 21/05/15 13:06:49 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:49,780] {docker.py:276} INFO - 21/05/15 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513053810287402920337113_0004_m_000097_276, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053810287402920337113_0004_m_000097_276}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513053810287402920337113_0004}; taskId=attempt_2021051513053810287402920337113_0004_m_000097_276, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@291f913b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:49,780] {docker.py:276} INFO - 21/05/15 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:49 INFO StagingCommitter: Starting: Task committer attempt_2021051513053810287402920337113_0004_m_000097_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053810287402920337113_0004_m_000097_276
[2021-05-15 10:06:49,783] {docker.py:276} INFO - 21/05/15 13:06:49 INFO StagingCommitter: Task committer attempt_2021051513053810287402920337113_0004_m_000097_276: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053810287402920337113_0004_m_000097_276 : duration 0:00.003s
[2021-05-15 10:06:50,698] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305381332358948164084700_0004_m_000094_273: needsTaskCommit() Task attempt_202105151305381332358948164084700_0004_m_000094_273
[2021-05-15 10:06:50,700] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Task committer attempt_202105151305381332358948164084700_0004_m_000094_273: needsTaskCommit() Task attempt_202105151305381332358948164084700_0004_m_000094_273: duration 0:00.001s
21/05/15 13:06:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381332358948164084700_0004_m_000094_273
[2021-05-15 10:06:50,701] {docker.py:276} INFO - 21/05/15 13:06:50 INFO Executor: Finished task 94.0 in stage 4.0 (TID 273). 4544 bytes result sent to driver
[2021-05-15 10:06:50,704] {docker.py:276} INFO - 21/05/15 13:06:50 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 277) (4703d70ea67c, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:50,705] {docker.py:276} INFO - 21/05/15 13:06:50 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 273) in 2530 ms on 4703d70ea67c (executor driver) (95/200)
21/05/15 13:06:50 INFO Executor: Running task 98.0 in stage 4.0 (TID 277)
[2021-05-15 10:06:50,714] {docker.py:276} INFO - 21/05/15 13:06:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:50,717] {docker.py:276} INFO - 21/05/15 13:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385116292509147715186_0004_m_000098_277, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385116292509147715186_0004_m_000098_277}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385116292509147715186_0004}; taskId=attempt_202105151305385116292509147715186_0004_m_000098_277, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4433762d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305385116292509147715186_0004_m_000098_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385116292509147715186_0004_m_000098_277
[2021-05-15 10:06:50,723] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Task committer attempt_202105151305385116292509147715186_0004_m_000098_277: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385116292509147715186_0004_m_000098_277 : duration 0:00.003s
[2021-05-15 10:06:50,817] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305383312780994858691000_0004_m_000095_274: needsTaskCommit() Task attempt_202105151305383312780994858691000_0004_m_000095_274
[2021-05-15 10:06:50,818] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Task committer attempt_202105151305383312780994858691000_0004_m_000095_274: needsTaskCommit() Task attempt_202105151305383312780994858691000_0004_m_000095_274: duration 0:00.001s
21/05/15 13:06:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383312780994858691000_0004_m_000095_274
[2021-05-15 10:06:50,819] {docker.py:276} INFO - 21/05/15 13:06:50 INFO Executor: Finished task 95.0 in stage 4.0 (TID 274). 4544 bytes result sent to driver
[2021-05-15 10:06:50,821] {docker.py:276} INFO - 21/05/15 13:06:50 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 278) (4703d70ea67c, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:50,822] {docker.py:276} INFO - 21/05/15 13:06:50 INFO Executor: Running task 99.0 in stage 4.0 (TID 278)
21/05/15 13:06:50 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 274) in 2531 ms on 4703d70ea67c (executor driver) (96/200)
[2021-05-15 10:06:50,833] {docker.py:276} INFO - 21/05/15 13:06:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:50,837] {docker.py:276} INFO - 21/05/15 13:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381341486698470466331_0004_m_000099_278, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381341486698470466331_0004_m_000099_278}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381341486698470466331_0004}; taskId=attempt_202105151305381341486698470466331_0004_m_000099_278, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2cd9c314}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305381341486698470466331_0004_m_000099_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381341486698470466331_0004_m_000099_278
[2021-05-15 10:06:50,840] {docker.py:276} INFO - 21/05/15 13:06:50 INFO StagingCommitter: Task committer attempt_202105151305381341486698470466331_0004_m_000099_278: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381341486698470466331_0004_m_000099_278 : duration 0:00.003s
[2021-05-15 10:06:51,056] {docker.py:276} INFO - 21/05/15 13:06:51 INFO StagingCommitter: Starting: Task committer attempt_202105151305389081497071705630932_0004_m_000096_275: needsTaskCommit() Task attempt_202105151305389081497071705630932_0004_m_000096_275
21/05/15 13:06:51 INFO StagingCommitter: Task committer attempt_202105151305389081497071705630932_0004_m_000096_275: needsTaskCommit() Task attempt_202105151305389081497071705630932_0004_m_000096_275: duration 0:00.000s
21/05/15 13:06:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305389081497071705630932_0004_m_000096_275
[2021-05-15 10:06:51,057] {docker.py:276} INFO - 21/05/15 13:06:51 INFO Executor: Finished task 96.0 in stage 4.0 (TID 275). 4544 bytes result sent to driver
[2021-05-15 10:06:51,059] {docker.py:276} INFO - 21/05/15 13:06:51 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 279) (4703d70ea67c, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:51,059] {docker.py:276} INFO - 21/05/15 13:06:51 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 275) in 2673 ms on 4703d70ea67c (executor driver) (97/200)
21/05/15 13:06:51 INFO Executor: Running task 100.0 in stage 4.0 (TID 279)
[2021-05-15 10:06:51,069] {docker.py:276} INFO - 21/05/15 13:06:51 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:51,070] {docker.py:276} INFO - 21/05/15 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384351277871446326110_0004_m_000100_279, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384351277871446326110_0004_m_000100_279}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384351277871446326110_0004}; taskId=attempt_202105151305384351277871446326110_0004_m_000100_279, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3722a7df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:51 INFO StagingCommitter: Starting: Task committer attempt_202105151305384351277871446326110_0004_m_000100_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384351277871446326110_0004_m_000100_279
[2021-05-15 10:06:51,073] {docker.py:276} INFO - 21/05/15 13:06:51 INFO StagingCommitter: Task committer attempt_202105151305384351277871446326110_0004_m_000100_279: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384351277871446326110_0004_m_000100_279 : duration 0:00.003s
[2021-05-15 10:06:52,270] {docker.py:276} INFO - 21/05/15 13:06:52 INFO StagingCommitter: Starting: Task committer attempt_2021051513053810287402920337113_0004_m_000097_276: needsTaskCommit() Task attempt_2021051513053810287402920337113_0004_m_000097_276
[2021-05-15 10:06:52,271] {docker.py:276} INFO - 21/05/15 13:06:52 INFO StagingCommitter: Task committer attempt_2021051513053810287402920337113_0004_m_000097_276: needsTaskCommit() Task attempt_2021051513053810287402920337113_0004_m_000097_276: duration 0:00.001s
21/05/15 13:06:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513053810287402920337113_0004_m_000097_276
[2021-05-15 10:06:52,274] {docker.py:276} INFO - 21/05/15 13:06:52 INFO Executor: Finished task 97.0 in stage 4.0 (TID 276). 4544 bytes result sent to driver
[2021-05-15 10:06:52,275] {docker.py:276} INFO - 21/05/15 13:06:52 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 280) (4703d70ea67c, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:52,276] {docker.py:276} INFO - 21/05/15 13:06:52 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 276) in 2510 ms on 4703d70ea67c (executor driver) (98/200)
[2021-05-15 10:06:52,276] {docker.py:276} INFO - 21/05/15 13:06:52 INFO Executor: Running task 101.0 in stage 4.0 (TID 280)
[2021-05-15 10:06:52,285] {docker.py:276} INFO - 21/05/15 13:06:52 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:52,286] {docker.py:276} INFO - 21/05/15 13:06:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383096406682657441662_0004_m_000101_280, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383096406682657441662_0004_m_000101_280}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383096406682657441662_0004}; taskId=attempt_202105151305383096406682657441662_0004_m_000101_280, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33ec585e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:52,287] {docker.py:276} INFO - 21/05/15 13:06:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:52 INFO StagingCommitter: Starting: Task committer attempt_202105151305383096406682657441662_0004_m_000101_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383096406682657441662_0004_m_000101_280
[2021-05-15 10:06:52,289] {docker.py:276} INFO - 21/05/15 13:06:52 INFO StagingCommitter: Task committer attempt_202105151305383096406682657441662_0004_m_000101_280: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383096406682657441662_0004_m_000101_280 : duration 0:00.002s
[2021-05-15 10:06:53,038] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305385116292509147715186_0004_m_000098_277: needsTaskCommit() Task attempt_202105151305385116292509147715186_0004_m_000098_277
21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_202105151305385116292509147715186_0004_m_000098_277: needsTaskCommit() Task attempt_202105151305385116292509147715186_0004_m_000098_277: duration 0:00.001s
[2021-05-15 10:06:53,039] {docker.py:276} INFO - 21/05/15 13:06:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385116292509147715186_0004_m_000098_277
[2021-05-15 10:06:53,042] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Finished task 98.0 in stage 4.0 (TID 277). 4544 bytes result sent to driver
[2021-05-15 10:06:53,043] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 281) (4703d70ea67c, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:53,044] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Running task 102.0 in stage 4.0 (TID 281)
[2021-05-15 10:06:53,045] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 277) in 2344 ms on 4703d70ea67c (executor driver) (99/200)
[2021-05-15 10:06:53,054] {docker.py:276} INFO - 21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:53,055] {docker.py:276} INFO - 21/05/15 13:06:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384166138801221770390_0004_m_000102_281, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384166138801221770390_0004_m_000102_281}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384166138801221770390_0004}; taskId=attempt_202105151305384166138801221770390_0004_m_000102_281, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7332bae3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305384166138801221770390_0004_m_000102_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384166138801221770390_0004_m_000102_281
[2021-05-15 10:06:53,059] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_202105151305384166138801221770390_0004_m_000102_281: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384166138801221770390_0004_m_000102_281 : duration 0:00.003s
[2021-05-15 10:06:53,350] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305381341486698470466331_0004_m_000099_278: needsTaskCommit() Task attempt_202105151305381341486698470466331_0004_m_000099_278
[2021-05-15 10:06:53,351] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_202105151305381341486698470466331_0004_m_000099_278: needsTaskCommit() Task attempt_202105151305381341486698470466331_0004_m_000099_278: duration 0:00.001s
21/05/15 13:06:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381341486698470466331_0004_m_000099_278
[2021-05-15 10:06:53,354] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Finished task 99.0 in stage 4.0 (TID 278). 4544 bytes result sent to driver
[2021-05-15 10:06:53,355] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 282) (4703d70ea67c, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:53,356] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Running task 103.0 in stage 4.0 (TID 282)
[2021-05-15 10:06:53,357] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 278) in 2540 ms on 4703d70ea67c (executor driver) (100/200)
[2021-05-15 10:06:53,366] {docker.py:276} INFO - 21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:53,368] {docker.py:276} INFO - 21/05/15 13:06:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:53,368] {docker.py:276} INFO - 21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538920958002366999149_0004_m_000103_282, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538920958002366999149_0004_m_000103_282}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538920958002366999149_0004}; taskId=attempt_20210515130538920958002366999149_0004_m_000103_282, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66dbde8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:53,369] {docker.py:276} INFO - 21/05/15 13:06:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:53,369] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_20210515130538920958002366999149_0004_m_000103_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538920958002366999149_0004_m_000103_282
[2021-05-15 10:06:53,372] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_20210515130538920958002366999149_0004_m_000103_282: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538920958002366999149_0004_m_000103_282 : duration 0:00.003s
[2021-05-15 10:06:53,513] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305384351277871446326110_0004_m_000100_279: needsTaskCommit() Task attempt_202105151305384351277871446326110_0004_m_000100_279
[2021-05-15 10:06:53,514] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_202105151305384351277871446326110_0004_m_000100_279: needsTaskCommit() Task attempt_202105151305384351277871446326110_0004_m_000100_279: duration 0:00.001s
[2021-05-15 10:06:53,514] {docker.py:276} INFO - 21/05/15 13:06:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384351277871446326110_0004_m_000100_279
[2021-05-15 10:06:53,516] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Finished task 100.0 in stage 4.0 (TID 279). 4544 bytes result sent to driver
[2021-05-15 10:06:53,517] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 283) (4703d70ea67c, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:53,518] {docker.py:276} INFO - 21/05/15 13:06:53 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 279) in 2463 ms on 4703d70ea67c (executor driver) (101/200)
[2021-05-15 10:06:53,521] {docker.py:276} INFO - 21/05/15 13:06:53 INFO Executor: Running task 104.0 in stage 4.0 (TID 283)
[2021-05-15 10:06:53,530] {docker.py:276} INFO - 21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:53,532] {docker.py:276} INFO - 21/05/15 13:06:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385555169898430837598_0004_m_000104_283, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385555169898430837598_0004_m_000104_283}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385555169898430837598_0004}; taskId=attempt_202105151305385555169898430837598_0004_m_000104_283, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6216c054}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:53,532] {docker.py:276} INFO - 21/05/15 13:06:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305385555169898430837598_0004_m_000104_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385555169898430837598_0004_m_000104_283
[2021-05-15 10:06:53,536] {docker.py:276} INFO - 21/05/15 13:06:53 INFO StagingCommitter: Task committer attempt_202105151305385555169898430837598_0004_m_000104_283: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385555169898430837598_0004_m_000104_283 : duration 0:00.003s
[2021-05-15 10:06:54,800] {docker.py:276} INFO - 21/05/15 13:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105151305383096406682657441662_0004_m_000101_280: needsTaskCommit() Task attempt_202105151305383096406682657441662_0004_m_000101_280
[2021-05-15 10:06:54,801] {docker.py:276} INFO - 21/05/15 13:06:54 INFO StagingCommitter: Task committer attempt_202105151305383096406682657441662_0004_m_000101_280: needsTaskCommit() Task attempt_202105151305383096406682657441662_0004_m_000101_280: duration 0:00.001s
21/05/15 13:06:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383096406682657441662_0004_m_000101_280
[2021-05-15 10:06:54,804] {docker.py:276} INFO - 21/05/15 13:06:54 INFO Executor: Finished task 101.0 in stage 4.0 (TID 280). 4544 bytes result sent to driver
[2021-05-15 10:06:54,805] {docker.py:276} INFO - 21/05/15 13:06:54 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 284) (4703d70ea67c, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:54,806] {docker.py:276} INFO - 21/05/15 13:06:54 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 280) in 2535 ms on 4703d70ea67c (executor driver) (102/200)
[2021-05-15 10:06:54,806] {docker.py:276} INFO - 21/05/15 13:06:54 INFO Executor: Running task 105.0 in stage 4.0 (TID 284)
[2021-05-15 10:06:54,815] {docker.py:276} INFO - 21/05/15 13:06:54 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:54,816] {docker.py:276} INFO - 21/05/15 13:06:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387431189748963769002_0004_m_000105_284, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387431189748963769002_0004_m_000105_284}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387431189748963769002_0004}; taskId=attempt_202105151305387431189748963769002_0004_m_000105_284, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28e5cede}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:54,817] {docker.py:276} INFO - 21/05/15 13:06:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:54 INFO StagingCommitter: Starting: Task committer attempt_202105151305387431189748963769002_0004_m_000105_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387431189748963769002_0004_m_000105_284
[2021-05-15 10:06:54,819] {docker.py:276} INFO - 21/05/15 13:06:54 INFO StagingCommitter: Task committer attempt_202105151305387431189748963769002_0004_m_000105_284: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387431189748963769002_0004_m_000105_284 : duration 0:00.002s
[2021-05-15 10:06:55,585] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305384166138801221770390_0004_m_000102_281: needsTaskCommit() Task attempt_202105151305384166138801221770390_0004_m_000102_281
[2021-05-15 10:06:55,586] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_202105151305384166138801221770390_0004_m_000102_281: needsTaskCommit() Task attempt_202105151305384166138801221770390_0004_m_000102_281: duration 0:00.002s
21/05/15 13:06:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384166138801221770390_0004_m_000102_281
[2021-05-15 10:06:55,588] {docker.py:276} INFO - 21/05/15 13:06:55 INFO Executor: Finished task 102.0 in stage 4.0 (TID 281). 4544 bytes result sent to driver
[2021-05-15 10:06:55,589] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 285) (4703d70ea67c, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:55,590] {docker.py:276} INFO - 21/05/15 13:06:55 INFO Executor: Running task 106.0 in stage 4.0 (TID 285)
[2021-05-15 10:06:55,591] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 281) in 2516 ms on 4703d70ea67c (executor driver) (103/200)
[2021-05-15 10:06:55,600] {docker.py:276} INFO - 21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:55,602] {docker.py:276} INFO - 21/05/15 13:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513053831517203108754803_0004_m_000106_285, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053831517203108754803_0004_m_000106_285}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513053831517203108754803_0004}; taskId=attempt_2021051513053831517203108754803_0004_m_000106_285, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@695d6189}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:55,603] {docker.py:276} INFO - 21/05/15 13:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_2021051513053831517203108754803_0004_m_000106_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053831517203108754803_0004_m_000106_285
[2021-05-15 10:06:55,605] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_2021051513053831517203108754803_0004_m_000106_285: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053831517203108754803_0004_m_000106_285 : duration 0:00.003s
[2021-05-15 10:06:55,782] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_20210515130538920958002366999149_0004_m_000103_282: needsTaskCommit() Task attempt_20210515130538920958002366999149_0004_m_000103_282
[2021-05-15 10:06:55,783] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_20210515130538920958002366999149_0004_m_000103_282: needsTaskCommit() Task attempt_20210515130538920958002366999149_0004_m_000103_282: duration 0:00.001s
21/05/15 13:06:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538920958002366999149_0004_m_000103_282
[2021-05-15 10:06:55,783] {docker.py:276} INFO - 21/05/15 13:06:55 INFO Executor: Finished task 103.0 in stage 4.0 (TID 282). 4544 bytes result sent to driver
[2021-05-15 10:06:55,785] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 286) (4703d70ea67c, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:55,787] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 282) in 2399 ms on 4703d70ea67c (executor driver) (104/200)
21/05/15 13:06:55 INFO Executor: Running task 107.0 in stage 4.0 (TID 286)
[2021-05-15 10:06:55,795] {docker.py:276} INFO - 21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:55,797] {docker.py:276} INFO - 21/05/15 13:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386949533975900809788_0004_m_000107_286, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386949533975900809788_0004_m_000107_286}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386949533975900809788_0004}; taskId=attempt_202105151305386949533975900809788_0004_m_000107_286, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20ef87d3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305386949533975900809788_0004_m_000107_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386949533975900809788_0004_m_000107_286
[2021-05-15 10:06:55,800] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_202105151305386949533975900809788_0004_m_000107_286: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386949533975900809788_0004_m_000107_286 : duration 0:00.003s
[2021-05-15 10:06:55,870] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305385555169898430837598_0004_m_000104_283: needsTaskCommit() Task attempt_202105151305385555169898430837598_0004_m_000104_283
21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_202105151305385555169898430837598_0004_m_000104_283: needsTaskCommit() Task attempt_202105151305385555169898430837598_0004_m_000104_283: duration 0:00.000s
[2021-05-15 10:06:55,871] {docker.py:276} INFO - 21/05/15 13:06:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385555169898430837598_0004_m_000104_283
[2021-05-15 10:06:55,872] {docker.py:276} INFO - 21/05/15 13:06:55 INFO Executor: Finished task 104.0 in stage 4.0 (TID 283). 4544 bytes result sent to driver
[2021-05-15 10:06:55,873] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 287) (4703d70ea67c, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:55,874] {docker.py:276} INFO - 21/05/15 13:06:55 INFO Executor: Running task 108.0 in stage 4.0 (TID 287)
[2021-05-15 10:06:55,874] {docker.py:276} INFO - 21/05/15 13:06:55 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 283) in 2324 ms on 4703d70ea67c (executor driver) (105/200)
[2021-05-15 10:06:55,883] {docker.py:276} INFO - 21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:55,885] {docker.py:276} INFO - 21/05/15 13:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387254729059240264815_0004_m_000108_287, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387254729059240264815_0004_m_000108_287}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387254729059240264815_0004}; taskId=attempt_202105151305387254729059240264815_0004_m_000108_287, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@495856f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:55,885] {docker.py:276} INFO - 21/05/15 13:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:55 INFO StagingCommitter: Starting: Task committer attempt_202105151305387254729059240264815_0004_m_000108_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387254729059240264815_0004_m_000108_287
[2021-05-15 10:06:55,888] {docker.py:276} INFO - 21/05/15 13:06:55 INFO StagingCommitter: Task committer attempt_202105151305387254729059240264815_0004_m_000108_287: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387254729059240264815_0004_m_000108_287 : duration 0:00.003s
[2021-05-15 10:06:57,268] {docker.py:276} INFO - 21/05/15 13:06:57 INFO StagingCommitter: Starting: Task committer attempt_202105151305387431189748963769002_0004_m_000105_284: needsTaskCommit() Task attempt_202105151305387431189748963769002_0004_m_000105_284
[2021-05-15 10:06:57,270] {docker.py:276} INFO - 21/05/15 13:06:57 INFO StagingCommitter: Task committer attempt_202105151305387431189748963769002_0004_m_000105_284: needsTaskCommit() Task attempt_202105151305387431189748963769002_0004_m_000105_284: duration 0:00.001s
[2021-05-15 10:06:57,270] {docker.py:276} INFO - 21/05/15 13:06:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387431189748963769002_0004_m_000105_284
[2021-05-15 10:06:57,272] {docker.py:276} INFO - 21/05/15 13:06:57 INFO Executor: Finished task 105.0 in stage 4.0 (TID 284). 4544 bytes result sent to driver
[2021-05-15 10:06:57,273] {docker.py:276} INFO - 21/05/15 13:06:57 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 288) (4703d70ea67c, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:57,274] {docker.py:276} INFO - 21/05/15 13:06:57 INFO Executor: Running task 109.0 in stage 4.0 (TID 288)
[2021-05-15 10:06:57,275] {docker.py:276} INFO - 21/05/15 13:06:57 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 284) in 2439 ms on 4703d70ea67c (executor driver) (106/200)
[2021-05-15 10:06:57,282] {docker.py:276} INFO - 21/05/15 13:06:57 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:57,284] {docker.py:276} INFO - 21/05/15 13:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538275417409967833716_0004_m_000109_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538275417409967833716_0004_m_000109_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538275417409967833716_0004}; taskId=attempt_20210515130538275417409967833716_0004_m_000109_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@213ccee5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:57,284] {docker.py:276} INFO - 21/05/15 13:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:57 INFO StagingCommitter: Starting: Task committer attempt_20210515130538275417409967833716_0004_m_000109_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538275417409967833716_0004_m_000109_288
[2021-05-15 10:06:57,287] {docker.py:276} INFO - 21/05/15 13:06:57 INFO StagingCommitter: Task committer attempt_20210515130538275417409967833716_0004_m_000109_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538275417409967833716_0004_m_000109_288 : duration 0:00.003s
[2021-05-15 10:06:58,025] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_2021051513053831517203108754803_0004_m_000106_285: needsTaskCommit() Task attempt_2021051513053831517203108754803_0004_m_000106_285
[2021-05-15 10:06:58,025] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_2021051513053831517203108754803_0004_m_000106_285: needsTaskCommit() Task attempt_2021051513053831517203108754803_0004_m_000106_285: duration 0:00.001s
[2021-05-15 10:06:58,026] {docker.py:276} INFO - 21/05/15 13:06:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513053831517203108754803_0004_m_000106_285
[2021-05-15 10:06:58,026] {docker.py:276} INFO - 21/05/15 13:06:58 INFO Executor: Finished task 106.0 in stage 4.0 (TID 285). 4544 bytes result sent to driver
[2021-05-15 10:06:58,026] {docker.py:276} INFO - 21/05/15 13:06:58 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 289) (4703d70ea67c, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:58,027] {docker.py:276} INFO - 21/05/15 13:06:58 INFO Executor: Running task 110.0 in stage 4.0 (TID 289)
[2021-05-15 10:06:58,028] {docker.py:276} INFO - 21/05/15 13:06:58 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 285) in 2442 ms on 4703d70ea67c (executor driver) (107/200)
[2021-05-15 10:06:58,035] {docker.py:276} INFO - 21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:58,036] {docker.py:276} INFO - 21/05/15 13:06:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:58,036] {docker.py:276} INFO - 21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385458171062452423224_0004_m_000110_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385458171062452423224_0004_m_000110_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385458171062452423224_0004}; taskId=attempt_202105151305385458171062452423224_0004_m_000110_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bbb4215}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:58,037] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305385458171062452423224_0004_m_000110_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385458171062452423224_0004_m_000110_289
[2021-05-15 10:06:58,039] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_202105151305385458171062452423224_0004_m_000110_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385458171062452423224_0004_m_000110_289 : duration 0:00.003s
[2021-05-15 10:06:58,367] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305386949533975900809788_0004_m_000107_286: needsTaskCommit() Task attempt_202105151305386949533975900809788_0004_m_000107_286
[2021-05-15 10:06:58,368] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_202105151305386949533975900809788_0004_m_000107_286: needsTaskCommit() Task attempt_202105151305386949533975900809788_0004_m_000107_286: duration 0:00.000s
21/05/15 13:06:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386949533975900809788_0004_m_000107_286
[2021-05-15 10:06:58,369] {docker.py:276} INFO - 21/05/15 13:06:58 INFO Executor: Finished task 107.0 in stage 4.0 (TID 286). 4544 bytes result sent to driver
[2021-05-15 10:06:58,370] {docker.py:276} INFO - 21/05/15 13:06:58 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 290) (4703d70ea67c, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:58,371] {docker.py:276} INFO - 21/05/15 13:06:58 INFO Executor: Running task 111.0 in stage 4.0 (TID 290)
21/05/15 13:06:58 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 286) in 2591 ms on 4703d70ea67c (executor driver) (108/200)
[2021-05-15 10:06:58,392] {docker.py:276} INFO - 21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:58,394] {docker.py:276} INFO - 21/05/15 13:06:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385013372556263107914_0004_m_000111_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385013372556263107914_0004_m_000111_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385013372556263107914_0004}; taskId=attempt_202105151305385013372556263107914_0004_m_000111_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@404b7f71}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:58,394] {docker.py:276} INFO - 21/05/15 13:06:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305385013372556263107914_0004_m_000111_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385013372556263107914_0004_m_000111_290
[2021-05-15 10:06:58,397] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_202105151305385013372556263107914_0004_m_000111_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385013372556263107914_0004_m_000111_290 : duration 0:00.003s
[2021-05-15 10:06:58,413] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305387254729059240264815_0004_m_000108_287: needsTaskCommit() Task attempt_202105151305387254729059240264815_0004_m_000108_287
[2021-05-15 10:06:58,413] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_202105151305387254729059240264815_0004_m_000108_287: needsTaskCommit() Task attempt_202105151305387254729059240264815_0004_m_000108_287: duration 0:00.000s
21/05/15 13:06:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387254729059240264815_0004_m_000108_287
[2021-05-15 10:06:58,414] {docker.py:276} INFO - 21/05/15 13:06:58 INFO Executor: Finished task 108.0 in stage 4.0 (TID 287). 4587 bytes result sent to driver
[2021-05-15 10:06:58,415] {docker.py:276} INFO - 21/05/15 13:06:58 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 291) (4703d70ea67c, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:58,416] {docker.py:276} INFO - 21/05/15 13:06:58 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 287) in 2547 ms on 4703d70ea67c (executor driver) (109/200)
21/05/15 13:06:58 INFO Executor: Running task 112.0 in stage 4.0 (TID 291)
[2021-05-15 10:06:58,423] {docker.py:276} INFO - 21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:06:58,423] {docker.py:276} INFO - 21/05/15 13:06:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:06:58,424] {docker.py:276} INFO - 21/05/15 13:06:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:06:58,425] {docker.py:276} INFO - 21/05/15 13:06:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388409892853027082983_0004_m_000112_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388409892853027082983_0004_m_000112_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388409892853027082983_0004}; taskId=attempt_202105151305388409892853027082983_0004_m_000112_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74244b23}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:58,425] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Starting: Task committer attempt_202105151305388409892853027082983_0004_m_000112_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388409892853027082983_0004_m_000112_291
[2021-05-15 10:06:58,429] {docker.py:276} INFO - 21/05/15 13:06:58 INFO StagingCommitter: Task committer attempt_202105151305388409892853027082983_0004_m_000112_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388409892853027082983_0004_m_000112_291 : duration 0:00.004s
[2021-05-15 10:06:59,823] {docker.py:276} INFO - 21/05/15 13:06:59 INFO StagingCommitter: Starting: Task committer attempt_20210515130538275417409967833716_0004_m_000109_288: needsTaskCommit() Task attempt_20210515130538275417409967833716_0004_m_000109_288
[2021-05-15 10:06:59,826] {docker.py:276} INFO - 21/05/15 13:06:59 INFO StagingCommitter: Task committer attempt_20210515130538275417409967833716_0004_m_000109_288: needsTaskCommit() Task attempt_20210515130538275417409967833716_0004_m_000109_288: duration 0:00.002s
21/05/15 13:06:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538275417409967833716_0004_m_000109_288
[2021-05-15 10:06:59,827] {docker.py:276} INFO - 21/05/15 13:06:59 INFO Executor: Finished task 109.0 in stage 4.0 (TID 288). 4587 bytes result sent to driver
[2021-05-15 10:06:59,829] {docker.py:276} INFO - 21/05/15 13:06:59 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 292) (4703d70ea67c, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:06:59,830] {docker.py:276} INFO - 21/05/15 13:06:59 INFO Executor: Running task 113.0 in stage 4.0 (TID 292)
21/05/15 13:06:59 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 288) in 2561 ms on 4703d70ea67c (executor driver) (110/200)
[2021-05-15 10:06:59,839] {docker.py:276} INFO - 21/05/15 13:06:59 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:06:59,842] {docker.py:276} INFO - 21/05/15 13:06:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:06:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:06:59,843] {docker.py:276} INFO - 21/05/15 13:06:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:06:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385285238792334117769_0004_m_000113_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385285238792334117769_0004_m_000113_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385285238792334117769_0004}; taskId=attempt_202105151305385285238792334117769_0004_m_000113_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51713213}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:06:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:06:59,843] {docker.py:276} INFO - 21/05/15 13:06:59 INFO StagingCommitter: Starting: Task committer attempt_202105151305385285238792334117769_0004_m_000113_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385285238792334117769_0004_m_000113_292
[2021-05-15 10:06:59,846] {docker.py:276} INFO - 21/05/15 13:06:59 INFO StagingCommitter: Task committer attempt_202105151305385285238792334117769_0004_m_000113_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385285238792334117769_0004_m_000113_292 : duration 0:00.004s
[2021-05-15 10:07:00,573] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305385458171062452423224_0004_m_000110_289: needsTaskCommit() Task attempt_202105151305385458171062452423224_0004_m_000110_289
[2021-05-15 10:07:00,574] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305385458171062452423224_0004_m_000110_289: needsTaskCommit() Task attempt_202105151305385458171062452423224_0004_m_000110_289: duration 0:00.002s
21/05/15 13:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385458171062452423224_0004_m_000110_289
[2021-05-15 10:07:00,576] {docker.py:276} INFO - 21/05/15 13:07:00 INFO Executor: Finished task 110.0 in stage 4.0 (TID 289). 4587 bytes result sent to driver
[2021-05-15 10:07:00,577] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 293) (4703d70ea67c, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:00,578] {docker.py:276} INFO - 21/05/15 13:07:00 INFO Executor: Running task 114.0 in stage 4.0 (TID 293)
[2021-05-15 10:07:00,578] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 289) in 2556 ms on 4703d70ea67c (executor driver) (111/200)
[2021-05-15 10:07:00,587] {docker.py:276} INFO - 21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:00,589] {docker.py:276} INFO - 21/05/15 13:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:00,589] {docker.py:276} INFO - 21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:00,590] {docker.py:276} INFO - 21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386762146489225161249_0004_m_000114_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386762146489225161249_0004_m_000114_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386762146489225161249_0004}; taskId=attempt_202105151305386762146489225161249_0004_m_000114_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fb96719}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:00,590] {docker.py:276} INFO - 21/05/15 13:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305386762146489225161249_0004_m_000114_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386762146489225161249_0004_m_000114_293
[2021-05-15 10:07:00,592] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305386762146489225161249_0004_m_000114_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386762146489225161249_0004_m_000114_293 : duration 0:00.003s
[2021-05-15 10:07:00,871] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305385013372556263107914_0004_m_000111_290: needsTaskCommit() Task attempt_202105151305385013372556263107914_0004_m_000111_290
[2021-05-15 10:07:00,872] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305385013372556263107914_0004_m_000111_290: needsTaskCommit() Task attempt_202105151305385013372556263107914_0004_m_000111_290: duration 0:00.001s
21/05/15 13:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385013372556263107914_0004_m_000111_290
[2021-05-15 10:07:00,874] {docker.py:276} INFO - 21/05/15 13:07:00 INFO Executor: Finished task 111.0 in stage 4.0 (TID 290). 4587 bytes result sent to driver
[2021-05-15 10:07:00,875] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 294) (4703d70ea67c, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:00,876] {docker.py:276} INFO - 21/05/15 13:07:00 INFO Executor: Running task 115.0 in stage 4.0 (TID 294)
[2021-05-15 10:07:00,876] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 290) in 2509 ms on 4703d70ea67c (executor driver) (112/200)
[2021-05-15 10:07:00,886] {docker.py:276} INFO - 21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:00,888] {docker.py:276} INFO - 21/05/15 13:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388535837580506544050_0004_m_000115_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388535837580506544050_0004_m_000115_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388535837580506544050_0004}; taskId=attempt_202105151305388535837580506544050_0004_m_000115_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c8de9f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:00,888] {docker.py:276} INFO - 21/05/15 13:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305388535837580506544050_0004_m_000115_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388535837580506544050_0004_m_000115_294
[2021-05-15 10:07:00,891] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305388535837580506544050_0004_m_000115_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388535837580506544050_0004_m_000115_294 : duration 0:00.003s
[2021-05-15 10:07:00,939] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305388409892853027082983_0004_m_000112_291: needsTaskCommit() Task attempt_202105151305388409892853027082983_0004_m_000112_291
21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305388409892853027082983_0004_m_000112_291: needsTaskCommit() Task attempt_202105151305388409892853027082983_0004_m_000112_291: duration 0:00.001s
[2021-05-15 10:07:00,939] {docker.py:276} INFO - 21/05/15 13:07:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388409892853027082983_0004_m_000112_291
[2021-05-15 10:07:00,940] {docker.py:276} INFO - 21/05/15 13:07:00 INFO Executor: Finished task 112.0 in stage 4.0 (TID 291). 4544 bytes result sent to driver
[2021-05-15 10:07:00,942] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 295) (4703d70ea67c, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:00,942] {docker.py:276} INFO - 21/05/15 13:07:00 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 291) in 2530 ms on 4703d70ea67c (executor driver) (113/200)
21/05/15 13:07:00 INFO Executor: Running task 116.0 in stage 4.0 (TID 295)
[2021-05-15 10:07:00,951] {docker.py:276} INFO - 21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:00,953] {docker.py:276} INFO - 21/05/15 13:07:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387201032925458151002_0004_m_000116_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201032925458151002_0004_m_000116_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387201032925458151002_0004}; taskId=attempt_202105151305387201032925458151002_0004_m_000116_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@716bac93}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:00,954] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Starting: Task committer attempt_202105151305387201032925458151002_0004_m_000116_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201032925458151002_0004_m_000116_295
[2021-05-15 10:07:00,956] {docker.py:276} INFO - 21/05/15 13:07:00 INFO StagingCommitter: Task committer attempt_202105151305387201032925458151002_0004_m_000116_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387201032925458151002_0004_m_000116_295 : duration 0:00.003s
[2021-05-15 10:07:01,864] {docker.py:276} INFO - 21/05/15 13:07:01 INFO StagingCommitter: Starting: Task committer attempt_202105151305385285238792334117769_0004_m_000113_292: needsTaskCommit() Task attempt_202105151305385285238792334117769_0004_m_000113_292
[2021-05-15 10:07:01,865] {docker.py:276} INFO - 21/05/15 13:07:01 INFO StagingCommitter: Task committer attempt_202105151305385285238792334117769_0004_m_000113_292: needsTaskCommit() Task attempt_202105151305385285238792334117769_0004_m_000113_292: duration 0:00.001s
21/05/15 13:07:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385285238792334117769_0004_m_000113_292
[2021-05-15 10:07:01,865] {docker.py:276} INFO - 21/05/15 13:07:01 INFO Executor: Finished task 113.0 in stage 4.0 (TID 292). 4544 bytes result sent to driver
[2021-05-15 10:07:01,867] {docker.py:276} INFO - 21/05/15 13:07:01 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 296) (4703d70ea67c, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:01,867] {docker.py:276} INFO - 21/05/15 13:07:01 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 292) in 2042 ms on 4703d70ea67c (executor driver) (114/200)
[2021-05-15 10:07:01,868] {docker.py:276} INFO - 21/05/15 13:07:01 INFO Executor: Running task 117.0 in stage 4.0 (TID 296)
[2021-05-15 10:07:01,877] {docker.py:276} INFO - 21/05/15 13:07:01 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:01,880] {docker.py:276} INFO - 21/05/15 13:07:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:01,880] {docker.py:276} INFO - 21/05/15 13:07:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538741175899366596618_0004_m_000117_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538741175899366596618_0004_m_000117_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538741175899366596618_0004}; taskId=attempt_20210515130538741175899366596618_0004_m_000117_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e85bfb9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:01 INFO StagingCommitter: Starting: Task committer attempt_20210515130538741175899366596618_0004_m_000117_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538741175899366596618_0004_m_000117_296
[2021-05-15 10:07:01,883] {docker.py:276} INFO - 21/05/15 13:07:01 INFO StagingCommitter: Task committer attempt_20210515130538741175899366596618_0004_m_000117_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538741175899366596618_0004_m_000117_296 : duration 0:00.002s
[2021-05-15 10:07:02,613] {docker.py:276} INFO - 21/05/15 13:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305386762146489225161249_0004_m_000114_293: needsTaskCommit() Task attempt_202105151305386762146489225161249_0004_m_000114_293
[2021-05-15 10:07:02,614] {docker.py:276} INFO - 21/05/15 13:07:02 INFO StagingCommitter: Task committer attempt_202105151305386762146489225161249_0004_m_000114_293: needsTaskCommit() Task attempt_202105151305386762146489225161249_0004_m_000114_293: duration 0:00.001s
21/05/15 13:07:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386762146489225161249_0004_m_000114_293
[2021-05-15 10:07:02,616] {docker.py:276} INFO - 21/05/15 13:07:02 INFO Executor: Finished task 114.0 in stage 4.0 (TID 293). 4544 bytes result sent to driver
[2021-05-15 10:07:02,617] {docker.py:276} INFO - 21/05/15 13:07:02 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 297) (4703d70ea67c, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:02,618] {docker.py:276} INFO - 21/05/15 13:07:02 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 293) in 2044 ms on 4703d70ea67c (executor driver) (115/200)
[2021-05-15 10:07:02,619] {docker.py:276} INFO - 21/05/15 13:07:02 INFO Executor: Running task 118.0 in stage 4.0 (TID 297)
[2021-05-15 10:07:02,628] {docker.py:276} INFO - 21/05/15 13:07:02 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:02,630] {docker.py:276} INFO - 21/05/15 13:07:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:02,631] {docker.py:276} INFO - 21/05/15 13:07:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386050538717653079692_0004_m_000118_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386050538717653079692_0004_m_000118_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386050538717653079692_0004}; taskId=attempt_202105151305386050538717653079692_0004_m_000118_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44c68cb4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:02,631] {docker.py:276} INFO - 21/05/15 13:07:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:02,631] {docker.py:276} INFO - 21/05/15 13:07:02 INFO StagingCommitter: Starting: Task committer attempt_202105151305386050538717653079692_0004_m_000118_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386050538717653079692_0004_m_000118_297
[2021-05-15 10:07:02,635] {docker.py:276} INFO - 21/05/15 13:07:02 INFO StagingCommitter: Task committer attempt_202105151305386050538717653079692_0004_m_000118_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386050538717653079692_0004_m_000118_297 : duration 0:00.004s
[2021-05-15 10:07:03,254] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305388535837580506544050_0004_m_000115_294: needsTaskCommit() Task attempt_202105151305388535837580506544050_0004_m_000115_294
21/05/15 13:07:03 INFO StagingCommitter: Task committer attempt_202105151305388535837580506544050_0004_m_000115_294: needsTaskCommit() Task attempt_202105151305388535837580506544050_0004_m_000115_294: duration 0:00.001s
21/05/15 13:07:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388535837580506544050_0004_m_000115_294
[2021-05-15 10:07:03,257] {docker.py:276} INFO - 21/05/15 13:07:03 INFO Executor: Finished task 115.0 in stage 4.0 (TID 294). 4544 bytes result sent to driver
[2021-05-15 10:07:03,258] {docker.py:276} INFO - 21/05/15 13:07:03 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 298) (4703d70ea67c, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:03,259] {docker.py:276} INFO - 21/05/15 13:07:03 INFO Executor: Running task 119.0 in stage 4.0 (TID 298)
21/05/15 13:07:03 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 294) in 2387 ms on 4703d70ea67c (executor driver) (116/200)
[2021-05-15 10:07:03,270] {docker.py:276} INFO - 21/05/15 13:07:03 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:03,272] {docker.py:276} INFO - 21/05/15 13:07:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:03,272] {docker.py:276} INFO - 21/05/15 13:07:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387054780040486587841_0004_m_000119_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054780040486587841_0004_m_000119_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387054780040486587841_0004}; taskId=attempt_202105151305387054780040486587841_0004_m_000119_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60c964ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305387054780040486587841_0004_m_000119_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054780040486587841_0004_m_000119_298
[2021-05-15 10:07:03,275] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Task committer attempt_202105151305387054780040486587841_0004_m_000119_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387054780040486587841_0004_m_000119_298 : duration 0:00.003s
[2021-05-15 10:07:03,778] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305387201032925458151002_0004_m_000116_295: needsTaskCommit() Task attempt_202105151305387201032925458151002_0004_m_000116_295
[2021-05-15 10:07:03,779] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Task committer attempt_202105151305387201032925458151002_0004_m_000116_295: needsTaskCommit() Task attempt_202105151305387201032925458151002_0004_m_000116_295: duration 0:00.001s
21/05/15 13:07:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387201032925458151002_0004_m_000116_295
[2021-05-15 10:07:03,782] {docker.py:276} INFO - 21/05/15 13:07:03 INFO Executor: Finished task 116.0 in stage 4.0 (TID 295). 4544 bytes result sent to driver
[2021-05-15 10:07:03,783] {docker.py:276} INFO - 21/05/15 13:07:03 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 299) (4703d70ea67c, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:03,784] {docker.py:276} INFO - 21/05/15 13:07:03 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 295) in 2846 ms on 4703d70ea67c (executor driver) (117/200)
[2021-05-15 10:07:03,785] {docker.py:276} INFO - 21/05/15 13:07:03 INFO Executor: Running task 120.0 in stage 4.0 (TID 299)
[2021-05-15 10:07:03,794] {docker.py:276} INFO - 21/05/15 13:07:03 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:03,796] {docker.py:276} INFO - 21/05/15 13:07:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:03,797] {docker.py:276} INFO - 21/05/15 13:07:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:03,797] {docker.py:276} INFO - 21/05/15 13:07:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388780389161869522778_0004_m_000120_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388780389161869522778_0004_m_000120_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388780389161869522778_0004}; taskId=attempt_202105151305388780389161869522778_0004_m_000120_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@396716a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:03,798] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Starting: Task committer attempt_202105151305388780389161869522778_0004_m_000120_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388780389161869522778_0004_m_000120_299
[2021-05-15 10:07:03,800] {docker.py:276} INFO - 21/05/15 13:07:03 INFO StagingCommitter: Task committer attempt_202105151305388780389161869522778_0004_m_000120_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388780389161869522778_0004_m_000120_299 : duration 0:00.004s
[2021-05-15 10:07:04,175] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Starting: Task committer attempt_20210515130538741175899366596618_0004_m_000117_296: needsTaskCommit() Task attempt_20210515130538741175899366596618_0004_m_000117_296
[2021-05-15 10:07:04,177] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Task committer attempt_20210515130538741175899366596618_0004_m_000117_296: needsTaskCommit() Task attempt_20210515130538741175899366596618_0004_m_000117_296: duration 0:00.002s
21/05/15 13:07:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538741175899366596618_0004_m_000117_296
[2021-05-15 10:07:04,180] {docker.py:276} INFO - 21/05/15 13:07:04 INFO Executor: Finished task 117.0 in stage 4.0 (TID 296). 4544 bytes result sent to driver
[2021-05-15 10:07:04,181] {docker.py:276} INFO - 21/05/15 13:07:04 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 300) (4703d70ea67c, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:04,182] {docker.py:276} INFO - 21/05/15 13:07:04 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 296) in 2319 ms on 4703d70ea67c (executor driver) (118/200)
[2021-05-15 10:07:04,183] {docker.py:276} INFO - 21/05/15 13:07:04 INFO Executor: Running task 121.0 in stage 4.0 (TID 300)
[2021-05-15 10:07:04,192] {docker.py:276} INFO - 21/05/15 13:07:04 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:04,194] {docker.py:276} INFO - 21/05/15 13:07:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:04,194] {docker.py:276} INFO - 21/05/15 13:07:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:04,195] {docker.py:276} INFO - 21/05/15 13:07:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385768622573396864946_0004_m_000121_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768622573396864946_0004_m_000121_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385768622573396864946_0004}; taskId=attempt_202105151305385768622573396864946_0004_m_000121_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e3abcab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:04,195] {docker.py:276} INFO - 21/05/15 13:07:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:04 INFO StagingCommitter: Starting: Task committer attempt_202105151305385768622573396864946_0004_m_000121_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768622573396864946_0004_m_000121_300
[2021-05-15 10:07:04,197] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Task committer attempt_202105151305385768622573396864946_0004_m_000121_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385768622573396864946_0004_m_000121_300 : duration 0:00.003s
[2021-05-15 10:07:04,655] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Starting: Task committer attempt_202105151305386050538717653079692_0004_m_000118_297: needsTaskCommit() Task attempt_202105151305386050538717653079692_0004_m_000118_297
[2021-05-15 10:07:04,656] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Task committer attempt_202105151305386050538717653079692_0004_m_000118_297: needsTaskCommit() Task attempt_202105151305386050538717653079692_0004_m_000118_297: duration 0:00.000s
[2021-05-15 10:07:04,657] {docker.py:276} INFO - 21/05/15 13:07:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386050538717653079692_0004_m_000118_297
[2021-05-15 10:07:04,659] {docker.py:276} INFO - 21/05/15 13:07:04 INFO Executor: Finished task 118.0 in stage 4.0 (TID 297). 4544 bytes result sent to driver
[2021-05-15 10:07:04,660] {docker.py:276} INFO - 21/05/15 13:07:04 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 301) (4703d70ea67c, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:04,661] {docker.py:276} INFO - 21/05/15 13:07:04 INFO Executor: Running task 122.0 in stage 4.0 (TID 301)
21/05/15 13:07:04 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 297) in 2047 ms on 4703d70ea67c (executor driver) (119/200)
[2021-05-15 10:07:04,671] {docker.py:276} INFO - 21/05/15 13:07:04 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:04,673] {docker.py:276} INFO - 21/05/15 13:07:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538850821786228873181_0004_m_000122_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850821786228873181_0004_m_000122_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538850821786228873181_0004}; taskId=attempt_20210515130538850821786228873181_0004_m_000122_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3255ae54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:04,673] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Starting: Task committer attempt_20210515130538850821786228873181_0004_m_000122_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850821786228873181_0004_m_000122_301
[2021-05-15 10:07:04,676] {docker.py:276} INFO - 21/05/15 13:07:04 INFO StagingCommitter: Task committer attempt_20210515130538850821786228873181_0004_m_000122_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850821786228873181_0004_m_000122_301 : duration 0:00.003s
[2021-05-15 10:07:05,764] {docker.py:276} INFO - 21/05/15 13:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305387054780040486587841_0004_m_000119_298: needsTaskCommit() Task attempt_202105151305387054780040486587841_0004_m_000119_298
[2021-05-15 10:07:05,766] {docker.py:276} INFO - 21/05/15 13:07:05 INFO StagingCommitter: Task committer attempt_202105151305387054780040486587841_0004_m_000119_298: needsTaskCommit() Task attempt_202105151305387054780040486587841_0004_m_000119_298: duration 0:00.001s
21/05/15 13:07:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387054780040486587841_0004_m_000119_298
[2021-05-15 10:07:05,768] {docker.py:276} INFO - 21/05/15 13:07:05 INFO Executor: Finished task 119.0 in stage 4.0 (TID 298). 4544 bytes result sent to driver
[2021-05-15 10:07:05,769] {docker.py:276} INFO - 21/05/15 13:07:05 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 302) (4703d70ea67c, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:05,769] {docker.py:276} INFO - 21/05/15 13:07:05 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 298) in 2516 ms on 4703d70ea67c (executor driver) (120/200)
[2021-05-15 10:07:05,770] {docker.py:276} INFO - 21/05/15 13:07:05 INFO Executor: Running task 123.0 in stage 4.0 (TID 302)
[2021-05-15 10:07:05,780] {docker.py:276} INFO - 21/05/15 13:07:05 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:05,782] {docker.py:276} INFO - 21/05/15 13:07:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386495004589508428986_0004_m_000123_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386495004589508428986_0004_m_000123_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386495004589508428986_0004}; taskId=attempt_202105151305386495004589508428986_0004_m_000123_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@616595a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:05,783] {docker.py:276} INFO - 21/05/15 13:07:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:05 INFO StagingCommitter: Starting: Task committer attempt_202105151305386495004589508428986_0004_m_000123_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386495004589508428986_0004_m_000123_302
[2021-05-15 10:07:05,785] {docker.py:276} INFO - 21/05/15 13:07:05 INFO StagingCommitter: Task committer attempt_202105151305386495004589508428986_0004_m_000123_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386495004589508428986_0004_m_000123_302 : duration 0:00.003s
[2021-05-15 10:07:06,329] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Starting: Task committer attempt_202105151305388780389161869522778_0004_m_000120_299: needsTaskCommit() Task attempt_202105151305388780389161869522778_0004_m_000120_299
[2021-05-15 10:07:06,330] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Task committer attempt_202105151305388780389161869522778_0004_m_000120_299: needsTaskCommit() Task attempt_202105151305388780389161869522778_0004_m_000120_299: duration 0:00.001s
21/05/15 13:07:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388780389161869522778_0004_m_000120_299
[2021-05-15 10:07:06,332] {docker.py:276} INFO - 21/05/15 13:07:06 INFO Executor: Finished task 120.0 in stage 4.0 (TID 299). 4544 bytes result sent to driver
[2021-05-15 10:07:06,333] {docker.py:276} INFO - 21/05/15 13:07:06 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 303) (4703d70ea67c, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:06,334] {docker.py:276} INFO - 21/05/15 13:07:06 INFO Executor: Running task 124.0 in stage 4.0 (TID 303)
[2021-05-15 10:07:06,335] {docker.py:276} INFO - 21/05/15 13:07:06 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 299) in 2554 ms on 4703d70ea67c (executor driver) (121/200)
[2021-05-15 10:07:06,345] {docker.py:276} INFO - 21/05/15 13:07:06 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:06,345] {docker.py:276} INFO - 21/05/15 13:07:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:06,347] {docker.py:276} INFO - 21/05/15 13:07:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:06,347] {docker.py:276} INFO - 21/05/15 13:07:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:06,348] {docker.py:276} INFO - 21/05/15 13:07:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:06,348] {docker.py:276} INFO - 21/05/15 13:07:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538525949413108469909_0004_m_000124_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538525949413108469909_0004_m_000124_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538525949413108469909_0004}; taskId=attempt_20210515130538525949413108469909_0004_m_000124_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b2a32f7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:06,348] {docker.py:276} INFO - 21/05/15 13:07:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:06,348] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Starting: Task committer attempt_20210515130538525949413108469909_0004_m_000124_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538525949413108469909_0004_m_000124_303
[2021-05-15 10:07:06,351] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Task committer attempt_20210515130538525949413108469909_0004_m_000124_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538525949413108469909_0004_m_000124_303 : duration 0:00.003s
[2021-05-15 10:07:06,916] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Starting: Task committer attempt_202105151305385768622573396864946_0004_m_000121_300: needsTaskCommit() Task attempt_202105151305385768622573396864946_0004_m_000121_300
[2021-05-15 10:07:06,917] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Task committer attempt_202105151305385768622573396864946_0004_m_000121_300: needsTaskCommit() Task attempt_202105151305385768622573396864946_0004_m_000121_300: duration 0:00.002s
21/05/15 13:07:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385768622573396864946_0004_m_000121_300
[2021-05-15 10:07:06,920] {docker.py:276} INFO - 21/05/15 13:07:06 INFO Executor: Finished task 121.0 in stage 4.0 (TID 300). 4544 bytes result sent to driver
[2021-05-15 10:07:06,921] {docker.py:276} INFO - 21/05/15 13:07:06 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 304) (4703d70ea67c, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:06,922] {docker.py:276} INFO - 21/05/15 13:07:06 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 300) in 2744 ms on 4703d70ea67c (executor driver) (122/200)
21/05/15 13:07:06 INFO Executor: Running task 125.0 in stage 4.0 (TID 304)
[2021-05-15 10:07:06,932] {docker.py:276} INFO - 21/05/15 13:07:06 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:06,934] {docker.py:276} INFO - 21/05/15 13:07:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:06,934] {docker.py:276} INFO - 21/05/15 13:07:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388471971823834802453_0004_m_000125_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388471971823834802453_0004_m_000125_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388471971823834802453_0004}; taskId=attempt_202105151305388471971823834802453_0004_m_000125_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@407ed697}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:06,935] {docker.py:276} INFO - 21/05/15 13:07:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:06,935] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Starting: Task committer attempt_202105151305388471971823834802453_0004_m_000125_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388471971823834802453_0004_m_000125_304
[2021-05-15 10:07:06,937] {docker.py:276} INFO - 21/05/15 13:07:06 INFO StagingCommitter: Task committer attempt_202105151305388471971823834802453_0004_m_000125_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388471971823834802453_0004_m_000125_304 : duration 0:00.003s
[2021-05-15 10:07:07,362] {docker.py:276} INFO - 21/05/15 13:07:07 INFO StagingCommitter: Starting: Task committer attempt_20210515130538850821786228873181_0004_m_000122_301: needsTaskCommit() Task attempt_20210515130538850821786228873181_0004_m_000122_301
[2021-05-15 10:07:07,362] {docker.py:276} INFO - 21/05/15 13:07:07 INFO StagingCommitter: Task committer attempt_20210515130538850821786228873181_0004_m_000122_301: needsTaskCommit() Task attempt_20210515130538850821786228873181_0004_m_000122_301: duration 0:00.000s
21/05/15 13:07:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538850821786228873181_0004_m_000122_301
[2021-05-15 10:07:07,363] {docker.py:276} INFO - 21/05/15 13:07:07 INFO Executor: Finished task 122.0 in stage 4.0 (TID 301). 4544 bytes result sent to driver
[2021-05-15 10:07:07,364] {docker.py:276} INFO - 21/05/15 13:07:07 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 305) (4703d70ea67c, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:07,367] {docker.py:276} INFO - 21/05/15 13:07:07 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 301) in 2710 ms on 4703d70ea67c (executor driver) (123/200)
[2021-05-15 10:07:07,367] {docker.py:276} INFO - 21/05/15 13:07:07 INFO Executor: Running task 126.0 in stage 4.0 (TID 305)
[2021-05-15 10:07:07,377] {docker.py:276} INFO - 21/05/15 13:07:07 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:07,379] {docker.py:276} INFO - 21/05/15 13:07:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:07,380] {docker.py:276} INFO - 21/05/15 13:07:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382201892625856092563_0004_m_000126_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382201892625856092563_0004_m_000126_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382201892625856092563_0004}; taskId=attempt_202105151305382201892625856092563_0004_m_000126_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49f88df6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:07,380] {docker.py:276} INFO - 21/05/15 13:07:07 INFO StagingCommitter: Starting: Task committer attempt_202105151305382201892625856092563_0004_m_000126_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382201892625856092563_0004_m_000126_305
[2021-05-15 10:07:07,383] {docker.py:276} INFO - 21/05/15 13:07:07 INFO StagingCommitter: Task committer attempt_202105151305382201892625856092563_0004_m_000126_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382201892625856092563_0004_m_000126_305 : duration 0:00.004s
[2021-05-15 10:07:08,267] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305386495004589508428986_0004_m_000123_302: needsTaskCommit() Task attempt_202105151305386495004589508428986_0004_m_000123_302
21/05/15 13:07:08 INFO StagingCommitter: Task committer attempt_202105151305386495004589508428986_0004_m_000123_302: needsTaskCommit() Task attempt_202105151305386495004589508428986_0004_m_000123_302: duration 0:00.000s
21/05/15 13:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386495004589508428986_0004_m_000123_302
[2021-05-15 10:07:08,268] {docker.py:276} INFO - 21/05/15 13:07:08 INFO Executor: Finished task 123.0 in stage 4.0 (TID 302). 4544 bytes result sent to driver
[2021-05-15 10:07:08,270] {docker.py:276} INFO - 21/05/15 13:07:08 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 306) (4703d70ea67c, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:08,271] {docker.py:276} INFO - 21/05/15 13:07:08 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 302) in 2505 ms on 4703d70ea67c (executor driver) (124/200)
[2021-05-15 10:07:08,272] {docker.py:276} INFO - 21/05/15 13:07:08 INFO Executor: Running task 127.0 in stage 4.0 (TID 306)
[2021-05-15 10:07:08,285] {docker.py:276} INFO - 21/05/15 13:07:08 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:08,286] {docker.py:276} INFO - 21/05/15 13:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:08,288] {docker.py:276} INFO - 21/05/15 13:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:08,289] {docker.py:276} INFO - 21/05/15 13:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:08,289] {docker.py:276} INFO - 21/05/15 13:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381389623618371843314_0004_m_000127_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381389623618371843314_0004_m_000127_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381389623618371843314_0004}; taskId=attempt_202105151305381389623618371843314_0004_m_000127_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@700804ce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:08,290] {docker.py:276} INFO - 21/05/15 13:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:08,290] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305381389623618371843314_0004_m_000127_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381389623618371843314_0004_m_000127_306
[2021-05-15 10:07:08,293] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Task committer attempt_202105151305381389623618371843314_0004_m_000127_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381389623618371843314_0004_m_000127_306 : duration 0:00.003s
[2021-05-15 10:07:08,723] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Starting: Task committer attempt_20210515130538525949413108469909_0004_m_000124_303: needsTaskCommit() Task attempt_20210515130538525949413108469909_0004_m_000124_303
[2021-05-15 10:07:08,724] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Task committer attempt_20210515130538525949413108469909_0004_m_000124_303: needsTaskCommit() Task attempt_20210515130538525949413108469909_0004_m_000124_303: duration 0:00.001s
[2021-05-15 10:07:08,725] {docker.py:276} INFO - 21/05/15 13:07:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538525949413108469909_0004_m_000124_303
[2021-05-15 10:07:08,726] {docker.py:276} INFO - 21/05/15 13:07:08 INFO Executor: Finished task 124.0 in stage 4.0 (TID 303). 4544 bytes result sent to driver
[2021-05-15 10:07:08,727] {docker.py:276} INFO - 21/05/15 13:07:08 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 307) (4703d70ea67c, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:08,728] {docker.py:276} INFO - 21/05/15 13:07:08 INFO Executor: Running task 128.0 in stage 4.0 (TID 307)
[2021-05-15 10:07:08,728] {docker.py:276} INFO - 21/05/15 13:07:08 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 303) in 2398 ms on 4703d70ea67c (executor driver) (125/200)
[2021-05-15 10:07:08,738] {docker.py:276} INFO - 21/05/15 13:07:08 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:08,738] {docker.py:276} INFO - 21/05/15 13:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:08,740] {docker.py:276} INFO - 21/05/15 13:07:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:08,740] {docker.py:276} INFO - 21/05/15 13:07:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387731262899031724402_0004_m_000128_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387731262899031724402_0004_m_000128_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387731262899031724402_0004}; taskId=attempt_202105151305387731262899031724402_0004_m_000128_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c4c1a37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:08,741] {docker.py:276} INFO - 21/05/15 13:07:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:08,741] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Starting: Task committer attempt_202105151305387731262899031724402_0004_m_000128_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387731262899031724402_0004_m_000128_307
[2021-05-15 10:07:08,743] {docker.py:276} INFO - 21/05/15 13:07:08 INFO StagingCommitter: Task committer attempt_202105151305387731262899031724402_0004_m_000128_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387731262899031724402_0004_m_000128_307 : duration 0:00.003s
[2021-05-15 10:07:09,253] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Starting: Task committer attempt_202105151305388471971823834802453_0004_m_000125_304: needsTaskCommit() Task attempt_202105151305388471971823834802453_0004_m_000125_304
[2021-05-15 10:07:09,254] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Task committer attempt_202105151305388471971823834802453_0004_m_000125_304: needsTaskCommit() Task attempt_202105151305388471971823834802453_0004_m_000125_304: duration 0:00.001s
21/05/15 13:07:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388471971823834802453_0004_m_000125_304
[2021-05-15 10:07:09,255] {docker.py:276} INFO - 21/05/15 13:07:09 INFO Executor: Finished task 125.0 in stage 4.0 (TID 304). 4544 bytes result sent to driver
[2021-05-15 10:07:09,256] {docker.py:276} INFO - 21/05/15 13:07:09 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 308) (4703d70ea67c, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:09,257] {docker.py:276} INFO - 21/05/15 13:07:09 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 304) in 2340 ms on 4703d70ea67c (executor driver) (126/200)
[2021-05-15 10:07:09,258] {docker.py:276} INFO - 21/05/15 13:07:09 INFO Executor: Running task 129.0 in stage 4.0 (TID 308)
[2021-05-15 10:07:09,278] {docker.py:276} INFO - 21/05/15 13:07:09 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2021-05-15 10:07:09,280] {docker.py:276} INFO - 21/05/15 13:07:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:09,281] {docker.py:276} INFO - 21/05/15 13:07:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:09,281] {docker.py:276} INFO - 21/05/15 13:07:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381894877452123939161_0004_m_000129_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381894877452123939161_0004_m_000129_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381894877452123939161_0004}; taskId=attempt_202105151305381894877452123939161_0004_m_000129_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23ce9b67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:09,281] {docker.py:276} INFO - 21/05/15 13:07:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:09,282] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Starting: Task committer attempt_202105151305381894877452123939161_0004_m_000129_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381894877452123939161_0004_m_000129_308
[2021-05-15 10:07:09,285] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Task committer attempt_202105151305381894877452123939161_0004_m_000129_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381894877452123939161_0004_m_000129_308 : duration 0:00.003s
[2021-05-15 10:07:09,888] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Starting: Task committer attempt_202105151305382201892625856092563_0004_m_000126_305: needsTaskCommit() Task attempt_202105151305382201892625856092563_0004_m_000126_305
[2021-05-15 10:07:09,889] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Task committer attempt_202105151305382201892625856092563_0004_m_000126_305: needsTaskCommit() Task attempt_202105151305382201892625856092563_0004_m_000126_305: duration 0:00.001s
[2021-05-15 10:07:09,889] {docker.py:276} INFO - 21/05/15 13:07:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382201892625856092563_0004_m_000126_305
[2021-05-15 10:07:09,891] {docker.py:276} INFO - 21/05/15 13:07:09 INFO Executor: Finished task 126.0 in stage 4.0 (TID 305). 4587 bytes result sent to driver
[2021-05-15 10:07:09,893] {docker.py:276} INFO - 21/05/15 13:07:09 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 309) (4703d70ea67c, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:09,894] {docker.py:276} INFO - 21/05/15 13:07:09 INFO Executor: Running task 130.0 in stage 4.0 (TID 309)
21/05/15 13:07:09 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 305) in 2532 ms on 4703d70ea67c (executor driver) (127/200)
[2021-05-15 10:07:09,904] {docker.py:276} INFO - 21/05/15 13:07:09 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:09,905] {docker.py:276} INFO - 21/05/15 13:07:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385835606497356827821_0004_m_000130_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385835606497356827821_0004_m_000130_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385835606497356827821_0004}; taskId=attempt_202105151305385835606497356827821_0004_m_000130_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@664070b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:09,906] {docker.py:276} INFO - 21/05/15 13:07:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:09,906] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Starting: Task committer attempt_202105151305385835606497356827821_0004_m_000130_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385835606497356827821_0004_m_000130_309
[2021-05-15 10:07:09,909] {docker.py:276} INFO - 21/05/15 13:07:09 INFO StagingCommitter: Task committer attempt_202105151305385835606497356827821_0004_m_000130_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385835606497356827821_0004_m_000130_309 : duration 0:00.004s
[2021-05-15 10:07:10,590] {docker.py:276} INFO - 21/05/15 13:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105151305381389623618371843314_0004_m_000127_306: needsTaskCommit() Task attempt_202105151305381389623618371843314_0004_m_000127_306
21/05/15 13:07:10 INFO StagingCommitter: Task committer attempt_202105151305381389623618371843314_0004_m_000127_306: needsTaskCommit() Task attempt_202105151305381389623618371843314_0004_m_000127_306: duration 0:00.000s
21/05/15 13:07:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381389623618371843314_0004_m_000127_306
[2021-05-15 10:07:10,591] {docker.py:276} INFO - 21/05/15 13:07:10 INFO Executor: Finished task 127.0 in stage 4.0 (TID 306). 4587 bytes result sent to driver
[2021-05-15 10:07:10,622] {docker.py:276} INFO - 21/05/15 13:07:10 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 310) (4703d70ea67c, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:10,622] {docker.py:276} INFO - 21/05/15 13:07:10 INFO Executor: Running task 131.0 in stage 4.0 (TID 310)
[2021-05-15 10:07:10,622] {docker.py:276} INFO - 21/05/15 13:07:10 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 306) in 2327 ms on 4703d70ea67c (executor driver) (128/200)
[2021-05-15 10:07:10,622] {docker.py:276} INFO - 21/05/15 13:07:10 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:10,623] {docker.py:276} INFO - 21/05/15 13:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:10,623] {docker.py:276} INFO - 21/05/15 13:07:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:10,623] {docker.py:276} INFO - 21/05/15 13:07:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:10,623] {docker.py:276} INFO - 21/05/15 13:07:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388122042109619001161_0004_m_000131_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388122042109619001161_0004_m_000131_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388122042109619001161_0004}; taskId=attempt_202105151305388122042109619001161_0004_m_000131_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@24fa1b2e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:10,624] {docker.py:276} INFO - 21/05/15 13:07:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:10,624] {docker.py:276} INFO - 21/05/15 13:07:10 INFO StagingCommitter: Starting: Task committer attempt_202105151305388122042109619001161_0004_m_000131_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388122042109619001161_0004_m_000131_310
[2021-05-15 10:07:10,624] {docker.py:276} INFO - 21/05/15 13:07:10 INFO StagingCommitter: Task committer attempt_202105151305388122042109619001161_0004_m_000131_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388122042109619001161_0004_m_000131_310 : duration 0:00.005s
[2021-05-15 10:07:11,233] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305387731262899031724402_0004_m_000128_307: needsTaskCommit() Task attempt_202105151305387731262899031724402_0004_m_000128_307
[2021-05-15 10:07:11,234] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Task committer attempt_202105151305387731262899031724402_0004_m_000128_307: needsTaskCommit() Task attempt_202105151305387731262899031724402_0004_m_000128_307: duration 0:00.001s
21/05/15 13:07:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387731262899031724402_0004_m_000128_307
[2021-05-15 10:07:11,236] {docker.py:276} INFO - 21/05/15 13:07:11 INFO Executor: Finished task 128.0 in stage 4.0 (TID 307). 4587 bytes result sent to driver
[2021-05-15 10:07:11,237] {docker.py:276} INFO - 21/05/15 13:07:11 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 311) (4703d70ea67c, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:11,238] {docker.py:276} INFO - 21/05/15 13:07:11 INFO Executor: Running task 132.0 in stage 4.0 (TID 311)
21/05/15 13:07:11 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 307) in 2514 ms on 4703d70ea67c (executor driver) (129/200)
[2021-05-15 10:07:11,247] {docker.py:276} INFO - 21/05/15 13:07:11 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:11,248] {docker.py:276} INFO - 21/05/15 13:07:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387170554597419284888_0004_m_000132_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387170554597419284888_0004_m_000132_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387170554597419284888_0004}; taskId=attempt_202105151305387170554597419284888_0004_m_000132_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f40b9b4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:11,249] {docker.py:276} INFO - 21/05/15 13:07:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305387170554597419284888_0004_m_000132_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387170554597419284888_0004_m_000132_311
[2021-05-15 10:07:11,252] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Task committer attempt_202105151305387170554597419284888_0004_m_000132_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387170554597419284888_0004_m_000132_311 : duration 0:00.003s
[2021-05-15 10:07:11,626] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305381894877452123939161_0004_m_000129_308: needsTaskCommit() Task attempt_202105151305381894877452123939161_0004_m_000129_308
[2021-05-15 10:07:11,627] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Task committer attempt_202105151305381894877452123939161_0004_m_000129_308: needsTaskCommit() Task attempt_202105151305381894877452123939161_0004_m_000129_308: duration 0:00.000s
21/05/15 13:07:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381894877452123939161_0004_m_000129_308
[2021-05-15 10:07:11,629] {docker.py:276} INFO - 21/05/15 13:07:11 INFO Executor: Finished task 129.0 in stage 4.0 (TID 308). 4587 bytes result sent to driver
[2021-05-15 10:07:11,630] {docker.py:276} INFO - 21/05/15 13:07:11 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 312) (4703d70ea67c, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:11,631] {docker.py:276} INFO - 21/05/15 13:07:11 INFO Executor: Running task 133.0 in stage 4.0 (TID 312)
[2021-05-15 10:07:11,632] {docker.py:276} INFO - 21/05/15 13:07:11 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 308) in 2377 ms on 4703d70ea67c (executor driver) (130/200)
[2021-05-15 10:07:11,641] {docker.py:276} INFO - 21/05/15 13:07:11 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:11,641] {docker.py:276} INFO - 21/05/15 13:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:11,643] {docker.py:276} INFO - 21/05/15 13:07:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:11,643] {docker.py:276} INFO - 21/05/15 13:07:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:11,643] {docker.py:276} INFO - 21/05/15 13:07:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383979334629283106080_0004_m_000133_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383979334629283106080_0004_m_000133_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383979334629283106080_0004}; taskId=attempt_202105151305383979334629283106080_0004_m_000133_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5798212c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:11,644] {docker.py:276} INFO - 21/05/15 13:07:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:11 INFO StagingCommitter: Starting: Task committer attempt_202105151305383979334629283106080_0004_m_000133_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383979334629283106080_0004_m_000133_312
[2021-05-15 10:07:11,646] {docker.py:276} INFO - 21/05/15 13:07:11 INFO StagingCommitter: Task committer attempt_202105151305383979334629283106080_0004_m_000133_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383979334629283106080_0004_m_000133_312 : duration 0:00.002s
[2021-05-15 10:07:12,540] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305385835606497356827821_0004_m_000130_309: needsTaskCommit() Task attempt_202105151305385835606497356827821_0004_m_000130_309
[2021-05-15 10:07:12,541] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Task committer attempt_202105151305385835606497356827821_0004_m_000130_309: needsTaskCommit() Task attempt_202105151305385835606497356827821_0004_m_000130_309: duration 0:00.001s
21/05/15 13:07:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385835606497356827821_0004_m_000130_309
[2021-05-15 10:07:12,543] {docker.py:276} INFO - 21/05/15 13:07:12 INFO Executor: Finished task 130.0 in stage 4.0 (TID 309). 4544 bytes result sent to driver
[2021-05-15 10:07:12,544] {docker.py:276} INFO - 21/05/15 13:07:12 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 313) (4703d70ea67c, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:12,545] {docker.py:276} INFO - 21/05/15 13:07:12 INFO Executor: Running task 134.0 in stage 4.0 (TID 313)
[2021-05-15 10:07:12,546] {docker.py:276} INFO - 21/05/15 13:07:12 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 309) in 2657 ms on 4703d70ea67c (executor driver) (131/200)
[2021-05-15 10:07:12,555] {docker.py:276} INFO - 21/05/15 13:07:12 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:12,557] {docker.py:276} INFO - 21/05/15 13:07:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:12,557] {docker.py:276} INFO - 21/05/15 13:07:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:12,558] {docker.py:276} INFO - 21/05/15 13:07:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387715671586949588898_0004_m_000134_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387715671586949588898_0004_m_000134_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387715671586949588898_0004}; taskId=attempt_202105151305387715671586949588898_0004_m_000134_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d8a96e8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:12,558] {docker.py:276} INFO - 21/05/15 13:07:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:12,558] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305387715671586949588898_0004_m_000134_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387715671586949588898_0004_m_000134_313
[2021-05-15 10:07:12,561] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Task committer attempt_202105151305387715671586949588898_0004_m_000134_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387715671586949588898_0004_m_000134_313 : duration 0:00.003s
[2021-05-15 10:07:12,920] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305388122042109619001161_0004_m_000131_310: needsTaskCommit() Task attempt_202105151305388122042109619001161_0004_m_000131_310
21/05/15 13:07:12 INFO StagingCommitter: Task committer attempt_202105151305388122042109619001161_0004_m_000131_310: needsTaskCommit() Task attempt_202105151305388122042109619001161_0004_m_000131_310: duration 0:00.001s
[2021-05-15 10:07:12,921] {docker.py:276} INFO - 21/05/15 13:07:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388122042109619001161_0004_m_000131_310
[2021-05-15 10:07:12,923] {docker.py:276} INFO - 21/05/15 13:07:12 INFO Executor: Finished task 131.0 in stage 4.0 (TID 310). 4544 bytes result sent to driver
[2021-05-15 10:07:12,924] {docker.py:276} INFO - 21/05/15 13:07:12 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 314) (4703d70ea67c, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:12,925] {docker.py:276} INFO - 21/05/15 13:07:12 INFO Executor: Running task 135.0 in stage 4.0 (TID 314)
[2021-05-15 10:07:12,926] {docker.py:276} INFO - 21/05/15 13:07:12 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 310) in 2336 ms on 4703d70ea67c (executor driver) (132/200)
[2021-05-15 10:07:12,935] {docker.py:276} INFO - 21/05/15 13:07:12 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:12,937] {docker.py:276} INFO - 21/05/15 13:07:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:12,937] {docker.py:276} INFO - 21/05/15 13:07:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:12,938] {docker.py:276} INFO - 21/05/15 13:07:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385101519766778980801_0004_m_000135_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385101519766778980801_0004_m_000135_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385101519766778980801_0004}; taskId=attempt_202105151305385101519766778980801_0004_m_000135_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bcb7fc0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:12,938] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Starting: Task committer attempt_202105151305385101519766778980801_0004_m_000135_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385101519766778980801_0004_m_000135_314
[2021-05-15 10:07:12,941] {docker.py:276} INFO - 21/05/15 13:07:12 INFO StagingCommitter: Task committer attempt_202105151305385101519766778980801_0004_m_000135_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385101519766778980801_0004_m_000135_314 : duration 0:00.004s
[2021-05-15 10:07:13,651] {docker.py:276} INFO - 21/05/15 13:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105151305387170554597419284888_0004_m_000132_311: needsTaskCommit() Task attempt_202105151305387170554597419284888_0004_m_000132_311
[2021-05-15 10:07:13,651] {docker.py:276} INFO - 21/05/15 13:07:13 INFO StagingCommitter: Task committer attempt_202105151305387170554597419284888_0004_m_000132_311: needsTaskCommit() Task attempt_202105151305387170554597419284888_0004_m_000132_311: duration 0:00.001s
[2021-05-15 10:07:13,652] {docker.py:276} INFO - 21/05/15 13:07:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387170554597419284888_0004_m_000132_311
[2021-05-15 10:07:13,655] {docker.py:276} INFO - 21/05/15 13:07:13 INFO Executor: Finished task 132.0 in stage 4.0 (TID 311). 4544 bytes result sent to driver
21/05/15 13:07:13 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 315) (4703d70ea67c, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:13,656] {docker.py:276} INFO - 21/05/15 13:07:13 INFO Executor: Running task 136.0 in stage 4.0 (TID 315)
[2021-05-15 10:07:13,657] {docker.py:276} INFO - 21/05/15 13:07:13 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 311) in 2424 ms on 4703d70ea67c (executor driver) (133/200)
[2021-05-15 10:07:13,665] {docker.py:276} INFO - 21/05/15 13:07:13 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:13,667] {docker.py:276} INFO - 21/05/15 13:07:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386787506981304220570_0004_m_000136_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386787506981304220570_0004_m_000136_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386787506981304220570_0004}; taskId=attempt_202105151305386787506981304220570_0004_m_000136_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f6b00b2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:13 INFO StagingCommitter: Starting: Task committer attempt_202105151305386787506981304220570_0004_m_000136_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386787506981304220570_0004_m_000136_315
[2021-05-15 10:07:13,670] {docker.py:276} INFO - 21/05/15 13:07:13 INFO StagingCommitter: Task committer attempt_202105151305386787506981304220570_0004_m_000136_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386787506981304220570_0004_m_000136_315 : duration 0:00.003s
[2021-05-15 10:07:14,173] {docker.py:276} INFO - 21/05/15 13:07:14 INFO StagingCommitter: Starting: Task committer attempt_202105151305383979334629283106080_0004_m_000133_312: needsTaskCommit() Task attempt_202105151305383979334629283106080_0004_m_000133_312
[2021-05-15 10:07:14,175] {docker.py:276} INFO - 21/05/15 13:07:14 INFO StagingCommitter: Task committer attempt_202105151305383979334629283106080_0004_m_000133_312: needsTaskCommit() Task attempt_202105151305383979334629283106080_0004_m_000133_312: duration 0:00.001s
[2021-05-15 10:07:14,175] {docker.py:276} INFO - 21/05/15 13:07:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383979334629283106080_0004_m_000133_312
[2021-05-15 10:07:14,177] {docker.py:276} INFO - 21/05/15 13:07:14 INFO Executor: Finished task 133.0 in stage 4.0 (TID 312). 4544 bytes result sent to driver
[2021-05-15 10:07:14,179] {docker.py:276} INFO - 21/05/15 13:07:14 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 316) (4703d70ea67c, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:14,180] {docker.py:276} INFO - 21/05/15 13:07:14 INFO Executor: Running task 137.0 in stage 4.0 (TID 316)
[2021-05-15 10:07:14,181] {docker.py:276} INFO - 21/05/15 13:07:14 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 312) in 2555 ms on 4703d70ea67c (executor driver) (134/200)
[2021-05-15 10:07:14,191] {docker.py:276} INFO - 21/05/15 13:07:14 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:14,192] {docker.py:276} INFO - 21/05/15 13:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:14,193] {docker.py:276} INFO - 21/05/15 13:07:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:14,194] {docker.py:276} INFO - 21/05/15 13:07:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:14,194] {docker.py:276} INFO - 21/05/15 13:07:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538475090472551385325_0004_m_000137_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538475090472551385325_0004_m_000137_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538475090472551385325_0004}; taskId=attempt_20210515130538475090472551385325_0004_m_000137_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b373b56}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:14,195] {docker.py:276} INFO - 21/05/15 13:07:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:14,195] {docker.py:276} INFO - 21/05/15 13:07:14 INFO StagingCommitter: Starting: Task committer attempt_20210515130538475090472551385325_0004_m_000137_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538475090472551385325_0004_m_000137_316
[2021-05-15 10:07:14,199] {docker.py:276} INFO - 21/05/15 13:07:14 INFO StagingCommitter: Task committer attempt_20210515130538475090472551385325_0004_m_000137_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538475090472551385325_0004_m_000137_316 : duration 0:00.005s
[2021-05-15 10:07:15,067] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305387715671586949588898_0004_m_000134_313: needsTaskCommit() Task attempt_202105151305387715671586949588898_0004_m_000134_313
21/05/15 13:07:15 INFO StagingCommitter: Task committer attempt_202105151305387715671586949588898_0004_m_000134_313: needsTaskCommit() Task attempt_202105151305387715671586949588898_0004_m_000134_313: duration 0:00.000s
21/05/15 13:07:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387715671586949588898_0004_m_000134_313
[2021-05-15 10:07:15,071] {docker.py:276} INFO - 21/05/15 13:07:15 INFO Executor: Finished task 134.0 in stage 4.0 (TID 313). 4544 bytes result sent to driver
[2021-05-15 10:07:15,072] {docker.py:276} INFO - 21/05/15 13:07:15 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 317) (4703d70ea67c, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/15 13:07:15 INFO Executor: Running task 138.0 in stage 4.0 (TID 317)
[2021-05-15 10:07:15,073] {docker.py:276} INFO - 21/05/15 13:07:15 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 313) in 2532 ms on 4703d70ea67c (executor driver) (135/200)
[2021-05-15 10:07:15,082] {docker.py:276} INFO - 21/05/15 13:07:15 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:15,084] {docker.py:276} INFO - 21/05/15 13:07:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:15,084] {docker.py:276} INFO - 21/05/15 13:07:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:15,084] {docker.py:276} INFO - 21/05/15 13:07:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381110730843999803415_0004_m_000138_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381110730843999803415_0004_m_000138_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381110730843999803415_0004}; taskId=attempt_202105151305381110730843999803415_0004_m_000138_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@392487ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:15,085] {docker.py:276} INFO - 21/05/15 13:07:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:15,085] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305381110730843999803415_0004_m_000138_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381110730843999803415_0004_m_000138_317
[2021-05-15 10:07:15,088] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Task committer attempt_202105151305381110730843999803415_0004_m_000138_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381110730843999803415_0004_m_000138_317 : duration 0:00.002s
[2021-05-15 10:07:15,288] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305385101519766778980801_0004_m_000135_314: needsTaskCommit() Task attempt_202105151305385101519766778980801_0004_m_000135_314
[2021-05-15 10:07:15,288] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Task committer attempt_202105151305385101519766778980801_0004_m_000135_314: needsTaskCommit() Task attempt_202105151305385101519766778980801_0004_m_000135_314: duration 0:00.000s
21/05/15 13:07:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385101519766778980801_0004_m_000135_314
[2021-05-15 10:07:15,291] {docker.py:276} INFO - 21/05/15 13:07:15 INFO Executor: Finished task 135.0 in stage 4.0 (TID 314). 4544 bytes result sent to driver
[2021-05-15 10:07:15,291] {docker.py:276} INFO - 21/05/15 13:07:15 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 318) (4703d70ea67c, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:15,292] {docker.py:276} INFO - 21/05/15 13:07:15 INFO Executor: Running task 139.0 in stage 4.0 (TID 318)
[2021-05-15 10:07:15,292] {docker.py:276} INFO - 21/05/15 13:07:15 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 314) in 2371 ms on 4703d70ea67c (executor driver) (136/200)
[2021-05-15 10:07:15,302] {docker.py:276} INFO - 21/05/15 13:07:15 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:15,304] {docker.py:276} INFO - 21/05/15 13:07:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:15,304] {docker.py:276} INFO - 21/05/15 13:07:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:15,305] {docker.py:276} INFO - 21/05/15 13:07:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381136994917353021051_0004_m_000139_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381136994917353021051_0004_m_000139_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381136994917353021051_0004}; taskId=attempt_202105151305381136994917353021051_0004_m_000139_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4741eba9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:15 INFO StagingCommitter: Starting: Task committer attempt_202105151305381136994917353021051_0004_m_000139_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381136994917353021051_0004_m_000139_318
[2021-05-15 10:07:15,307] {docker.py:276} INFO - 21/05/15 13:07:15 INFO StagingCommitter: Task committer attempt_202105151305381136994917353021051_0004_m_000139_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381136994917353021051_0004_m_000139_318 : duration 0:00.003s
[2021-05-15 10:07:16,184] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305386787506981304220570_0004_m_000136_315: needsTaskCommit() Task attempt_202105151305386787506981304220570_0004_m_000136_315
[2021-05-15 10:07:16,186] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Task committer attempt_202105151305386787506981304220570_0004_m_000136_315: needsTaskCommit() Task attempt_202105151305386787506981304220570_0004_m_000136_315: duration 0:00.001s
21/05/15 13:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386787506981304220570_0004_m_000136_315
[2021-05-15 10:07:16,187] {docker.py:276} INFO - 21/05/15 13:07:16 INFO Executor: Finished task 136.0 in stage 4.0 (TID 315). 4544 bytes result sent to driver
[2021-05-15 10:07:16,189] {docker.py:276} INFO - 21/05/15 13:07:16 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 319) (4703d70ea67c, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:16,190] {docker.py:276} INFO - 21/05/15 13:07:16 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 315) in 2538 ms on 4703d70ea67c (executor driver) (137/200)
[2021-05-15 10:07:16,192] {docker.py:276} INFO - 21/05/15 13:07:16 INFO Executor: Running task 140.0 in stage 4.0 (TID 319)
[2021-05-15 10:07:16,202] {docker.py:276} INFO - 21/05/15 13:07:16 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:16,204] {docker.py:276} INFO - 21/05/15 13:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:16,205] {docker.py:276} INFO - 21/05/15 13:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384880646354646424453_0004_m_000140_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384880646354646424453_0004_m_000140_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384880646354646424453_0004}; taskId=attempt_202105151305384880646354646424453_0004_m_000140_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2de08c8b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:16,205] {docker.py:276} INFO - 21/05/15 13:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305384880646354646424453_0004_m_000140_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384880646354646424453_0004_m_000140_319
[2021-05-15 10:07:16,208] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Task committer attempt_202105151305384880646354646424453_0004_m_000140_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384880646354646424453_0004_m_000140_319 : duration 0:00.003s
[2021-05-15 10:07:16,741] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Starting: Task committer attempt_20210515130538475090472551385325_0004_m_000137_316: needsTaskCommit() Task attempt_20210515130538475090472551385325_0004_m_000137_316
[2021-05-15 10:07:16,742] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Task committer attempt_20210515130538475090472551385325_0004_m_000137_316: needsTaskCommit() Task attempt_20210515130538475090472551385325_0004_m_000137_316: duration 0:00.002s
21/05/15 13:07:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538475090472551385325_0004_m_000137_316
[2021-05-15 10:07:16,744] {docker.py:276} INFO - 21/05/15 13:07:16 INFO Executor: Finished task 137.0 in stage 4.0 (TID 316). 4544 bytes result sent to driver
[2021-05-15 10:07:16,745] {docker.py:276} INFO - 21/05/15 13:07:16 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 320) (4703d70ea67c, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:16,746] {docker.py:276} INFO - 21/05/15 13:07:16 INFO Executor: Running task 141.0 in stage 4.0 (TID 320)
[2021-05-15 10:07:16,747] {docker.py:276} INFO - 21/05/15 13:07:16 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 316) in 2571 ms on 4703d70ea67c (executor driver) (138/200)
[2021-05-15 10:07:16,756] {docker.py:276} INFO - 21/05/15 13:07:16 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:16,758] {docker.py:276} INFO - 21/05/15 13:07:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384493181178154250665_0004_m_000141_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384493181178154250665_0004_m_000141_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384493181178154250665_0004}; taskId=attempt_202105151305384493181178154250665_0004_m_000141_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ba717af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:16,758] {docker.py:276} INFO - 21/05/15 13:07:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:16,759] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Starting: Task committer attempt_202105151305384493181178154250665_0004_m_000141_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384493181178154250665_0004_m_000141_320
[2021-05-15 10:07:16,762] {docker.py:276} INFO - 21/05/15 13:07:16 INFO StagingCommitter: Task committer attempt_202105151305384493181178154250665_0004_m_000141_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384493181178154250665_0004_m_000141_320 : duration 0:00.004s
[2021-05-15 10:07:17,771] {docker.py:276} INFO - 21/05/15 13:07:17 INFO StagingCommitter: Starting: Task committer attempt_202105151305381110730843999803415_0004_m_000138_317: needsTaskCommit() Task attempt_202105151305381110730843999803415_0004_m_000138_317
[2021-05-15 10:07:17,772] {docker.py:276} INFO - 21/05/15 13:07:17 INFO StagingCommitter: Task committer attempt_202105151305381110730843999803415_0004_m_000138_317: needsTaskCommit() Task attempt_202105151305381110730843999803415_0004_m_000138_317: duration 0:00.002s
[2021-05-15 10:07:17,775] {docker.py:276} INFO - 21/05/15 13:07:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381110730843999803415_0004_m_000138_317
[2021-05-15 10:07:17,776] {docker.py:276} INFO - 21/05/15 13:07:17 INFO Executor: Finished task 138.0 in stage 4.0 (TID 317). 4544 bytes result sent to driver
[2021-05-15 10:07:17,778] {docker.py:276} INFO - 21/05/15 13:07:17 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 321) (4703d70ea67c, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:17,779] {docker.py:276} INFO - 21/05/15 13:07:17 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 317) in 2712 ms on 4703d70ea67c (executor driver) (139/200)
[2021-05-15 10:07:17,780] {docker.py:276} INFO - 21/05/15 13:07:17 INFO Executor: Running task 142.0 in stage 4.0 (TID 321)
[2021-05-15 10:07:17,788] {docker.py:276} INFO - 21/05/15 13:07:17 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:17,790] {docker.py:276} INFO - 21/05/15 13:07:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:17,790] {docker.py:276} INFO - 21/05/15 13:07:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386683513900059199411_0004_m_000142_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386683513900059199411_0004_m_000142_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386683513900059199411_0004}; taskId=attempt_202105151305386683513900059199411_0004_m_000142_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@331a8205}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:17 INFO StagingCommitter: Starting: Task committer attempt_202105151305386683513900059199411_0004_m_000142_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386683513900059199411_0004_m_000142_321
[2021-05-15 10:07:17,793] {docker.py:276} INFO - 21/05/15 13:07:17 INFO StagingCommitter: Task committer attempt_202105151305386683513900059199411_0004_m_000142_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386683513900059199411_0004_m_000142_321 : duration 0:00.003s
[2021-05-15 10:07:18,085] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305381136994917353021051_0004_m_000139_318: needsTaskCommit() Task attempt_202105151305381136994917353021051_0004_m_000139_318
[2021-05-15 10:07:18,086] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Task committer attempt_202105151305381136994917353021051_0004_m_000139_318: needsTaskCommit() Task attempt_202105151305381136994917353021051_0004_m_000139_318: duration 0:00.000s
21/05/15 13:07:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381136994917353021051_0004_m_000139_318
[2021-05-15 10:07:18,088] {docker.py:276} INFO - 21/05/15 13:07:18 INFO Executor: Finished task 139.0 in stage 4.0 (TID 318). 4544 bytes result sent to driver
[2021-05-15 10:07:18,089] {docker.py:276} INFO - 21/05/15 13:07:18 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 322) (4703d70ea67c, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:18,090] {docker.py:276} INFO - 21/05/15 13:07:18 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 318) in 2804 ms on 4703d70ea67c (executor driver) (140/200)
[2021-05-15 10:07:18,091] {docker.py:276} INFO - 21/05/15 13:07:18 INFO Executor: Running task 143.0 in stage 4.0 (TID 322)
[2021-05-15 10:07:18,100] {docker.py:276} INFO - 21/05/15 13:07:18 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:18,103] {docker.py:276} INFO - 21/05/15 13:07:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:18,103] {docker.py:276} INFO - 21/05/15 13:07:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:18,104] {docker.py:276} INFO - 21/05/15 13:07:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381511121468512780472_0004_m_000143_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381511121468512780472_0004_m_000143_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381511121468512780472_0004}; taskId=attempt_202105151305381511121468512780472_0004_m_000143_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@191c886a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:18,104] {docker.py:276} INFO - 21/05/15 13:07:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:18,104] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305381511121468512780472_0004_m_000143_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381511121468512780472_0004_m_000143_322
[2021-05-15 10:07:18,106] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Task committer attempt_202105151305381511121468512780472_0004_m_000143_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381511121468512780472_0004_m_000143_322 : duration 0:00.003s
[2021-05-15 10:07:18,659] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305384880646354646424453_0004_m_000140_319: needsTaskCommit() Task attempt_202105151305384880646354646424453_0004_m_000140_319
[2021-05-15 10:07:18,660] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Task committer attempt_202105151305384880646354646424453_0004_m_000140_319: needsTaskCommit() Task attempt_202105151305384880646354646424453_0004_m_000140_319: duration 0:00.000s
21/05/15 13:07:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384880646354646424453_0004_m_000140_319
[2021-05-15 10:07:18,661] {docker.py:276} INFO - 21/05/15 13:07:18 INFO Executor: Finished task 140.0 in stage 4.0 (TID 319). 4544 bytes result sent to driver
[2021-05-15 10:07:18,662] {docker.py:276} INFO - 21/05/15 13:07:18 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 323) (4703d70ea67c, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:18,663] {docker.py:276} INFO - 21/05/15 13:07:18 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 319) in 2477 ms on 4703d70ea67c (executor driver) (141/200)
[2021-05-15 10:07:18,664] {docker.py:276} INFO - 21/05/15 13:07:18 INFO Executor: Running task 144.0 in stage 4.0 (TID 323)
[2021-05-15 10:07:18,674] {docker.py:276} INFO - 21/05/15 13:07:18 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:18,674] {docker.py:276} INFO - 21/05/15 13:07:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:18,676] {docker.py:276} INFO - 21/05/15 13:07:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:18,676] {docker.py:276} INFO - 21/05/15 13:07:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:18,677] {docker.py:276} INFO - 21/05/15 13:07:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:18,677] {docker.py:276} INFO - 21/05/15 13:07:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388657793175713263405_0004_m_000144_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388657793175713263405_0004_m_000144_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388657793175713263405_0004}; taskId=attempt_202105151305388657793175713263405_0004_m_000144_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bde8bf6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:18,677] {docker.py:276} INFO - 21/05/15 13:07:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:18,678] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Starting: Task committer attempt_202105151305388657793175713263405_0004_m_000144_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388657793175713263405_0004_m_000144_323
[2021-05-15 10:07:18,680] {docker.py:276} INFO - 21/05/15 13:07:18 INFO StagingCommitter: Task committer attempt_202105151305388657793175713263405_0004_m_000144_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388657793175713263405_0004_m_000144_323 : duration 0:00.003s
[2021-05-15 10:07:19,104] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305384493181178154250665_0004_m_000141_320: needsTaskCommit() Task attempt_202105151305384493181178154250665_0004_m_000141_320
[2021-05-15 10:07:19,106] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Task committer attempt_202105151305384493181178154250665_0004_m_000141_320: needsTaskCommit() Task attempt_202105151305384493181178154250665_0004_m_000141_320: duration 0:00.001s
21/05/15 13:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384493181178154250665_0004_m_000141_320
[2021-05-15 10:07:19,108] {docker.py:276} INFO - 21/05/15 13:07:19 INFO Executor: Finished task 141.0 in stage 4.0 (TID 320). 4544 bytes result sent to driver
[2021-05-15 10:07:19,113] {docker.py:276} INFO - 21/05/15 13:07:19 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 324) (4703d70ea67c, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:19,113] {docker.py:276} INFO - 21/05/15 13:07:19 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 320) in 2372 ms on 4703d70ea67c (executor driver) (142/200)
[2021-05-15 10:07:19,114] {docker.py:276} INFO - 21/05/15 13:07:19 INFO Executor: Running task 145.0 in stage 4.0 (TID 324)
[2021-05-15 10:07:19,123] {docker.py:276} INFO - 21/05/15 13:07:19 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:19,124] {docker.py:276} INFO - 21/05/15 13:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:07:19,126] {docker.py:276} INFO - 21/05/15 13:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:19,126] {docker.py:276} INFO - 21/05/15 13:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385755353712916329184_0004_m_000145_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385755353712916329184_0004_m_000145_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385755353712916329184_0004}; taskId=attempt_202105151305385755353712916329184_0004_m_000145_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d7fe0e9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:19,126] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305385755353712916329184_0004_m_000145_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385755353712916329184_0004_m_000145_324
[2021-05-15 10:07:19,129] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Task committer attempt_202105151305385755353712916329184_0004_m_000145_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385755353712916329184_0004_m_000145_324 : duration 0:00.003s
[2021-05-15 10:07:19,289] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Starting: Task committer attempt_202105151305386683513900059199411_0004_m_000142_321: needsTaskCommit() Task attempt_202105151305386683513900059199411_0004_m_000142_321
[2021-05-15 10:07:19,290] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Task committer attempt_202105151305386683513900059199411_0004_m_000142_321: needsTaskCommit() Task attempt_202105151305386683513900059199411_0004_m_000142_321: duration 0:00.000s
21/05/15 13:07:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386683513900059199411_0004_m_000142_321
[2021-05-15 10:07:19,291] {docker.py:276} INFO - 21/05/15 13:07:19 INFO Executor: Finished task 142.0 in stage 4.0 (TID 321). 4544 bytes result sent to driver
[2021-05-15 10:07:19,293] {docker.py:276} INFO - 21/05/15 13:07:19 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 325) (4703d70ea67c, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:19,294] {docker.py:276} INFO - 21/05/15 13:07:19 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 321) in 1518 ms on 4703d70ea67c (executor driver) (143/200)
21/05/15 13:07:19 INFO Executor: Running task 146.0 in stage 4.0 (TID 325)
[2021-05-15 10:07:19,303] {docker.py:276} INFO - 21/05/15 13:07:19 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:19,305] {docker.py:276} INFO - 21/05/15 13:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538100238811024698630_0004_m_000146_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538100238811024698630_0004_m_000146_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538100238811024698630_0004}; taskId=attempt_20210515130538100238811024698630_0004_m_000146_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@799a2df4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:19,305] {docker.py:276} INFO - 21/05/15 13:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:19,306] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Starting: Task committer attempt_20210515130538100238811024698630_0004_m_000146_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538100238811024698630_0004_m_000146_325
[2021-05-15 10:07:19,309] {docker.py:276} INFO - 21/05/15 13:07:19 INFO StagingCommitter: Task committer attempt_20210515130538100238811024698630_0004_m_000146_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538100238811024698630_0004_m_000146_325 : duration 0:00.004s
[2021-05-15 10:07:20,650] {docker.py:276} INFO - 21/05/15 13:07:20 INFO StagingCommitter: Starting: Task committer attempt_202105151305381511121468512780472_0004_m_000143_322: needsTaskCommit() Task attempt_202105151305381511121468512780472_0004_m_000143_322
21/05/15 13:07:20 INFO StagingCommitter: Task committer attempt_202105151305381511121468512780472_0004_m_000143_322: needsTaskCommit() Task attempt_202105151305381511121468512780472_0004_m_000143_322: duration 0:00.001s
[2021-05-15 10:07:20,651] {docker.py:276} INFO - 21/05/15 13:07:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381511121468512780472_0004_m_000143_322
[2021-05-15 10:07:20,652] {docker.py:276} INFO - 21/05/15 13:07:20 INFO Executor: Finished task 143.0 in stage 4.0 (TID 322). 4544 bytes result sent to driver
[2021-05-15 10:07:20,654] {docker.py:276} INFO - 21/05/15 13:07:20 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 326) (4703d70ea67c, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:20,655] {docker.py:276} INFO - 21/05/15 13:07:20 INFO Executor: Running task 147.0 in stage 4.0 (TID 326)
21/05/15 13:07:20 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 322) in 2569 ms on 4703d70ea67c (executor driver) (144/200)
[2021-05-15 10:07:20,664] {docker.py:276} INFO - 21/05/15 13:07:20 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:20,666] {docker.py:276} INFO - 21/05/15 13:07:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387154630832033309933_0004_m_000147_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387154630832033309933_0004_m_000147_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387154630832033309933_0004}; taskId=attempt_202105151305387154630832033309933_0004_m_000147_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@772ff631}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:20,667] {docker.py:276} INFO - 21/05/15 13:07:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:20 INFO StagingCommitter: Starting: Task committer attempt_202105151305387154630832033309933_0004_m_000147_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387154630832033309933_0004_m_000147_326
[2021-05-15 10:07:20,669] {docker.py:276} INFO - 21/05/15 13:07:20 INFO StagingCommitter: Task committer attempt_202105151305387154630832033309933_0004_m_000147_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387154630832033309933_0004_m_000147_326 : duration 0:00.003s
[2021-05-15 10:07:20,996] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305388657793175713263405_0004_m_000144_323: needsTaskCommit() Task attempt_202105151305388657793175713263405_0004_m_000144_323
[2021-05-15 10:07:20,997] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Task committer attempt_202105151305388657793175713263405_0004_m_000144_323: needsTaskCommit() Task attempt_202105151305388657793175713263405_0004_m_000144_323: duration 0:00.001s
[2021-05-15 10:07:20,998] {docker.py:276} INFO - 21/05/15 13:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388657793175713263405_0004_m_000144_323
[2021-05-15 10:07:20,999] {docker.py:276} INFO - 21/05/15 13:07:21 INFO Executor: Finished task 144.0 in stage 4.0 (TID 323). 4544 bytes result sent to driver
[2021-05-15 10:07:21,000] {docker.py:276} INFO - 21/05/15 13:07:21 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 327) (4703d70ea67c, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:21,002] {docker.py:276} INFO - 21/05/15 13:07:21 INFO Executor: Running task 148.0 in stage 4.0 (TID 327)
[2021-05-15 10:07:21,004] {docker.py:276} INFO - 21/05/15 13:07:21 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 323) in 2344 ms on 4703d70ea67c (executor driver) (145/200)
[2021-05-15 10:07:21,027] {docker.py:276} INFO - 21/05/15 13:07:21 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:21,029] {docker.py:276} INFO - 21/05/15 13:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:21,030] {docker.py:276} INFO - 21/05/15 13:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:21,030] {docker.py:276} INFO - 21/05/15 13:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538963579241723414508_0004_m_000148_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538963579241723414508_0004_m_000148_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538963579241723414508_0004}; taskId=attempt_20210515130538963579241723414508_0004_m_000148_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c188c4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:21,031] {docker.py:276} INFO - 21/05/15 13:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:21,031] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Starting: Task committer attempt_20210515130538963579241723414508_0004_m_000148_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538963579241723414508_0004_m_000148_327
[2021-05-15 10:07:21,033] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Task committer attempt_20210515130538963579241723414508_0004_m_000148_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538963579241723414508_0004_m_000148_327 : duration 0:00.003s
[2021-05-15 10:07:21,646] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Starting: Task committer attempt_20210515130538100238811024698630_0004_m_000146_325: needsTaskCommit() Task attempt_20210515130538100238811024698630_0004_m_000146_325
[2021-05-15 10:07:21,647] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Task committer attempt_20210515130538100238811024698630_0004_m_000146_325: needsTaskCommit() Task attempt_20210515130538100238811024698630_0004_m_000146_325: duration 0:00.001s
21/05/15 13:07:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538100238811024698630_0004_m_000146_325
[2021-05-15 10:07:21,648] {docker.py:276} INFO - 21/05/15 13:07:21 INFO Executor: Finished task 146.0 in stage 4.0 (TID 325). 4587 bytes result sent to driver
[2021-05-15 10:07:21,649] {docker.py:276} INFO - 21/05/15 13:07:21 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 328) (4703d70ea67c, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:21,650] {docker.py:276} INFO - 21/05/15 13:07:21 INFO Executor: Running task 149.0 in stage 4.0 (TID 328)
[2021-05-15 10:07:21,651] {docker.py:276} INFO - 21/05/15 13:07:21 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 325) in 2361 ms on 4703d70ea67c (executor driver) (146/200)
[2021-05-15 10:07:21,660] {docker.py:276} INFO - 21/05/15 13:07:21 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:21,662] {docker.py:276} INFO - 21/05/15 13:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386136770312440070786_0004_m_000149_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386136770312440070786_0004_m_000149_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386136770312440070786_0004}; taskId=attempt_202105151305386136770312440070786_0004_m_000149_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c3539b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:21,663] {docker.py:276} INFO - 21/05/15 13:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:21 INFO StagingCommitter: Starting: Task committer attempt_202105151305386136770312440070786_0004_m_000149_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386136770312440070786_0004_m_000149_328
[2021-05-15 10:07:21,665] {docker.py:276} INFO - 21/05/15 13:07:21 INFO StagingCommitter: Task committer attempt_202105151305386136770312440070786_0004_m_000149_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386136770312440070786_0004_m_000149_328 : duration 0:00.003s
[2021-05-15 10:07:22,113] {docker.py:276} INFO - 21/05/15 13:07:22 INFO StagingCommitter: Starting: Task committer attempt_202105151305385755353712916329184_0004_m_000145_324: needsTaskCommit() Task attempt_202105151305385755353712916329184_0004_m_000145_324
[2021-05-15 10:07:22,114] {docker.py:276} INFO - 21/05/15 13:07:22 INFO StagingCommitter: Task committer attempt_202105151305385755353712916329184_0004_m_000145_324: needsTaskCommit() Task attempt_202105151305385755353712916329184_0004_m_000145_324: duration 0:00.000s
21/05/15 13:07:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385755353712916329184_0004_m_000145_324
[2021-05-15 10:07:22,116] {docker.py:276} INFO - 21/05/15 13:07:22 INFO Executor: Finished task 145.0 in stage 4.0 (TID 324). 4587 bytes result sent to driver
[2021-05-15 10:07:22,117] {docker.py:276} INFO - 21/05/15 13:07:22 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 329) (4703d70ea67c, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:22,118] {docker.py:276} INFO - 21/05/15 13:07:22 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 324) in 3010 ms on 4703d70ea67c (executor driver) (147/200)
21/05/15 13:07:22 INFO Executor: Running task 150.0 in stage 4.0 (TID 329)
[2021-05-15 10:07:22,127] {docker.py:276} INFO - 21/05/15 13:07:22 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:22,130] {docker.py:276} INFO - 21/05/15 13:07:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:22,130] {docker.py:276} INFO - 21/05/15 13:07:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:22,131] {docker.py:276} INFO - 21/05/15 13:07:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382797933290890648602_0004_m_000150_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382797933290890648602_0004_m_000150_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382797933290890648602_0004}; taskId=attempt_202105151305382797933290890648602_0004_m_000150_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@503dd60f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:22 INFO StagingCommitter: Starting: Task committer attempt_202105151305382797933290890648602_0004_m_000150_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382797933290890648602_0004_m_000150_329
[2021-05-15 10:07:22,134] {docker.py:276} INFO - 21/05/15 13:07:22 INFO StagingCommitter: Task committer attempt_202105151305382797933290890648602_0004_m_000150_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382797933290890648602_0004_m_000150_329 : duration 0:00.004s
[2021-05-15 10:07:23,175] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305387154630832033309933_0004_m_000147_326: needsTaskCommit() Task attempt_202105151305387154630832033309933_0004_m_000147_326
[2021-05-15 10:07:23,176] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Task committer attempt_202105151305387154630832033309933_0004_m_000147_326: needsTaskCommit() Task attempt_202105151305387154630832033309933_0004_m_000147_326: duration 0:00.001s
21/05/15 13:07:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387154630832033309933_0004_m_000147_326
[2021-05-15 10:07:23,179] {docker.py:276} INFO - 21/05/15 13:07:23 INFO Executor: Finished task 147.0 in stage 4.0 (TID 326). 4587 bytes result sent to driver
[2021-05-15 10:07:23,180] {docker.py:276} INFO - 21/05/15 13:07:23 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 330) (4703d70ea67c, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:23,181] {docker.py:276} INFO - 21/05/15 13:07:23 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 326) in 2530 ms on 4703d70ea67c (executor driver) (148/200)
[2021-05-15 10:07:23,182] {docker.py:276} INFO - 21/05/15 13:07:23 INFO Executor: Running task 151.0 in stage 4.0 (TID 330)
[2021-05-15 10:07:23,191] {docker.py:276} INFO - 21/05/15 13:07:23 INFO ShuffleBlockFetcherIterator: Getting 4 (11.3 KiB) non-empty blocks including 4 (11.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:23,193] {docker.py:276} INFO - 21/05/15 13:07:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:23,193] {docker.py:276} INFO - 21/05/15 13:07:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388150628270764496914_0004_m_000151_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388150628270764496914_0004_m_000151_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388150628270764496914_0004}; taskId=attempt_202105151305388150628270764496914_0004_m_000151_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4afe61c5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305388150628270764496914_0004_m_000151_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388150628270764496914_0004_m_000151_330
[2021-05-15 10:07:23,196] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Task committer attempt_202105151305388150628270764496914_0004_m_000151_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388150628270764496914_0004_m_000151_330 : duration 0:00.002s
[2021-05-15 10:07:23,450] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Starting: Task committer attempt_20210515130538963579241723414508_0004_m_000148_327: needsTaskCommit() Task attempt_20210515130538963579241723414508_0004_m_000148_327
[2021-05-15 10:07:23,452] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Task committer attempt_20210515130538963579241723414508_0004_m_000148_327: needsTaskCommit() Task attempt_20210515130538963579241723414508_0004_m_000148_327: duration 0:00.002s
[2021-05-15 10:07:23,453] {docker.py:276} INFO - 21/05/15 13:07:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538963579241723414508_0004_m_000148_327
[2021-05-15 10:07:23,455] {docker.py:276} INFO - 21/05/15 13:07:23 INFO Executor: Finished task 148.0 in stage 4.0 (TID 327). 4587 bytes result sent to driver
[2021-05-15 10:07:23,456] {docker.py:276} INFO - 21/05/15 13:07:23 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 331) (4703d70ea67c, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:23,457] {docker.py:276} INFO - 21/05/15 13:07:23 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 327) in 2460 ms on 4703d70ea67c (executor driver) (149/200)
[2021-05-15 10:07:23,458] {docker.py:276} INFO - 21/05/15 13:07:23 INFO Executor: Running task 152.0 in stage 4.0 (TID 331)
[2021-05-15 10:07:23,467] {docker.py:276} INFO - 21/05/15 13:07:23 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:23,469] {docker.py:276} INFO - 21/05/15 13:07:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384985684707389267070_0004_m_000152_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384985684707389267070_0004_m_000152_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384985684707389267070_0004}; taskId=attempt_202105151305384985684707389267070_0004_m_000152_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@768fe083}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:23 INFO StagingCommitter: Starting: Task committer attempt_202105151305384985684707389267070_0004_m_000152_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384985684707389267070_0004_m_000152_331
[2021-05-15 10:07:23,472] {docker.py:276} INFO - 21/05/15 13:07:23 INFO StagingCommitter: Task committer attempt_202105151305384985684707389267070_0004_m_000152_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384985684707389267070_0004_m_000152_331 : duration 0:00.002s
[2021-05-15 10:07:24,065] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305386136770312440070786_0004_m_000149_328: needsTaskCommit() Task attempt_202105151305386136770312440070786_0004_m_000149_328
[2021-05-15 10:07:24,065] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Task committer attempt_202105151305386136770312440070786_0004_m_000149_328: needsTaskCommit() Task attempt_202105151305386136770312440070786_0004_m_000149_328: duration 0:00.001s
21/05/15 13:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386136770312440070786_0004_m_000149_328
[2021-05-15 10:07:24,067] {docker.py:276} INFO - 21/05/15 13:07:24 INFO Executor: Finished task 149.0 in stage 4.0 (TID 328). 4544 bytes result sent to driver
[2021-05-15 10:07:24,068] {docker.py:276} INFO - 21/05/15 13:07:24 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 332) (4703d70ea67c, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:24,068] {docker.py:276} INFO - 21/05/15 13:07:24 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 328) in 2423 ms on 4703d70ea67c (executor driver) (150/200)
[2021-05-15 10:07:24,069] {docker.py:276} INFO - 21/05/15 13:07:24 INFO Executor: Running task 153.0 in stage 4.0 (TID 332)
[2021-05-15 10:07:24,076] {docker.py:276} INFO - 21/05/15 13:07:24 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:24,076] {docker.py:276} INFO - 21/05/15 13:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:24,078] {docker.py:276} INFO - 21/05/15 13:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:24,078] {docker.py:276} INFO - 21/05/15 13:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:24,079] {docker.py:276} INFO - 21/05/15 13:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386062204820033823458_0004_m_000153_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386062204820033823458_0004_m_000153_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386062204820033823458_0004}; taskId=attempt_202105151305386062204820033823458_0004_m_000153_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@493e34dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:24,079] {docker.py:276} INFO - 21/05/15 13:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:24,079] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305386062204820033823458_0004_m_000153_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386062204820033823458_0004_m_000153_332
[2021-05-15 10:07:24,083] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Task committer attempt_202105151305386062204820033823458_0004_m_000153_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386062204820033823458_0004_m_000153_332 : duration 0:00.003s
[2021-05-15 10:07:24,629] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305382797933290890648602_0004_m_000150_329: needsTaskCommit() Task attempt_202105151305382797933290890648602_0004_m_000150_329
21/05/15 13:07:24 INFO StagingCommitter: Task committer attempt_202105151305382797933290890648602_0004_m_000150_329: needsTaskCommit() Task attempt_202105151305382797933290890648602_0004_m_000150_329: duration 0:00.000s
21/05/15 13:07:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382797933290890648602_0004_m_000150_329
[2021-05-15 10:07:24,631] {docker.py:276} INFO - 21/05/15 13:07:24 INFO Executor: Finished task 150.0 in stage 4.0 (TID 329). 4544 bytes result sent to driver
[2021-05-15 10:07:24,633] {docker.py:276} INFO - 21/05/15 13:07:24 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 333) (4703d70ea67c, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:24,633] {docker.py:276} INFO - 21/05/15 13:07:24 INFO Executor: Running task 154.0 in stage 4.0 (TID 333)
[2021-05-15 10:07:24,634] {docker.py:276} INFO - 21/05/15 13:07:24 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 329) in 2520 ms on 4703d70ea67c (executor driver) (151/200)
[2021-05-15 10:07:24,644] {docker.py:276} INFO - 21/05/15 13:07:24 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:24,645] {docker.py:276} INFO - 21/05/15 13:07:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388208621372355764689_0004_m_000154_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388208621372355764689_0004_m_000154_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388208621372355764689_0004}; taskId=attempt_202105151305388208621372355764689_0004_m_000154_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@567c8f78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:24 INFO StagingCommitter: Starting: Task committer attempt_202105151305388208621372355764689_0004_m_000154_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388208621372355764689_0004_m_000154_333
[2021-05-15 10:07:24,648] {docker.py:276} INFO - 21/05/15 13:07:24 INFO StagingCommitter: Task committer attempt_202105151305388208621372355764689_0004_m_000154_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388208621372355764689_0004_m_000154_333 : duration 0:00.002s
[2021-05-15 10:07:25,653] {docker.py:276} INFO - 21/05/15 13:07:25 INFO StagingCommitter: Starting: Task committer attempt_202105151305388150628270764496914_0004_m_000151_330: needsTaskCommit() Task attempt_202105151305388150628270764496914_0004_m_000151_330
21/05/15 13:07:25 INFO StagingCommitter: Task committer attempt_202105151305388150628270764496914_0004_m_000151_330: needsTaskCommit() Task attempt_202105151305388150628270764496914_0004_m_000151_330: duration 0:00.001s
21/05/15 13:07:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388150628270764496914_0004_m_000151_330
[2021-05-15 10:07:25,655] {docker.py:276} INFO - 21/05/15 13:07:25 INFO Executor: Finished task 151.0 in stage 4.0 (TID 330). 4544 bytes result sent to driver
[2021-05-15 10:07:25,656] {docker.py:276} INFO - 21/05/15 13:07:25 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 334) (4703d70ea67c, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:25,656] {docker.py:276} INFO - 21/05/15 13:07:25 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 330) in 2445 ms on 4703d70ea67c (executor driver) (152/200)
21/05/15 13:07:25 INFO Executor: Running task 155.0 in stage 4.0 (TID 334)
[2021-05-15 10:07:25,665] {docker.py:276} INFO - 21/05/15 13:07:25 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:25,666] {docker.py:276} INFO - 21/05/15 13:07:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:25,667] {docker.py:276} INFO - 21/05/15 13:07:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387566492019925467120_0004_m_000155_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387566492019925467120_0004_m_000155_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387566492019925467120_0004}; taskId=attempt_202105151305387566492019925467120_0004_m_000155_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cb71cc4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:25 INFO StagingCommitter: Starting: Task committer attempt_202105151305387566492019925467120_0004_m_000155_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387566492019925467120_0004_m_000155_334
[2021-05-15 10:07:25,669] {docker.py:276} INFO - 21/05/15 13:07:25 INFO StagingCommitter: Task committer attempt_202105151305387566492019925467120_0004_m_000155_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387566492019925467120_0004_m_000155_334 : duration 0:00.003s
[2021-05-15 10:07:25,987] {docker.py:276} INFO - 21/05/15 13:07:25 INFO StagingCommitter: Starting: Task committer attempt_202105151305384985684707389267070_0004_m_000152_331: needsTaskCommit() Task attempt_202105151305384985684707389267070_0004_m_000152_331
[2021-05-15 10:07:25,988] {docker.py:276} INFO - 21/05/15 13:07:25 INFO StagingCommitter: Task committer attempt_202105151305384985684707389267070_0004_m_000152_331: needsTaskCommit() Task attempt_202105151305384985684707389267070_0004_m_000152_331: duration 0:00.001s
21/05/15 13:07:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384985684707389267070_0004_m_000152_331
[2021-05-15 10:07:25,991] {docker.py:276} INFO - 21/05/15 13:07:25 INFO Executor: Finished task 152.0 in stage 4.0 (TID 331). 4544 bytes result sent to driver
[2021-05-15 10:07:25,993] {docker.py:276} INFO - 21/05/15 13:07:25 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 335) (4703d70ea67c, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:25,994] {docker.py:276} INFO - 21/05/15 13:07:25 INFO Executor: Running task 156.0 in stage 4.0 (TID 335)
21/05/15 13:07:25 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 331) in 2506 ms on 4703d70ea67c (executor driver) (153/200)
[2021-05-15 10:07:26,003] {docker.py:276} INFO - 21/05/15 13:07:26 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:26,005] {docker.py:276} INFO - 21/05/15 13:07:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538705737652004946182_0004_m_000156_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538705737652004946182_0004_m_000156_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538705737652004946182_0004}; taskId=attempt_20210515130538705737652004946182_0004_m_000156_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29348963}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:26 INFO StagingCommitter: Starting: Task committer attempt_20210515130538705737652004946182_0004_m_000156_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538705737652004946182_0004_m_000156_335
[2021-05-15 10:07:26,008] {docker.py:276} INFO - 21/05/15 13:07:26 INFO StagingCommitter: Task committer attempt_20210515130538705737652004946182_0004_m_000156_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538705737652004946182_0004_m_000156_335 : duration 0:00.003s
[2021-05-15 10:07:26,575] {docker.py:276} INFO - 21/05/15 13:07:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305386062204820033823458_0004_m_000153_332: needsTaskCommit() Task attempt_202105151305386062204820033823458_0004_m_000153_332
[2021-05-15 10:07:26,576] {docker.py:276} INFO - 21/05/15 13:07:26 INFO StagingCommitter: Task committer attempt_202105151305386062204820033823458_0004_m_000153_332: needsTaskCommit() Task attempt_202105151305386062204820033823458_0004_m_000153_332: duration 0:00.001s
21/05/15 13:07:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386062204820033823458_0004_m_000153_332
[2021-05-15 10:07:26,577] {docker.py:276} INFO - 21/05/15 13:07:26 INFO Executor: Finished task 153.0 in stage 4.0 (TID 332). 4544 bytes result sent to driver
[2021-05-15 10:07:26,579] {docker.py:276} INFO - 21/05/15 13:07:26 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 336) (4703d70ea67c, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:26,580] {docker.py:276} INFO - 21/05/15 13:07:26 INFO Executor: Running task 157.0 in stage 4.0 (TID 336)
[2021-05-15 10:07:26,580] {docker.py:276} INFO - 21/05/15 13:07:26 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 332) in 2479 ms on 4703d70ea67c (executor driver) (154/200)
[2021-05-15 10:07:26,589] {docker.py:276} INFO - 21/05/15 13:07:26 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:26,591] {docker.py:276} INFO - 21/05/15 13:07:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384775471144970387014_0004_m_000157_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384775471144970387014_0004_m_000157_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384775471144970387014_0004}; taskId=attempt_202105151305384775471144970387014_0004_m_000157_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c20ad61}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:26 INFO StagingCommitter: Starting: Task committer attempt_202105151305384775471144970387014_0004_m_000157_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384775471144970387014_0004_m_000157_336
[2021-05-15 10:07:26,593] {docker.py:276} INFO - 21/05/15 13:07:26 INFO StagingCommitter: Task committer attempt_202105151305384775471144970387014_0004_m_000157_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384775471144970387014_0004_m_000157_336 : duration 0:00.002s
[2021-05-15 10:07:27,022] {docker.py:276} INFO - 21/05/15 13:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105151305388208621372355764689_0004_m_000154_333: needsTaskCommit() Task attempt_202105151305388208621372355764689_0004_m_000154_333
[2021-05-15 10:07:27,023] {docker.py:276} INFO - 21/05/15 13:07:27 INFO StagingCommitter: Task committer attempt_202105151305388208621372355764689_0004_m_000154_333: needsTaskCommit() Task attempt_202105151305388208621372355764689_0004_m_000154_333: duration 0:00.001s
21/05/15 13:07:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388208621372355764689_0004_m_000154_333
[2021-05-15 10:07:27,025] {docker.py:276} INFO - 21/05/15 13:07:27 INFO Executor: Finished task 154.0 in stage 4.0 (TID 333). 4544 bytes result sent to driver
[2021-05-15 10:07:27,026] {docker.py:276} INFO - 21/05/15 13:07:27 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 337) (4703d70ea67c, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:27,027] {docker.py:276} INFO - 21/05/15 13:07:27 INFO Executor: Running task 158.0 in stage 4.0 (TID 337)
[2021-05-15 10:07:27,028] {docker.py:276} INFO - 21/05/15 13:07:27 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 333) in 2363 ms on 4703d70ea67c (executor driver) (155/200)
[2021-05-15 10:07:27,038] {docker.py:276} INFO - 21/05/15 13:07:27 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:27,040] {docker.py:276} INFO - 21/05/15 13:07:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:27,041] {docker.py:276} INFO - 21/05/15 13:07:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386907625977119783234_0004_m_000158_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386907625977119783234_0004_m_000158_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386907625977119783234_0004}; taskId=attempt_202105151305386907625977119783234_0004_m_000158_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76dfb43d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:27 INFO StagingCommitter: Starting: Task committer attempt_202105151305386907625977119783234_0004_m_000158_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386907625977119783234_0004_m_000158_337
[2021-05-15 10:07:27,044] {docker.py:276} INFO - 21/05/15 13:07:27 INFO StagingCommitter: Task committer attempt_202105151305386907625977119783234_0004_m_000158_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386907625977119783234_0004_m_000158_337 : duration 0:00.003s
[2021-05-15 10:07:28,132] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305387566492019925467120_0004_m_000155_334: needsTaskCommit() Task attempt_202105151305387566492019925467120_0004_m_000155_334
21/05/15 13:07:28 INFO StagingCommitter: Task committer attempt_202105151305387566492019925467120_0004_m_000155_334: needsTaskCommit() Task attempt_202105151305387566492019925467120_0004_m_000155_334: duration 0:00.001s
[2021-05-15 10:07:28,133] {docker.py:276} INFO - 21/05/15 13:07:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387566492019925467120_0004_m_000155_334
[2021-05-15 10:07:28,134] {docker.py:276} INFO - 21/05/15 13:07:28 INFO Executor: Finished task 155.0 in stage 4.0 (TID 334). 4544 bytes result sent to driver
[2021-05-15 10:07:28,135] {docker.py:276} INFO - 21/05/15 13:07:28 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 338) (4703d70ea67c, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:28,136] {docker.py:276} INFO - 21/05/15 13:07:28 INFO Executor: Running task 159.0 in stage 4.0 (TID 338)
[2021-05-15 10:07:28,137] {docker.py:276} INFO - 21/05/15 13:07:28 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 334) in 2484 ms on 4703d70ea67c (executor driver) (156/200)
[2021-05-15 10:07:28,148] {docker.py:276} INFO - 21/05/15 13:07:28 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:28,148] {docker.py:276} INFO - 21/05/15 13:07:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-15 10:07:28,150] {docker.py:276} INFO - 21/05/15 13:07:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:28,150] {docker.py:276} INFO - 21/05/15 13:07:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:28,151] {docker.py:276} INFO - 21/05/15 13:07:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386739795109574917549_0004_m_000159_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386739795109574917549_0004_m_000159_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386739795109574917549_0004}; taskId=attempt_202105151305386739795109574917549_0004_m_000159_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@550650f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305386739795109574917549_0004_m_000159_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386739795109574917549_0004_m_000159_338
[2021-05-15 10:07:28,154] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Task committer attempt_202105151305386739795109574917549_0004_m_000159_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386739795109574917549_0004_m_000159_338 : duration 0:00.003s
[2021-05-15 10:07:28,542] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Starting: Task committer attempt_20210515130538705737652004946182_0004_m_000156_335: needsTaskCommit() Task attempt_20210515130538705737652004946182_0004_m_000156_335
[2021-05-15 10:07:28,543] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Task committer attempt_20210515130538705737652004946182_0004_m_000156_335: needsTaskCommit() Task attempt_20210515130538705737652004946182_0004_m_000156_335: duration 0:00.001s
21/05/15 13:07:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538705737652004946182_0004_m_000156_335
[2021-05-15 10:07:28,544] {docker.py:276} INFO - 21/05/15 13:07:28 INFO Executor: Finished task 156.0 in stage 4.0 (TID 335). 4544 bytes result sent to driver
[2021-05-15 10:07:28,546] {docker.py:276} INFO - 21/05/15 13:07:28 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 339) (4703d70ea67c, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:28,547] {docker.py:276} INFO - 21/05/15 13:07:28 INFO Executor: Running task 160.0 in stage 4.0 (TID 339)
[2021-05-15 10:07:28,548] {docker.py:276} INFO - 21/05/15 13:07:28 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 335) in 2558 ms on 4703d70ea67c (executor driver) (157/200)
[2021-05-15 10:07:28,556] {docker.py:276} INFO - 21/05/15 13:07:28 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:28,558] {docker.py:276} INFO - 21/05/15 13:07:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383394070658139475248_0004_m_000160_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383394070658139475248_0004_m_000160_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383394070658139475248_0004}; taskId=attempt_202105151305383394070658139475248_0004_m_000160_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c4bf43c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:28,558] {docker.py:276} INFO - 21/05/15 13:07:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:28,559] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Starting: Task committer attempt_202105151305383394070658139475248_0004_m_000160_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383394070658139475248_0004_m_000160_339
[2021-05-15 10:07:28,561] {docker.py:276} INFO - 21/05/15 13:07:28 INFO StagingCommitter: Task committer attempt_202105151305383394070658139475248_0004_m_000160_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383394070658139475248_0004_m_000160_339 : duration 0:00.004s
[2021-05-15 10:07:29,137] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305384775471144970387014_0004_m_000157_336: needsTaskCommit() Task attempt_202105151305384775471144970387014_0004_m_000157_336
21/05/15 13:07:29 INFO StagingCommitter: Task committer attempt_202105151305384775471144970387014_0004_m_000157_336: needsTaskCommit() Task attempt_202105151305384775471144970387014_0004_m_000157_336: duration 0:00.000s
21/05/15 13:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384775471144970387014_0004_m_000157_336
21/05/15 13:07:29 INFO Executor: Finished task 157.0 in stage 4.0 (TID 336). 4544 bytes result sent to driver
[2021-05-15 10:07:29,138] {docker.py:276} INFO - 21/05/15 13:07:29 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 340) (4703d70ea67c, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:29,139] {docker.py:276} INFO - 21/05/15 13:07:29 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 336) in 2564 ms on 4703d70ea67c (executor driver) (158/200)
[2021-05-15 10:07:29,140] {docker.py:276} INFO - 21/05/15 13:07:29 INFO Executor: Running task 161.0 in stage 4.0 (TID 340)
[2021-05-15 10:07:29,150] {docker.py:276} INFO - 21/05/15 13:07:29 INFO ShuffleBlockFetcherIterator: Getting 4 (10.7 KiB) non-empty blocks including 4 (10.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:29,152] {docker.py:276} INFO - 21/05/15 13:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:29,152] {docker.py:276} INFO - 21/05/15 13:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387749280764005786267_0004_m_000161_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387749280764005786267_0004_m_000161_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387749280764005786267_0004}; taskId=attempt_202105151305387749280764005786267_0004_m_000161_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b719544}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305387749280764005786267_0004_m_000161_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387749280764005786267_0004_m_000161_340
[2021-05-15 10:07:29,155] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Task committer attempt_202105151305387749280764005786267_0004_m_000161_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387749280764005786267_0004_m_000161_340 : duration 0:00.003s
[2021-05-15 10:07:29,508] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305386907625977119783234_0004_m_000158_337: needsTaskCommit() Task attempt_202105151305386907625977119783234_0004_m_000158_337
[2021-05-15 10:07:29,509] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Task committer attempt_202105151305386907625977119783234_0004_m_000158_337: needsTaskCommit() Task attempt_202105151305386907625977119783234_0004_m_000158_337: duration 0:00.000s
[2021-05-15 10:07:29,509] {docker.py:276} INFO - 21/05/15 13:07:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386907625977119783234_0004_m_000158_337
[2021-05-15 10:07:29,511] {docker.py:276} INFO - 21/05/15 13:07:29 INFO Executor: Finished task 158.0 in stage 4.0 (TID 337). 4544 bytes result sent to driver
[2021-05-15 10:07:29,512] {docker.py:276} INFO - 21/05/15 13:07:29 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 341) (4703d70ea67c, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:29,513] {docker.py:276} INFO - 21/05/15 13:07:29 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 337) in 2491 ms on 4703d70ea67c (executor driver) (159/200)
[2021-05-15 10:07:29,514] {docker.py:276} INFO - 21/05/15 13:07:29 INFO Executor: Running task 162.0 in stage 4.0 (TID 341)
[2021-05-15 10:07:29,523] {docker.py:276} INFO - 21/05/15 13:07:29 INFO ShuffleBlockFetcherIterator: Getting 4 (12.7 KiB) non-empty blocks including 4 (12.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:29,523] {docker.py:276} INFO - 21/05/15 13:07:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:29,525] {docker.py:276} INFO - 21/05/15 13:07:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:29,526] {docker.py:276} INFO - 21/05/15 13:07:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:29,526] {docker.py:276} INFO - 21/05/15 13:07:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388971949724894790352_0004_m_000162_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388971949724894790352_0004_m_000162_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388971949724894790352_0004}; taskId=attempt_202105151305388971949724894790352_0004_m_000162_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44008ed0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:29,526] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Starting: Task committer attempt_202105151305388971949724894790352_0004_m_000162_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388971949724894790352_0004_m_000162_341
[2021-05-15 10:07:29,530] {docker.py:276} INFO - 21/05/15 13:07:29 INFO StagingCommitter: Task committer attempt_202105151305388971949724894790352_0004_m_000162_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388971949724894790352_0004_m_000162_341 : duration 0:00.004s
[2021-05-15 10:07:30,588] {docker.py:276} INFO - 21/05/15 13:07:30 INFO StagingCommitter: Starting: Task committer attempt_202105151305386739795109574917549_0004_m_000159_338: needsTaskCommit() Task attempt_202105151305386739795109574917549_0004_m_000159_338
[2021-05-15 10:07:30,589] {docker.py:276} INFO - 21/05/15 13:07:30 INFO StagingCommitter: Task committer attempt_202105151305386739795109574917549_0004_m_000159_338: needsTaskCommit() Task attempt_202105151305386739795109574917549_0004_m_000159_338: duration 0:00.001s
21/05/15 13:07:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386739795109574917549_0004_m_000159_338
[2021-05-15 10:07:30,590] {docker.py:276} INFO - 21/05/15 13:07:30 INFO Executor: Finished task 159.0 in stage 4.0 (TID 338). 4544 bytes result sent to driver
[2021-05-15 10:07:30,591] {docker.py:276} INFO - 21/05/15 13:07:30 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 342) (4703d70ea67c, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:30,592] {docker.py:276} INFO - 21/05/15 13:07:30 INFO Executor: Running task 163.0 in stage 4.0 (TID 342)
[2021-05-15 10:07:30,592] {docker.py:276} INFO - 21/05/15 13:07:30 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 338) in 2460 ms on 4703d70ea67c (executor driver) (160/200)
[2021-05-15 10:07:30,600] {docker.py:276} INFO - 21/05/15 13:07:30 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:30,600] {docker.py:276} INFO - 21/05/15 13:07:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:30,602] {docker.py:276} INFO - 21/05/15 13:07:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:30,602] {docker.py:276} INFO - 21/05/15 13:07:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538856690853582160565_0004_m_000163_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538856690853582160565_0004_m_000163_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538856690853582160565_0004}; taskId=attempt_20210515130538856690853582160565_0004_m_000163_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a7eea4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:30 INFO StagingCommitter: Starting: Task committer attempt_20210515130538856690853582160565_0004_m_000163_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538856690853582160565_0004_m_000163_342
[2021-05-15 10:07:30,607] {docker.py:276} INFO - 21/05/15 13:07:30 INFO StagingCommitter: Task committer attempt_20210515130538856690853582160565_0004_m_000163_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538856690853582160565_0004_m_000163_342 : duration 0:00.004s
[2021-05-15 10:07:31,106] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305383394070658139475248_0004_m_000160_339: needsTaskCommit() Task attempt_202105151305383394070658139475248_0004_m_000160_339
[2021-05-15 10:07:31,107] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Task committer attempt_202105151305383394070658139475248_0004_m_000160_339: needsTaskCommit() Task attempt_202105151305383394070658139475248_0004_m_000160_339: duration 0:00.001s
21/05/15 13:07:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383394070658139475248_0004_m_000160_339
[2021-05-15 10:07:31,111] {docker.py:276} INFO - 21/05/15 13:07:31 INFO Executor: Finished task 160.0 in stage 4.0 (TID 339). 4544 bytes result sent to driver
[2021-05-15 10:07:31,112] {docker.py:276} INFO - 21/05/15 13:07:31 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 343) (4703d70ea67c, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:31,113] {docker.py:276} INFO - 21/05/15 13:07:31 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 339) in 2571 ms on 4703d70ea67c (executor driver) (161/200)
21/05/15 13:07:31 INFO Executor: Running task 164.0 in stage 4.0 (TID 343)
[2021-05-15 10:07:31,122] {docker.py:276} INFO - 21/05/15 13:07:31 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:31,124] {docker.py:276} INFO - 21/05/15 13:07:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:31,125] {docker.py:276} INFO - 21/05/15 13:07:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:31,125] {docker.py:276} INFO - 21/05/15 13:07:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386717259582803568763_0004_m_000164_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386717259582803568763_0004_m_000164_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386717259582803568763_0004}; taskId=attempt_202105151305386717259582803568763_0004_m_000164_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17434ad}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:31,126] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305386717259582803568763_0004_m_000164_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386717259582803568763_0004_m_000164_343
[2021-05-15 10:07:31,128] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Task committer attempt_202105151305386717259582803568763_0004_m_000164_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386717259582803568763_0004_m_000164_343 : duration 0:00.003s
[2021-05-15 10:07:31,628] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305387749280764005786267_0004_m_000161_340: needsTaskCommit() Task attempt_202105151305387749280764005786267_0004_m_000161_340
[2021-05-15 10:07:31,629] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Task committer attempt_202105151305387749280764005786267_0004_m_000161_340: needsTaskCommit() Task attempt_202105151305387749280764005786267_0004_m_000161_340: duration 0:00.001s
21/05/15 13:07:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387749280764005786267_0004_m_000161_340
[2021-05-15 10:07:31,630] {docker.py:276} INFO - 21/05/15 13:07:31 INFO Executor: Finished task 161.0 in stage 4.0 (TID 340). 4544 bytes result sent to driver
[2021-05-15 10:07:31,631] {docker.py:276} INFO - 21/05/15 13:07:31 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 344) (4703d70ea67c, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:31,631] {docker.py:276} INFO - 21/05/15 13:07:31 INFO Executor: Running task 165.0 in stage 4.0 (TID 344)
[2021-05-15 10:07:31,632] {docker.py:276} INFO - 21/05/15 13:07:31 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 340) in 2499 ms on 4703d70ea67c (executor driver) (162/200)
[2021-05-15 10:07:31,642] {docker.py:276} INFO - 21/05/15 13:07:31 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:31,644] {docker.py:276} INFO - 21/05/15 13:07:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:31,645] {docker.py:276} INFO - 21/05/15 13:07:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383678932993416255396_0004_m_000165_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383678932993416255396_0004_m_000165_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383678932993416255396_0004}; taskId=attempt_202105151305383678932993416255396_0004_m_000165_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d4a3f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:31,645] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Starting: Task committer attempt_202105151305383678932993416255396_0004_m_000165_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383678932993416255396_0004_m_000165_344
[2021-05-15 10:07:31,647] {docker.py:276} INFO - 21/05/15 13:07:31 INFO StagingCommitter: Task committer attempt_202105151305383678932993416255396_0004_m_000165_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383678932993416255396_0004_m_000165_344 : duration 0:00.003s
[2021-05-15 10:07:32,023] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105151305388971949724894790352_0004_m_000162_341: needsTaskCommit() Task attempt_202105151305388971949724894790352_0004_m_000162_341
[2021-05-15 10:07:32,024] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Task committer attempt_202105151305388971949724894790352_0004_m_000162_341: needsTaskCommit() Task attempt_202105151305388971949724894790352_0004_m_000162_341: duration 0:00.001s
21/05/15 13:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388971949724894790352_0004_m_000162_341
[2021-05-15 10:07:32,025] {docker.py:276} INFO - 21/05/15 13:07:32 INFO Executor: Finished task 162.0 in stage 4.0 (TID 341). 4544 bytes result sent to driver
[2021-05-15 10:07:32,026] {docker.py:276} INFO - 21/05/15 13:07:32 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 345) (4703d70ea67c, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:32,028] {docker.py:276} INFO - 21/05/15 13:07:32 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 341) in 2518 ms on 4703d70ea67c (executor driver) (163/200)
[2021-05-15 10:07:32,028] {docker.py:276} INFO - 21/05/15 13:07:32 INFO Executor: Running task 166.0 in stage 4.0 (TID 345)
[2021-05-15 10:07:32,037] {docker.py:276} INFO - 21/05/15 13:07:32 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:32,040] {docker.py:276} INFO - 21/05/15 13:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385017181291820149618_0004_m_000166_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385017181291820149618_0004_m_000166_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385017181291820149618_0004}; taskId=attempt_202105151305385017181291820149618_0004_m_000166_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47aa8d7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:32,040] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105151305385017181291820149618_0004_m_000166_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385017181291820149618_0004_m_000166_345
[2021-05-15 10:07:32,042] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Task committer attempt_202105151305385017181291820149618_0004_m_000166_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385017181291820149618_0004_m_000166_345 : duration 0:00.002s
[2021-05-15 10:07:32,635] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Starting: Task committer attempt_20210515130538856690853582160565_0004_m_000163_342: needsTaskCommit() Task attempt_20210515130538856690853582160565_0004_m_000163_342
21/05/15 13:07:32 INFO StagingCommitter: Task committer attempt_20210515130538856690853582160565_0004_m_000163_342: needsTaskCommit() Task attempt_20210515130538856690853582160565_0004_m_000163_342: duration 0:00.000s
[2021-05-15 10:07:32,636] {docker.py:276} INFO - 21/05/15 13:07:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538856690853582160565_0004_m_000163_342
[2021-05-15 10:07:32,637] {docker.py:276} INFO - 21/05/15 13:07:32 INFO Executor: Finished task 163.0 in stage 4.0 (TID 342). 4587 bytes result sent to driver
[2021-05-15 10:07:32,639] {docker.py:276} INFO - 21/05/15 13:07:32 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 346) (4703d70ea67c, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:32,640] {docker.py:276} INFO - 21/05/15 13:07:32 INFO Executor: Running task 167.0 in stage 4.0 (TID 346)
[2021-05-15 10:07:32,640] {docker.py:276} INFO - 21/05/15 13:07:32 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 342) in 2051 ms on 4703d70ea67c (executor driver) (164/200)
[2021-05-15 10:07:32,650] {docker.py:276} INFO - 21/05/15 13:07:32 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:32,652] {docker.py:276} INFO - 21/05/15 13:07:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:32,652] {docker.py:276} INFO - 21/05/15 13:07:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381930720587916577495_0004_m_000167_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381930720587916577495_0004_m_000167_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381930720587916577495_0004}; taskId=attempt_202105151305381930720587916577495_0004_m_000167_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b2c549b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:32,652] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Starting: Task committer attempt_202105151305381930720587916577495_0004_m_000167_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381930720587916577495_0004_m_000167_346
[2021-05-15 10:07:32,655] {docker.py:276} INFO - 21/05/15 13:07:32 INFO StagingCommitter: Task committer attempt_202105151305381930720587916577495_0004_m_000167_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381930720587916577495_0004_m_000167_346 : duration 0:00.003s
[2021-05-15 10:07:33,516] {docker.py:276} INFO - 21/05/15 13:07:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305386717259582803568763_0004_m_000164_343: needsTaskCommit() Task attempt_202105151305386717259582803568763_0004_m_000164_343
[2021-05-15 10:07:33,517] {docker.py:276} INFO - 21/05/15 13:07:33 INFO StagingCommitter: Task committer attempt_202105151305386717259582803568763_0004_m_000164_343: needsTaskCommit() Task attempt_202105151305386717259582803568763_0004_m_000164_343: duration 0:00.001s
21/05/15 13:07:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386717259582803568763_0004_m_000164_343
[2021-05-15 10:07:33,519] {docker.py:276} INFO - 21/05/15 13:07:33 INFO Executor: Finished task 164.0 in stage 4.0 (TID 343). 4587 bytes result sent to driver
[2021-05-15 10:07:33,520] {docker.py:276} INFO - 21/05/15 13:07:33 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 347) (4703d70ea67c, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:33,520] {docker.py:276} INFO - 21/05/15 13:07:33 INFO Executor: Running task 168.0 in stage 4.0 (TID 347)
[2021-05-15 10:07:33,521] {docker.py:276} INFO - 21/05/15 13:07:33 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 343) in 2412 ms on 4703d70ea67c (executor driver) (165/200)
[2021-05-15 10:07:33,529] {docker.py:276} INFO - 21/05/15 13:07:33 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:33,531] {docker.py:276} INFO - 21/05/15 13:07:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:33,531] {docker.py:276} INFO - 21/05/15 13:07:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:33,532] {docker.py:276} INFO - 21/05/15 13:07:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:33,532] {docker.py:276} INFO - 21/05/15 13:07:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382931790139745885094_0004_m_000168_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382931790139745885094_0004_m_000168_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382931790139745885094_0004}; taskId=attempt_202105151305382931790139745885094_0004_m_000168_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21f8b77f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:33,533] {docker.py:276} INFO - 21/05/15 13:07:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:33 INFO StagingCommitter: Starting: Task committer attempt_202105151305382931790139745885094_0004_m_000168_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382931790139745885094_0004_m_000168_347
[2021-05-15 10:07:33,536] {docker.py:276} INFO - 21/05/15 13:07:33 INFO StagingCommitter: Task committer attempt_202105151305382931790139745885094_0004_m_000168_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382931790139745885094_0004_m_000168_347 : duration 0:00.003s
[2021-05-15 10:07:34,165] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Starting: Task committer attempt_202105151305383678932993416255396_0004_m_000165_344: needsTaskCommit() Task attempt_202105151305383678932993416255396_0004_m_000165_344
[2021-05-15 10:07:34,166] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Task committer attempt_202105151305383678932993416255396_0004_m_000165_344: needsTaskCommit() Task attempt_202105151305383678932993416255396_0004_m_000165_344: duration 0:00.001s
21/05/15 13:07:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383678932993416255396_0004_m_000165_344
[2021-05-15 10:07:34,169] {docker.py:276} INFO - 21/05/15 13:07:34 INFO Executor: Finished task 165.0 in stage 4.0 (TID 344). 4587 bytes result sent to driver
[2021-05-15 10:07:34,170] {docker.py:276} INFO - 21/05/15 13:07:34 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 348) (4703d70ea67c, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:34,171] {docker.py:276} INFO - 21/05/15 13:07:34 INFO Executor: Running task 169.0 in stage 4.0 (TID 348)
[2021-05-15 10:07:34,172] {docker.py:276} INFO - 21/05/15 13:07:34 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 344) in 2544 ms on 4703d70ea67c (executor driver) (166/200)
[2021-05-15 10:07:34,182] {docker.py:276} INFO - 21/05/15 13:07:34 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:34,183] {docker.py:276} INFO - 21/05/15 13:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:34,184] {docker.py:276} INFO - 21/05/15 13:07:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385504544441002524186_0004_m_000169_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385504544441002524186_0004_m_000169_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385504544441002524186_0004}; taskId=attempt_202105151305385504544441002524186_0004_m_000169_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44f53ce4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:34 INFO StagingCommitter: Starting: Task committer attempt_202105151305385504544441002524186_0004_m_000169_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385504544441002524186_0004_m_000169_348
[2021-05-15 10:07:34,187] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Task committer attempt_202105151305385504544441002524186_0004_m_000169_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385504544441002524186_0004_m_000169_348 : duration 0:00.003s
[2021-05-15 10:07:34,535] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Starting: Task committer attempt_202105151305385017181291820149618_0004_m_000166_345: needsTaskCommit() Task attempt_202105151305385017181291820149618_0004_m_000166_345
[2021-05-15 10:07:34,536] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Task committer attempt_202105151305385017181291820149618_0004_m_000166_345: needsTaskCommit() Task attempt_202105151305385017181291820149618_0004_m_000166_345: duration 0:00.001s
21/05/15 13:07:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385017181291820149618_0004_m_000166_345
[2021-05-15 10:07:34,538] {docker.py:276} INFO - 21/05/15 13:07:34 INFO Executor: Finished task 166.0 in stage 4.0 (TID 345). 4587 bytes result sent to driver
[2021-05-15 10:07:34,539] {docker.py:276} INFO - 21/05/15 13:07:34 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 349) (4703d70ea67c, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:34,540] {docker.py:276} INFO - 21/05/15 13:07:34 INFO Executor: Running task 170.0 in stage 4.0 (TID 349)
[2021-05-15 10:07:34,541] {docker.py:276} INFO - 21/05/15 13:07:34 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 345) in 2517 ms on 4703d70ea67c (executor driver) (167/200)
[2021-05-15 10:07:34,550] {docker.py:276} INFO - 21/05/15 13:07:34 INFO ShuffleBlockFetcherIterator: Getting 4 (12.4 KiB) non-empty blocks including 4 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:34,551] {docker.py:276} INFO - 21/05/15 13:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538420514383341388341_0004_m_000170_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538420514383341388341_0004_m_000170_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538420514383341388341_0004}; taskId=attempt_20210515130538420514383341388341_0004_m_000170_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a5792e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:34,552] {docker.py:276} INFO - 21/05/15 13:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:34 INFO StagingCommitter: Starting: Task committer attempt_20210515130538420514383341388341_0004_m_000170_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538420514383341388341_0004_m_000170_349
[2021-05-15 10:07:34,554] {docker.py:276} INFO - 21/05/15 13:07:34 INFO StagingCommitter: Task committer attempt_20210515130538420514383341388341_0004_m_000170_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538420514383341388341_0004_m_000170_349 : duration 0:00.003s
[2021-05-15 10:07:35,122] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105151305381930720587916577495_0004_m_000167_346: needsTaskCommit() Task attempt_202105151305381930720587916577495_0004_m_000167_346
[2021-05-15 10:07:35,123] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Task committer attempt_202105151305381930720587916577495_0004_m_000167_346: needsTaskCommit() Task attempt_202105151305381930720587916577495_0004_m_000167_346: duration 0:00.001s
21/05/15 13:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381930720587916577495_0004_m_000167_346
[2021-05-15 10:07:35,125] {docker.py:276} INFO - 21/05/15 13:07:35 INFO Executor: Finished task 167.0 in stage 4.0 (TID 346). 4544 bytes result sent to driver
[2021-05-15 10:07:35,127] {docker.py:276} INFO - 21/05/15 13:07:35 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 350) (4703d70ea67c, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:35,128] {docker.py:276} INFO - 21/05/15 13:07:35 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 346) in 2492 ms on 4703d70ea67c (executor driver) (168/200)
[2021-05-15 10:07:35,129] {docker.py:276} INFO - 21/05/15 13:07:35 INFO Executor: Running task 171.0 in stage 4.0 (TID 350)
[2021-05-15 10:07:35,138] {docker.py:276} INFO - 21/05/15 13:07:35 INFO ShuffleBlockFetcherIterator: Getting 4 (13.2 KiB) non-empty blocks including 4 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:35,139] {docker.py:276} INFO - 21/05/15 13:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:35,140] {docker.py:276} INFO - 21/05/15 13:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:35,140] {docker.py:276} INFO - 21/05/15 13:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388391430243565081983_0004_m_000171_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388391430243565081983_0004_m_000171_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388391430243565081983_0004}; taskId=attempt_202105151305388391430243565081983_0004_m_000171_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35bff6d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:35,141] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105151305388391430243565081983_0004_m_000171_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388391430243565081983_0004_m_000171_350
[2021-05-15 10:07:35,144] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Task committer attempt_202105151305388391430243565081983_0004_m_000171_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388391430243565081983_0004_m_000171_350 : duration 0:00.004s
[2021-05-15 10:07:35,309] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Starting: Task committer attempt_202105151305382931790139745885094_0004_m_000168_347: needsTaskCommit() Task attempt_202105151305382931790139745885094_0004_m_000168_347
[2021-05-15 10:07:35,310] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Task committer attempt_202105151305382931790139745885094_0004_m_000168_347: needsTaskCommit() Task attempt_202105151305382931790139745885094_0004_m_000168_347: duration 0:00.002s
[2021-05-15 10:07:35,311] {docker.py:276} INFO - 21/05/15 13:07:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382931790139745885094_0004_m_000168_347
[2021-05-15 10:07:35,312] {docker.py:276} INFO - 21/05/15 13:07:35 INFO Executor: Finished task 168.0 in stage 4.0 (TID 347). 4544 bytes result sent to driver
[2021-05-15 10:07:35,313] {docker.py:276} INFO - 21/05/15 13:07:35 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 351) (4703d70ea67c, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:35,314] {docker.py:276} INFO - 21/05/15 13:07:35 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 347) in 1797 ms on 4703d70ea67c (executor driver) (169/200)
[2021-05-15 10:07:35,315] {docker.py:276} INFO - 21/05/15 13:07:35 INFO Executor: Running task 172.0 in stage 4.0 (TID 351)
[2021-05-15 10:07:35,324] {docker.py:276} INFO - 21/05/15 13:07:35 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:35,326] {docker.py:276} INFO - 21/05/15 13:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:35,326] {docker.py:276} INFO - 21/05/15 13:07:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:35,326] {docker.py:276} INFO - 21/05/15 13:07:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538193109621027118132_0004_m_000172_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538193109621027118132_0004_m_000172_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538193109621027118132_0004}; taskId=attempt_20210515130538193109621027118132_0004_m_000172_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6ad5b3d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:35,327] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Starting: Task committer attempt_20210515130538193109621027118132_0004_m_000172_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538193109621027118132_0004_m_000172_351
[2021-05-15 10:07:35,329] {docker.py:276} INFO - 21/05/15 13:07:35 INFO StagingCommitter: Task committer attempt_20210515130538193109621027118132_0004_m_000172_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538193109621027118132_0004_m_000172_351 : duration 0:00.003s
[2021-05-15 10:07:36,641] {docker.py:276} INFO - 21/05/15 13:07:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305385504544441002524186_0004_m_000169_348: needsTaskCommit() Task attempt_202105151305385504544441002524186_0004_m_000169_348
[2021-05-15 10:07:36,643] {docker.py:276} INFO - 21/05/15 13:07:36 INFO StagingCommitter: Task committer attempt_202105151305385504544441002524186_0004_m_000169_348: needsTaskCommit() Task attempt_202105151305385504544441002524186_0004_m_000169_348: duration 0:00.001s
21/05/15 13:07:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385504544441002524186_0004_m_000169_348
[2021-05-15 10:07:36,644] {docker.py:276} INFO - 21/05/15 13:07:36 INFO Executor: Finished task 169.0 in stage 4.0 (TID 348). 4544 bytes result sent to driver
[2021-05-15 10:07:36,645] {docker.py:276} INFO - 21/05/15 13:07:36 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 352) (4703d70ea67c, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:36,646] {docker.py:276} INFO - 21/05/15 13:07:36 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 348) in 2480 ms on 4703d70ea67c (executor driver) (170/200)
[2021-05-15 10:07:36,647] {docker.py:276} INFO - 21/05/15 13:07:36 INFO Executor: Running task 173.0 in stage 4.0 (TID 352)
[2021-05-15 10:07:36,656] {docker.py:276} INFO - 21/05/15 13:07:36 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:36,657] {docker.py:276} INFO - 21/05/15 13:07:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:36,658] {docker.py:276} INFO - 21/05/15 13:07:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388151940173502799152_0004_m_000173_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388151940173502799152_0004_m_000173_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388151940173502799152_0004}; taskId=attempt_202105151305388151940173502799152_0004_m_000173_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d969dc6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:36 INFO StagingCommitter: Starting: Task committer attempt_202105151305388151940173502799152_0004_m_000173_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388151940173502799152_0004_m_000173_352
[2021-05-15 10:07:36,660] {docker.py:276} INFO - 21/05/15 13:07:36 INFO StagingCommitter: Task committer attempt_202105151305388151940173502799152_0004_m_000173_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388151940173502799152_0004_m_000173_352 : duration 0:00.003s
[2021-05-15 10:07:37,049] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Starting: Task committer attempt_20210515130538420514383341388341_0004_m_000170_349: needsTaskCommit() Task attempt_20210515130538420514383341388341_0004_m_000170_349
[2021-05-15 10:07:37,050] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Task committer attempt_20210515130538420514383341388341_0004_m_000170_349: needsTaskCommit() Task attempt_20210515130538420514383341388341_0004_m_000170_349: duration 0:00.001s
[2021-05-15 10:07:37,051] {docker.py:276} INFO - 21/05/15 13:07:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538420514383341388341_0004_m_000170_349
[2021-05-15 10:07:37,052] {docker.py:276} INFO - 21/05/15 13:07:37 INFO Executor: Finished task 170.0 in stage 4.0 (TID 349). 4544 bytes result sent to driver
[2021-05-15 10:07:37,053] {docker.py:276} INFO - 21/05/15 13:07:37 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 353) (4703d70ea67c, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:37,054] {docker.py:276} INFO - 21/05/15 13:07:37 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 349) in 2518 ms on 4703d70ea67c (executor driver) (171/200)
[2021-05-15 10:07:37,055] {docker.py:276} INFO - 21/05/15 13:07:37 INFO Executor: Running task 174.0 in stage 4.0 (TID 353)
[2021-05-15 10:07:37,064] {docker.py:276} INFO - 21/05/15 13:07:37 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:37,065] {docker.py:276} INFO - 21/05/15 13:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:37,066] {docker.py:276} INFO - 21/05/15 13:07:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:37,066] {docker.py:276} INFO - 21/05/15 13:07:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386025233180801778258_0004_m_000174_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386025233180801778258_0004_m_000174_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386025233180801778258_0004}; taskId=attempt_202105151305386025233180801778258_0004_m_000174_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62bbca00}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:37 INFO StagingCommitter: Starting: Task committer attempt_202105151305386025233180801778258_0004_m_000174_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386025233180801778258_0004_m_000174_353
[2021-05-15 10:07:37,068] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Task committer attempt_202105151305386025233180801778258_0004_m_000174_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386025233180801778258_0004_m_000174_353 : duration 0:00.002s
[2021-05-15 10:07:37,693] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Starting: Task committer attempt_20210515130538193109621027118132_0004_m_000172_351: needsTaskCommit() Task attempt_20210515130538193109621027118132_0004_m_000172_351
[2021-05-15 10:07:37,694] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Task committer attempt_20210515130538193109621027118132_0004_m_000172_351: needsTaskCommit() Task attempt_20210515130538193109621027118132_0004_m_000172_351: duration 0:00.001s
21/05/15 13:07:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538193109621027118132_0004_m_000172_351
[2021-05-15 10:07:37,696] {docker.py:276} INFO - 21/05/15 13:07:37 INFO Executor: Finished task 172.0 in stage 4.0 (TID 351). 4544 bytes result sent to driver
[2021-05-15 10:07:37,698] {docker.py:276} INFO - 21/05/15 13:07:37 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 354) (4703d70ea67c, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:37,699] {docker.py:276} INFO - 21/05/15 13:07:37 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 351) in 2388 ms on 4703d70ea67c (executor driver) (172/200)
[2021-05-15 10:07:37,699] {docker.py:276} INFO - 21/05/15 13:07:37 INFO Executor: Running task 175.0 in stage 4.0 (TID 354)
[2021-05-15 10:07:37,709] {docker.py:276} INFO - 21/05/15 13:07:37 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:37,710] {docker.py:276} INFO - 21/05/15 13:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305389220446851972323356_0004_m_000175_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389220446851972323356_0004_m_000175_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305389220446851972323356_0004}; taskId=attempt_202105151305389220446851972323356_0004_m_000175_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e99ff6e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:37,711] {docker.py:276} INFO - 21/05/15 13:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:37 INFO StagingCommitter: Starting: Task committer attempt_202105151305389220446851972323356_0004_m_000175_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389220446851972323356_0004_m_000175_354
[2021-05-15 10:07:37,714] {docker.py:276} INFO - 21/05/15 13:07:37 INFO StagingCommitter: Task committer attempt_202105151305389220446851972323356_0004_m_000175_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305389220446851972323356_0004_m_000175_354 : duration 0:00.003s
[2021-05-15 10:07:38,256] {docker.py:276} INFO - 21/05/15 13:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305388391430243565081983_0004_m_000171_350: needsTaskCommit() Task attempt_202105151305388391430243565081983_0004_m_000171_350
[2021-05-15 10:07:38,257] {docker.py:276} INFO - 21/05/15 13:07:38 INFO StagingCommitter: Task committer attempt_202105151305388391430243565081983_0004_m_000171_350: needsTaskCommit() Task attempt_202105151305388391430243565081983_0004_m_000171_350: duration 0:00.000s
21/05/15 13:07:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388391430243565081983_0004_m_000171_350
[2021-05-15 10:07:38,258] {docker.py:276} INFO - 21/05/15 13:07:38 INFO Executor: Finished task 171.0 in stage 4.0 (TID 350). 4544 bytes result sent to driver
[2021-05-15 10:07:38,259] {docker.py:276} INFO - 21/05/15 13:07:38 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 355) (4703d70ea67c, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:38,260] {docker.py:276} INFO - 21/05/15 13:07:38 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 350) in 3137 ms on 4703d70ea67c (executor driver) (173/200)
[2021-05-15 10:07:38,261] {docker.py:276} INFO - 21/05/15 13:07:38 INFO Executor: Running task 176.0 in stage 4.0 (TID 355)
[2021-05-15 10:07:38,268] {docker.py:276} INFO - 21/05/15 13:07:38 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:38,270] {docker.py:276} INFO - 21/05/15 13:07:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381411743983191646445_0004_m_000176_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381411743983191646445_0004_m_000176_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381411743983191646445_0004}; taskId=attempt_202105151305381411743983191646445_0004_m_000176_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@759cb808}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:38 INFO StagingCommitter: Starting: Task committer attempt_202105151305381411743983191646445_0004_m_000176_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381411743983191646445_0004_m_000176_355
[2021-05-15 10:07:38,273] {docker.py:276} INFO - 21/05/15 13:07:38 INFO StagingCommitter: Task committer attempt_202105151305381411743983191646445_0004_m_000176_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381411743983191646445_0004_m_000176_355 : duration 0:00.003s
[2021-05-15 10:07:39,156] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Starting: Task committer attempt_202105151305388151940173502799152_0004_m_000173_352: needsTaskCommit() Task attempt_202105151305388151940173502799152_0004_m_000173_352
[2021-05-15 10:07:39,158] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Task committer attempt_202105151305388151940173502799152_0004_m_000173_352: needsTaskCommit() Task attempt_202105151305388151940173502799152_0004_m_000173_352: duration 0:00.000s
[2021-05-15 10:07:39,159] {docker.py:276} INFO - 21/05/15 13:07:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388151940173502799152_0004_m_000173_352
[2021-05-15 10:07:39,161] {docker.py:276} INFO - 21/05/15 13:07:39 INFO Executor: Finished task 173.0 in stage 4.0 (TID 352). 4544 bytes result sent to driver
[2021-05-15 10:07:39,163] {docker.py:276} INFO - 21/05/15 13:07:39 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 356) (4703d70ea67c, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:39,164] {docker.py:276} INFO - 21/05/15 13:07:39 INFO Executor: Running task 177.0 in stage 4.0 (TID 356)
21/05/15 13:07:39 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 352) in 2522 ms on 4703d70ea67c (executor driver) (174/200)
[2021-05-15 10:07:39,173] {docker.py:276} INFO - 21/05/15 13:07:39 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:39,174] {docker.py:276} INFO - 21/05/15 13:07:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305387827447897626379442_0004_m_000177_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387827447897626379442_0004_m_000177_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305387827447897626379442_0004}; taskId=attempt_202105151305387827447897626379442_0004_m_000177_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4e857304}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:39,175] {docker.py:276} INFO - 21/05/15 13:07:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:39 INFO StagingCommitter: Starting: Task committer attempt_202105151305387827447897626379442_0004_m_000177_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387827447897626379442_0004_m_000177_356
[2021-05-15 10:07:39,177] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Task committer attempt_202105151305387827447897626379442_0004_m_000177_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305387827447897626379442_0004_m_000177_356 : duration 0:00.003s
[2021-05-15 10:07:39,592] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Starting: Task committer attempt_202105151305386025233180801778258_0004_m_000174_353: needsTaskCommit() Task attempt_202105151305386025233180801778258_0004_m_000174_353
[2021-05-15 10:07:39,592] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Task committer attempt_202105151305386025233180801778258_0004_m_000174_353: needsTaskCommit() Task attempt_202105151305386025233180801778258_0004_m_000174_353: duration 0:00.002s
21/05/15 13:07:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386025233180801778258_0004_m_000174_353
[2021-05-15 10:07:39,595] {docker.py:276} INFO - 21/05/15 13:07:39 INFO Executor: Finished task 174.0 in stage 4.0 (TID 353). 4544 bytes result sent to driver
[2021-05-15 10:07:39,597] {docker.py:276} INFO - 21/05/15 13:07:39 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 357) (4703d70ea67c, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:39,598] {docker.py:276} INFO - 21/05/15 13:07:39 INFO Executor: Running task 178.0 in stage 4.0 (TID 357)
21/05/15 13:07:39 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 353) in 2548 ms on 4703d70ea67c (executor driver) (175/200)
[2021-05-15 10:07:39,607] {docker.py:276} INFO - 21/05/15 13:07:39 INFO ShuffleBlockFetcherIterator: Getting 4 (11.4 KiB) non-empty blocks including 4 (11.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:39,609] {docker.py:276} INFO - 21/05/15 13:07:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538987092403771942391_0004_m_000178_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538987092403771942391_0004_m_000178_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538987092403771942391_0004}; taskId=attempt_20210515130538987092403771942391_0004_m_000178_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4560271d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:39 INFO StagingCommitter: Starting: Task committer attempt_20210515130538987092403771942391_0004_m_000178_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538987092403771942391_0004_m_000178_357
[2021-05-15 10:07:39,611] {docker.py:276} INFO - 21/05/15 13:07:39 INFO StagingCommitter: Task committer attempt_20210515130538987092403771942391_0004_m_000178_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538987092403771942391_0004_m_000178_357 : duration 0:00.002s
[2021-05-15 10:07:40,288] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305389220446851972323356_0004_m_000175_354: needsTaskCommit() Task attempt_202105151305389220446851972323356_0004_m_000175_354
[2021-05-15 10:07:40,289] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Task committer attempt_202105151305389220446851972323356_0004_m_000175_354: needsTaskCommit() Task attempt_202105151305389220446851972323356_0004_m_000175_354: duration 0:00.001s
21/05/15 13:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305389220446851972323356_0004_m_000175_354
[2021-05-15 10:07:40,291] {docker.py:276} INFO - 21/05/15 13:07:40 INFO Executor: Finished task 175.0 in stage 4.0 (TID 354). 4544 bytes result sent to driver
[2021-05-15 10:07:40,292] {docker.py:276} INFO - 21/05/15 13:07:40 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 358) (4703d70ea67c, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:40,295] {docker.py:276} INFO - 21/05/15 13:07:40 INFO Executor: Running task 179.0 in stage 4.0 (TID 358)
[2021-05-15 10:07:40,295] {docker.py:276} INFO - 21/05/15 13:07:40 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 354) in 2600 ms on 4703d70ea67c (executor driver) (176/200)
[2021-05-15 10:07:40,305] {docker.py:276} INFO - 21/05/15 13:07:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.6 KiB) non-empty blocks including 4 (12.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:40,306] {docker.py:276} INFO - 21/05/15 13:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:40,307] {docker.py:276} INFO - 21/05/15 13:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381975619738855464650_0004_m_000179_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381975619738855464650_0004_m_000179_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381975619738855464650_0004}; taskId=attempt_202105151305381975619738855464650_0004_m_000179_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18792180}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305381975619738855464650_0004_m_000179_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381975619738855464650_0004_m_000179_358
[2021-05-15 10:07:40,309] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Task committer attempt_202105151305381975619738855464650_0004_m_000179_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381975619738855464650_0004_m_000179_358 : duration 0:00.003s
[2021-05-15 10:07:40,780] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305381411743983191646445_0004_m_000176_355: needsTaskCommit() Task attempt_202105151305381411743983191646445_0004_m_000176_355
[2021-05-15 10:07:40,781] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Task committer attempt_202105151305381411743983191646445_0004_m_000176_355: needsTaskCommit() Task attempt_202105151305381411743983191646445_0004_m_000176_355: duration 0:00.001s
21/05/15 13:07:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381411743983191646445_0004_m_000176_355
[2021-05-15 10:07:40,782] {docker.py:276} INFO - 21/05/15 13:07:40 INFO Executor: Finished task 176.0 in stage 4.0 (TID 355). 4544 bytes result sent to driver
21/05/15 13:07:40 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 359) (4703d70ea67c, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:40,783] {docker.py:276} INFO - 21/05/15 13:07:40 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 355) in 2528 ms on 4703d70ea67c (executor driver) (177/200)
[2021-05-15 10:07:40,784] {docker.py:276} INFO - 21/05/15 13:07:40 INFO Executor: Running task 180.0 in stage 4.0 (TID 359)
[2021-05-15 10:07:40,792] {docker.py:276} INFO - 21/05/15 13:07:40 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:40,794] {docker.py:276} INFO - 21/05/15 13:07:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388974801433157568091_0004_m_000180_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388974801433157568091_0004_m_000180_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388974801433157568091_0004}; taskId=attempt_202105151305388974801433157568091_0004_m_000180_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32ec0486}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:40,795] {docker.py:276} INFO - 21/05/15 13:07:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:40 INFO StagingCommitter: Starting: Task committer attempt_202105151305388974801433157568091_0004_m_000180_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388974801433157568091_0004_m_000180_359
[2021-05-15 10:07:40,798] {docker.py:276} INFO - 21/05/15 13:07:40 INFO StagingCommitter: Task committer attempt_202105151305388974801433157568091_0004_m_000180_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388974801433157568091_0004_m_000180_359 : duration 0:00.003s
[2021-05-15 10:07:41,675] {docker.py:276} INFO - 21/05/15 13:07:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305387827447897626379442_0004_m_000177_356: needsTaskCommit() Task attempt_202105151305387827447897626379442_0004_m_000177_356
[2021-05-15 10:07:41,677] {docker.py:276} INFO - 21/05/15 13:07:41 INFO StagingCommitter: Task committer attempt_202105151305387827447897626379442_0004_m_000177_356: needsTaskCommit() Task attempt_202105151305387827447897626379442_0004_m_000177_356: duration 0:00.001s
21/05/15 13:07:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305387827447897626379442_0004_m_000177_356
[2021-05-15 10:07:41,680] {docker.py:276} INFO - 21/05/15 13:07:41 INFO Executor: Finished task 177.0 in stage 4.0 (TID 356). 4544 bytes result sent to driver
[2021-05-15 10:07:41,681] {docker.py:276} INFO - 21/05/15 13:07:41 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 360) (4703d70ea67c, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:41,682] {docker.py:276} INFO - 21/05/15 13:07:41 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 356) in 2523 ms on 4703d70ea67c (executor driver) (178/200)
[2021-05-15 10:07:41,683] {docker.py:276} INFO - 21/05/15 13:07:41 INFO Executor: Running task 181.0 in stage 4.0 (TID 360)
[2021-05-15 10:07:41,692] {docker.py:276} INFO - 21/05/15 13:07:41 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:41,693] {docker.py:276} INFO - 21/05/15 13:07:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386264495686603110385_0004_m_000181_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386264495686603110385_0004_m_000181_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386264495686603110385_0004}; taskId=attempt_202105151305386264495686603110385_0004_m_000181_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a5852b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:41 INFO StagingCommitter: Starting: Task committer attempt_202105151305386264495686603110385_0004_m_000181_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386264495686603110385_0004_m_000181_360
[2021-05-15 10:07:41,696] {docker.py:276} INFO - 21/05/15 13:07:41 INFO StagingCommitter: Task committer attempt_202105151305386264495686603110385_0004_m_000181_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386264495686603110385_0004_m_000181_360 : duration 0:00.003s
[2021-05-15 10:07:42,062] {docker.py:276} INFO - 21/05/15 13:07:42 INFO StagingCommitter: Starting: Task committer attempt_20210515130538987092403771942391_0004_m_000178_357: needsTaskCommit() Task attempt_20210515130538987092403771942391_0004_m_000178_357
[2021-05-15 10:07:42,063] {docker.py:276} INFO - 21/05/15 13:07:42 INFO StagingCommitter: Task committer attempt_20210515130538987092403771942391_0004_m_000178_357: needsTaskCommit() Task attempt_20210515130538987092403771942391_0004_m_000178_357: duration 0:00.001s
[2021-05-15 10:07:42,064] {docker.py:276} INFO - 21/05/15 13:07:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538987092403771942391_0004_m_000178_357
[2021-05-15 10:07:42,066] {docker.py:276} INFO - 21/05/15 13:07:42 INFO Executor: Finished task 178.0 in stage 4.0 (TID 357). 4544 bytes result sent to driver
[2021-05-15 10:07:42,067] {docker.py:276} INFO - 21/05/15 13:07:42 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 361) (4703d70ea67c, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:42,068] {docker.py:276} INFO - 21/05/15 13:07:42 INFO Executor: Running task 182.0 in stage 4.0 (TID 361)
[2021-05-15 10:07:42,069] {docker.py:276} INFO - 21/05/15 13:07:42 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 357) in 2475 ms on 4703d70ea67c (executor driver) (179/200)
[2021-05-15 10:07:42,077] {docker.py:276} INFO - 21/05/15 13:07:42 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:42,079] {docker.py:276} INFO - 21/05/15 13:07:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:42,079] {docker.py:276} INFO - 21/05/15 13:07:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382564524422324650285_0004_m_000182_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382564524422324650285_0004_m_000182_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382564524422324650285_0004}; taskId=attempt_202105151305382564524422324650285_0004_m_000182_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c8d1e32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:42 INFO StagingCommitter: Starting: Task committer attempt_202105151305382564524422324650285_0004_m_000182_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382564524422324650285_0004_m_000182_361
[2021-05-15 10:07:42,083] {docker.py:276} INFO - 21/05/15 13:07:42 INFO StagingCommitter: Task committer attempt_202105151305382564524422324650285_0004_m_000182_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382564524422324650285_0004_m_000182_361 : duration 0:00.004s
[2021-05-15 10:07:43,082] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305381975619738855464650_0004_m_000179_358: needsTaskCommit() Task attempt_202105151305381975619738855464650_0004_m_000179_358
21/05/15 13:07:43 INFO StagingCommitter: Task committer attempt_202105151305381975619738855464650_0004_m_000179_358: needsTaskCommit() Task attempt_202105151305381975619738855464650_0004_m_000179_358: duration 0:00.001s
21/05/15 13:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381975619738855464650_0004_m_000179_358
[2021-05-15 10:07:43,085] {docker.py:276} INFO - 21/05/15 13:07:43 INFO Executor: Finished task 179.0 in stage 4.0 (TID 358). 4544 bytes result sent to driver
[2021-05-15 10:07:43,086] {docker.py:276} INFO - 21/05/15 13:07:43 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 362) (4703d70ea67c, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:43,086] {docker.py:276} INFO - 21/05/15 13:07:43 INFO Executor: Running task 183.0 in stage 4.0 (TID 362)
21/05/15 13:07:43 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 358) in 2797 ms on 4703d70ea67c (executor driver) (180/200)
[2021-05-15 10:07:43,096] {docker.py:276} INFO - 21/05/15 13:07:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.8 KiB) non-empty blocks including 4 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:43,098] {docker.py:276} INFO - 21/05/15 13:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:43,098] {docker.py:276} INFO - 21/05/15 13:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383011335928522844930_0004_m_000183_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383011335928522844930_0004_m_000183_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383011335928522844930_0004}; taskId=attempt_202105151305383011335928522844930_0004_m_000183_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@af92713}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305383011335928522844930_0004_m_000183_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383011335928522844930_0004_m_000183_362
[2021-05-15 10:07:43,101] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Task committer attempt_202105151305383011335928522844930_0004_m_000183_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383011335928522844930_0004_m_000183_362 : duration 0:00.003s
[2021-05-15 10:07:43,305] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305388974801433157568091_0004_m_000180_359: needsTaskCommit() Task attempt_202105151305388974801433157568091_0004_m_000180_359
[2021-05-15 10:07:43,306] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Task committer attempt_202105151305388974801433157568091_0004_m_000180_359: needsTaskCommit() Task attempt_202105151305388974801433157568091_0004_m_000180_359: duration 0:00.001s
21/05/15 13:07:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388974801433157568091_0004_m_000180_359
[2021-05-15 10:07:43,308] {docker.py:276} INFO - 21/05/15 13:07:43 INFO Executor: Finished task 180.0 in stage 4.0 (TID 359). 4544 bytes result sent to driver
[2021-05-15 10:07:43,309] {docker.py:276} INFO - 21/05/15 13:07:43 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 363) (4703d70ea67c, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:43,310] {docker.py:276} INFO - 21/05/15 13:07:43 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 359) in 2531 ms on 4703d70ea67c (executor driver) (181/200)
[2021-05-15 10:07:43,311] {docker.py:276} INFO - 21/05/15 13:07:43 INFO Executor: Running task 184.0 in stage 4.0 (TID 363)
[2021-05-15 10:07:43,320] {docker.py:276} INFO - 21/05/15 13:07:43 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:43,320] {docker.py:276} INFO - 21/05/15 13:07:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:43,322] {docker.py:276} INFO - 21/05/15 13:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:43,322] {docker.py:276} INFO - 21/05/15 13:07:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:43,323] {docker.py:276} INFO - 21/05/15 13:07:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383179490000232579918_0004_m_000184_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383179490000232579918_0004_m_000184_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383179490000232579918_0004}; taskId=attempt_202105151305383179490000232579918_0004_m_000184_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bed2aa2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:43,323] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Starting: Task committer attempt_202105151305383179490000232579918_0004_m_000184_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383179490000232579918_0004_m_000184_363
[2021-05-15 10:07:43,325] {docker.py:276} INFO - 21/05/15 13:07:43 INFO StagingCommitter: Task committer attempt_202105151305383179490000232579918_0004_m_000184_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383179490000232579918_0004_m_000184_363 : duration 0:00.003s
[2021-05-15 10:07:44,296] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Starting: Task committer attempt_202105151305386264495686603110385_0004_m_000181_360: needsTaskCommit() Task attempt_202105151305386264495686603110385_0004_m_000181_360
[2021-05-15 10:07:44,297] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Task committer attempt_202105151305386264495686603110385_0004_m_000181_360: needsTaskCommit() Task attempt_202105151305386264495686603110385_0004_m_000181_360: duration 0:00.000s
21/05/15 13:07:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386264495686603110385_0004_m_000181_360
[2021-05-15 10:07:44,298] {docker.py:276} INFO - 21/05/15 13:07:44 INFO Executor: Finished task 181.0 in stage 4.0 (TID 360). 4544 bytes result sent to driver
[2021-05-15 10:07:44,299] {docker.py:276} INFO - 21/05/15 13:07:44 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 364) (4703d70ea67c, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:44,300] {docker.py:276} INFO - 21/05/15 13:07:44 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 360) in 2622 ms on 4703d70ea67c (executor driver) (182/200)
21/05/15 13:07:44 INFO Executor: Running task 185.0 in stage 4.0 (TID 364)
[2021-05-15 10:07:44,318] {docker.py:276} INFO - 21/05/15 13:07:44 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:44,320] {docker.py:276} INFO - 21/05/15 13:07:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381403362160241344285_0004_m_000185_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381403362160241344285_0004_m_000185_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381403362160241344285_0004}; taskId=attempt_202105151305381403362160241344285_0004_m_000185_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@f55c51f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:44,320] {docker.py:276} INFO - 21/05/15 13:07:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:44 INFO StagingCommitter: Starting: Task committer attempt_202105151305381403362160241344285_0004_m_000185_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381403362160241344285_0004_m_000185_364
[2021-05-15 10:07:44,323] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Task committer attempt_202105151305381403362160241344285_0004_m_000185_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381403362160241344285_0004_m_000185_364 : duration 0:00.002s
[2021-05-15 10:07:44,626] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Starting: Task committer attempt_202105151305382564524422324650285_0004_m_000182_361: needsTaskCommit() Task attempt_202105151305382564524422324650285_0004_m_000182_361
[2021-05-15 10:07:44,627] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Task committer attempt_202105151305382564524422324650285_0004_m_000182_361: needsTaskCommit() Task attempt_202105151305382564524422324650285_0004_m_000182_361: duration 0:00.001s
[2021-05-15 10:07:44,628] {docker.py:276} INFO - 21/05/15 13:07:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382564524422324650285_0004_m_000182_361
[2021-05-15 10:07:44,629] {docker.py:276} INFO - 21/05/15 13:07:44 INFO Executor: Finished task 182.0 in stage 4.0 (TID 361). 4587 bytes result sent to driver
[2021-05-15 10:07:44,631] {docker.py:276} INFO - 21/05/15 13:07:44 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 365) (4703d70ea67c, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:44,633] {docker.py:276} INFO - 21/05/15 13:07:44 INFO Executor: Running task 186.0 in stage 4.0 (TID 365)
21/05/15 13:07:44 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 361) in 2569 ms on 4703d70ea67c (executor driver) (183/200)
[2021-05-15 10:07:44,641] {docker.py:276} INFO - 21/05/15 13:07:44 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:44,642] {docker.py:276} INFO - 21/05/15 13:07:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:44,642] {docker.py:276} INFO - 21/05/15 13:07:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538230639580626620400_0004_m_000186_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538230639580626620400_0004_m_000186_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538230639580626620400_0004}; taskId=attempt_20210515130538230639580626620400_0004_m_000186_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@578dd450}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:44 INFO StagingCommitter: Starting: Task committer attempt_20210515130538230639580626620400_0004_m_000186_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538230639580626620400_0004_m_000186_365
[2021-05-15 10:07:44,645] {docker.py:276} INFO - 21/05/15 13:07:44 INFO StagingCommitter: Task committer attempt_20210515130538230639580626620400_0004_m_000186_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538230639580626620400_0004_m_000186_365 : duration 0:00.002s
[2021-05-15 10:07:45,581] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305383011335928522844930_0004_m_000183_362: needsTaskCommit() Task attempt_202105151305383011335928522844930_0004_m_000183_362
21/05/15 13:07:45 INFO StagingCommitter: Task committer attempt_202105151305383011335928522844930_0004_m_000183_362: needsTaskCommit() Task attempt_202105151305383011335928522844930_0004_m_000183_362: duration 0:00.001s
21/05/15 13:07:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383011335928522844930_0004_m_000183_362
[2021-05-15 10:07:45,582] {docker.py:276} INFO - 21/05/15 13:07:45 INFO Executor: Finished task 183.0 in stage 4.0 (TID 362). 4587 bytes result sent to driver
[2021-05-15 10:07:45,583] {docker.py:276} INFO - 21/05/15 13:07:45 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 366) (4703d70ea67c, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:45,584] {docker.py:276} INFO - 21/05/15 13:07:45 INFO Executor: Running task 187.0 in stage 4.0 (TID 366)
21/05/15 13:07:45 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 362) in 2502 ms on 4703d70ea67c (executor driver) (184/200)
[2021-05-15 10:07:45,595] {docker.py:276} INFO - 21/05/15 13:07:45 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:45,596] {docker.py:276} INFO - 21/05/15 13:07:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:45,597] {docker.py:276} INFO - 21/05/15 13:07:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:45,597] {docker.py:276} INFO - 21/05/15 13:07:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:45,598] {docker.py:276} INFO - 21/05/15 13:07:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383106938582266233557_0004_m_000187_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383106938582266233557_0004_m_000187_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383106938582266233557_0004}; taskId=attempt_202105151305383106938582266233557_0004_m_000187_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@77ed0150}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:45,598] {docker.py:276} INFO - 21/05/15 13:07:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:45,598] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305383106938582266233557_0004_m_000187_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383106938582266233557_0004_m_000187_366
[2021-05-15 10:07:45,600] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Task committer attempt_202105151305383106938582266233557_0004_m_000187_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383106938582266233557_0004_m_000187_366 : duration 0:00.003s
[2021-05-15 10:07:45,847] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Starting: Task committer attempt_202105151305383179490000232579918_0004_m_000184_363: needsTaskCommit() Task attempt_202105151305383179490000232579918_0004_m_000184_363
[2021-05-15 10:07:45,848] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Task committer attempt_202105151305383179490000232579918_0004_m_000184_363: needsTaskCommit() Task attempt_202105151305383179490000232579918_0004_m_000184_363: duration 0:00.000s
21/05/15 13:07:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383179490000232579918_0004_m_000184_363
[2021-05-15 10:07:45,850] {docker.py:276} INFO - 21/05/15 13:07:45 INFO Executor: Finished task 184.0 in stage 4.0 (TID 363). 4587 bytes result sent to driver
[2021-05-15 10:07:45,851] {docker.py:276} INFO - 21/05/15 13:07:45 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 367) (4703d70ea67c, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:45,852] {docker.py:276} INFO - 21/05/15 13:07:45 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 363) in 2546 ms on 4703d70ea67c (executor driver) (185/200)
[2021-05-15 10:07:45,853] {docker.py:276} INFO - 21/05/15 13:07:45 INFO Executor: Running task 188.0 in stage 4.0 (TID 367)
[2021-05-15 10:07:45,862] {docker.py:276} INFO - 21/05/15 13:07:45 INFO ShuffleBlockFetcherIterator: Getting 4 (12.3 KiB) non-empty blocks including 4 (12.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:45,864] {docker.py:276} INFO - 21/05/15 13:07:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:45,865] {docker.py:276} INFO - 21/05/15 13:07:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:45,865] {docker.py:276} INFO - 21/05/15 13:07:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051513053822635690522284712_0004_m_000188_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053822635690522284712_0004_m_000188_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051513053822635690522284712_0004}; taskId=attempt_2021051513053822635690522284712_0004_m_000188_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d80ff5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:45,865] {docker.py:276} INFO - 21/05/15 13:07:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:45,866] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Starting: Task committer attempt_2021051513053822635690522284712_0004_m_000188_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053822635690522284712_0004_m_000188_367
[2021-05-15 10:07:45,869] {docker.py:276} INFO - 21/05/15 13:07:45 INFO StagingCommitter: Task committer attempt_2021051513053822635690522284712_0004_m_000188_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_2021051513053822635690522284712_0004_m_000188_367 : duration 0:00.005s
[2021-05-15 10:07:46,846] {docker.py:276} INFO - 21/05/15 13:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305381403362160241344285_0004_m_000185_364: needsTaskCommit() Task attempt_202105151305381403362160241344285_0004_m_000185_364
[2021-05-15 10:07:46,847] {docker.py:276} INFO - 21/05/15 13:07:46 INFO StagingCommitter: Task committer attempt_202105151305381403362160241344285_0004_m_000185_364: needsTaskCommit() Task attempt_202105151305381403362160241344285_0004_m_000185_364: duration 0:00.001s
21/05/15 13:07:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381403362160241344285_0004_m_000185_364
[2021-05-15 10:07:46,849] {docker.py:276} INFO - 21/05/15 13:07:46 INFO Executor: Finished task 185.0 in stage 4.0 (TID 364). 4587 bytes result sent to driver
[2021-05-15 10:07:46,850] {docker.py:276} INFO - 21/05/15 13:07:46 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 368) (4703d70ea67c, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:46,851] {docker.py:276} INFO - 21/05/15 13:07:46 INFO Executor: Running task 189.0 in stage 4.0 (TID 368)
[2021-05-15 10:07:46,852] {docker.py:276} INFO - 21/05/15 13:07:46 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 364) in 2556 ms on 4703d70ea67c (executor driver) (186/200)
[2021-05-15 10:07:46,861] {docker.py:276} INFO - 21/05/15 13:07:46 INFO ShuffleBlockFetcherIterator: Getting 4 (11.7 KiB) non-empty blocks including 4 (11.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:46,861] {docker.py:276} INFO - 21/05/15 13:07:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:46,863] {docker.py:276} INFO - 21/05/15 13:07:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-15 10:07:46,863] {docker.py:276} INFO - 21/05/15 13:07:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-15 10:07:46,864] {docker.py:276} INFO - 21/05/15 13:07:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:46,864] {docker.py:276} INFO - 21/05/15 13:07:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385035350994015748722_0004_m_000189_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385035350994015748722_0004_m_000189_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385035350994015748722_0004}; taskId=attempt_202105151305385035350994015748722_0004_m_000189_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3aa5a267}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:46,864] {docker.py:276} INFO - 21/05/15 13:07:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:46 INFO StagingCommitter: Starting: Task committer attempt_202105151305385035350994015748722_0004_m_000189_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385035350994015748722_0004_m_000189_368
[2021-05-15 10:07:46,867] {docker.py:276} INFO - 21/05/15 13:07:46 INFO StagingCommitter: Task committer attempt_202105151305385035350994015748722_0004_m_000189_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385035350994015748722_0004_m_000189_368 : duration 0:00.003s
[2021-05-15 10:07:47,089] {docker.py:276} INFO - 21/05/15 13:07:47 INFO StagingCommitter: Starting: Task committer attempt_20210515130538230639580626620400_0004_m_000186_365: needsTaskCommit() Task attempt_20210515130538230639580626620400_0004_m_000186_365
21/05/15 13:07:47 INFO StagingCommitter: Task committer attempt_20210515130538230639580626620400_0004_m_000186_365: needsTaskCommit() Task attempt_20210515130538230639580626620400_0004_m_000186_365: duration 0:00.001s
[2021-05-15 10:07:47,090] {docker.py:276} INFO - 21/05/15 13:07:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538230639580626620400_0004_m_000186_365
[2021-05-15 10:07:47,092] {docker.py:276} INFO - 21/05/15 13:07:47 INFO Executor: Finished task 186.0 in stage 4.0 (TID 365). 4544 bytes result sent to driver
[2021-05-15 10:07:47,093] {docker.py:276} INFO - 21/05/15 13:07:47 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 369) (4703d70ea67c, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:47,094] {docker.py:276} INFO - 21/05/15 13:07:47 INFO Executor: Running task 190.0 in stage 4.0 (TID 369)
[2021-05-15 10:07:47,095] {docker.py:276} INFO - 21/05/15 13:07:47 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 365) in 2467 ms on 4703d70ea67c (executor driver) (187/200)
[2021-05-15 10:07:47,104] {docker.py:276} INFO - 21/05/15 13:07:47 INFO ShuffleBlockFetcherIterator: Getting 4 (12.1 KiB) non-empty blocks including 4 (12.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:47,106] {docker.py:276} INFO - 21/05/15 13:07:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:47,107] {docker.py:276} INFO - 21/05/15 13:07:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382293916763114953190_0004_m_000190_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382293916763114953190_0004_m_000190_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382293916763114953190_0004}; taskId=attempt_202105151305382293916763114953190_0004_m_000190_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@341a3611}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:47 INFO StagingCommitter: Starting: Task committer attempt_202105151305382293916763114953190_0004_m_000190_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382293916763114953190_0004_m_000190_369
[2021-05-15 10:07:47,110] {docker.py:276} INFO - 21/05/15 13:07:47 INFO StagingCommitter: Task committer attempt_202105151305382293916763114953190_0004_m_000190_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382293916763114953190_0004_m_000190_369 : duration 0:00.003s
[2021-05-15 10:07:47,982] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305383106938582266233557_0004_m_000187_366: needsTaskCommit() Task attempt_202105151305383106938582266233557_0004_m_000187_366
[2021-05-15 10:07:47,984] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Task committer attempt_202105151305383106938582266233557_0004_m_000187_366: needsTaskCommit() Task attempt_202105151305383106938582266233557_0004_m_000187_366: duration 0:00.002s
[2021-05-15 10:07:47,984] {docker.py:276} INFO - 21/05/15 13:07:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383106938582266233557_0004_m_000187_366
[2021-05-15 10:07:47,986] {docker.py:276} INFO - 21/05/15 13:07:48 INFO Executor: Finished task 187.0 in stage 4.0 (TID 366). 4544 bytes result sent to driver
[2021-05-15 10:07:47,987] {docker.py:276} INFO - 21/05/15 13:07:48 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 370) (4703d70ea67c, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:47,988] {docker.py:276} INFO - 21/05/15 13:07:48 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 366) in 2409 ms on 4703d70ea67c (executor driver) (188/200)
[2021-05-15 10:07:47,989] {docker.py:276} INFO - 21/05/15 13:07:48 INFO Executor: Running task 191.0 in stage 4.0 (TID 370)
[2021-05-15 10:07:47,998] {docker.py:276} INFO - 21/05/15 13:07:48 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:47,998] {docker.py:276} INFO - 21/05/15 13:07:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:48,000] {docker.py:276} INFO - 21/05/15 13:07:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:48,001] {docker.py:276} INFO - 21/05/15 13:07:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305384790773037389479594_0004_m_000191_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384790773037389479594_0004_m_000191_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305384790773037389479594_0004}; taskId=attempt_202105151305384790773037389479594_0004_m_000191_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e95be5e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:48,001] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305384790773037389479594_0004_m_000191_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384790773037389479594_0004_m_000191_370
[2021-05-15 10:07:48,004] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Task committer attempt_202105151305384790773037389479594_0004_m_000191_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305384790773037389479594_0004_m_000191_370 : duration 0:00.004s
[2021-05-15 10:07:48,423] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Starting: Task committer attempt_2021051513053822635690522284712_0004_m_000188_367: needsTaskCommit() Task attempt_2021051513053822635690522284712_0004_m_000188_367
[2021-05-15 10:07:48,424] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Task committer attempt_2021051513053822635690522284712_0004_m_000188_367: needsTaskCommit() Task attempt_2021051513053822635690522284712_0004_m_000188_367: duration 0:00.000s
21/05/15 13:07:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051513053822635690522284712_0004_m_000188_367
[2021-05-15 10:07:48,426] {docker.py:276} INFO - 21/05/15 13:07:48 INFO Executor: Finished task 188.0 in stage 4.0 (TID 367). 4544 bytes result sent to driver
[2021-05-15 10:07:48,427] {docker.py:276} INFO - 21/05/15 13:07:48 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 371) (4703d70ea67c, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:48,428] {docker.py:276} INFO - 21/05/15 13:07:48 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 367) in 2580 ms on 4703d70ea67c (executor driver) (189/200)
[2021-05-15 10:07:48,429] {docker.py:276} INFO - 21/05/15 13:07:48 INFO Executor: Running task 192.0 in stage 4.0 (TID 371)
[2021-05-15 10:07:48,439] {docker.py:276} INFO - 21/05/15 13:07:48 INFO ShuffleBlockFetcherIterator: Getting 4 (12.0 KiB) non-empty blocks including 4 (12.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-15 10:07:48,439] {docker.py:276} INFO - 21/05/15 13:07:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:48,441] {docker.py:276} INFO - 21/05/15 13:07:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:48,441] {docker.py:276} INFO - 21/05/15 13:07:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382294507535361012398_0004_m_000192_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382294507535361012398_0004_m_000192_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382294507535361012398_0004}; taskId=attempt_202105151305382294507535361012398_0004_m_000192_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74507aaf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:48 INFO StagingCommitter: Starting: Task committer attempt_202105151305382294507535361012398_0004_m_000192_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382294507535361012398_0004_m_000192_371
[2021-05-15 10:07:48,443] {docker.py:276} INFO - 21/05/15 13:07:48 INFO StagingCommitter: Task committer attempt_202105151305382294507535361012398_0004_m_000192_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382294507535361012398_0004_m_000192_371 : duration 0:00.003s
[2021-05-15 10:07:49,342] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105151305385035350994015748722_0004_m_000189_368: needsTaskCommit() Task attempt_202105151305385035350994015748722_0004_m_000189_368
[2021-05-15 10:07:49,343] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Task committer attempt_202105151305385035350994015748722_0004_m_000189_368: needsTaskCommit() Task attempt_202105151305385035350994015748722_0004_m_000189_368: duration 0:00.001s
[2021-05-15 10:07:49,344] {docker.py:276} INFO - 21/05/15 13:07:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385035350994015748722_0004_m_000189_368
[2021-05-15 10:07:49,345] {docker.py:276} INFO - 21/05/15 13:07:49 INFO Executor: Finished task 189.0 in stage 4.0 (TID 368). 4544 bytes result sent to driver
[2021-05-15 10:07:49,346] {docker.py:276} INFO - 21/05/15 13:07:49 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 372) (4703d70ea67c, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:49,347] {docker.py:276} INFO - 21/05/15 13:07:49 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 368) in 2501 ms on 4703d70ea67c (executor driver) (190/200)
[2021-05-15 10:07:49,348] {docker.py:276} INFO - 21/05/15 13:07:49 INFO Executor: Running task 193.0 in stage 4.0 (TID 372)
[2021-05-15 10:07:49,357] {docker.py:276} INFO - 21/05/15 13:07:49 INFO ShuffleBlockFetcherIterator: Getting 4 (10.9 KiB) non-empty blocks including 4 (10.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:49,358] {docker.py:276} INFO - 21/05/15 13:07:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305386215271513839524236_0004_m_000193_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386215271513839524236_0004_m_000193_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305386215271513839524236_0004}; taskId=attempt_202105151305386215271513839524236_0004_m_000193_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58c4af52}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:49,359] {docker.py:276} INFO - 21/05/15 13:07:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105151305386215271513839524236_0004_m_000193_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386215271513839524236_0004_m_000193_372
[2021-05-15 10:07:49,361] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Task committer attempt_202105151305386215271513839524236_0004_m_000193_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305386215271513839524236_0004_m_000193_372 : duration 0:00.003s
[2021-05-15 10:07:49,629] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105151305382293916763114953190_0004_m_000190_369: needsTaskCommit() Task attempt_202105151305382293916763114953190_0004_m_000190_369
[2021-05-15 10:07:49,630] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Task committer attempt_202105151305382293916763114953190_0004_m_000190_369: needsTaskCommit() Task attempt_202105151305382293916763114953190_0004_m_000190_369: duration 0:00.000s
21/05/15 13:07:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382293916763114953190_0004_m_000190_369
[2021-05-15 10:07:49,631] {docker.py:276} INFO - 21/05/15 13:07:49 INFO Executor: Finished task 190.0 in stage 4.0 (TID 369). 4544 bytes result sent to driver
[2021-05-15 10:07:49,633] {docker.py:276} INFO - 21/05/15 13:07:49 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 373) (4703d70ea67c, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:49,633] {docker.py:276} INFO - 21/05/15 13:07:49 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 369) in 2544 ms on 4703d70ea67c (executor driver) (191/200)
21/05/15 13:07:49 INFO Executor: Running task 194.0 in stage 4.0 (TID 373)
[2021-05-15 10:07:49,644] {docker.py:276} INFO - 21/05/15 13:07:49 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:49,646] {docker.py:276} INFO - 21/05/15 13:07:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305388862570552245946935_0004_m_000194_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388862570552245946935_0004_m_000194_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305388862570552245946935_0004}; taskId=attempt_202105151305388862570552245946935_0004_m_000194_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4180313a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:49 INFO StagingCommitter: Starting: Task committer attempt_202105151305388862570552245946935_0004_m_000194_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388862570552245946935_0004_m_000194_373
[2021-05-15 10:07:49,649] {docker.py:276} INFO - 21/05/15 13:07:49 INFO StagingCommitter: Task committer attempt_202105151305388862570552245946935_0004_m_000194_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305388862570552245946935_0004_m_000194_373 : duration 0:00.002s
[2021-05-15 10:07:49,981] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305384790773037389479594_0004_m_000191_370: needsTaskCommit() Task attempt_202105151305384790773037389479594_0004_m_000191_370
[2021-05-15 10:07:49,982] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Task committer attempt_202105151305384790773037389479594_0004_m_000191_370: needsTaskCommit() Task attempt_202105151305384790773037389479594_0004_m_000191_370: duration 0:00.000s
[2021-05-15 10:07:49,982] {docker.py:276} INFO - 21/05/15 13:07:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305384790773037389479594_0004_m_000191_370
[2021-05-15 10:07:49,983] {docker.py:276} INFO - 21/05/15 13:07:50 INFO Executor: Finished task 191.0 in stage 4.0 (TID 370). 4544 bytes result sent to driver
[2021-05-15 10:07:49,985] {docker.py:276} INFO - 21/05/15 13:07:50 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 374) (4703d70ea67c, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:49,987] {docker.py:276} INFO - 21/05/15 13:07:50 INFO Executor: Running task 195.0 in stage 4.0 (TID 374)
21/05/15 13:07:50 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 370) in 2003 ms on 4703d70ea67c (executor driver) (192/200)
[2021-05-15 10:07:49,995] {docker.py:276} INFO - 21/05/15 13:07:50 INFO ShuffleBlockFetcherIterator: Getting 4 (11.2 KiB) non-empty blocks including 4 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:49,997] {docker.py:276} INFO - 21/05/15 13:07:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:49,997] {docker.py:276} INFO - 21/05/15 13:07:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305383441092192884511329_0004_m_000195_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383441092192884511329_0004_m_000195_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305383441092192884511329_0004}; taskId=attempt_202105151305383441092192884511329_0004_m_000195_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c585b19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:49,998] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305383441092192884511329_0004_m_000195_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383441092192884511329_0004_m_000195_374
[2021-05-15 10:07:50,000] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Task committer attempt_202105151305383441092192884511329_0004_m_000195_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305383441092192884511329_0004_m_000195_374 : duration 0:00.003s
[2021-05-15 10:07:50,908] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305382294507535361012398_0004_m_000192_371: needsTaskCommit() Task attempt_202105151305382294507535361012398_0004_m_000192_371
[2021-05-15 10:07:50,909] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Task committer attempt_202105151305382294507535361012398_0004_m_000192_371: needsTaskCommit() Task attempt_202105151305382294507535361012398_0004_m_000192_371: duration 0:00.001s
21/05/15 13:07:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382294507535361012398_0004_m_000192_371
[2021-05-15 10:07:50,910] {docker.py:276} INFO - 21/05/15 13:07:50 INFO Executor: Finished task 192.0 in stage 4.0 (TID 371). 4544 bytes result sent to driver
[2021-05-15 10:07:50,912] {docker.py:276} INFO - 21/05/15 13:07:50 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 375) (4703d70ea67c, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:50,913] {docker.py:276} INFO - 21/05/15 13:07:50 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 371) in 2490 ms on 4703d70ea67c (executor driver) (193/200)
[2021-05-15 10:07:50,913] {docker.py:276} INFO - 21/05/15 13:07:50 INFO Executor: Running task 196.0 in stage 4.0 (TID 375)
[2021-05-15 10:07:50,922] {docker.py:276} INFO - 21/05/15 13:07:50 INFO ShuffleBlockFetcherIterator: Getting 4 (12.9 KiB) non-empty blocks including 4 (12.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:50,924] {docker.py:276} INFO - 21/05/15 13:07:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305381865013974978264107_0004_m_000196_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381865013974978264107_0004_m_000196_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305381865013974978264107_0004}; taskId=attempt_202105151305381865013974978264107_0004_m_000196_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49a03a06}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:50,924] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Starting: Task committer attempt_202105151305381865013974978264107_0004_m_000196_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381865013974978264107_0004_m_000196_375
[2021-05-15 10:07:50,927] {docker.py:276} INFO - 21/05/15 13:07:50 INFO StagingCommitter: Task committer attempt_202105151305381865013974978264107_0004_m_000196_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305381865013974978264107_0004_m_000196_375 : duration 0:00.003s
[2021-05-15 10:07:51,870] {docker.py:276} INFO - 21/05/15 13:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105151305386215271513839524236_0004_m_000193_372: needsTaskCommit() Task attempt_202105151305386215271513839524236_0004_m_000193_372
[2021-05-15 10:07:51,871] {docker.py:276} INFO - 21/05/15 13:07:51 INFO StagingCommitter: Task committer attempt_202105151305386215271513839524236_0004_m_000193_372: needsTaskCommit() Task attempt_202105151305386215271513839524236_0004_m_000193_372: duration 0:00.000s
[2021-05-15 10:07:51,872] {docker.py:276} INFO - 21/05/15 13:07:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305386215271513839524236_0004_m_000193_372
[2021-05-15 10:07:51,872] {docker.py:276} INFO - 21/05/15 13:07:51 INFO Executor: Finished task 193.0 in stage 4.0 (TID 372). 4544 bytes result sent to driver
[2021-05-15 10:07:51,873] {docker.py:276} INFO - 21/05/15 13:07:51 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 376) (4703d70ea67c, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:51,874] {docker.py:276} INFO - 21/05/15 13:07:51 INFO Executor: Running task 197.0 in stage 4.0 (TID 376)
21/05/15 13:07:51 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 372) in 2531 ms on 4703d70ea67c (executor driver) (194/200)
[2021-05-15 10:07:51,882] {docker.py:276} INFO - 21/05/15 13:07:51 INFO ShuffleBlockFetcherIterator: Getting 4 (10.5 KiB) non-empty blocks including 4 (10.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:51,884] {docker.py:276} INFO - 21/05/15 13:07:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305382867035533814461259_0004_m_000197_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382867035533814461259_0004_m_000197_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305382867035533814461259_0004}; taskId=attempt_202105151305382867035533814461259_0004_m_000197_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16f41abd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:51 INFO StagingCommitter: Starting: Task committer attempt_202105151305382867035533814461259_0004_m_000197_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382867035533814461259_0004_m_000197_376
[2021-05-15 10:07:51,887] {docker.py:276} INFO - 21/05/15 13:07:51 INFO StagingCommitter: Task committer attempt_202105151305382867035533814461259_0004_m_000197_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305382867035533814461259_0004_m_000197_376 : duration 0:00.003s
[2021-05-15 10:07:52,054] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Starting: Task committer attempt_202105151305388862570552245946935_0004_m_000194_373: needsTaskCommit() Task attempt_202105151305388862570552245946935_0004_m_000194_373
[2021-05-15 10:07:52,055] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Task committer attempt_202105151305388862570552245946935_0004_m_000194_373: needsTaskCommit() Task attempt_202105151305388862570552245946935_0004_m_000194_373: duration 0:00.001s
[2021-05-15 10:07:52,055] {docker.py:276} INFO - 21/05/15 13:07:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305388862570552245946935_0004_m_000194_373
[2021-05-15 10:07:52,058] {docker.py:276} INFO - 21/05/15 13:07:52 INFO Executor: Finished task 194.0 in stage 4.0 (TID 373). 4544 bytes result sent to driver
[2021-05-15 10:07:52,059] {docker.py:276} INFO - 21/05/15 13:07:52 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 377) (4703d70ea67c, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:52,060] {docker.py:276} INFO - 21/05/15 13:07:52 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 373) in 2430 ms on 4703d70ea67c (executor driver) (195/200)
[2021-05-15 10:07:52,061] {docker.py:276} INFO - 21/05/15 13:07:52 INFO Executor: Running task 198.0 in stage 4.0 (TID 377)
[2021-05-15 10:07:52,070] {docker.py:276} INFO - 21/05/15 13:07:52 INFO ShuffleBlockFetcherIterator: Getting 4 (11.0 KiB) non-empty blocks including 4 (11.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:52,072] {docker.py:276} INFO - 21/05/15 13:07:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105151305385997557891257949202_0004_m_000198_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385997557891257949202_0004_m_000198_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105151305385997557891257949202_0004}; taskId=attempt_202105151305385997557891257949202_0004_m_000198_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fc5fe8e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/15 13:07:52 INFO StagingCommitter: Starting: Task committer attempt_202105151305385997557891257949202_0004_m_000198_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385997557891257949202_0004_m_000198_377
[2021-05-15 10:07:52,074] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Task committer attempt_202105151305385997557891257949202_0004_m_000198_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_202105151305385997557891257949202_0004_m_000198_377 : duration 0:00.002s
[2021-05-15 10:07:52,491] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Starting: Task committer attempt_202105151305383441092192884511329_0004_m_000195_374: needsTaskCommit() Task attempt_202105151305383441092192884511329_0004_m_000195_374
[2021-05-15 10:07:52,492] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Task committer attempt_202105151305383441092192884511329_0004_m_000195_374: needsTaskCommit() Task attempt_202105151305383441092192884511329_0004_m_000195_374: duration 0:00.001s
21/05/15 13:07:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305383441092192884511329_0004_m_000195_374
[2021-05-15 10:07:52,493] {docker.py:276} INFO - 21/05/15 13:07:52 INFO Executor: Finished task 195.0 in stage 4.0 (TID 374). 4544 bytes result sent to driver
[2021-05-15 10:07:52,495] {docker.py:276} INFO - 21/05/15 13:07:52 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 378) (4703d70ea67c, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-15 10:07:52,496] {docker.py:276} INFO - 21/05/15 13:07:52 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 374) in 2514 ms on 4703d70ea67c (executor driver) (196/200)
[2021-05-15 10:07:52,498] {docker.py:276} INFO - 21/05/15 13:07:52 INFO Executor: Running task 199.0 in stage 4.0 (TID 378)
[2021-05-15 10:07:52,507] {docker.py:276} INFO - 21/05/15 13:07:52 INFO ShuffleBlockFetcherIterator: Getting 4 (11.5 KiB) non-empty blocks including 4 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/15 13:07:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-15 10:07:52,509] {docker.py:276} INFO - 21/05/15 13:07:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/15 13:07:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/15 13:07:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/15 13:07:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210515130538850780431300994954_0004_m_000199_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850780431300994954_0004_m_000199_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210515130538850780431300994954_0004}; taskId=attempt_20210515130538850780431300994954_0004_m_000199_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50e70a63}; outputPath=file:/home/jovyan/tmp/staging/jovyan/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/15 13:07:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-15 10:07:52,509] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Starting: Task committer attempt_20210515130538850780431300994954_0004_m_000199_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850780431300994954_0004_m_000199_378
[2021-05-15 10:07:52,512] {docker.py:276} INFO - 21/05/15 13:07:52 INFO StagingCommitter: Task committer attempt_20210515130538850780431300994954_0004_m_000199_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/219ed9c8-ac1a-4c35-b5c0-a46e52310adf/_temporary/0/_temporary/attempt_20210515130538850780431300994954_0004_m_000199_378 : duration 0:00.003s
[2021-05-15 10:07:53,516] {docker.py:276} INFO - 21/05/15 13:07:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305381865013974978264107_0004_m_000196_375: needsTaskCommit() Task attempt_202105151305381865013974978264107_0004_m_000196_375
21/05/15 13:07:53 INFO StagingCommitter: Task committer attempt_202105151305381865013974978264107_0004_m_000196_375: needsTaskCommit() Task attempt_202105151305381865013974978264107_0004_m_000196_375: duration 0:00.001s
21/05/15 13:07:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305381865013974978264107_0004_m_000196_375
[2021-05-15 10:07:53,518] {docker.py:276} INFO - 21/05/15 13:07:53 INFO Executor: Finished task 196.0 in stage 4.0 (TID 375). 4544 bytes result sent to driver
[2021-05-15 10:07:53,520] {docker.py:276} INFO - 21/05/15 13:07:53 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 375) in 2611 ms on 4703d70ea67c (executor driver) (197/200)
[2021-05-15 10:07:53,913] {docker.py:276} INFO - 21/05/15 13:07:53 INFO StagingCommitter: Starting: Task committer attempt_202105151305385997557891257949202_0004_m_000198_377: needsTaskCommit() Task attempt_202105151305385997557891257949202_0004_m_000198_377
[2021-05-15 10:07:53,924] {docker.py:276} INFO - 21/05/15 13:07:53 INFO StagingCommitter: Task committer attempt_202105151305385997557891257949202_0004_m_000198_377: needsTaskCommit() Task attempt_202105151305385997557891257949202_0004_m_000198_377: duration 0:00.001s
[2021-05-15 10:07:53,924] {docker.py:276} INFO - 21/05/15 13:07:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305385997557891257949202_0004_m_000198_377
[2021-05-15 10:07:53,925] {docker.py:276} INFO - 21/05/15 13:07:53 INFO Executor: Finished task 198.0 in stage 4.0 (TID 377). 4544 bytes result sent to driver
[2021-05-15 10:07:53,925] {docker.py:276} INFO - 21/05/15 13:07:53 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 377) in 1861 ms on 4703d70ea67c (executor driver) (198/200)
[2021-05-15 10:07:54,176] {docker.py:276} INFO - 21/05/15 13:07:54 INFO StagingCommitter: Starting: Task committer attempt_202105151305382867035533814461259_0004_m_000197_376: needsTaskCommit() Task attempt_202105151305382867035533814461259_0004_m_000197_376
[2021-05-15 10:07:54,178] {docker.py:276} INFO - 21/05/15 13:07:54 INFO StagingCommitter: Task committer attempt_202105151305382867035533814461259_0004_m_000197_376: needsTaskCommit() Task attempt_202105151305382867035533814461259_0004_m_000197_376: duration 0:00.001s
21/05/15 13:07:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105151305382867035533814461259_0004_m_000197_376
[2021-05-15 10:07:54,180] {docker.py:276} INFO - 21/05/15 13:07:54 INFO Executor: Finished task 197.0 in stage 4.0 (TID 376). 4544 bytes result sent to driver
[2021-05-15 10:07:54,182] {docker.py:276} INFO - 21/05/15 13:07:54 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 376) in 2310 ms on 4703d70ea67c (executor driver) (199/200)
[2021-05-15 10:07:54,355] {docker.py:276} INFO - 21/05/15 13:07:54 INFO StagingCommitter: Starting: Task committer attempt_20210515130538850780431300994954_0004_m_000199_378: needsTaskCommit() Task attempt_20210515130538850780431300994954_0004_m_000199_378
[2021-05-15 10:07:54,358] {docker.py:276} INFO - 21/05/15 13:07:54 INFO StagingCommitter: Task committer attempt_20210515130538850780431300994954_0004_m_000199_378: needsTaskCommit() Task attempt_20210515130538850780431300994954_0004_m_000199_378: duration 0:00.003s
21/05/15 13:07:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210515130538850780431300994954_0004_m_000199_378
[2021-05-15 10:07:54,360] {docker.py:276} INFO - 21/05/15 13:07:54 INFO Executor: Finished task 199.0 in stage 4.0 (TID 378). 4544 bytes result sent to driver
[2021-05-15 10:07:54,361] {docker.py:276} INFO - 21/05/15 13:07:54 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 378) in 1869 ms on 4703d70ea67c (executor driver) (200/200)
21/05/15 13:07:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-15 10:07:54,362] {docker.py:276} INFO - 21/05/15 13:07:54 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 125.841 s
[2021-05-15 10:07:54,363] {docker.py:276} INFO - 21/05/15 13:07:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/15 13:07:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-15 10:07:54,364] {docker.py:276} INFO - 21/05/15 13:07:54 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 135.782501 s
[2021-05-15 10:07:54,366] {docker.py:276} INFO - 21/05/15 13:07:54 INFO AbstractS3ACommitter: Starting: Task committer attempt_20210515130538415247702313742323_0000_m_000000_0: commitJob((no job ID))
[2021-05-15 10:07:54,392] {docker.py:276} INFO - 21/05/15 13:07:54 WARN AbstractS3ACommitter: Task committer attempt_20210515130538415247702313742323_0000_m_000000_0: No pending uploads to commit
[2021-05-15 10:07:54,904] {docker.py:276} INFO - 21/05/15 13:07:54 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/15 13:07:54 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-15 10:07:55,083] {docker.py:276} INFO - 21/05/15 13:07:55 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.179s
21/05/15 13:07:55 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.179s
[2021-05-15 10:07:55,084] {docker.py:276} INFO - 21/05/15 13:07:55 INFO AbstractS3ACommitter: Task committer attempt_20210515130538415247702313742323_0000_m_000000_0: commitJob((no job ID)): duration 0:00.719s
[2021-05-15 10:07:55,608] {docker.py:276} INFO - 21/05/15 13:07:55 INFO FileFormatWriter: Write Job 219ed9c8-ac1a-4c35-b5c0-a46e52310adf committed.
[2021-05-15 10:07:55,617] {docker.py:276} INFO - 21/05/15 13:07:55 INFO FileFormatWriter: Finished processing stats for write job 219ed9c8-ac1a-4c35-b5c0-a46e52310adf.
[2021-05-15 10:07:55,730] {docker.py:276} INFO - 21/05/15 13:07:55 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-15 10:07:55,749] {docker.py:276} INFO - 21/05/15 13:07:55 INFO SparkUI: Stopped Spark web UI at http://4703d70ea67c:4040
[2021-05-15 10:07:55,775] {docker.py:276} INFO - 21/05/15 13:07:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-15 10:07:55,806] {docker.py:276} INFO - 21/05/15 13:07:55 INFO MemoryStore: MemoryStore cleared
[2021-05-15 10:07:55,808] {docker.py:276} INFO - 21/05/15 13:07:55 INFO BlockManager: BlockManager stopped
[2021-05-15 10:07:55,811] {docker.py:276} INFO - 21/05/15 13:07:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-15 10:07:55,818] {docker.py:276} INFO - 21/05/15 13:07:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-15 10:07:55,826] {docker.py:276} INFO - 21/05/15 13:07:55 INFO SparkContext: Successfully stopped SparkContext
[2021-05-15 10:07:55,827] {docker.py:276} INFO - 21/05/15 13:07:55 INFO ShutdownHookManager: Shutdown hook called
[2021-05-15 10:07:55,828] {docker.py:276} INFO - 21/05/15 13:07:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfcaf8a9-c5ea-4779-b09a-3144ebaff9c7
[2021-05-15 10:07:55,832] {docker.py:276} INFO - 21/05/15 13:07:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d2c37d4-4f89-40f2-b2e0-16732ce9f44b/pyspark-3dbbbc9f-2e1e-4644-9ffe-1c8039c73b1b
[2021-05-15 10:07:55,836] {docker.py:276} INFO - 21/05/15 13:07:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d2c37d4-4f89-40f2-b2e0-16732ce9f44b
[2021-05-15 10:07:55,845] {docker.py:276} INFO - 21/05/15 13:07:55 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-15 10:07:55,846] {docker.py:276} INFO - 21/05/15 13:07:55 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-15 10:07:55,847] {docker.py:276} INFO - 21/05/15 13:07:55 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-15 10:07:56,089] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T050000, start_date=20210515T130500, end_date=20210515T130756
[2021-05-15 10:07:56,135] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-15 10:07:56,163] {local_task_job.py:146} INFO - Task exited with return code 0
