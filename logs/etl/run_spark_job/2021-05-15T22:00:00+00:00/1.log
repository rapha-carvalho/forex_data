[2021-05-16 20:11:40,311] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T22:00:00+00:00 [queued]>
[2021-05-16 20:11:40,317] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-15T22:00:00+00:00 [queued]>
[2021-05-16 20:11:40,317] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 20:11:40,317] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-16 20:11:40,317] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-16 20:11:40,323] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-15T22:00:00+00:00
[2021-05-16 20:11:40,326] {standard_task_runner.py:52} INFO - Started process 31060 to run task
[2021-05-16 20:11:40,332] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-15T22:00:00+00:00', '--job-id', '770', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpycqhl_u_', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpcc6h7p27']
[2021-05-16 20:11:40,334] {standard_task_runner.py:77} INFO - Job 770: Subtask run_spark_job
[2021-05-16 20:11:40,367] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-15T22:00:00+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-16 20:11:40,419] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-15T22:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-15T22:00:00+00:00
[2021-05-16 20:11:40,422] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-16 20:11:44,101] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-16 20:11:44,105] {docker.py:312} INFO - Digest: sha256:4e46a1dd36dff0cd54870612109ad855e6261637c4ee65f5dbc48a05c92675ea
[2021-05-16 20:11:44,106] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-16 20:11:44,113] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-16 20:11:46,495] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-16 20:11:47,083] {docker.py:276} INFO - 21/05/16 23:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-16 20:11:49,552] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-16 20:11:49,566] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SparkContext: Running Spark version 3.1.1
[2021-05-16 20:11:49,652] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceUtils: ==============================================================
[2021-05-16 20:11:49,653] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-16 20:11:49,653] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceUtils: ==============================================================
[2021-05-16 20:11:49,654] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SparkContext: Submitted application: spark.py
[2021-05-16 20:11:49,690] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-16 20:11:49,705] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceProfile: Limiting resource is cpu
[2021-05-16 20:11:49,706] {docker.py:276} INFO - 21/05/16 23:11:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-16 20:11:49,782] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-16 20:11:49,783] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SecurityManager: Changing modify acls to: jovyan
21/05/16 23:11:49 INFO SecurityManager: Changing view acls groups to:
[2021-05-16 20:11:49,784] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SecurityManager: Changing modify acls groups to:
[2021-05-16 20:11:49,784] {docker.py:276} INFO - 21/05/16 23:11:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-16 20:11:50,160] {docker.py:276} INFO - 21/05/16 23:11:50 INFO Utils: Successfully started service 'sparkDriver' on port 44093.
[2021-05-16 20:11:50,205] {docker.py:276} INFO - 21/05/16 23:11:50 INFO SparkEnv: Registering MapOutputTracker
[2021-05-16 20:11:50,252] {docker.py:276} INFO - 21/05/16 23:11:50 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-16 20:11:50,290] {docker.py:276} INFO - 21/05/16 23:11:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-16 20:11:50,290] {docker.py:276} INFO - 21/05/16 23:11:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-16 20:11:50,298] {docker.py:276} INFO - 21/05/16 23:11:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-16 20:11:50,317] {docker.py:276} INFO - 21/05/16 23:11:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d293c72-7254-477d-8003-74d2b386673f
[2021-05-16 20:11:50,355] {docker.py:276} INFO - 21/05/16 23:11:50 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-16 20:11:50,383] {docker.py:276} INFO - 21/05/16 23:11:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-16 20:11:50,716] {docker.py:276} INFO - 21/05/16 23:11:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-16 20:11:50,821] {docker.py:276} INFO - 21/05/16 23:11:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://764b1db4ecfd:4040
[2021-05-16 20:11:51,088] {docker.py:276} INFO - 21/05/16 23:11:51 INFO Executor: Starting executor ID driver on host 764b1db4ecfd
[2021-05-16 20:11:51,144] {docker.py:276} INFO - 21/05/16 23:11:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38389.
[2021-05-16 20:11:51,144] {docker.py:276} INFO - 21/05/16 23:11:51 INFO NettyBlockTransferService: Server created on 764b1db4ecfd:38389
[2021-05-16 20:11:51,148] {docker.py:276} INFO - 21/05/16 23:11:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-16 20:11:51,160] {docker.py:276} INFO - 21/05/16 23:11:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 764b1db4ecfd, 38389, None)
[2021-05-16 20:11:51,168] {docker.py:276} INFO - 21/05/16 23:11:51 INFO BlockManagerMasterEndpoint: Registering block manager 764b1db4ecfd:38389 with 934.4 MiB RAM, BlockManagerId(driver, 764b1db4ecfd, 38389, None)
[2021-05-16 20:11:51,172] {docker.py:276} INFO - 21/05/16 23:11:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 764b1db4ecfd, 38389, None)
[2021-05-16 20:11:51,174] {docker.py:276} INFO - 21/05/16 23:11:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 764b1db4ecfd, 38389, None)
[2021-05-16 20:11:51,796] {docker.py:276} INFO - 21/05/16 23:11:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-16 20:11:51,797] {docker.py:276} INFO - 21/05/16 23:11:51 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-16 20:11:52,897] {docker.py:276} INFO - 21/05/16 23:11:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-16 20:11:52,948] {docker.py:276} INFO - 21/05/16 23:11:52 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/16 23:11:52 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-16 20:11:58,837] {docker.py:276} INFO - 21/05/16 23:11:58 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 186 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621095045_to_1621096845.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621096845_to_1621098645.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621098645_to_1621100445.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621100445_to_1621102245.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621102245_to_1621104045.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621104045_to_1621105845.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621105845_to_1621107645.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621107645_to_1621109445.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621109445_to_1621111245.csv.
[2021-05-16 20:11:59,372] {docker.py:276} INFO - 21/05/16 23:11:59 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:11:59,402] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 186 output partitions
[2021-05-16 20:11:59,403] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 20:11:59,410] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 20:11:59,413] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Missing parents: List()
[2021-05-16 20:11:59,424] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 20:11:59,571] {docker.py:276} INFO - 21/05/16 23:11:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.9 KiB, free 934.3 MiB)
[2021-05-16 20:11:59,645] {docker.py:276} INFO - 21/05/16 23:11:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.3 MiB)
[2021-05-16 20:11:59,649] {docker.py:276} INFO - 21/05/16 23:11:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 764b1db4ecfd:38389 (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 20:11:59,663] {docker.py:276} INFO - 21/05/16 23:11:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-16 20:11:59,690] {docker.py:276} INFO - 21/05/16 23:11:59 INFO DAGScheduler: Submitting 186 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-16 20:11:59,692] {docker.py:276} INFO - 21/05/16 23:11:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 186 tasks resource profile 0
[2021-05-16 20:11:59,796] {docker.py:276} INFO - 21/05/16 23:11:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (764b1db4ecfd, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:11:59,816] {docker.py:276} INFO - 21/05/16 23:11:59 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (764b1db4ecfd, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:11:59,817] {docker.py:276} INFO - 21/05/16 23:11:59 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (764b1db4ecfd, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:11:59,819] {docker.py:276} INFO - 21/05/16 23:11:59 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (764b1db4ecfd, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:11:59,844] {docker.py:276} INFO - 21/05/16 23:11:59 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-16 20:11:59,846] {docker.py:276} INFO - 21/05/16 23:11:59 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-16 20:11:59,846] {docker.py:276} INFO - 21/05/16 23:11:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-16 20:11:59,847] {docker.py:276} INFO - 21/05/16 23:11:59 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-16 20:12:00,278] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1886 bytes result sent to driver
[2021-05-16 20:12:00,285] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (764b1db4ecfd, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,285] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-16 20:12:00,291] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 472 ms on 764b1db4ecfd (executor driver) (1/186)
[2021-05-16 20:12:00,478] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
[2021-05-16 20:12:00,486] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (764b1db4ecfd, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:00 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
21/05/16 23:12:00 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 201 ms on 764b1db4ecfd (executor driver) (2/186)
[2021-05-16 20:12:00,676] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1843 bytes result sent to driver
[2021-05-16 20:12:00,679] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (764b1db4ecfd, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,680] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-16 20:12:00,681] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 200 ms on 764b1db4ecfd (executor driver) (3/186)
[2021-05-16 20:12:00,756] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver
[2021-05-16 20:12:00,758] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (764b1db4ecfd, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,759] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-16 20:12:00,760] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1002 ms on 764b1db4ecfd (executor driver) (4/186)
[2021-05-16 20:12:00,805] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1886 bytes result sent to driver
[2021-05-16 20:12:00,808] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (764b1db4ecfd, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,809] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 994 ms on 764b1db4ecfd (executor driver) (5/186)
[2021-05-16 20:12:00,810] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1886 bytes result sent to driver
[2021-05-16 20:12:00,812] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-16 20:12:00,813] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (764b1db4ecfd, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,814] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
21/05/16 23:12:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1015 ms on 764b1db4ecfd (executor driver) (6/186)
[2021-05-16 20:12:00,877] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1886 bytes result sent to driver
[2021-05-16 20:12:00,879] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (764b1db4ecfd, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,880] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 203 ms on 764b1db4ecfd (executor driver) (7/186)
[2021-05-16 20:12:00,881] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-16 20:12:00,935] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1843 bytes result sent to driver
[2021-05-16 20:12:00,936] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (764b1db4ecfd, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:00,937] {docker.py:276} INFO - 21/05/16 23:12:00 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 179 ms on 764b1db4ecfd (executor driver) (8/186)
[2021-05-16 20:12:00,937] {docker.py:276} INFO - 21/05/16 23:12:00 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-16 20:12:00,998] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1843 bytes result sent to driver
[2021-05-16 20:12:00,999] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (764b1db4ecfd, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,000] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-16 20:12:01,001] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 189 ms on 764b1db4ecfd (executor driver) (9/186)
[2021-05-16 20:12:01,007] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
[2021-05-16 20:12:01,008] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (764b1db4ecfd, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,009] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 202 ms on 764b1db4ecfd (executor driver) (10/186)
[2021-05-16 20:12:01,010] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-16 20:12:01,058] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1843 bytes result sent to driver
[2021-05-16 20:12:01,060] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (764b1db4ecfd, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,061] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 183 ms on 764b1db4ecfd (executor driver) (11/186)
[2021-05-16 20:12:01,061] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-16 20:12:01,111] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1843 bytes result sent to driver
[2021-05-16 20:12:01,113] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (764b1db4ecfd, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,114] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
21/05/16 23:12:01 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 179 ms on 764b1db4ecfd (executor driver) (12/186)
[2021-05-16 20:12:01,190] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1843 bytes result sent to driver
[2021-05-16 20:12:01,191] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1843 bytes result sent to driver
[2021-05-16 20:12:01,192] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (764b1db4ecfd, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,193] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-16 20:12:01,194] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (764b1db4ecfd, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,195] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 196 ms on 764b1db4ecfd (executor driver) (13/186)
[2021-05-16 20:12:01,195] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-16 20:12:01,198] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 190 ms on 764b1db4ecfd (executor driver) (14/186)
[2021-05-16 20:12:01,237] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
[2021-05-16 20:12:01,239] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (764b1db4ecfd, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,241] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 182 ms on 764b1db4ecfd (executor driver) (15/186)
21/05/16 23:12:01 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
[2021-05-16 20:12:01,291] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1843 bytes result sent to driver
[2021-05-16 20:12:01,293] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (764b1db4ecfd, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,294] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-16 20:12:01,295] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 183 ms on 764b1db4ecfd (executor driver) (16/186)
[2021-05-16 20:12:01,376] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1843 bytes result sent to driver
[2021-05-16 20:12:01,377] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 185 ms on 764b1db4ecfd (executor driver) (17/186)
[2021-05-16 20:12:01,379] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (764b1db4ecfd, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,382] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-16 20:12:01,383] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1843 bytes result sent to driver
[2021-05-16 20:12:01,385] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (764b1db4ecfd, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,386] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-16 20:12:01,387] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 195 ms on 764b1db4ecfd (executor driver) (18/186)
[2021-05-16 20:12:01,417] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1886 bytes result sent to driver
[2021-05-16 20:12:01,418] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (764b1db4ecfd, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,419] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-16 20:12:01,420] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 182 ms on 764b1db4ecfd (executor driver) (19/186)
[2021-05-16 20:12:01,479] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1886 bytes result sent to driver
[2021-05-16 20:12:01,483] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (764b1db4ecfd, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,484] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 192 ms on 764b1db4ecfd (executor driver) (20/186)
[2021-05-16 20:12:01,486] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-16 20:12:01,574] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1886 bytes result sent to driver
[2021-05-16 20:12:01,575] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1886 bytes result sent to driver
[2021-05-16 20:12:01,578] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (764b1db4ecfd, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,581] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-16 20:12:01,582] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (764b1db4ecfd, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,582] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-16 20:12:01,583] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 198 ms on 764b1db4ecfd (executor driver) (21/186)
[2021-05-16 20:12:01,583] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 205 ms on 764b1db4ecfd (executor driver) (22/186)
[2021-05-16 20:12:01,597] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1843 bytes result sent to driver
[2021-05-16 20:12:01,598] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (764b1db4ecfd, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,599] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 181 ms on 764b1db4ecfd (executor driver) (23/186)
[2021-05-16 20:12:01,600] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-16 20:12:01,663] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1843 bytes result sent to driver
[2021-05-16 20:12:01,664] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (764b1db4ecfd, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,666] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 184 ms on 764b1db4ecfd (executor driver) (24/186)
[2021-05-16 20:12:01,666] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-16 20:12:01,771] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1843 bytes result sent to driver
[2021-05-16 20:12:01,772] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (764b1db4ecfd, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,773] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 193 ms on 764b1db4ecfd (executor driver) (25/186)
[2021-05-16 20:12:01,774] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-16 20:12:01,778] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1843 bytes result sent to driver
[2021-05-16 20:12:01,779] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (764b1db4ecfd, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,780] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 204 ms on 764b1db4ecfd (executor driver) (26/186)
[2021-05-16 20:12:01,781] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-16 20:12:01,786] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
[2021-05-16 20:12:01,787] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (764b1db4ecfd, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,788] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 191 ms on 764b1db4ecfd (executor driver) (27/186)
[2021-05-16 20:12:01,789] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-16 20:12:01,840] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1843 bytes result sent to driver
[2021-05-16 20:12:01,841] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (764b1db4ecfd, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,843] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 179 ms on 764b1db4ecfd (executor driver) (28/186)
[2021-05-16 20:12:01,843] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-16 20:12:01,958] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1843 bytes result sent to driver
[2021-05-16 20:12:01,959] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1843 bytes result sent to driver
[2021-05-16 20:12:01,960] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (764b1db4ecfd, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,962] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-16 20:12:01,963] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (764b1db4ecfd, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,964] {docker.py:276} INFO - 21/05/16 23:12:01 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-16 20:12:01,965] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 193 ms on 764b1db4ecfd (executor driver) (29/186)
[2021-05-16 20:12:01,966] {docker.py:276} INFO - 21/05/16 23:12:01 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 187 ms on 764b1db4ecfd (executor driver) (30/186)
[2021-05-16 20:12:01,971] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1843 bytes result sent to driver
[2021-05-16 20:12:01,975] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (764b1db4ecfd, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:01,977] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 190 ms on 764b1db4ecfd (executor driver) (31/186)
[2021-05-16 20:12:01,977] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-16 20:12:02,038] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1843 bytes result sent to driver
[2021-05-16 20:12:02,040] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (764b1db4ecfd, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,041] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-16 20:12:02,041] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 200 ms on 764b1db4ecfd (executor driver) (32/186)
[2021-05-16 20:12:02,142] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1843 bytes result sent to driver
[2021-05-16 20:12:02,145] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (764b1db4ecfd, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,146] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 185 ms on 764b1db4ecfd (executor driver) (33/186)
[2021-05-16 20:12:02,149] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
21/05/16 23:12:02 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
[2021-05-16 20:12:02,150] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (764b1db4ecfd, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,154] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 193 ms on 764b1db4ecfd (executor driver) (34/186)
[2021-05-16 20:12:02,155] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-16 20:12:02,156] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1843 bytes result sent to driver
[2021-05-16 20:12:02,168] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (764b1db4ecfd, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,173] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
21/05/16 23:12:02 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 194 ms on 764b1db4ecfd (executor driver) (35/186)
[2021-05-16 20:12:02,218] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1886 bytes result sent to driver
[2021-05-16 20:12:02,219] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (764b1db4ecfd, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,221] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 181 ms on 764b1db4ecfd (executor driver) (36/186)
[2021-05-16 20:12:02,221] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-16 20:12:02,341] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1886 bytes result sent to driver
[2021-05-16 20:12:02,343] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (764b1db4ecfd, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,345] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 202 ms on 764b1db4ecfd (executor driver) (37/186)
[2021-05-16 20:12:02,345] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
21/05/16 23:12:02 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1886 bytes result sent to driver
[2021-05-16 20:12:02,346] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-16 20:12:02,347] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (764b1db4ecfd, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,349] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-16 20:12:02,350] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (764b1db4ecfd, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,351] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-16 20:12:02,351] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 184 ms on 764b1db4ecfd (executor driver) (38/186)
[2021-05-16 20:12:02,352] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 202 ms on 764b1db4ecfd (executor driver) (39/186)
[2021-05-16 20:12:02,391] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1843 bytes result sent to driver
[2021-05-16 20:12:02,393] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (764b1db4ecfd, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,394] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 175 ms on 764b1db4ecfd (executor driver) (40/186)
[2021-05-16 20:12:02,395] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-16 20:12:02,528] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1843 bytes result sent to driver
[2021-05-16 20:12:02,529] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1843 bytes result sent to driver
[2021-05-16 20:12:02,530] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (764b1db4ecfd, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,532] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-16 20:12:02,532] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1843 bytes result sent to driver
[2021-05-16 20:12:02,534] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (764b1db4ecfd, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,536] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-16 20:12:02,537] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 187 ms on 764b1db4ecfd (executor driver) (41/186)
[2021-05-16 20:12:02,537] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 190 ms on 764b1db4ecfd (executor driver) (42/186)
[2021-05-16 20:12:02,539] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (764b1db4ecfd, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,541] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 199 ms on 764b1db4ecfd (executor driver) (43/186)
[2021-05-16 20:12:02,541] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-16 20:12:02,571] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1843 bytes result sent to driver
[2021-05-16 20:12:02,572] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (764b1db4ecfd, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,573] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 180 ms on 764b1db4ecfd (executor driver) (44/186)
[2021-05-16 20:12:02,574] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-16 20:12:02,714] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
[2021-05-16 20:12:02,715] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (764b1db4ecfd, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,718] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1843 bytes result sent to driver
[2021-05-16 20:12:02,719] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
21/05/16 23:12:02 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 188 ms on 764b1db4ecfd (executor driver) (45/186)
[2021-05-16 20:12:02,720] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (764b1db4ecfd, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,724] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1843 bytes result sent to driver
[2021-05-16 20:12:02,726] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-16 20:12:02,727] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (764b1db4ecfd, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,729] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 189 ms on 764b1db4ecfd (executor driver) (46/186)
[2021-05-16 20:12:02,730] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-16 20:12:02,731] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 197 ms on 764b1db4ecfd (executor driver) (47/186)
[2021-05-16 20:12:02,750] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1843 bytes result sent to driver
[2021-05-16 20:12:02,754] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (764b1db4ecfd, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,756] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 184 ms on 764b1db4ecfd (executor driver) (48/186)
[2021-05-16 20:12:02,756] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-16 20:12:02,914] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
[2021-05-16 20:12:02,916] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (764b1db4ecfd, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:02,918] {docker.py:276} INFO - 21/05/16 23:12:02 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 192 ms on 764b1db4ecfd (executor driver) (49/186)
[2021-05-16 20:12:02,919] {docker.py:276} INFO - 21/05/16 23:12:02 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-16 20:12:02,999] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1843 bytes result sent to driver
[2021-05-16 20:12:03,001] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (764b1db4ecfd, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,004] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 287 ms on 764b1db4ecfd (executor driver) (50/186)
[2021-05-16 20:12:03,004] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-16 20:12:03,005] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1886 bytes result sent to driver
[2021-05-16 20:12:03,006] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1843 bytes result sent to driver
[2021-05-16 20:12:03,007] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (764b1db4ecfd, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,008] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 256 ms on 764b1db4ecfd (executor driver) (51/186)
[2021-05-16 20:12:03,009] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 291 ms on 764b1db4ecfd (executor driver) (52/186)
[2021-05-16 20:12:03,010] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-16 20:12:03,012] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (764b1db4ecfd, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,033] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-16 20:12:03,105] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1886 bytes result sent to driver
[2021-05-16 20:12:03,109] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (764b1db4ecfd, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,111] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 195 ms on 764b1db4ecfd (executor driver) (53/186)
[2021-05-16 20:12:03,112] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-16 20:12:03,208] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1886 bytes result sent to driver
[2021-05-16 20:12:03,209] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (764b1db4ecfd, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,210] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-16 20:12:03,211] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 205 ms on 764b1db4ecfd (executor driver) (54/186)
[2021-05-16 20:12:03,218] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
[2021-05-16 20:12:03,221] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (764b1db4ecfd, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,222] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 210 ms on 764b1db4ecfd (executor driver) (55/186)
[2021-05-16 20:12:03,223] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1886 bytes result sent to driver
[2021-05-16 20:12:03,224] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-16 20:12:03,225] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (764b1db4ecfd, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,226] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 226 ms on 764b1db4ecfd (executor driver) (56/186)
[2021-05-16 20:12:03,227] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-16 20:12:03,296] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1843 bytes result sent to driver
[2021-05-16 20:12:03,298] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (764b1db4ecfd, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,300] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 193 ms on 764b1db4ecfd (executor driver) (57/186)
[2021-05-16 20:12:03,301] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-16 20:12:03,383] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1843 bytes result sent to driver
[2021-05-16 20:12:03,386] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (764b1db4ecfd, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,388] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
21/05/16 23:12:03 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 179 ms on 764b1db4ecfd (executor driver) (58/186)
[2021-05-16 20:12:03,403] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1843 bytes result sent to driver
[2021-05-16 20:12:03,405] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (764b1db4ecfd, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,407] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 188 ms on 764b1db4ecfd (executor driver) (59/186)
[2021-05-16 20:12:03,408] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-16 20:12:03,409] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1843 bytes result sent to driver
[2021-05-16 20:12:03,411] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (764b1db4ecfd, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,413] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 189 ms on 764b1db4ecfd (executor driver) (60/186)
[2021-05-16 20:12:03,414] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-16 20:12:03,480] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1843 bytes result sent to driver
[2021-05-16 20:12:03,483] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (764b1db4ecfd, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,484] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-16 20:12:03,486] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 186 ms on 764b1db4ecfd (executor driver) (61/186)
[2021-05-16 20:12:03,567] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
[2021-05-16 20:12:03,569] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (764b1db4ecfd, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,571] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
21/05/16 23:12:03 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 186 ms on 764b1db4ecfd (executor driver) (62/186)
[2021-05-16 20:12:03,593] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1843 bytes result sent to driver
[2021-05-16 20:12:03,594] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1843 bytes result sent to driver
[2021-05-16 20:12:03,595] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 191 ms on 764b1db4ecfd (executor driver) (63/186)
[2021-05-16 20:12:03,599] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (764b1db4ecfd, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,601] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-16 20:12:03,603] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (764b1db4ecfd, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,606] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-16 20:12:03,606] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 195 ms on 764b1db4ecfd (executor driver) (64/186)
[2021-05-16 20:12:03,712] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1843 bytes result sent to driver
[2021-05-16 20:12:03,715] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (764b1db4ecfd, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,717] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 234 ms on 764b1db4ecfd (executor driver) (65/186)
[2021-05-16 20:12:03,718] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
[2021-05-16 20:12:03,748] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1843 bytes result sent to driver
[2021-05-16 20:12:03,749] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (764b1db4ecfd, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,750] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 182 ms on 764b1db4ecfd (executor driver) (66/186)
[2021-05-16 20:12:03,751] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-16 20:12:03,785] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1843 bytes result sent to driver
[2021-05-16 20:12:03,786] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
[2021-05-16 20:12:03,787] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (764b1db4ecfd, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,789] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (764b1db4ecfd, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:03 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-16 20:12:03,790] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
[2021-05-16 20:12:03,792] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 189 ms on 764b1db4ecfd (executor driver) (67/186)
[2021-05-16 20:12:03,792] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 195 ms on 764b1db4ecfd (executor driver) (68/186)
[2021-05-16 20:12:03,901] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1886 bytes result sent to driver
[2021-05-16 20:12:03,904] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 189 ms on 764b1db4ecfd (executor driver) (69/186)
[2021-05-16 20:12:03,905] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (764b1db4ecfd, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,906] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-16 20:12:03,931] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1886 bytes result sent to driver
[2021-05-16 20:12:03,933] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (764b1db4ecfd, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:03,934] {docker.py:276} INFO - 21/05/16 23:12:03 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 186 ms on 764b1db4ecfd (executor driver) (70/186)
[2021-05-16 20:12:03,935] {docker.py:276} INFO - 21/05/16 23:12:03 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-16 20:12:04,032] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1886 bytes result sent to driver
21/05/16 23:12:04 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (764b1db4ecfd, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:04 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
21/05/16 23:12:04 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 245 ms on 764b1db4ecfd (executor driver) (71/186)
[2021-05-16 20:12:04,049] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1886 bytes result sent to driver
[2021-05-16 20:12:04,053] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (764b1db4ecfd, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,057] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 270 ms on 764b1db4ecfd (executor driver) (72/186)
[2021-05-16 20:12:04,065] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-16 20:12:04,095] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1843 bytes result sent to driver
[2021-05-16 20:12:04,095] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (764b1db4ecfd, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,099] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 194 ms on 764b1db4ecfd (executor driver) (73/186)
[2021-05-16 20:12:04,103] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-16 20:12:04,192] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
[2021-05-16 20:12:04,194] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (764b1db4ecfd, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,196] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
21/05/16 23:12:04 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 265 ms on 764b1db4ecfd (executor driver) (74/186)
[2021-05-16 20:12:04,237] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1843 bytes result sent to driver
21/05/16 23:12:04 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (764b1db4ecfd, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,238] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-16 20:12:04,239] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 212 ms on 764b1db4ecfd (executor driver) (75/186)
[2021-05-16 20:12:04,251] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1843 bytes result sent to driver
[2021-05-16 20:12:04,253] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (764b1db4ecfd, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,257] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 205 ms on 764b1db4ecfd (executor driver) (76/186)
[2021-05-16 20:12:04,257] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-16 20:12:04,283] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1843 bytes result sent to driver
[2021-05-16 20:12:04,285] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (764b1db4ecfd, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,287] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-16 20:12:04,288] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 193 ms on 764b1db4ecfd (executor driver) (77/186)
[2021-05-16 20:12:04,379] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
[2021-05-16 20:12:04,381] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (764b1db4ecfd, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,382] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 189 ms on 764b1db4ecfd (executor driver) (78/186)
[2021-05-16 20:12:04,382] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-16 20:12:04,414] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1843 bytes result sent to driver
[2021-05-16 20:12:04,415] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (764b1db4ecfd, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,416] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 180 ms on 764b1db4ecfd (executor driver) (79/186)
21/05/16 23:12:04 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-16 20:12:04,438] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1843 bytes result sent to driver
[2021-05-16 20:12:04,456] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (764b1db4ecfd, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,477] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 188 ms on 764b1db4ecfd (executor driver) (80/186)
[2021-05-16 20:12:04,478] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-16 20:12:04,479] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1843 bytes result sent to driver
[2021-05-16 20:12:04,479] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (764b1db4ecfd, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,480] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-16 20:12:04,480] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 186 ms on 764b1db4ecfd (executor driver) (81/186)
[2021-05-16 20:12:04,555] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
[2021-05-16 20:12:04,556] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (764b1db4ecfd, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,557] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 176 ms on 764b1db4ecfd (executor driver) (82/186)
[2021-05-16 20:12:04,558] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-16 20:12:04,603] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1843 bytes result sent to driver
[2021-05-16 20:12:04,608] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (764b1db4ecfd, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,610] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 194 ms on 764b1db4ecfd (executor driver) (83/186)
[2021-05-16 20:12:04,611] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-16 20:12:04,623] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1843 bytes result sent to driver
[2021-05-16 20:12:04,624] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (764b1db4ecfd, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,626] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 184 ms on 764b1db4ecfd (executor driver) (84/186)
21/05/16 23:12:04 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
[2021-05-16 20:12:04,648] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1886 bytes result sent to driver
[2021-05-16 20:12:04,649] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (764b1db4ecfd, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,650] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 181 ms on 764b1db4ecfd (executor driver) (85/186)
21/05/16 23:12:04 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-16 20:12:04,755] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1886 bytes result sent to driver
[2021-05-16 20:12:04,758] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (764b1db4ecfd, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:04 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-16 20:12:04,759] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 204 ms on 764b1db4ecfd (executor driver) (86/186)
[2021-05-16 20:12:04,799] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1886 bytes result sent to driver
[2021-05-16 20:12:04,801] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (764b1db4ecfd, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,802] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 195 ms on 764b1db4ecfd (executor driver) (87/186)
[2021-05-16 20:12:04,803] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-16 20:12:04,810] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1886 bytes result sent to driver
[2021-05-16 20:12:04,811] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (764b1db4ecfd, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,812] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
21/05/16 23:12:04 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 191 ms on 764b1db4ecfd (executor driver) (88/186)
[2021-05-16 20:12:04,824] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1843 bytes result sent to driver
[2021-05-16 20:12:04,826] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (764b1db4ecfd, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,827] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 178 ms on 764b1db4ecfd (executor driver) (89/186)
[2021-05-16 20:12:04,828] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-16 20:12:04,930] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1843 bytes result sent to driver
[2021-05-16 20:12:04,931] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (764b1db4ecfd, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,932] {docker.py:276} INFO - 21/05/16 23:12:04 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 176 ms on 764b1db4ecfd (executor driver) (90/186)
[2021-05-16 20:12:04,932] {docker.py:276} INFO - 21/05/16 23:12:04 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-16 20:12:04,977] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1843 bytes result sent to driver
[2021-05-16 20:12:04,979] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (764b1db4ecfd, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,980] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 180 ms on 764b1db4ecfd (executor driver) (91/186)
[2021-05-16 20:12:04,980] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-16 20:12:04,990] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1843 bytes result sent to driver
[2021-05-16 20:12:04,990] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (764b1db4ecfd, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:04,991] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
21/05/16 23:12:05 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 181 ms on 764b1db4ecfd (executor driver) (92/186)
[2021-05-16 20:12:05,002] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1843 bytes result sent to driver
[2021-05-16 20:12:05,003] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (764b1db4ecfd, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,004] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 179 ms on 764b1db4ecfd (executor driver) (93/186)
[2021-05-16 20:12:05,004] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-16 20:12:05,106] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1843 bytes result sent to driver
[2021-05-16 20:12:05,109] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (764b1db4ecfd, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,110] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
21/05/16 23:12:05 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 179 ms on 764b1db4ecfd (executor driver) (94/186)
[2021-05-16 20:12:05,149] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1843 bytes result sent to driver
[2021-05-16 20:12:05,151] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (764b1db4ecfd, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,152] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 174 ms on 764b1db4ecfd (executor driver) (95/186)
[2021-05-16 20:12:05,152] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-16 20:12:05,166] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1843 bytes result sent to driver
[2021-05-16 20:12:05,167] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (764b1db4ecfd, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,168] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
21/05/16 23:12:05 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 178 ms on 764b1db4ecfd (executor driver) (96/186)
[2021-05-16 20:12:05,175] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1843 bytes result sent to driver
[2021-05-16 20:12:05,176] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (764b1db4ecfd, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,177] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-16 20:12:05,178] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 174 ms on 764b1db4ecfd (executor driver) (97/186)
[2021-05-16 20:12:05,283] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1843 bytes result sent to driver
[2021-05-16 20:12:05,285] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (764b1db4ecfd, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,285] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 177 ms on 764b1db4ecfd (executor driver) (98/186)
[2021-05-16 20:12:05,286] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-16 20:12:05,324] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1843 bytes result sent to driver
[2021-05-16 20:12:05,325] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (764b1db4ecfd, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,326] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 175 ms on 764b1db4ecfd (executor driver) (99/186)
21/05/16 23:12:05 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-16 20:12:05,348] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1843 bytes result sent to driver
[2021-05-16 20:12:05,350] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (764b1db4ecfd, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,351] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 184 ms on 764b1db4ecfd (executor driver) (100/186)
[2021-05-16 20:12:05,352] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1843 bytes result sent to driver
[2021-05-16 20:12:05,353] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
[2021-05-16 20:12:05,354] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (764b1db4ecfd, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,355] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 179 ms on 764b1db4ecfd (executor driver) (101/186)
[2021-05-16 20:12:05,356] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-16 20:12:05,459] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1886 bytes result sent to driver
[2021-05-16 20:12:05,461] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (764b1db4ecfd, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,462] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-16 20:12:05,463] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 177 ms on 764b1db4ecfd (executor driver) (102/186)
[2021-05-16 20:12:05,498] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1886 bytes result sent to driver
[2021-05-16 20:12:05,500] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (764b1db4ecfd, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,500] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 176 ms on 764b1db4ecfd (executor driver) (103/186)
[2021-05-16 20:12:05,501] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-16 20:12:05,535] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1886 bytes result sent to driver
[2021-05-16 20:12:05,537] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (764b1db4ecfd, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,538] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 188 ms on 764b1db4ecfd (executor driver) (104/186)
[2021-05-16 20:12:05,539] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-16 20:12:05,543] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1886 bytes result sent to driver
[2021-05-16 20:12:05,544] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (764b1db4ecfd, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,545] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 191 ms on 764b1db4ecfd (executor driver) (105/186)
[2021-05-16 20:12:05,545] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-16 20:12:05,633] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1843 bytes result sent to driver
[2021-05-16 20:12:05,634] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (764b1db4ecfd, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,635] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 176 ms on 764b1db4ecfd (executor driver) (106/186)
[2021-05-16 20:12:05,635] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-16 20:12:05,670] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1843 bytes result sent to driver
[2021-05-16 20:12:05,671] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (764b1db4ecfd, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,671] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 172 ms on 764b1db4ecfd (executor driver) (107/186)
[2021-05-16 20:12:05,672] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-16 20:12:05,723] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1843 bytes result sent to driver
[2021-05-16 20:12:05,725] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1843 bytes result sent to driver
[2021-05-16 20:12:05,726] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (764b1db4ecfd, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,727] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 191 ms on 764b1db4ecfd (executor driver) (108/186)
[2021-05-16 20:12:05,729] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-16 20:12:05,730] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (764b1db4ecfd, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,731] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 188 ms on 764b1db4ecfd (executor driver) (109/186)
[2021-05-16 20:12:05,732] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-16 20:12:05,814] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1843 bytes result sent to driver
[2021-05-16 20:12:05,816] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (764b1db4ecfd, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,817] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
21/05/16 23:12:05 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 183 ms on 764b1db4ecfd (executor driver) (110/186)
[2021-05-16 20:12:05,844] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1843 bytes result sent to driver
[2021-05-16 20:12:05,846] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (764b1db4ecfd, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,847] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
21/05/16 23:12:05 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 176 ms on 764b1db4ecfd (executor driver) (111/186)
[2021-05-16 20:12:05,906] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1843 bytes result sent to driver
[2021-05-16 20:12:05,908] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (764b1db4ecfd, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,910] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 181 ms on 764b1db4ecfd (executor driver) (112/186)
[2021-05-16 20:12:05,910] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-16 20:12:05,912] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1843 bytes result sent to driver
[2021-05-16 20:12:05,914] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (764b1db4ecfd, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,915] {docker.py:276} INFO - 21/05/16 23:12:05 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 191 ms on 764b1db4ecfd (executor driver) (113/186)
[2021-05-16 20:12:05,916] {docker.py:276} INFO - 21/05/16 23:12:05 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-16 20:12:05,993] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1843 bytes result sent to driver
[2021-05-16 20:12:05,994] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (764b1db4ecfd, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:05,995] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 179 ms on 764b1db4ecfd (executor driver) (114/186)
[2021-05-16 20:12:06,031] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1843 bytes result sent to driver
[2021-05-16 20:12:06,041] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (764b1db4ecfd, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,042] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 189 ms on 764b1db4ecfd (executor driver) (115/186)
[2021-05-16 20:12:06,042] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-16 20:12:06,088] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1843 bytes result sent to driver
[2021-05-16 20:12:06,090] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (764b1db4ecfd, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,091] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-16 20:12:06,092] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 184 ms on 764b1db4ecfd (executor driver) (116/186)
[2021-05-16 20:12:06,148] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1843 bytes result sent to driver
[2021-05-16 20:12:06,150] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (764b1db4ecfd, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,151] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 237 ms on 764b1db4ecfd (executor driver) (117/186)
[2021-05-16 20:12:06,168] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1886 bytes result sent to driver
[2021-05-16 20:12:06,169] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (764b1db4ecfd, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,170] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 177 ms on 764b1db4ecfd (executor driver) (118/186)
[2021-05-16 20:12:06,217] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1886 bytes result sent to driver
[2021-05-16 20:12:06,219] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (764b1db4ecfd, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,221] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 189 ms on 764b1db4ecfd (executor driver) (119/186)
21/05/16 23:12:06 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
[2021-05-16 20:12:06,274] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1886 bytes result sent to driver
[2021-05-16 20:12:06,276] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (764b1db4ecfd, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,277] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-16 20:12:06,278] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 188 ms on 764b1db4ecfd (executor driver) (120/186)
[2021-05-16 20:12:06,332] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1886 bytes result sent to driver
[2021-05-16 20:12:06,335] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (764b1db4ecfd, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,336] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-16 20:12:06,336] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 150 ms on 764b1db4ecfd (executor driver) (121/186)
[2021-05-16 20:12:06,368] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1843 bytes result sent to driver
[2021-05-16 20:12:06,370] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (764b1db4ecfd, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,371] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 165 ms on 764b1db4ecfd (executor driver) (122/186)
[2021-05-16 20:12:06,401] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1843 bytes result sent to driver
[2021-05-16 20:12:06,402] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (764b1db4ecfd, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,403] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 148 ms on 764b1db4ecfd (executor driver) (123/186)
[2021-05-16 20:12:06,459] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1843 bytes result sent to driver
[2021-05-16 20:12:06,462] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (764b1db4ecfd, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,463] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 152 ms on 764b1db4ecfd (executor driver) (124/186)
[2021-05-16 20:12:06,465] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-16 20:12:06,510] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1843 bytes result sent to driver
[2021-05-16 20:12:06,512] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (764b1db4ecfd, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,513] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-16 20:12:06,514] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 144 ms on 764b1db4ecfd (executor driver) (125/186)
[2021-05-16 20:12:06,548] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1843 bytes result sent to driver
[2021-05-16 20:12:06,550] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (764b1db4ecfd, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,551] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 182 ms on 764b1db4ecfd (executor driver) (126/186)
[2021-05-16 20:12:06,552] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-16 20:12:06,586] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1843 bytes result sent to driver
[2021-05-16 20:12:06,587] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (764b1db4ecfd, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,590] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 187 ms on 764b1db4ecfd (executor driver) (127/186)
21/05/16 23:12:06 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-16 20:12:06,656] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1843 bytes result sent to driver
[2021-05-16 20:12:06,658] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (764b1db4ecfd, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,659] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 198 ms on 764b1db4ecfd (executor driver) (128/186)
[2021-05-16 20:12:06,692] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1843 bytes result sent to driver
[2021-05-16 20:12:06,694] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (764b1db4ecfd, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,696] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 184 ms on 764b1db4ecfd (executor driver) (129/186)
21/05/16 23:12:06 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-16 20:12:06,732] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1843 bytes result sent to driver
[2021-05-16 20:12:06,734] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (764b1db4ecfd, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,736] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-16 20:12:06,737] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 187 ms on 764b1db4ecfd (executor driver) (130/186)
[2021-05-16 20:12:06,766] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1843 bytes result sent to driver
[2021-05-16 20:12:06,768] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (764b1db4ecfd, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,769] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
21/05/16 23:12:06 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 183 ms on 764b1db4ecfd (executor driver) (131/186)
[2021-05-16 20:12:06,839] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1843 bytes result sent to driver
[2021-05-16 20:12:06,842] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (764b1db4ecfd, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,843] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 185 ms on 764b1db4ecfd (executor driver) (132/186)
[2021-05-16 20:12:06,843] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-16 20:12:06,912] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1843 bytes result sent to driver
[2021-05-16 20:12:06,913] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1843 bytes result sent to driver
[2021-05-16 20:12:06,914] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (764b1db4ecfd, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,915] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 181 ms on 764b1db4ecfd (executor driver) (133/186)
[2021-05-16 20:12:06,916] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-16 20:12:06,917] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (764b1db4ecfd, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,918] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-16 20:12:06,919] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 225 ms on 764b1db4ecfd (executor driver) (134/186)
[2021-05-16 20:12:06,955] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1843 bytes result sent to driver
[2021-05-16 20:12:06,957] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (764b1db4ecfd, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:06,958] {docker.py:276} INFO - 21/05/16 23:12:06 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-16 20:12:06,959] {docker.py:276} INFO - 21/05/16 23:12:06 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 193 ms on 764b1db4ecfd (executor driver) (135/186)
[2021-05-16 20:12:07,023] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1886 bytes result sent to driver
[2021-05-16 20:12:07,024] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (764b1db4ecfd, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,025] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 185 ms on 764b1db4ecfd (executor driver) (136/186)
[2021-05-16 20:12:07,027] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-16 20:12:07,094] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1886 bytes result sent to driver
[2021-05-16 20:12:07,095] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (764b1db4ecfd, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,096] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
[2021-05-16 20:12:07,097] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 184 ms on 764b1db4ecfd (executor driver) (137/186)
[2021-05-16 20:12:07,102] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1886 bytes result sent to driver
[2021-05-16 20:12:07,102] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 141.0 in stage 0.0 (TID 141) (764b1db4ecfd, executor driver, partition 141, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,103] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 187 ms on 764b1db4ecfd (executor driver) (138/186)
[2021-05-16 20:12:07,104] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 141.0 in stage 0.0 (TID 141)
[2021-05-16 20:12:07,153] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1886 bytes result sent to driver
[2021-05-16 20:12:07,155] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 142.0 in stage 0.0 (TID 142) (764b1db4ecfd, executor driver, partition 142, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,156] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 142.0 in stage 0.0 (TID 142)
21/05/16 23:12:07 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 200 ms on 764b1db4ecfd (executor driver) (139/186)
[2021-05-16 20:12:07,208] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1843 bytes result sent to driver
[2021-05-16 20:12:07,209] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 143.0 in stage 0.0 (TID 143) (764b1db4ecfd, executor driver, partition 143, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,211] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 143.0 in stage 0.0 (TID 143)
21/05/16 23:12:07 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 188 ms on 764b1db4ecfd (executor driver) (140/186)
[2021-05-16 20:12:07,279] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1843 bytes result sent to driver
[2021-05-16 20:12:07,280] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 144.0 in stage 0.0 (TID 144) (764b1db4ecfd, executor driver, partition 144, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,281] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 187 ms on 764b1db4ecfd (executor driver) (141/186)
[2021-05-16 20:12:07,282] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 144.0 in stage 0.0 (TID 144)
[2021-05-16 20:12:07,284] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 141.0 in stage 0.0 (TID 141). 1843 bytes result sent to driver
[2021-05-16 20:12:07,286] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 141.0 in stage 0.0 (TID 141) in 184 ms on 764b1db4ecfd (executor driver) (142/186)
[2021-05-16 20:12:07,287] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 145.0 in stage 0.0 (TID 145) (764b1db4ecfd, executor driver, partition 145, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,289] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 145.0 in stage 0.0 (TID 145)
[2021-05-16 20:12:07,343] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 142.0 in stage 0.0 (TID 142). 1843 bytes result sent to driver
[2021-05-16 20:12:07,344] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 146.0 in stage 0.0 (TID 146) (764b1db4ecfd, executor driver, partition 146, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,346] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 146.0 in stage 0.0 (TID 146)
[2021-05-16 20:12:07,347] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 142.0 in stage 0.0 (TID 142) in 192 ms on 764b1db4ecfd (executor driver) (143/186)
[2021-05-16 20:12:07,389] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 143.0 in stage 0.0 (TID 143). 1843 bytes result sent to driver
[2021-05-16 20:12:07,391] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 147.0 in stage 0.0 (TID 147) (764b1db4ecfd, executor driver, partition 147, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,392] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 147.0 in stage 0.0 (TID 147)
21/05/16 23:12:07 INFO TaskSetManager: Finished task 143.0 in stage 0.0 (TID 143) in 184 ms on 764b1db4ecfd (executor driver) (144/186)
[2021-05-16 20:12:07,461] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 144.0 in stage 0.0 (TID 144). 1843 bytes result sent to driver
[2021-05-16 20:12:07,462] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 148.0 in stage 0.0 (TID 148) (764b1db4ecfd, executor driver, partition 148, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,464] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 144.0 in stage 0.0 (TID 144) in 184 ms on 764b1db4ecfd (executor driver) (145/186)
[2021-05-16 20:12:07,464] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 148.0 in stage 0.0 (TID 148)
[2021-05-16 20:12:07,479] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 145.0 in stage 0.0 (TID 145). 1843 bytes result sent to driver
[2021-05-16 20:12:07,480] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 145.0 in stage 0.0 (TID 145) in 194 ms on 764b1db4ecfd (executor driver) (146/186)
[2021-05-16 20:12:07,481] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 149.0 in stage 0.0 (TID 149) (764b1db4ecfd, executor driver, partition 149, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,482] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 149.0 in stage 0.0 (TID 149)
[2021-05-16 20:12:07,521] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 146.0 in stage 0.0 (TID 146). 1843 bytes result sent to driver
[2021-05-16 20:12:07,522] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 150.0 in stage 0.0 (TID 150) (764b1db4ecfd, executor driver, partition 150, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,523] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 146.0 in stage 0.0 (TID 146) in 179 ms on 764b1db4ecfd (executor driver) (147/186)
[2021-05-16 20:12:07,524] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 150.0 in stage 0.0 (TID 150)
[2021-05-16 20:12:07,570] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 147.0 in stage 0.0 (TID 147). 1843 bytes result sent to driver
[2021-05-16 20:12:07,572] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 151.0 in stage 0.0 (TID 151) (764b1db4ecfd, executor driver, partition 151, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,573] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 147.0 in stage 0.0 (TID 147) in 182 ms on 764b1db4ecfd (executor driver) (148/186)
[2021-05-16 20:12:07,574] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 151.0 in stage 0.0 (TID 151)
[2021-05-16 20:12:07,658] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 148.0 in stage 0.0 (TID 148). 1843 bytes result sent to driver
[2021-05-16 20:12:07,661] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 152.0 in stage 0.0 (TID 152) (764b1db4ecfd, executor driver, partition 152, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,661] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 152.0 in stage 0.0 (TID 152)
[2021-05-16 20:12:07,662] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 148.0 in stage 0.0 (TID 148) in 201 ms on 764b1db4ecfd (executor driver) (149/186)
[2021-05-16 20:12:07,664] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 149.0 in stage 0.0 (TID 149). 1843 bytes result sent to driver
[2021-05-16 20:12:07,666] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 153.0 in stage 0.0 (TID 153) (764b1db4ecfd, executor driver, partition 153, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,666] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 149.0 in stage 0.0 (TID 149) in 185 ms on 764b1db4ecfd (executor driver) (150/186)
[2021-05-16 20:12:07,667] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 153.0 in stage 0.0 (TID 153)
[2021-05-16 20:12:07,697] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 150.0 in stage 0.0 (TID 150). 1843 bytes result sent to driver
[2021-05-16 20:12:07,698] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 154.0 in stage 0.0 (TID 154) (764b1db4ecfd, executor driver, partition 154, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,700] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 154.0 in stage 0.0 (TID 154)
[2021-05-16 20:12:07,700] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 150.0 in stage 0.0 (TID 150) in 179 ms on 764b1db4ecfd (executor driver) (151/186)
[2021-05-16 20:12:07,755] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 151.0 in stage 0.0 (TID 151). 1843 bytes result sent to driver
[2021-05-16 20:12:07,756] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 155.0 in stage 0.0 (TID 155) (764b1db4ecfd, executor driver, partition 155, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,758] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 151.0 in stage 0.0 (TID 151) in 187 ms on 764b1db4ecfd (executor driver) (152/186)
[2021-05-16 20:12:07,758] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 155.0 in stage 0.0 (TID 155)
[2021-05-16 20:12:07,839] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 152.0 in stage 0.0 (TID 152). 1886 bytes result sent to driver
[2021-05-16 20:12:07,840] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 153.0 in stage 0.0 (TID 153). 1886 bytes result sent to driver
[2021-05-16 20:12:07,840] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 156.0 in stage 0.0 (TID 156) (764b1db4ecfd, executor driver, partition 156, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,841] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 156.0 in stage 0.0 (TID 156)
[2021-05-16 20:12:07,842] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 157.0 in stage 0.0 (TID 157) (764b1db4ecfd, executor driver, partition 157, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,843] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 157.0 in stage 0.0 (TID 157)
[2021-05-16 20:12:07,844] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 153.0 in stage 0.0 (TID 153) in 179 ms on 764b1db4ecfd (executor driver) (153/186)
[2021-05-16 20:12:07,844] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 152.0 in stage 0.0 (TID 152) in 185 ms on 764b1db4ecfd (executor driver) (154/186)
[2021-05-16 20:12:07,919] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 154.0 in stage 0.0 (TID 154). 1886 bytes result sent to driver
[2021-05-16 20:12:07,921] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 158.0 in stage 0.0 (TID 158) (764b1db4ecfd, executor driver, partition 158, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,922] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 158.0 in stage 0.0 (TID 158)
[2021-05-16 20:12:07,923] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 154.0 in stage 0.0 (TID 154) in 225 ms on 764b1db4ecfd (executor driver) (155/186)
[2021-05-16 20:12:07,954] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Finished task 155.0 in stage 0.0 (TID 155). 1886 bytes result sent to driver
[2021-05-16 20:12:07,956] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Starting task 159.0 in stage 0.0 (TID 159) (764b1db4ecfd, executor driver, partition 159, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:07,957] {docker.py:276} INFO - 21/05/16 23:12:07 INFO TaskSetManager: Finished task 155.0 in stage 0.0 (TID 155) in 202 ms on 764b1db4ecfd (executor driver) (156/186)
[2021-05-16 20:12:07,959] {docker.py:276} INFO - 21/05/16 23:12:07 INFO Executor: Running task 159.0 in stage 0.0 (TID 159)
[2021-05-16 20:12:08,017] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 157.0 in stage 0.0 (TID 157). 1843 bytes result sent to driver
[2021-05-16 20:12:08,018] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 156.0 in stage 0.0 (TID 156). 1843 bytes result sent to driver
[2021-05-16 20:12:08,019] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 160.0 in stage 0.0 (TID 160) (764b1db4ecfd, executor driver, partition 160, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,021] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 160.0 in stage 0.0 (TID 160)
[2021-05-16 20:12:08,022] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 161.0 in stage 0.0 (TID 161) (764b1db4ecfd, executor driver, partition 161, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,023] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 157.0 in stage 0.0 (TID 157) in 182 ms on 764b1db4ecfd (executor driver) (157/186)
[2021-05-16 20:12:08,024] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 156.0 in stage 0.0 (TID 156) in 183 ms on 764b1db4ecfd (executor driver) (158/186)
[2021-05-16 20:12:08,027] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 161.0 in stage 0.0 (TID 161)
[2021-05-16 20:12:08,102] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 158.0 in stage 0.0 (TID 158). 1843 bytes result sent to driver
[2021-05-16 20:12:08,104] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 162.0 in stage 0.0 (TID 162) (764b1db4ecfd, executor driver, partition 162, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:08 INFO Executor: Running task 162.0 in stage 0.0 (TID 162)
[2021-05-16 20:12:08,105] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 158.0 in stage 0.0 (TID 158) in 184 ms on 764b1db4ecfd (executor driver) (159/186)
[2021-05-16 20:12:08,137] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 159.0 in stage 0.0 (TID 159). 1843 bytes result sent to driver
[2021-05-16 20:12:08,138] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 163.0 in stage 0.0 (TID 163) (764b1db4ecfd, executor driver, partition 163, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,139] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 159.0 in stage 0.0 (TID 159) in 184 ms on 764b1db4ecfd (executor driver) (160/186)
[2021-05-16 20:12:08,140] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 163.0 in stage 0.0 (TID 163)
[2021-05-16 20:12:08,199] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 161.0 in stage 0.0 (TID 161). 1843 bytes result sent to driver
[2021-05-16 20:12:08,201] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 164.0 in stage 0.0 (TID 164) (764b1db4ecfd, executor driver, partition 164, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,202] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 161.0 in stage 0.0 (TID 161) in 181 ms on 764b1db4ecfd (executor driver) (161/186)
[2021-05-16 20:12:08,203] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 164.0 in stage 0.0 (TID 164)
[2021-05-16 20:12:08,204] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 160.0 in stage 0.0 (TID 160). 1843 bytes result sent to driver
[2021-05-16 20:12:08,205] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 165.0 in stage 0.0 (TID 165) (764b1db4ecfd, executor driver, partition 165, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,207] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 160.0 in stage 0.0 (TID 160) in 189 ms on 764b1db4ecfd (executor driver) (162/186)
21/05/16 23:12:08 INFO Executor: Running task 165.0 in stage 0.0 (TID 165)
[2021-05-16 20:12:08,281] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 162.0 in stage 0.0 (TID 162). 1843 bytes result sent to driver
[2021-05-16 20:12:08,283] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 166.0 in stage 0.0 (TID 166) (764b1db4ecfd, executor driver, partition 166, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,284] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 162.0 in stage 0.0 (TID 162) in 182 ms on 764b1db4ecfd (executor driver) (163/186)
[2021-05-16 20:12:08,285] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 166.0 in stage 0.0 (TID 166)
[2021-05-16 20:12:08,320] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 163.0 in stage 0.0 (TID 163). 1843 bytes result sent to driver
[2021-05-16 20:12:08,321] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 167.0 in stage 0.0 (TID 167) (764b1db4ecfd, executor driver, partition 167, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,322] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 163.0 in stage 0.0 (TID 163) in 184 ms on 764b1db4ecfd (executor driver) (164/186)
21/05/16 23:12:08 INFO Executor: Running task 167.0 in stage 0.0 (TID 167)
[2021-05-16 20:12:08,379] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 165.0 in stage 0.0 (TID 165). 1843 bytes result sent to driver
[2021-05-16 20:12:08,381] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 164.0 in stage 0.0 (TID 164). 1843 bytes result sent to driver
[2021-05-16 20:12:08,382] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 168.0 in stage 0.0 (TID 168) (764b1db4ecfd, executor driver, partition 168, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,383] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 168.0 in stage 0.0 (TID 168)
[2021-05-16 20:12:08,384] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 169.0 in stage 0.0 (TID 169) (764b1db4ecfd, executor driver, partition 169, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,385] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 165.0 in stage 0.0 (TID 165) in 180 ms on 764b1db4ecfd (executor driver) (165/186)
[2021-05-16 20:12:08,387] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 164.0 in stage 0.0 (TID 164) in 186 ms on 764b1db4ecfd (executor driver) (166/186)
21/05/16 23:12:08 INFO Executor: Running task 169.0 in stage 0.0 (TID 169)
[2021-05-16 20:12:08,464] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 166.0 in stage 0.0 (TID 166). 1843 bytes result sent to driver
[2021-05-16 20:12:08,466] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 170.0 in stage 0.0 (TID 170) (764b1db4ecfd, executor driver, partition 170, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,468] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 166.0 in stage 0.0 (TID 166) in 185 ms on 764b1db4ecfd (executor driver) (167/186)
[2021-05-16 20:12:08,469] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 170.0 in stage 0.0 (TID 170)
[2021-05-16 20:12:08,502] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 167.0 in stage 0.0 (TID 167). 1843 bytes result sent to driver
[2021-05-16 20:12:08,504] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 171.0 in stage 0.0 (TID 171) (764b1db4ecfd, executor driver, partition 171, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,505] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 171.0 in stage 0.0 (TID 171)
21/05/16 23:12:08 INFO TaskSetManager: Finished task 167.0 in stage 0.0 (TID 167) in 184 ms on 764b1db4ecfd (executor driver) (168/186)
[2021-05-16 20:12:08,564] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 169.0 in stage 0.0 (TID 169). 1843 bytes result sent to driver
[2021-05-16 20:12:08,564] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 168.0 in stage 0.0 (TID 168). 1843 bytes result sent to driver
[2021-05-16 20:12:08,565] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 172.0 in stage 0.0 (TID 172) (764b1db4ecfd, executor driver, partition 172, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,567] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 172.0 in stage 0.0 (TID 172)
[2021-05-16 20:12:08,568] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 173.0 in stage 0.0 (TID 173) (764b1db4ecfd, executor driver, partition 173, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,568] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 169.0 in stage 0.0 (TID 169) in 186 ms on 764b1db4ecfd (executor driver) (169/186)
[2021-05-16 20:12:08,569] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 173.0 in stage 0.0 (TID 173)
[2021-05-16 20:12:08,570] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 168.0 in stage 0.0 (TID 168) in 190 ms on 764b1db4ecfd (executor driver) (170/186)
[2021-05-16 20:12:08,642] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 170.0 in stage 0.0 (TID 170). 1886 bytes result sent to driver
[2021-05-16 20:12:08,643] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 174.0 in stage 0.0 (TID 174) (764b1db4ecfd, executor driver, partition 174, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,644] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 170.0 in stage 0.0 (TID 170) in 179 ms on 764b1db4ecfd (executor driver) (171/186)
[2021-05-16 20:12:08,645] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 174.0 in stage 0.0 (TID 174)
[2021-05-16 20:12:08,681] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 171.0 in stage 0.0 (TID 171). 1886 bytes result sent to driver
[2021-05-16 20:12:08,683] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 175.0 in stage 0.0 (TID 175) (764b1db4ecfd, executor driver, partition 175, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,684] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 175.0 in stage 0.0 (TID 175)
[2021-05-16 20:12:08,685] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 171.0 in stage 0.0 (TID 171) in 182 ms on 764b1db4ecfd (executor driver) (172/186)
[2021-05-16 20:12:08,756] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 172.0 in stage 0.0 (TID 172). 1886 bytes result sent to driver
[2021-05-16 20:12:08,757] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 173.0 in stage 0.0 (TID 173). 1886 bytes result sent to driver
[2021-05-16 20:12:08,758] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 176.0 in stage 0.0 (TID 176) (764b1db4ecfd, executor driver, partition 176, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,760] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 176.0 in stage 0.0 (TID 176)
[2021-05-16 20:12:08,760] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 177.0 in stage 0.0 (TID 177) (764b1db4ecfd, executor driver, partition 177, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,762] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 172.0 in stage 0.0 (TID 172) in 196 ms on 764b1db4ecfd (executor driver) (173/186)
21/05/16 23:12:08 INFO TaskSetManager: Finished task 173.0 in stage 0.0 (TID 173) in 195 ms on 764b1db4ecfd (executor driver) (174/186)
[2021-05-16 20:12:08,762] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 177.0 in stage 0.0 (TID 177)
[2021-05-16 20:12:08,818] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 174.0 in stage 0.0 (TID 174). 1843 bytes result sent to driver
[2021-05-16 20:12:08,820] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 178.0 in stage 0.0 (TID 178) (764b1db4ecfd, executor driver, partition 178, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,821] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 174.0 in stage 0.0 (TID 174) in 177 ms on 764b1db4ecfd (executor driver) (175/186)
[2021-05-16 20:12:08,822] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 178.0 in stage 0.0 (TID 178)
[2021-05-16 20:12:08,866] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 175.0 in stage 0.0 (TID 175). 1843 bytes result sent to driver
[2021-05-16 20:12:08,868] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 179.0 in stage 0.0 (TID 179) (764b1db4ecfd, executor driver, partition 179, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,869] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 175.0 in stage 0.0 (TID 175) in 187 ms on 764b1db4ecfd (executor driver) (176/186)
[2021-05-16 20:12:08,870] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 179.0 in stage 0.0 (TID 179)
[2021-05-16 20:12:08,939] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 176.0 in stage 0.0 (TID 176). 1843 bytes result sent to driver
[2021-05-16 20:12:08,941] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 177.0 in stage 0.0 (TID 177). 1843 bytes result sent to driver
21/05/16 23:12:08 INFO TaskSetManager: Starting task 180.0 in stage 0.0 (TID 180) (764b1db4ecfd, executor driver, partition 180, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,942] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 180.0 in stage 0.0 (TID 180)
[2021-05-16 20:12:08,944] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 181.0 in stage 0.0 (TID 181) (764b1db4ecfd, executor driver, partition 181, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,945] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 176.0 in stage 0.0 (TID 176) in 187 ms on 764b1db4ecfd (executor driver) (177/186)
[2021-05-16 20:12:08,947] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 177.0 in stage 0.0 (TID 177) in 187 ms on 764b1db4ecfd (executor driver) (178/186)
[2021-05-16 20:12:08,948] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 181.0 in stage 0.0 (TID 181)
[2021-05-16 20:12:08,993] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Finished task 178.0 in stage 0.0 (TID 178). 1843 bytes result sent to driver
[2021-05-16 20:12:08,994] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Starting task 182.0 in stage 0.0 (TID 182) (764b1db4ecfd, executor driver, partition 182, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:08,994] {docker.py:276} INFO - 21/05/16 23:12:08 INFO TaskSetManager: Finished task 178.0 in stage 0.0 (TID 178) in 176 ms on 764b1db4ecfd (executor driver) (179/186)
[2021-05-16 20:12:08,995] {docker.py:276} INFO - 21/05/16 23:12:08 INFO Executor: Running task 182.0 in stage 0.0 (TID 182)
[2021-05-16 20:12:09,044] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 179.0 in stage 0.0 (TID 179). 1843 bytes result sent to driver
[2021-05-16 20:12:09,045] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 183.0 in stage 0.0 (TID 183) (764b1db4ecfd, executor driver, partition 183, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,047] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 183.0 in stage 0.0 (TID 183)
[2021-05-16 20:12:09,047] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 179.0 in stage 0.0 (TID 179) in 180 ms on 764b1db4ecfd (executor driver) (180/186)
[2021-05-16 20:12:09,119] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 181.0 in stage 0.0 (TID 181). 1843 bytes result sent to driver
21/05/16 23:12:09 INFO Executor: Finished task 180.0 in stage 0.0 (TID 180). 1843 bytes result sent to driver
[2021-05-16 20:12:09,120] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 184.0 in stage 0.0 (TID 184) (764b1db4ecfd, executor driver, partition 184, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,121] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 184.0 in stage 0.0 (TID 184)
21/05/16 23:12:09 INFO TaskSetManager: Starting task 185.0 in stage 0.0 (TID 185) (764b1db4ecfd, executor driver, partition 185, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,122] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 180.0 in stage 0.0 (TID 180) in 183 ms on 764b1db4ecfd (executor driver) (181/186)
21/05/16 23:12:09 INFO Executor: Running task 185.0 in stage 0.0 (TID 185)
[2021-05-16 20:12:09,123] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 181.0 in stage 0.0 (TID 181) in 179 ms on 764b1db4ecfd (executor driver) (182/186)
[2021-05-16 20:12:09,166] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 182.0 in stage 0.0 (TID 182). 1843 bytes result sent to driver
[2021-05-16 20:12:09,173] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 182.0 in stage 0.0 (TID 182) in 180 ms on 764b1db4ecfd (executor driver) (183/186)
[2021-05-16 20:12:09,227] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 183.0 in stage 0.0 (TID 183). 1843 bytes result sent to driver
[2021-05-16 20:12:09,229] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 183.0 in stage 0.0 (TID 183) in 183 ms on 764b1db4ecfd (executor driver) (184/186)
[2021-05-16 20:12:09,302] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 184.0 in stage 0.0 (TID 184). 1843 bytes result sent to driver
[2021-05-16 20:12:09,303] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 184.0 in stage 0.0 (TID 184) in 183 ms on 764b1db4ecfd (executor driver) (185/186)
[2021-05-16 20:12:09,310] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 185.0 in stage 0.0 (TID 185). 1843 bytes result sent to driver
[2021-05-16 20:12:09,311] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 185.0 in stage 0.0 (TID 185) in 190 ms on 764b1db4ecfd (executor driver) (186/186)
[2021-05-16 20:12:09,314] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-16 20:12:09,315] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 9.805 s
[2021-05-16 20:12:09,322] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 20:12:09,322] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-16 20:12:09,326] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 9.965563 s
[2021-05-16 20:12:09,367] {docker.py:276} INFO - 21/05/16 23:12:09 INFO InMemoryFileIndex: It took 10556 ms to list leaf files for 186 paths.
[2021-05-16 20:12:09,515] {docker.py:276} INFO - 21/05/16 23:12:09 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 186 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621095045_to_1621096845.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621096845_to_1621098645.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621098645_to_1621100445.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621100445_to_1621102245.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621102245_to_1621104045.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621104045_to_1621105845.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621105845_to_1621107645.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621107645_to_1621109445.csv, s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621109445_to_1621111245.csv.
[2021-05-16 20:12:09,569] {docker.py:276} INFO - 21/05/16 23:12:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:09,587] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 186 output partitions
21/05/16 23:12:09 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/16 23:12:09 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 20:12:09,612] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Missing parents: List()
[2021-05-16 20:12:09,613] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 20:12:09,613] {docker.py:276} INFO - 21/05/16 23:12:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 85.0 KiB, free 934.2 MiB)
[2021-05-16 20:12:09,614] {docker.py:276} INFO - 21/05/16 23:12:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-16 20:12:09,615] {docker.py:276} INFO - 21/05/16 23:12:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 764b1db4ecfd:38389 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-16 20:12:09,621] {docker.py:276} INFO - 21/05/16 23:12:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-16 20:12:09,627] {docker.py:276} INFO - 21/05/16 23:12:09 INFO DAGScheduler: Submitting 186 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-16 20:12:09,628] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 186 tasks resource profile 0
[2021-05-16 20:12:09,636] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 186) (764b1db4ecfd, executor driver, partition 0, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,637] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 187) (764b1db4ecfd, executor driver, partition 1, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,638] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 188) (764b1db4ecfd, executor driver, partition 2, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,639] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 189) (764b1db4ecfd, executor driver, partition 3, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,643] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 186)
[2021-05-16 20:12:09,643] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 2.0 in stage 1.0 (TID 188)
[2021-05-16 20:12:09,644] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 1.0 in stage 1.0 (TID 187)
[2021-05-16 20:12:09,644] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 3.0 in stage 1.0 (TID 189)
[2021-05-16 20:12:09,647] {docker.py:276} INFO - 21/05/16 23:12:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 764b1db4ecfd:38389 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 20:12:09,906] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 2.0 in stage 1.0 (TID 188). 1843 bytes result sent to driver
21/05/16 23:12:09 INFO Executor: Finished task 3.0 in stage 1.0 (TID 189). 1843 bytes result sent to driver
[2021-05-16 20:12:09,907] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 1.0 in stage 1.0 (TID 187). 1843 bytes result sent to driver
[2021-05-16 20:12:09,909] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 186). 1843 bytes result sent to driver
[2021-05-16 20:12:09,909] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 190) (764b1db4ecfd, executor driver, partition 4, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,912] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 188) in 274 ms on 764b1db4ecfd (executor driver) (1/186)
[2021-05-16 20:12:09,913] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 189) in 274 ms on 764b1db4ecfd (executor driver) (2/186)
[2021-05-16 20:12:09,914] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 4.0 in stage 1.0 (TID 190)
[2021-05-16 20:12:09,914] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 191) (764b1db4ecfd, executor driver, partition 5, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,920] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 5.0 in stage 1.0 (TID 191)
[2021-05-16 20:12:09,922] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 192) (764b1db4ecfd, executor driver, partition 6, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,923] {docker.py:276} INFO - 21/05/16 23:12:09 INFO Executor: Running task 6.0 in stage 1.0 (TID 192)
[2021-05-16 20:12:09,924] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 187) in 286 ms on 764b1db4ecfd (executor driver) (3/186)
[2021-05-16 20:12:09,924] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 193) (764b1db4ecfd, executor driver, partition 7, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:09,930] {docker.py:276} INFO - 21/05/16 23:12:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 186) in 295 ms on 764b1db4ecfd (executor driver) (4/186)
21/05/16 23:12:09 INFO Executor: Running task 7.0 in stage 1.0 (TID 193)
[2021-05-16 20:12:10,087] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 4.0 in stage 1.0 (TID 190). 1843 bytes result sent to driver
[2021-05-16 20:12:10,088] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 194) (764b1db4ecfd, executor driver, partition 8, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,089] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 8.0 in stage 1.0 (TID 194)
21/05/16 23:12:10 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 190) in 183 ms on 764b1db4ecfd (executor driver) (5/186)
[2021-05-16 20:12:10,091] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 5.0 in stage 1.0 (TID 191). 1843 bytes result sent to driver
[2021-05-16 20:12:10,092] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 195) (764b1db4ecfd, executor driver, partition 9, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,093] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 9.0 in stage 1.0 (TID 195)
21/05/16 23:12:10 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 191) in 181 ms on 764b1db4ecfd (executor driver) (6/186)
[2021-05-16 20:12:10,098] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 7.0 in stage 1.0 (TID 193). 1843 bytes result sent to driver
[2021-05-16 20:12:10,099] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 196) (764b1db4ecfd, executor driver, partition 10, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,100] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 193) in 177 ms on 764b1db4ecfd (executor driver) (7/186)
[2021-05-16 20:12:10,102] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 6.0 in stage 1.0 (TID 192). 1843 bytes result sent to driver
[2021-05-16 20:12:10,103] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 10.0 in stage 1.0 (TID 196)
[2021-05-16 20:12:10,104] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 197) (764b1db4ecfd, executor driver, partition 11, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,105] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 11.0 in stage 1.0 (TID 197)
[2021-05-16 20:12:10,106] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 192) in 185 ms on 764b1db4ecfd (executor driver) (8/186)
[2021-05-16 20:12:10,267] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 8.0 in stage 1.0 (TID 194). 1843 bytes result sent to driver
[2021-05-16 20:12:10,268] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 198) (764b1db4ecfd, executor driver, partition 12, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,269] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 12.0 in stage 1.0 (TID 198)
[2021-05-16 20:12:10,271] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 194) in 182 ms on 764b1db4ecfd (executor driver) (9/186)
[2021-05-16 20:12:10,272] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 9.0 in stage 1.0 (TID 195). 1843 bytes result sent to driver
[2021-05-16 20:12:10,274] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 199) (764b1db4ecfd, executor driver, partition 13, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,274] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 13.0 in stage 1.0 (TID 199)
[2021-05-16 20:12:10,284] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 195) in 192 ms on 764b1db4ecfd (executor driver) (10/186)
[2021-05-16 20:12:10,286] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 11.0 in stage 1.0 (TID 197). 1886 bytes result sent to driver
[2021-05-16 20:12:10,287] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 200) (764b1db4ecfd, executor driver, partition 14, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,288] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 197) in 185 ms on 764b1db4ecfd (executor driver) (11/186)
[2021-05-16 20:12:10,289] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 14.0 in stage 1.0 (TID 200)
[2021-05-16 20:12:10,309] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 10.0 in stage 1.0 (TID 196). 1886 bytes result sent to driver
[2021-05-16 20:12:10,310] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 201) (764b1db4ecfd, executor driver, partition 15, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,311] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 196) in 212 ms on 764b1db4ecfd (executor driver) (12/186)
[2021-05-16 20:12:10,311] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 15.0 in stage 1.0 (TID 201)
[2021-05-16 20:12:10,457] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 13.0 in stage 1.0 (TID 199). 1886 bytes result sent to driver
[2021-05-16 20:12:10,459] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 202) (764b1db4ecfd, executor driver, partition 16, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,461] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 199) in 187 ms on 764b1db4ecfd (executor driver) (13/186)
21/05/16 23:12:10 INFO Executor: Running task 16.0 in stage 1.0 (TID 202)
[2021-05-16 20:12:10,462] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 14.0 in stage 1.0 (TID 200). 1843 bytes result sent to driver
[2021-05-16 20:12:10,464] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 203) (764b1db4ecfd, executor driver, partition 17, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,465] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 17.0 in stage 1.0 (TID 203)
[2021-05-16 20:12:10,466] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 200) in 178 ms on 764b1db4ecfd (executor driver) (14/186)
[2021-05-16 20:12:10,482] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 15.0 in stage 1.0 (TID 201). 1843 bytes result sent to driver
[2021-05-16 20:12:10,483] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 204) (764b1db4ecfd, executor driver, partition 18, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,484] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 201) in 174 ms on 764b1db4ecfd (executor driver) (15/186)
[2021-05-16 20:12:10,484] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 18.0 in stage 1.0 (TID 204)
[2021-05-16 20:12:10,524] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 12.0 in stage 1.0 (TID 198). 1886 bytes result sent to driver
[2021-05-16 20:12:10,527] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 205) (764b1db4ecfd, executor driver, partition 19, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,528] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 198) in 260 ms on 764b1db4ecfd (executor driver) (16/186)
[2021-05-16 20:12:10,529] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 19.0 in stage 1.0 (TID 205)
[2021-05-16 20:12:10,638] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 16.0 in stage 1.0 (TID 202). 1843 bytes result sent to driver
[2021-05-16 20:12:10,639] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 206) (764b1db4ecfd, executor driver, partition 20, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,640] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 20.0 in stage 1.0 (TID 206)
[2021-05-16 20:12:10,641] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 202) in 182 ms on 764b1db4ecfd (executor driver) (17/186)
[2021-05-16 20:12:10,642] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 17.0 in stage 1.0 (TID 203). 1843 bytes result sent to driver
[2021-05-16 20:12:10,644] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 207) (764b1db4ecfd, executor driver, partition 21, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,644] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 203) in 181 ms on 764b1db4ecfd (executor driver) (18/186)
[2021-05-16 20:12:10,645] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 21.0 in stage 1.0 (TID 207)
[2021-05-16 20:12:10,650] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 18.0 in stage 1.0 (TID 204). 1843 bytes result sent to driver
[2021-05-16 20:12:10,651] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 208) (764b1db4ecfd, executor driver, partition 22, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,652] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 22.0 in stage 1.0 (TID 208)
[2021-05-16 20:12:10,653] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 204) in 170 ms on 764b1db4ecfd (executor driver) (19/186)
[2021-05-16 20:12:10,706] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 19.0 in stage 1.0 (TID 205). 1843 bytes result sent to driver
[2021-05-16 20:12:10,708] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 209) (764b1db4ecfd, executor driver, partition 23, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,709] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 23.0 in stage 1.0 (TID 209)
21/05/16 23:12:10 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 205) in 184 ms on 764b1db4ecfd (executor driver) (20/186)
[2021-05-16 20:12:10,819] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 21.0 in stage 1.0 (TID 207). 1843 bytes result sent to driver
21/05/16 23:12:10 INFO Executor: Finished task 20.0 in stage 1.0 (TID 206). 1843 bytes result sent to driver
[2021-05-16 20:12:10,821] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 210) (764b1db4ecfd, executor driver, partition 24, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,822] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 22.0 in stage 1.0 (TID 208). 1843 bytes result sent to driver
[2021-05-16 20:12:10,824] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 24.0 in stage 1.0 (TID 210)
[2021-05-16 20:12:10,825] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 211) (764b1db4ecfd, executor driver, partition 25, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,825] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 207) in 182 ms on 764b1db4ecfd (executor driver) (21/186)
[2021-05-16 20:12:10,825] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 206) in 188 ms on 764b1db4ecfd (executor driver) (22/186)
[2021-05-16 20:12:10,827] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 212) (764b1db4ecfd, executor driver, partition 26, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,827] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 208) in 176 ms on 764b1db4ecfd (executor driver) (23/186)
[2021-05-16 20:12:10,828] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 26.0 in stage 1.0 (TID 212)
[2021-05-16 20:12:10,829] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 25.0 in stage 1.0 (TID 211)
[2021-05-16 20:12:10,897] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Finished task 23.0 in stage 1.0 (TID 209). 1843 bytes result sent to driver
[2021-05-16 20:12:10,899] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 213) (764b1db4ecfd, executor driver, partition 27, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:10,900] {docker.py:276} INFO - 21/05/16 23:12:10 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 209) in 192 ms on 764b1db4ecfd (executor driver) (24/186)
[2021-05-16 20:12:10,901] {docker.py:276} INFO - 21/05/16 23:12:10 INFO Executor: Running task 27.0 in stage 1.0 (TID 213)
[2021-05-16 20:12:10,998] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 26.0 in stage 1.0 (TID 212). 1843 bytes result sent to driver
[2021-05-16 20:12:11,000] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 24.0 in stage 1.0 (TID 210). 1843 bytes result sent to driver
21/05/16 23:12:11 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 214) (764b1db4ecfd, executor driver, partition 28, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,001] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 28.0 in stage 1.0 (TID 214)
21/05/16 23:12:11 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 215) (764b1db4ecfd, executor driver, partition 29, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,002] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 212) in 175 ms on 764b1db4ecfd (executor driver) (25/186)
[2021-05-16 20:12:11,003] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 25.0 in stage 1.0 (TID 211). 1843 bytes result sent to driver
[2021-05-16 20:12:11,004] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 216) (764b1db4ecfd, executor driver, partition 30, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,005] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 210) in 184 ms on 764b1db4ecfd (executor driver) (26/186)
[2021-05-16 20:12:11,005] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 29.0 in stage 1.0 (TID 215)
[2021-05-16 20:12:11,006] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 211) in 182 ms on 764b1db4ecfd (executor driver) (27/186)
[2021-05-16 20:12:11,006] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 30.0 in stage 1.0 (TID 216)
[2021-05-16 20:12:11,075] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 27.0 in stage 1.0 (TID 213). 1886 bytes result sent to driver
[2021-05-16 20:12:11,076] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 217) (764b1db4ecfd, executor driver, partition 31, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,077] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 31.0 in stage 1.0 (TID 217)
21/05/16 23:12:11 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 213) in 179 ms on 764b1db4ecfd (executor driver) (28/186)
[2021-05-16 20:12:11,188] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 28.0 in stage 1.0 (TID 214). 1886 bytes result sent to driver
[2021-05-16 20:12:11,189] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 30.0 in stage 1.0 (TID 216). 1886 bytes result sent to driver
21/05/16 23:12:11 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 218) (764b1db4ecfd, executor driver, partition 32, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,191] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 32.0 in stage 1.0 (TID 218)
[2021-05-16 20:12:11,191] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 219) (764b1db4ecfd, executor driver, partition 33, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,192] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 214) in 193 ms on 764b1db4ecfd (executor driver) (29/186)
21/05/16 23:12:11 INFO Executor: Running task 33.0 in stage 1.0 (TID 219)
[2021-05-16 20:12:11,193] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 216) in 190 ms on 764b1db4ecfd (executor driver) (30/186)
[2021-05-16 20:12:11,194] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 29.0 in stage 1.0 (TID 215). 1886 bytes result sent to driver
[2021-05-16 20:12:11,195] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 220) (764b1db4ecfd, executor driver, partition 34, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,196] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 215) in 196 ms on 764b1db4ecfd (executor driver) (31/186)
[2021-05-16 20:12:11,197] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 34.0 in stage 1.0 (TID 220)
[2021-05-16 20:12:11,255] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 31.0 in stage 1.0 (TID 217). 1843 bytes result sent to driver
[2021-05-16 20:12:11,256] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 221) (764b1db4ecfd, executor driver, partition 35, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,257] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 217) in 181 ms on 764b1db4ecfd (executor driver) (32/186)
[2021-05-16 20:12:11,258] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 35.0 in stage 1.0 (TID 221)
[2021-05-16 20:12:11,361] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 32.0 in stage 1.0 (TID 218). 1843 bytes result sent to driver
[2021-05-16 20:12:11,363] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 222) (764b1db4ecfd, executor driver, partition 36, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,365] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 218) in 175 ms on 764b1db4ecfd (executor driver) (33/186)
21/05/16 23:12:11 INFO Executor: Running task 36.0 in stage 1.0 (TID 222)
[2021-05-16 20:12:11,367] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 34.0 in stage 1.0 (TID 220). 1843 bytes result sent to driver
[2021-05-16 20:12:11,368] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 223) (764b1db4ecfd, executor driver, partition 37, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,369] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 37.0 in stage 1.0 (TID 223)
[2021-05-16 20:12:11,370] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 220) in 175 ms on 764b1db4ecfd (executor driver) (34/186)
[2021-05-16 20:12:11,373] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 33.0 in stage 1.0 (TID 219). 1843 bytes result sent to driver
[2021-05-16 20:12:11,374] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 224) (764b1db4ecfd, executor driver, partition 38, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,375] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 219) in 185 ms on 764b1db4ecfd (executor driver) (35/186)
[2021-05-16 20:12:11,377] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 38.0 in stage 1.0 (TID 224)
[2021-05-16 20:12:11,437] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 35.0 in stage 1.0 (TID 221). 1843 bytes result sent to driver
[2021-05-16 20:12:11,438] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 225) (764b1db4ecfd, executor driver, partition 39, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,439] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 221) in 184 ms on 764b1db4ecfd (executor driver) (36/186)
[2021-05-16 20:12:11,441] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 39.0 in stage 1.0 (TID 225)
[2021-05-16 20:12:11,550] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 36.0 in stage 1.0 (TID 222). 1843 bytes result sent to driver
[2021-05-16 20:12:11,563] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 37.0 in stage 1.0 (TID 223). 1843 bytes result sent to driver
[2021-05-16 20:12:11,564] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 226) (764b1db4ecfd, executor driver, partition 40, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,564] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 38.0 in stage 1.0 (TID 224). 1843 bytes result sent to driver
[2021-05-16 20:12:11,564] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 40.0 in stage 1.0 (TID 226)
[2021-05-16 20:12:11,565] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 227) (764b1db4ecfd, executor driver, partition 41, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,565] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 41.0 in stage 1.0 (TID 227)
[2021-05-16 20:12:11,566] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 228) (764b1db4ecfd, executor driver, partition 42, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,566] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 223) in 190 ms on 764b1db4ecfd (executor driver) (37/186)
[2021-05-16 20:12:11,566] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 222) in 196 ms on 764b1db4ecfd (executor driver) (38/186)
[2021-05-16 20:12:11,567] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 42.0 in stage 1.0 (TID 228)
[2021-05-16 20:12:11,567] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 224) in 186 ms on 764b1db4ecfd (executor driver) (39/186)
[2021-05-16 20:12:11,627] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 39.0 in stage 1.0 (TID 225). 1843 bytes result sent to driver
[2021-05-16 20:12:11,628] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 229) (764b1db4ecfd, executor driver, partition 43, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,630] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 43.0 in stage 1.0 (TID 229)
[2021-05-16 20:12:11,631] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 225) in 192 ms on 764b1db4ecfd (executor driver) (40/186)
[2021-05-16 20:12:11,732] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 42.0 in stage 1.0 (TID 228). 1843 bytes result sent to driver
[2021-05-16 20:12:11,733] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 40.0 in stage 1.0 (TID 226). 1843 bytes result sent to driver
[2021-05-16 20:12:11,734] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 230) (764b1db4ecfd, executor driver, partition 44, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,735] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 41.0 in stage 1.0 (TID 227). 1843 bytes result sent to driver
[2021-05-16 20:12:11,736] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 228) in 179 ms on 764b1db4ecfd (executor driver) (41/186)
[2021-05-16 20:12:11,736] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 44.0 in stage 1.0 (TID 230)
[2021-05-16 20:12:11,737] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 231) (764b1db4ecfd, executor driver, partition 45, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,738] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 45.0 in stage 1.0 (TID 231)
[2021-05-16 20:12:11,739] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 232) (764b1db4ecfd, executor driver, partition 46, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,740] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 227) in 185 ms on 764b1db4ecfd (executor driver) (42/186)
[2021-05-16 20:12:11,741] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 226) in 188 ms on 764b1db4ecfd (executor driver) (43/186)
[2021-05-16 20:12:11,741] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 46.0 in stage 1.0 (TID 232)
[2021-05-16 20:12:11,808] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 43.0 in stage 1.0 (TID 229). 1843 bytes result sent to driver
[2021-05-16 20:12:11,810] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 233) (764b1db4ecfd, executor driver, partition 47, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,812] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 229) in 185 ms on 764b1db4ecfd (executor driver) (44/186)
21/05/16 23:12:11 INFO Executor: Running task 47.0 in stage 1.0 (TID 233)
[2021-05-16 20:12:11,926] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Finished task 44.0 in stage 1.0 (TID 230). 1886 bytes result sent to driver
21/05/16 23:12:11 INFO Executor: Finished task 46.0 in stage 1.0 (TID 232). 1886 bytes result sent to driver
[2021-05-16 20:12:11,927] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 234) (764b1db4ecfd, executor driver, partition 48, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:11 INFO Executor: Finished task 45.0 in stage 1.0 (TID 231). 1886 bytes result sent to driver
[2021-05-16 20:12:11,928] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 48.0 in stage 1.0 (TID 234)
[2021-05-16 20:12:11,929] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 235) (764b1db4ecfd, executor driver, partition 49, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,931] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 232) in 192 ms on 764b1db4ecfd (executor driver) (45/186)
[2021-05-16 20:12:11,932] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 230) in 199 ms on 764b1db4ecfd (executor driver) (46/186)
[2021-05-16 20:12:11,933] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 49.0 in stage 1.0 (TID 235)
[2021-05-16 20:12:11,934] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 236) (764b1db4ecfd, executor driver, partition 50, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,935] {docker.py:276} INFO - 21/05/16 23:12:11 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 231) in 198 ms on 764b1db4ecfd (executor driver) (47/186)
[2021-05-16 20:12:11,935] {docker.py:276} INFO - 21/05/16 23:12:11 INFO Executor: Running task 50.0 in stage 1.0 (TID 236)
[2021-05-16 20:12:11,996] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 47.0 in stage 1.0 (TID 233). 1886 bytes result sent to driver
[2021-05-16 20:12:11,998] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 237) (764b1db4ecfd, executor driver, partition 51, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:11,999] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 233) in 189 ms on 764b1db4ecfd (executor driver) (48/186)
[2021-05-16 20:12:12,000] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 51.0 in stage 1.0 (TID 237)
[2021-05-16 20:12:12,106] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 49.0 in stage 1.0 (TID 235). 1843 bytes result sent to driver
21/05/16 23:12:12 INFO Executor: Finished task 48.0 in stage 1.0 (TID 234). 1843 bytes result sent to driver
[2021-05-16 20:12:12,109] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 238) (764b1db4ecfd, executor driver, partition 52, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:12 INFO Executor: Finished task 50.0 in stage 1.0 (TID 236). 1843 bytes result sent to driver
[2021-05-16 20:12:12,110] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 52.0 in stage 1.0 (TID 238)
[2021-05-16 20:12:12,111] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 239) (764b1db4ecfd, executor driver, partition 53, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,114] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 234) in 185 ms on 764b1db4ecfd (executor driver) (49/186)
[2021-05-16 20:12:12,115] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 240) (764b1db4ecfd, executor driver, partition 54, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,115] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 235) in 183 ms on 764b1db4ecfd (executor driver) (50/186)
[2021-05-16 20:12:12,116] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 54.0 in stage 1.0 (TID 240)
[2021-05-16 20:12:12,116] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 236) in 180 ms on 764b1db4ecfd (executor driver) (51/186)
[2021-05-16 20:12:12,117] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 53.0 in stage 1.0 (TID 239)
[2021-05-16 20:12:12,174] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 51.0 in stage 1.0 (TID 237). 1843 bytes result sent to driver
[2021-05-16 20:12:12,176] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 241) (764b1db4ecfd, executor driver, partition 55, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,177] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 55.0 in stage 1.0 (TID 241)
[2021-05-16 20:12:12,178] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 237) in 180 ms on 764b1db4ecfd (executor driver) (52/186)
[2021-05-16 20:12:12,286] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 54.0 in stage 1.0 (TID 240). 1843 bytes result sent to driver
[2021-05-16 20:12:12,288] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 52.0 in stage 1.0 (TID 238). 1843 bytes result sent to driver
[2021-05-16 20:12:12,292] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 242) (764b1db4ecfd, executor driver, partition 56, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,293] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 56.0 in stage 1.0 (TID 242)
[2021-05-16 20:12:12,294] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 243) (764b1db4ecfd, executor driver, partition 57, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,295] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 240) in 178 ms on 764b1db4ecfd (executor driver) (53/186)
[2021-05-16 20:12:12,295] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 238) in 183 ms on 764b1db4ecfd (executor driver) (54/186)
[2021-05-16 20:12:12,296] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 57.0 in stage 1.0 (TID 243)
[2021-05-16 20:12:12,296] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 53.0 in stage 1.0 (TID 239). 1843 bytes result sent to driver
[2021-05-16 20:12:12,298] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 244) (764b1db4ecfd, executor driver, partition 58, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,298] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 239) in 186 ms on 764b1db4ecfd (executor driver) (55/186)
[2021-05-16 20:12:12,299] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 58.0 in stage 1.0 (TID 244)
[2021-05-16 20:12:12,356] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 55.0 in stage 1.0 (TID 241). 1843 bytes result sent to driver
[2021-05-16 20:12:12,358] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 245) (764b1db4ecfd, executor driver, partition 59, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,359] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 59.0 in stage 1.0 (TID 245)
[2021-05-16 20:12:12,360] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 241) in 184 ms on 764b1db4ecfd (executor driver) (56/186)
[2021-05-16 20:12:12,463] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 57.0 in stage 1.0 (TID 243). 1843 bytes result sent to driver
21/05/16 23:12:12 INFO Executor: Finished task 56.0 in stage 1.0 (TID 242). 1843 bytes result sent to driver
[2021-05-16 20:12:12,465] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 246) (764b1db4ecfd, executor driver, partition 60, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,467] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 60.0 in stage 1.0 (TID 246)
21/05/16 23:12:12 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 247) (764b1db4ecfd, executor driver, partition 61, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,468] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 61.0 in stage 1.0 (TID 247)
21/05/16 23:12:12 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 242) in 181 ms on 764b1db4ecfd (executor driver) (57/186)
[2021-05-16 20:12:12,469] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 243) in 179 ms on 764b1db4ecfd (executor driver) (58/186)
[2021-05-16 20:12:12,472] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 58.0 in stage 1.0 (TID 244). 1843 bytes result sent to driver
[2021-05-16 20:12:12,473] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 248) (764b1db4ecfd, executor driver, partition 62, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,474] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 244) in 179 ms on 764b1db4ecfd (executor driver) (59/186)
[2021-05-16 20:12:12,474] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 62.0 in stage 1.0 (TID 248)
[2021-05-16 20:12:12,538] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 59.0 in stage 1.0 (TID 245). 1843 bytes result sent to driver
[2021-05-16 20:12:12,539] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 249) (764b1db4ecfd, executor driver, partition 63, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,541] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 63.0 in stage 1.0 (TID 249)
[2021-05-16 20:12:12,542] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 245) in 184 ms on 764b1db4ecfd (executor driver) (60/186)
[2021-05-16 20:12:12,643] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 61.0 in stage 1.0 (TID 247). 1843 bytes result sent to driver
[2021-05-16 20:12:12,647] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 250) (764b1db4ecfd, executor driver, partition 64, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,648] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 247) in 180 ms on 764b1db4ecfd (executor driver) (61/186)
[2021-05-16 20:12:12,648] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 62.0 in stage 1.0 (TID 248). 1843 bytes result sent to driver
[2021-05-16 20:12:12,649] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 64.0 in stage 1.0 (TID 250)
[2021-05-16 20:12:12,650] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 251) (764b1db4ecfd, executor driver, partition 65, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,651] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 248) in 179 ms on 764b1db4ecfd (executor driver) (62/186)
[2021-05-16 20:12:12,652] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 65.0 in stage 1.0 (TID 251)
[2021-05-16 20:12:12,653] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 60.0 in stage 1.0 (TID 246). 1843 bytes result sent to driver
[2021-05-16 20:12:12,664] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 252) (764b1db4ecfd, executor driver, partition 66, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,665] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 66.0 in stage 1.0 (TID 252)
[2021-05-16 20:12:12,667] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 246) in 203 ms on 764b1db4ecfd (executor driver) (63/186)
[2021-05-16 20:12:12,718] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 63.0 in stage 1.0 (TID 249). 1886 bytes result sent to driver
[2021-05-16 20:12:12,719] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 253) (764b1db4ecfd, executor driver, partition 67, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,720] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 249) in 182 ms on 764b1db4ecfd (executor driver) (64/186)
21/05/16 23:12:12 INFO Executor: Running task 67.0 in stage 1.0 (TID 253)
[2021-05-16 20:12:12,833] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 64.0 in stage 1.0 (TID 250). 1886 bytes result sent to driver
21/05/16 23:12:12 INFO Executor: Finished task 65.0 in stage 1.0 (TID 251). 1886 bytes result sent to driver
[2021-05-16 20:12:12,834] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 254) (764b1db4ecfd, executor driver, partition 68, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,836] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 68.0 in stage 1.0 (TID 254)
21/05/16 23:12:12 INFO Executor: Finished task 66.0 in stage 1.0 (TID 252). 1843 bytes result sent to driver
[2021-05-16 20:12:12,837] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 255) (764b1db4ecfd, executor driver, partition 69, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,838] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 256) (764b1db4ecfd, executor driver, partition 70, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,839] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 250) in 194 ms on 764b1db4ecfd (executor driver) (65/186)
21/05/16 23:12:12 INFO Executor: Running task 70.0 in stage 1.0 (TID 256)
[2021-05-16 20:12:12,840] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 252) in 174 ms on 764b1db4ecfd (executor driver) (66/186)
21/05/16 23:12:12 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 251) in 190 ms on 764b1db4ecfd (executor driver) (67/186)
[2021-05-16 20:12:12,845] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 69.0 in stage 1.0 (TID 255)
[2021-05-16 20:12:12,896] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Finished task 67.0 in stage 1.0 (TID 253). 1843 bytes result sent to driver
[2021-05-16 20:12:12,899] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 257) (764b1db4ecfd, executor driver, partition 71, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:12,900] {docker.py:276} INFO - 21/05/16 23:12:12 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 253) in 180 ms on 764b1db4ecfd (executor driver) (68/186)
[2021-05-16 20:12:12,901] {docker.py:276} INFO - 21/05/16 23:12:12 INFO Executor: Running task 71.0 in stage 1.0 (TID 257)
[2021-05-16 20:12:13,013] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 70.0 in stage 1.0 (TID 256). 1843 bytes result sent to driver
[2021-05-16 20:12:13,015] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 68.0 in stage 1.0 (TID 254). 1843 bytes result sent to driver
[2021-05-16 20:12:13,017] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 258) (764b1db4ecfd, executor driver, partition 72, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,018] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 72.0 in stage 1.0 (TID 258)
[2021-05-16 20:12:13,019] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 259) (764b1db4ecfd, executor driver, partition 73, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,020] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 256) in 183 ms on 764b1db4ecfd (executor driver) (69/186)
[2021-05-16 20:12:13,021] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 254) in 187 ms on 764b1db4ecfd (executor driver) (70/186)
[2021-05-16 20:12:13,022] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 73.0 in stage 1.0 (TID 259)
[2021-05-16 20:12:13,023] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 69.0 in stage 1.0 (TID 255). 1843 bytes result sent to driver
[2021-05-16 20:12:13,024] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 260) (764b1db4ecfd, executor driver, partition 74, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,025] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 255) in 190 ms on 764b1db4ecfd (executor driver) (71/186)
[2021-05-16 20:12:13,026] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 74.0 in stage 1.0 (TID 260)
[2021-05-16 20:12:13,081] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 71.0 in stage 1.0 (TID 257). 1843 bytes result sent to driver
[2021-05-16 20:12:13,082] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 261) (764b1db4ecfd, executor driver, partition 75, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,084] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 75.0 in stage 1.0 (TID 261)
[2021-05-16 20:12:13,084] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 257) in 186 ms on 764b1db4ecfd (executor driver) (72/186)
[2021-05-16 20:12:13,194] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 72.0 in stage 1.0 (TID 258). 1843 bytes result sent to driver
[2021-05-16 20:12:13,196] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 262) (764b1db4ecfd, executor driver, partition 76, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,196] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 258) in 181 ms on 764b1db4ecfd (executor driver) (73/186)
[2021-05-16 20:12:13,197] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 76.0 in stage 1.0 (TID 262)
[2021-05-16 20:12:13,199] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 73.0 in stage 1.0 (TID 259). 1843 bytes result sent to driver
21/05/16 23:12:13 INFO Executor: Finished task 74.0 in stage 1.0 (TID 260). 1886 bytes result sent to driver
[2021-05-16 20:12:13,200] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 263) (764b1db4ecfd, executor driver, partition 77, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,201] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 264) (764b1db4ecfd, executor driver, partition 78, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,201] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 259) in 184 ms on 764b1db4ecfd (executor driver) (74/186)
[2021-05-16 20:12:13,202] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 78.0 in stage 1.0 (TID 264)
[2021-05-16 20:12:13,202] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 260) in 179 ms on 764b1db4ecfd (executor driver) (75/186)
[2021-05-16 20:12:13,205] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 77.0 in stage 1.0 (TID 263)
[2021-05-16 20:12:13,269] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 75.0 in stage 1.0 (TID 261). 1843 bytes result sent to driver
[2021-05-16 20:12:13,270] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 265) (764b1db4ecfd, executor driver, partition 79, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,271] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 79.0 in stage 1.0 (TID 265)
21/05/16 23:12:13 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 261) in 190 ms on 764b1db4ecfd (executor driver) (76/186)
[2021-05-16 20:12:13,371] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 78.0 in stage 1.0 (TID 264). 1843 bytes result sent to driver
[2021-05-16 20:12:13,372] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 266) (764b1db4ecfd, executor driver, partition 80, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,373] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 264) in 173 ms on 764b1db4ecfd (executor driver) (77/186)
[2021-05-16 20:12:13,374] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 80.0 in stage 1.0 (TID 266)
[2021-05-16 20:12:13,375] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 76.0 in stage 1.0 (TID 262). 1843 bytes result sent to driver
[2021-05-16 20:12:13,375] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 267) (764b1db4ecfd, executor driver, partition 81, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,377] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 262) in 182 ms on 764b1db4ecfd (executor driver) (78/186)
[2021-05-16 20:12:13,380] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 77.0 in stage 1.0 (TID 263). 1843 bytes result sent to driver
21/05/16 23:12:13 INFO Executor: Running task 81.0 in stage 1.0 (TID 267)
[2021-05-16 20:12:13,380] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 268) (764b1db4ecfd, executor driver, partition 82, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,381] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 263) in 182 ms on 764b1db4ecfd (executor driver) (79/186)
[2021-05-16 20:12:13,382] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 82.0 in stage 1.0 (TID 268)
[2021-05-16 20:12:13,445] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 79.0 in stage 1.0 (TID 265). 1886 bytes result sent to driver
[2021-05-16 20:12:13,446] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 269) (764b1db4ecfd, executor driver, partition 83, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,447] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 83.0 in stage 1.0 (TID 269)
21/05/16 23:12:13 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 265) in 178 ms on 764b1db4ecfd (executor driver) (80/186)
[2021-05-16 20:12:13,546] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 80.0 in stage 1.0 (TID 266). 1886 bytes result sent to driver
[2021-05-16 20:12:13,547] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 270) (764b1db4ecfd, executor driver, partition 84, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,548] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 266) in 177 ms on 764b1db4ecfd (executor driver) (81/186)
[2021-05-16 20:12:13,549] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 84.0 in stage 1.0 (TID 270)
[2021-05-16 20:12:13,563] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 81.0 in stage 1.0 (TID 267). 1886 bytes result sent to driver
[2021-05-16 20:12:13,563] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 82.0 in stage 1.0 (TID 268). 1886 bytes result sent to driver
[2021-05-16 20:12:13,564] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 271) (764b1db4ecfd, executor driver, partition 85, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,565] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 267) in 190 ms on 764b1db4ecfd (executor driver) (82/186)
[2021-05-16 20:12:13,565] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 85.0 in stage 1.0 (TID 271)
[2021-05-16 20:12:13,567] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 272) (764b1db4ecfd, executor driver, partition 86, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,568] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 86.0 in stage 1.0 (TID 272)
[2021-05-16 20:12:13,569] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 268) in 188 ms on 764b1db4ecfd (executor driver) (83/186)
[2021-05-16 20:12:13,620] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 83.0 in stage 1.0 (TID 269). 1843 bytes result sent to driver
[2021-05-16 20:12:13,621] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 273) (764b1db4ecfd, executor driver, partition 87, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,621] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 269) in 176 ms on 764b1db4ecfd (executor driver) (84/186)
[2021-05-16 20:12:13,623] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 87.0 in stage 1.0 (TID 273)
[2021-05-16 20:12:13,717] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 84.0 in stage 1.0 (TID 270). 1843 bytes result sent to driver
[2021-05-16 20:12:13,718] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 274) (764b1db4ecfd, executor driver, partition 88, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,719] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 88.0 in stage 1.0 (TID 274)
21/05/16 23:12:13 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 270) in 173 ms on 764b1db4ecfd (executor driver) (85/186)
[2021-05-16 20:12:13,735] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 86.0 in stage 1.0 (TID 272). 1843 bytes result sent to driver
[2021-05-16 20:12:13,735] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 275) (764b1db4ecfd, executor driver, partition 89, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,736] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 89.0 in stage 1.0 (TID 275)
[2021-05-16 20:12:13,737] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 272) in 171 ms on 764b1db4ecfd (executor driver) (86/186)
[2021-05-16 20:12:13,740] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 85.0 in stage 1.0 (TID 271). 1843 bytes result sent to driver
[2021-05-16 20:12:13,741] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 276) (764b1db4ecfd, executor driver, partition 90, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,742] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 271) in 178 ms on 764b1db4ecfd (executor driver) (87/186)
[2021-05-16 20:12:13,742] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 90.0 in stage 1.0 (TID 276)
[2021-05-16 20:12:13,796] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 87.0 in stage 1.0 (TID 273). 1843 bytes result sent to driver
[2021-05-16 20:12:13,797] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 277) (764b1db4ecfd, executor driver, partition 91, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,798] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 273) in 178 ms on 764b1db4ecfd (executor driver) (88/186)
21/05/16 23:12:13 INFO Executor: Running task 91.0 in stage 1.0 (TID 277)
[2021-05-16 20:12:13,889] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 88.0 in stage 1.0 (TID 274). 1843 bytes result sent to driver
[2021-05-16 20:12:13,890] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 278) (764b1db4ecfd, executor driver, partition 92, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,890] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 274) in 173 ms on 764b1db4ecfd (executor driver) (89/186)
[2021-05-16 20:12:13,891] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 92.0 in stage 1.0 (TID 278)
[2021-05-16 20:12:13,909] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 90.0 in stage 1.0 (TID 276). 1843 bytes result sent to driver
[2021-05-16 20:12:13,910] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 89.0 in stage 1.0 (TID 275). 1843 bytes result sent to driver
[2021-05-16 20:12:13,911] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 279) (764b1db4ecfd, executor driver, partition 93, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,912] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 93.0 in stage 1.0 (TID 279)
[2021-05-16 20:12:13,912] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 276) in 171 ms on 764b1db4ecfd (executor driver) (90/186)
[2021-05-16 20:12:13,913] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 280) (764b1db4ecfd, executor driver, partition 94, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,914] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 275) in 178 ms on 764b1db4ecfd (executor driver) (91/186)
[2021-05-16 20:12:13,916] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 94.0 in stage 1.0 (TID 280)
[2021-05-16 20:12:13,972] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Finished task 91.0 in stage 1.0 (TID 277). 1843 bytes result sent to driver
[2021-05-16 20:12:13,973] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 281) (764b1db4ecfd, executor driver, partition 95, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:13,975] {docker.py:276} INFO - 21/05/16 23:12:13 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 277) in 177 ms on 764b1db4ecfd (executor driver) (92/186)
[2021-05-16 20:12:13,976] {docker.py:276} INFO - 21/05/16 23:12:13 INFO Executor: Running task 95.0 in stage 1.0 (TID 281)
[2021-05-16 20:12:14,063] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 92.0 in stage 1.0 (TID 278). 1843 bytes result sent to driver
[2021-05-16 20:12:14,065] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 282) (764b1db4ecfd, executor driver, partition 96, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,067] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 278) in 177 ms on 764b1db4ecfd (executor driver) (93/186)
[2021-05-16 20:12:14,067] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 96.0 in stage 1.0 (TID 282)
[2021-05-16 20:12:14,084] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 93.0 in stage 1.0 (TID 279). 1843 bytes result sent to driver
[2021-05-16 20:12:14,085] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 283) (764b1db4ecfd, executor driver, partition 97, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,086] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 97.0 in stage 1.0 (TID 283)
[2021-05-16 20:12:14,086] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 279) in 176 ms on 764b1db4ecfd (executor driver) (94/186)
[2021-05-16 20:12:14,087] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 94.0 in stage 1.0 (TID 280). 1886 bytes result sent to driver
[2021-05-16 20:12:14,088] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 284) (764b1db4ecfd, executor driver, partition 98, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,089] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 280) in 177 ms on 764b1db4ecfd (executor driver) (95/186)
[2021-05-16 20:12:14,089] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 98.0 in stage 1.0 (TID 284)
[2021-05-16 20:12:14,146] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 95.0 in stage 1.0 (TID 281). 1843 bytes result sent to driver
[2021-05-16 20:12:14,147] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 281) in 175 ms on 764b1db4ecfd (executor driver) (96/186)
[2021-05-16 20:12:14,148] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 285) (764b1db4ecfd, executor driver, partition 99, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,149] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 99.0 in stage 1.0 (TID 285)
[2021-05-16 20:12:14,268] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 97.0 in stage 1.0 (TID 283). 1886 bytes result sent to driver
[2021-05-16 20:12:14,269] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 98.0 in stage 1.0 (TID 284). 1886 bytes result sent to driver
[2021-05-16 20:12:14,270] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 286) (764b1db4ecfd, executor driver, partition 100, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,272] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 100.0 in stage 1.0 (TID 286)
[2021-05-16 20:12:14,273] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 287) (764b1db4ecfd, executor driver, partition 101, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,274] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 283) in 189 ms on 764b1db4ecfd (executor driver) (97/186)
[2021-05-16 20:12:14,275] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 284) in 186 ms on 764b1db4ecfd (executor driver) (98/186)
21/05/16 23:12:14 INFO Executor: Running task 101.0 in stage 1.0 (TID 287)
[2021-05-16 20:12:14,282] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 96.0 in stage 1.0 (TID 282). 1886 bytes result sent to driver
[2021-05-16 20:12:14,283] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 288) (764b1db4ecfd, executor driver, partition 102, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,284] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 282) in 221 ms on 764b1db4ecfd (executor driver) (99/186)
[2021-05-16 20:12:14,285] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 102.0 in stage 1.0 (TID 288)
[2021-05-16 20:12:14,332] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 99.0 in stage 1.0 (TID 285). 1886 bytes result sent to driver
[2021-05-16 20:12:14,333] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 289) (764b1db4ecfd, executor driver, partition 103, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,335] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 285) in 186 ms on 764b1db4ecfd (executor driver) (100/186)
21/05/16 23:12:14 INFO Executor: Running task 103.0 in stage 1.0 (TID 289)
[2021-05-16 20:12:14,447] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 100.0 in stage 1.0 (TID 286). 1843 bytes result sent to driver
[2021-05-16 20:12:14,449] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 290) (764b1db4ecfd, executor driver, partition 104, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,450] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 104.0 in stage 1.0 (TID 290)
[2021-05-16 20:12:14,451] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 286) in 181 ms on 764b1db4ecfd (executor driver) (101/186)
21/05/16 23:12:14 INFO Executor: Finished task 101.0 in stage 1.0 (TID 287). 1843 bytes result sent to driver
[2021-05-16 20:12:14,453] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 291) (764b1db4ecfd, executor driver, partition 105, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,454] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 105.0 in stage 1.0 (TID 291)
[2021-05-16 20:12:14,455] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 287) in 183 ms on 764b1db4ecfd (executor driver) (102/186)
[2021-05-16 20:12:14,457] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 102.0 in stage 1.0 (TID 288). 1843 bytes result sent to driver
[2021-05-16 20:12:14,459] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 292) (764b1db4ecfd, executor driver, partition 106, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,460] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 106.0 in stage 1.0 (TID 292)
[2021-05-16 20:12:14,461] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 288) in 177 ms on 764b1db4ecfd (executor driver) (103/186)
[2021-05-16 20:12:14,581] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 103.0 in stage 1.0 (TID 289). 1843 bytes result sent to driver
[2021-05-16 20:12:14,583] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 293) (764b1db4ecfd, executor driver, partition 107, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,585] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 289) in 251 ms on 764b1db4ecfd (executor driver) (104/186)
21/05/16 23:12:14 INFO Executor: Running task 107.0 in stage 1.0 (TID 293)
[2021-05-16 20:12:14,629] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 105.0 in stage 1.0 (TID 291). 1843 bytes result sent to driver
[2021-05-16 20:12:14,632] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 106.0 in stage 1.0 (TID 292). 1843 bytes result sent to driver
[2021-05-16 20:12:14,633] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 294) (764b1db4ecfd, executor driver, partition 108, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,634] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 108.0 in stage 1.0 (TID 294)
21/05/16 23:12:14 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 295) (764b1db4ecfd, executor driver, partition 109, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,635] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 292) in 174 ms on 764b1db4ecfd (executor driver) (105/186)
[2021-05-16 20:12:14,636] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 109.0 in stage 1.0 (TID 295)
[2021-05-16 20:12:14,637] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 291) in 180 ms on 764b1db4ecfd (executor driver) (106/186)
[2021-05-16 20:12:14,637] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 104.0 in stage 1.0 (TID 290). 1843 bytes result sent to driver
[2021-05-16 20:12:14,638] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 296) (764b1db4ecfd, executor driver, partition 110, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,639] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 110.0 in stage 1.0 (TID 296)
21/05/16 23:12:14 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 290) in 189 ms on 764b1db4ecfd (executor driver) (107/186)
[2021-05-16 20:12:14,763] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 107.0 in stage 1.0 (TID 293). 1843 bytes result sent to driver
[2021-05-16 20:12:14,765] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 297) (764b1db4ecfd, executor driver, partition 111, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,767] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 293) in 184 ms on 764b1db4ecfd (executor driver) (108/186)
[2021-05-16 20:12:14,768] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 111.0 in stage 1.0 (TID 297)
[2021-05-16 20:12:14,807] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 108.0 in stage 1.0 (TID 294). 1843 bytes result sent to driver
[2021-05-16 20:12:14,808] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 298) (764b1db4ecfd, executor driver, partition 112, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,809] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 112.0 in stage 1.0 (TID 298)
[2021-05-16 20:12:14,810] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 294) in 179 ms on 764b1db4ecfd (executor driver) (109/186)
[2021-05-16 20:12:14,811] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 110.0 in stage 1.0 (TID 296). 1843 bytes result sent to driver
[2021-05-16 20:12:14,812] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 109.0 in stage 1.0 (TID 295). 1843 bytes result sent to driver
[2021-05-16 20:12:14,813] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 299) (764b1db4ecfd, executor driver, partition 113, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,814] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 296) in 178 ms on 764b1db4ecfd (executor driver) (110/186)
[2021-05-16 20:12:14,815] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 113.0 in stage 1.0 (TID 299)
[2021-05-16 20:12:14,816] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 300) (764b1db4ecfd, executor driver, partition 114, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,816] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 295) in 185 ms on 764b1db4ecfd (executor driver) (111/186)
[2021-05-16 20:12:14,817] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 114.0 in stage 1.0 (TID 300)
[2021-05-16 20:12:14,946] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 111.0 in stage 1.0 (TID 297). 1843 bytes result sent to driver
[2021-05-16 20:12:14,947] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 301) (764b1db4ecfd, executor driver, partition 115, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,948] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Running task 115.0 in stage 1.0 (TID 301)
[2021-05-16 20:12:14,949] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 297) in 185 ms on 764b1db4ecfd (executor driver) (112/186)
[2021-05-16 20:12:14,986] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 112.0 in stage 1.0 (TID 298). 1843 bytes result sent to driver
[2021-05-16 20:12:14,987] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 113.0 in stage 1.0 (TID 299). 1843 bytes result sent to driver
[2021-05-16 20:12:14,987] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 302) (764b1db4ecfd, executor driver, partition 116, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,988] {docker.py:276} INFO - 21/05/16 23:12:14 INFO Executor: Finished task 114.0 in stage 1.0 (TID 300). 1843 bytes result sent to driver
[2021-05-16 20:12:14,989] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 298) in 182 ms on 764b1db4ecfd (executor driver) (113/186)
[2021-05-16 20:12:14,990] {docker.py:276} INFO - 21/05/16 23:12:14 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 303) (764b1db4ecfd, executor driver, partition 117, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,990] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 116.0 in stage 1.0 (TID 302)
[2021-05-16 20:12:14,991] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 304) (764b1db4ecfd, executor driver, partition 118, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:14,992] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 299) in 180 ms on 764b1db4ecfd (executor driver) (114/186)
[2021-05-16 20:12:14,992] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 300) in 177 ms on 764b1db4ecfd (executor driver) (115/186)
[2021-05-16 20:12:14,992] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 118.0 in stage 1.0 (TID 304)
[2021-05-16 20:12:14,993] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 117.0 in stage 1.0 (TID 303)
[2021-05-16 20:12:15,125] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 115.0 in stage 1.0 (TID 301). 1886 bytes result sent to driver
[2021-05-16 20:12:15,127] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 305) (764b1db4ecfd, executor driver, partition 119, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,128] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 119.0 in stage 1.0 (TID 305)
21/05/16 23:12:15 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 301) in 182 ms on 764b1db4ecfd (executor driver) (116/186)
[2021-05-16 20:12:15,165] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 116.0 in stage 1.0 (TID 302). 1886 bytes result sent to driver
[2021-05-16 20:12:15,166] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 118.0 in stage 1.0 (TID 304). 1886 bytes result sent to driver
[2021-05-16 20:12:15,168] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 306) (764b1db4ecfd, executor driver, partition 120, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,170] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 302) in 182 ms on 764b1db4ecfd (executor driver) (117/186)
[2021-05-16 20:12:15,171] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 120.0 in stage 1.0 (TID 306)
[2021-05-16 20:12:15,171] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 307) (764b1db4ecfd, executor driver, partition 121, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,172] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 304) in 181 ms on 764b1db4ecfd (executor driver) (118/186)
[2021-05-16 20:12:15,173] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 121.0 in stage 1.0 (TID 307)
[2021-05-16 20:12:15,177] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 117.0 in stage 1.0 (TID 303). 1886 bytes result sent to driver
[2021-05-16 20:12:15,177] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 308) (764b1db4ecfd, executor driver, partition 122, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,178] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 303) in 188 ms on 764b1db4ecfd (executor driver) (119/186)
[2021-05-16 20:12:15,179] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 122.0 in stage 1.0 (TID 308)
[2021-05-16 20:12:15,304] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 119.0 in stage 1.0 (TID 305). 1843 bytes result sent to driver
[2021-05-16 20:12:15,305] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 309) (764b1db4ecfd, executor driver, partition 123, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,306] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 305) in 180 ms on 764b1db4ecfd (executor driver) (120/186)
[2021-05-16 20:12:15,306] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 123.0 in stage 1.0 (TID 309)
[2021-05-16 20:12:15,343] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 121.0 in stage 1.0 (TID 307). 1843 bytes result sent to driver
[2021-05-16 20:12:15,344] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 310) (764b1db4ecfd, executor driver, partition 124, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,345] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 124.0 in stage 1.0 (TID 310)
21/05/16 23:12:15 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 307) in 176 ms on 764b1db4ecfd (executor driver) (121/186)
[2021-05-16 20:12:15,347] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 120.0 in stage 1.0 (TID 306). 1843 bytes result sent to driver
[2021-05-16 20:12:15,348] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 311) (764b1db4ecfd, executor driver, partition 125, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,351] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 122.0 in stage 1.0 (TID 308). 1843 bytes result sent to driver
[2021-05-16 20:12:15,352] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 125.0 in stage 1.0 (TID 311)
[2021-05-16 20:12:15,352] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 306) in 185 ms on 764b1db4ecfd (executor driver) (122/186)
[2021-05-16 20:12:15,353] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 312) (764b1db4ecfd, executor driver, partition 126, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,353] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 308) in 179 ms on 764b1db4ecfd (executor driver) (123/186)
[2021-05-16 20:12:15,354] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 126.0 in stage 1.0 (TID 312)
[2021-05-16 20:12:15,479] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 123.0 in stage 1.0 (TID 309). 1843 bytes result sent to driver
[2021-05-16 20:12:15,480] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 313) (764b1db4ecfd, executor driver, partition 127, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,481] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 309) in 176 ms on 764b1db4ecfd (executor driver) (124/186)
[2021-05-16 20:12:15,482] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 127.0 in stage 1.0 (TID 313)
[2021-05-16 20:12:15,522] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 126.0 in stage 1.0 (TID 312). 1843 bytes result sent to driver
21/05/16 23:12:15 INFO Executor: Finished task 124.0 in stage 1.0 (TID 310). 1843 bytes result sent to driver
[2021-05-16 20:12:15,522] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 314) (764b1db4ecfd, executor driver, partition 128, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,523] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 128.0 in stage 1.0 (TID 314)
[2021-05-16 20:12:15,523] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 315) (764b1db4ecfd, executor driver, partition 129, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,524] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 312) in 171 ms on 764b1db4ecfd (executor driver) (125/186)
[2021-05-16 20:12:15,524] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 310) in 181 ms on 764b1db4ecfd (executor driver) (126/186)
21/05/16 23:12:15 INFO Executor: Running task 129.0 in stage 1.0 (TID 315)
[2021-05-16 20:12:15,541] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 125.0 in stage 1.0 (TID 311). 1843 bytes result sent to driver
[2021-05-16 20:12:15,542] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 316) (764b1db4ecfd, executor driver, partition 130, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,543] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 311) in 196 ms on 764b1db4ecfd (executor driver) (127/186)
[2021-05-16 20:12:15,544] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 130.0 in stage 1.0 (TID 316)
[2021-05-16 20:12:15,659] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 127.0 in stage 1.0 (TID 313). 1843 bytes result sent to driver
[2021-05-16 20:12:15,662] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 317) (764b1db4ecfd, executor driver, partition 131, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,663] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 313) in 183 ms on 764b1db4ecfd (executor driver) (128/186)
[2021-05-16 20:12:15,663] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 131.0 in stage 1.0 (TID 317)
[2021-05-16 20:12:15,693] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 129.0 in stage 1.0 (TID 315). 1843 bytes result sent to driver
[2021-05-16 20:12:15,695] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 318) (764b1db4ecfd, executor driver, partition 132, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,696] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 315) in 172 ms on 764b1db4ecfd (executor driver) (129/186)
[2021-05-16 20:12:15,697] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 132.0 in stage 1.0 (TID 318)
[2021-05-16 20:12:15,713] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 130.0 in stage 1.0 (TID 316). 1843 bytes result sent to driver
[2021-05-16 20:12:15,715] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 319) (764b1db4ecfd, executor driver, partition 133, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,715] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 316) in 174 ms on 764b1db4ecfd (executor driver) (130/186)
21/05/16 23:12:15 INFO Executor: Running task 133.0 in stage 1.0 (TID 319)
[2021-05-16 20:12:15,837] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 131.0 in stage 1.0 (TID 317). 1886 bytes result sent to driver
[2021-05-16 20:12:15,839] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 320) (764b1db4ecfd, executor driver, partition 134, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,840] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 317) in 180 ms on 764b1db4ecfd (executor driver) (131/186)
[2021-05-16 20:12:15,842] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 134.0 in stage 1.0 (TID 320)
[2021-05-16 20:12:15,866] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 132.0 in stage 1.0 (TID 318). 1886 bytes result sent to driver
[2021-05-16 20:12:15,866] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 321) (764b1db4ecfd, executor driver, partition 135, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,867] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 318) in 173 ms on 764b1db4ecfd (executor driver) (132/186)
[2021-05-16 20:12:15,868] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 135.0 in stage 1.0 (TID 321)
[2021-05-16 20:12:15,899] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Finished task 133.0 in stage 1.0 (TID 319). 1886 bytes result sent to driver
[2021-05-16 20:12:15,901] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 322) (764b1db4ecfd, executor driver, partition 136, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:15,902] {docker.py:276} INFO - 21/05/16 23:12:15 INFO Executor: Running task 136.0 in stage 1.0 (TID 322)
[2021-05-16 20:12:15,903] {docker.py:276} INFO - 21/05/16 23:12:15 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 319) in 187 ms on 764b1db4ecfd (executor driver) (133/186)
[2021-05-16 20:12:16,019] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 134.0 in stage 1.0 (TID 320). 1843 bytes result sent to driver
[2021-05-16 20:12:16,022] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 323) (764b1db4ecfd, executor driver, partition 137, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,023] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 320) in 183 ms on 764b1db4ecfd (executor driver) (134/186)
[2021-05-16 20:12:16,024] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 137.0 in stage 1.0 (TID 323)
[2021-05-16 20:12:16,037] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 135.0 in stage 1.0 (TID 321). 1843 bytes result sent to driver
[2021-05-16 20:12:16,038] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 324) (764b1db4ecfd, executor driver, partition 138, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,039] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 321) in 173 ms on 764b1db4ecfd (executor driver) (135/186)
[2021-05-16 20:12:16,040] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 138.0 in stage 1.0 (TID 324)
[2021-05-16 20:12:16,077] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 136.0 in stage 1.0 (TID 322). 1843 bytes result sent to driver
[2021-05-16 20:12:16,079] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 325) (764b1db4ecfd, executor driver, partition 139, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,080] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 139.0 in stage 1.0 (TID 325)
[2021-05-16 20:12:16,080] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 322) in 180 ms on 764b1db4ecfd (executor driver) (136/186)
[2021-05-16 20:12:16,200] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 137.0 in stage 1.0 (TID 323). 1843 bytes result sent to driver
[2021-05-16 20:12:16,201] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 326) (764b1db4ecfd, executor driver, partition 140, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,202] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 323) in 182 ms on 764b1db4ecfd (executor driver) (137/186)
[2021-05-16 20:12:16,203] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 140.0 in stage 1.0 (TID 326)
[2021-05-16 20:12:16,209] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 138.0 in stage 1.0 (TID 324). 1843 bytes result sent to driver
[2021-05-16 20:12:16,210] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 128.0 in stage 1.0 (TID 314). 1886 bytes result sent to driver
[2021-05-16 20:12:16,211] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 327) (764b1db4ecfd, executor driver, partition 141, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,212] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 141.0 in stage 1.0 (TID 327)
[2021-05-16 20:12:16,212] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 328) (764b1db4ecfd, executor driver, partition 142, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,213] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 314) in 691 ms on 764b1db4ecfd (executor driver) (138/186)
21/05/16 23:12:16 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 324) in 176 ms on 764b1db4ecfd (executor driver) (139/186)
[2021-05-16 20:12:16,214] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 142.0 in stage 1.0 (TID 328)
[2021-05-16 20:12:16,257] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 139.0 in stage 1.0 (TID 325). 1843 bytes result sent to driver
[2021-05-16 20:12:16,258] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 329) (764b1db4ecfd, executor driver, partition 143, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,259] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 325) in 181 ms on 764b1db4ecfd (executor driver) (140/186)
[2021-05-16 20:12:16,260] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 143.0 in stage 1.0 (TID 329)
[2021-05-16 20:12:16,383] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 140.0 in stage 1.0 (TID 326). 1843 bytes result sent to driver
[2021-05-16 20:12:16,385] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 330) (764b1db4ecfd, executor driver, partition 144, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,387] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 141.0 in stage 1.0 (TID 327). 1843 bytes result sent to driver
[2021-05-16 20:12:16,388] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 326) in 187 ms on 764b1db4ecfd (executor driver) (141/186)
[2021-05-16 20:12:16,389] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 144.0 in stage 1.0 (TID 330)
[2021-05-16 20:12:16,389] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 331) (764b1db4ecfd, executor driver, partition 145, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,390] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 327) in 179 ms on 764b1db4ecfd (executor driver) (142/186)
[2021-05-16 20:12:16,391] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 145.0 in stage 1.0 (TID 331)
[2021-05-16 20:12:16,393] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 142.0 in stage 1.0 (TID 328). 1843 bytes result sent to driver
[2021-05-16 20:12:16,395] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 332) (764b1db4ecfd, executor driver, partition 146, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,395] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 328) in 184 ms on 764b1db4ecfd (executor driver) (143/186)
[2021-05-16 20:12:16,396] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 146.0 in stage 1.0 (TID 332)
[2021-05-16 20:12:16,428] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 143.0 in stage 1.0 (TID 329). 1843 bytes result sent to driver
[2021-05-16 20:12:16,429] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 333) (764b1db4ecfd, executor driver, partition 147, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,430] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 147.0 in stage 1.0 (TID 333)
[2021-05-16 20:12:16,430] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 329) in 173 ms on 764b1db4ecfd (executor driver) (144/186)
[2021-05-16 20:12:16,566] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 145.0 in stage 1.0 (TID 331). 1843 bytes result sent to driver
[2021-05-16 20:12:16,567] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 334) (764b1db4ecfd, executor driver, partition 148, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,569] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 148.0 in stage 1.0 (TID 334)
[2021-05-16 20:12:16,569] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 331) in 180 ms on 764b1db4ecfd (executor driver) (145/186)
[2021-05-16 20:12:16,571] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 144.0 in stage 1.0 (TID 330). 1843 bytes result sent to driver
[2021-05-16 20:12:16,572] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 335) (764b1db4ecfd, executor driver, partition 149, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,573] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 330) in 189 ms on 764b1db4ecfd (executor driver) (146/186)
[2021-05-16 20:12:16,573] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 146.0 in stage 1.0 (TID 332). 1843 bytes result sent to driver
[2021-05-16 20:12:16,574] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 149.0 in stage 1.0 (TID 335)
[2021-05-16 20:12:16,575] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 336) (764b1db4ecfd, executor driver, partition 150, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,576] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 332) in 182 ms on 764b1db4ecfd (executor driver) (147/186)
[2021-05-16 20:12:16,577] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 150.0 in stage 1.0 (TID 336)
[2021-05-16 20:12:16,600] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 147.0 in stage 1.0 (TID 333). 1843 bytes result sent to driver
[2021-05-16 20:12:16,602] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 337) (764b1db4ecfd, executor driver, partition 151, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,603] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 333) in 174 ms on 764b1db4ecfd (executor driver) (148/186)
[2021-05-16 20:12:16,603] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 151.0 in stage 1.0 (TID 337)
[2021-05-16 20:12:16,748] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 148.0 in stage 1.0 (TID 334). 1886 bytes result sent to driver
21/05/16 23:12:16 INFO Executor: Finished task 149.0 in stage 1.0 (TID 335). 1886 bytes result sent to driver
[2021-05-16 20:12:16,750] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 338) (764b1db4ecfd, executor driver, partition 152, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,751] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 334) in 184 ms on 764b1db4ecfd (executor driver) (149/186)
[2021-05-16 20:12:16,752] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 152.0 in stage 1.0 (TID 338)
[2021-05-16 20:12:16,753] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 339) (764b1db4ecfd, executor driver, partition 153, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,753] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 335) in 181 ms on 764b1db4ecfd (executor driver) (150/186)
21/05/16 23:12:16 INFO Executor: Running task 153.0 in stage 1.0 (TID 339)
[2021-05-16 20:12:16,756] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 150.0 in stage 1.0 (TID 336). 1886 bytes result sent to driver
[2021-05-16 20:12:16,756] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 340) (764b1db4ecfd, executor driver, partition 154, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,757] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 154.0 in stage 1.0 (TID 340)
[2021-05-16 20:12:16,758] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 336) in 183 ms on 764b1db4ecfd (executor driver) (151/186)
[2021-05-16 20:12:16,778] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 151.0 in stage 1.0 (TID 337). 1886 bytes result sent to driver
[2021-05-16 20:12:16,779] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 341) (764b1db4ecfd, executor driver, partition 155, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,780] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 155.0 in stage 1.0 (TID 341)
[2021-05-16 20:12:16,781] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 337) in 180 ms on 764b1db4ecfd (executor driver) (152/186)
[2021-05-16 20:12:16,924] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 154.0 in stage 1.0 (TID 340). 1843 bytes result sent to driver
[2021-05-16 20:12:16,925] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 342) (764b1db4ecfd, executor driver, partition 156, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,926] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 340) in 170 ms on 764b1db4ecfd (executor driver) (153/186)
[2021-05-16 20:12:16,927] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 156.0 in stage 1.0 (TID 342)
[2021-05-16 20:12:16,928] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 153.0 in stage 1.0 (TID 339). 1843 bytes result sent to driver
[2021-05-16 20:12:16,929] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 343) (764b1db4ecfd, executor driver, partition 157, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,929] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 339) in 179 ms on 764b1db4ecfd (executor driver) (154/186)
21/05/16 23:12:16 INFO Executor: Running task 157.0 in stage 1.0 (TID 343)
[2021-05-16 20:12:16,933] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 152.0 in stage 1.0 (TID 338). 1843 bytes result sent to driver
[2021-05-16 20:12:16,935] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 344) (764b1db4ecfd, executor driver, partition 158, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,936] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 158.0 in stage 1.0 (TID 344)
[2021-05-16 20:12:16,937] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 338) in 187 ms on 764b1db4ecfd (executor driver) (155/186)
[2021-05-16 20:12:16,947] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Finished task 155.0 in stage 1.0 (TID 341). 1843 bytes result sent to driver
[2021-05-16 20:12:16,948] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 345) (764b1db4ecfd, executor driver, partition 159, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:16,949] {docker.py:276} INFO - 21/05/16 23:12:16 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 341) in 170 ms on 764b1db4ecfd (executor driver) (156/186)
[2021-05-16 20:12:16,949] {docker.py:276} INFO - 21/05/16 23:12:16 INFO Executor: Running task 159.0 in stage 1.0 (TID 345)
[2021-05-16 20:12:17,109] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 157.0 in stage 1.0 (TID 343). 1843 bytes result sent to driver
[2021-05-16 20:12:17,110] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 346) (764b1db4ecfd, executor driver, partition 160, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,111] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 160.0 in stage 1.0 (TID 346)
[2021-05-16 20:12:17,112] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 343) in 183 ms on 764b1db4ecfd (executor driver) (157/186)
[2021-05-16 20:12:17,112] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 156.0 in stage 1.0 (TID 342). 1843 bytes result sent to driver
[2021-05-16 20:12:17,112] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 158.0 in stage 1.0 (TID 344). 1843 bytes result sent to driver
[2021-05-16 20:12:17,113] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 347) (764b1db4ecfd, executor driver, partition 161, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,113] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 342) in 189 ms on 764b1db4ecfd (executor driver) (158/186)
[2021-05-16 20:12:17,115] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 348) (764b1db4ecfd, executor driver, partition 162, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,116] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 344) in 181 ms on 764b1db4ecfd (executor driver) (159/186)
21/05/16 23:12:17 INFO Executor: Running task 161.0 in stage 1.0 (TID 347)
[2021-05-16 20:12:17,117] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 162.0 in stage 1.0 (TID 348)
[2021-05-16 20:12:17,119] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 159.0 in stage 1.0 (TID 345). 1843 bytes result sent to driver
[2021-05-16 20:12:17,120] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 349) (764b1db4ecfd, executor driver, partition 163, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,122] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 345) in 174 ms on 764b1db4ecfd (executor driver) (160/186)
[2021-05-16 20:12:17,123] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 163.0 in stage 1.0 (TID 349)
[2021-05-16 20:12:17,287] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 160.0 in stage 1.0 (TID 346). 1843 bytes result sent to driver
[2021-05-16 20:12:17,290] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 350) (764b1db4ecfd, executor driver, partition 164, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,291] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 346) in 180 ms on 764b1db4ecfd (executor driver) (161/186)
[2021-05-16 20:12:17,292] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 164.0 in stage 1.0 (TID 350)
[2021-05-16 20:12:17,302] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 161.0 in stage 1.0 (TID 347). 1843 bytes result sent to driver
[2021-05-16 20:12:17,303] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 351) (764b1db4ecfd, executor driver, partition 165, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,303] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 165.0 in stage 1.0 (TID 351)
[2021-05-16 20:12:17,304] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 347) in 183 ms on 764b1db4ecfd (executor driver) (162/186)
[2021-05-16 20:12:17,305] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 162.0 in stage 1.0 (TID 348). 1843 bytes result sent to driver
21/05/16 23:12:17 INFO Executor: Finished task 163.0 in stage 1.0 (TID 349). 1843 bytes result sent to driver
21/05/16 23:12:17 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 352) (764b1db4ecfd, executor driver, partition 166, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,305] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 166.0 in stage 1.0 (TID 352)
[2021-05-16 20:12:17,306] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 348) in 186 ms on 764b1db4ecfd (executor driver) (163/186)
[2021-05-16 20:12:17,307] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 353) (764b1db4ecfd, executor driver, partition 167, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,307] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 167.0 in stage 1.0 (TID 353)
21/05/16 23:12:17 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 349) in 184 ms on 764b1db4ecfd (executor driver) (164/186)
[2021-05-16 20:12:17,468] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 164.0 in stage 1.0 (TID 350). 1843 bytes result sent to driver
[2021-05-16 20:12:17,469] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 354) (764b1db4ecfd, executor driver, partition 168, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,470] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 350) in 182 ms on 764b1db4ecfd (executor driver) (165/186)
[2021-05-16 20:12:17,470] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 168.0 in stage 1.0 (TID 354)
[2021-05-16 20:12:17,471] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 166.0 in stage 1.0 (TID 352). 1843 bytes result sent to driver
[2021-05-16 20:12:17,481] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 165.0 in stage 1.0 (TID 351). 1886 bytes result sent to driver
[2021-05-16 20:12:17,482] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 167.0 in stage 1.0 (TID 353). 1886 bytes result sent to driver
[2021-05-16 20:12:17,482] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 355) (764b1db4ecfd, executor driver, partition 169, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,483] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 352) in 183 ms on 764b1db4ecfd (executor driver) (166/186)
21/05/16 23:12:17 INFO Executor: Running task 169.0 in stage 1.0 (TID 355)
[2021-05-16 20:12:17,484] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 351) in 191 ms on 764b1db4ecfd (executor driver) (167/186)
[2021-05-16 20:12:17,485] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 356) (764b1db4ecfd, executor driver, partition 170, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,486] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 357) (764b1db4ecfd, executor driver, partition 171, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,487] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 353) in 185 ms on 764b1db4ecfd (executor driver) (168/186)
[2021-05-16 20:12:17,488] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 171.0 in stage 1.0 (TID 357)
[2021-05-16 20:12:17,488] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 170.0 in stage 1.0 (TID 356)
[2021-05-16 20:12:17,655] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 168.0 in stage 1.0 (TID 354). 1886 bytes result sent to driver
[2021-05-16 20:12:17,656] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 358) (764b1db4ecfd, executor driver, partition 172, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,657] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 354) in 189 ms on 764b1db4ecfd (executor driver) (169/186)
[2021-05-16 20:12:17,658] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 172.0 in stage 1.0 (TID 358)
[2021-05-16 20:12:17,659] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 170.0 in stage 1.0 (TID 356). 1843 bytes result sent to driver
[2021-05-16 20:12:17,660] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 359) (764b1db4ecfd, executor driver, partition 173, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,661] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 356) in 177 ms on 764b1db4ecfd (executor driver) (170/186)
[2021-05-16 20:12:17,662] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 173.0 in stage 1.0 (TID 359)
[2021-05-16 20:12:17,664] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 169.0 in stage 1.0 (TID 355). 1843 bytes result sent to driver
[2021-05-16 20:12:17,665] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 360) (764b1db4ecfd, executor driver, partition 174, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,668] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 355) in 184 ms on 764b1db4ecfd (executor driver) (171/186)
[2021-05-16 20:12:17,668] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 174.0 in stage 1.0 (TID 360)
[2021-05-16 20:12:17,670] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 171.0 in stage 1.0 (TID 357). 1843 bytes result sent to driver
[2021-05-16 20:12:17,671] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 361) (764b1db4ecfd, executor driver, partition 175, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,672] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 175.0 in stage 1.0 (TID 361)
21/05/16 23:12:17 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 357) in 186 ms on 764b1db4ecfd (executor driver) (172/186)
[2021-05-16 20:12:17,844] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 174.0 in stage 1.0 (TID 360). 1843 bytes result sent to driver
[2021-05-16 20:12:17,845] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 173.0 in stage 1.0 (TID 359). 1843 bytes result sent to driver
21/05/16 23:12:17 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 362) (764b1db4ecfd, executor driver, partition 176, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:17 INFO Executor: Finished task 172.0 in stage 1.0 (TID 358). 1843 bytes result sent to driver
[2021-05-16 20:12:17,846] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 360) in 181 ms on 764b1db4ecfd (executor driver) (173/186)
[2021-05-16 20:12:17,848] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 363) (764b1db4ecfd, executor driver, partition 177, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
21/05/16 23:12:17 INFO Executor: Running task 176.0 in stage 1.0 (TID 362)
[2021-05-16 20:12:17,849] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Finished task 175.0 in stage 1.0 (TID 361). 1843 bytes result sent to driver
[2021-05-16 20:12:17,850] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 359) in 189 ms on 764b1db4ecfd (executor driver) (174/186)
[2021-05-16 20:12:17,851] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 177.0 in stage 1.0 (TID 363)
21/05/16 23:12:17 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 364) (764b1db4ecfd, executor driver, partition 178, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,852] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 178.0 in stage 1.0 (TID 364)
[2021-05-16 20:12:17,852] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 365) (764b1db4ecfd, executor driver, partition 179, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:17,854] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 358) in 199 ms on 764b1db4ecfd (executor driver) (175/186)
[2021-05-16 20:12:17,855] {docker.py:276} INFO - 21/05/16 23:12:17 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 361) in 183 ms on 764b1db4ecfd (executor driver) (176/186)
[2021-05-16 20:12:17,855] {docker.py:276} INFO - 21/05/16 23:12:17 INFO Executor: Running task 179.0 in stage 1.0 (TID 365)
[2021-05-16 20:12:18,023] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 177.0 in stage 1.0 (TID 363). 1843 bytes result sent to driver
[2021-05-16 20:12:18,025] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 179.0 in stage 1.0 (TID 365). 1843 bytes result sent to driver
21/05/16 23:12:18 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 366) (764b1db4ecfd, executor driver, partition 180, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,026] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 180.0 in stage 1.0 (TID 366)
[2021-05-16 20:12:18,027] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 367) (764b1db4ecfd, executor driver, partition 181, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,027] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 365) in 175 ms on 764b1db4ecfd (executor driver) (177/186)
[2021-05-16 20:12:18,028] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 363) in 180 ms on 764b1db4ecfd (executor driver) (178/186)
[2021-05-16 20:12:18,028] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 181.0 in stage 1.0 (TID 367)
[2021-05-16 20:12:18,030] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 176.0 in stage 1.0 (TID 362). 1843 bytes result sent to driver
[2021-05-16 20:12:18,031] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 368) (764b1db4ecfd, executor driver, partition 182, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,032] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 182.0 in stage 1.0 (TID 368)
[2021-05-16 20:12:18,032] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 362) in 187 ms on 764b1db4ecfd (executor driver) (179/186)
[2021-05-16 20:12:18,034] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 178.0 in stage 1.0 (TID 364). 1843 bytes result sent to driver
[2021-05-16 20:12:18,036] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 369) (764b1db4ecfd, executor driver, partition 183, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,040] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 183.0 in stage 1.0 (TID 369)
[2021-05-16 20:12:18,041] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 364) in 190 ms on 764b1db4ecfd (executor driver) (180/186)
[2021-05-16 20:12:18,206] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 182.0 in stage 1.0 (TID 368). 1843 bytes result sent to driver
[2021-05-16 20:12:18,207] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 370) (764b1db4ecfd, executor driver, partition 184, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,209] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 181.0 in stage 1.0 (TID 367). 1843 bytes result sent to driver
[2021-05-16 20:12:18,211] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 184.0 in stage 1.0 (TID 370)
[2021-05-16 20:12:18,212] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 183.0 in stage 1.0 (TID 369). 1843 bytes result sent to driver
[2021-05-16 20:12:18,213] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 180.0 in stage 1.0 (TID 366). 1843 bytes result sent to driver
21/05/16 23:12:18 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 371) (764b1db4ecfd, executor driver, partition 185, PROCESS_LOCAL, 4560 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:18,214] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Running task 185.0 in stage 1.0 (TID 371)
[2021-05-16 20:12:18,214] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 368) in 184 ms on 764b1db4ecfd (executor driver) (181/186)
[2021-05-16 20:12:18,215] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 367) in 190 ms on 764b1db4ecfd (executor driver) (182/186)
[2021-05-16 20:12:18,216] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 366) in 192 ms on 764b1db4ecfd (executor driver) (183/186)
[2021-05-16 20:12:18,216] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 369) in 180 ms on 764b1db4ecfd (executor driver) (184/186)
[2021-05-16 20:12:18,391] {docker.py:276} INFO - 21/05/16 23:12:18 INFO Executor: Finished task 185.0 in stage 1.0 (TID 371). 1843 bytes result sent to driver
21/05/16 23:12:18 INFO Executor: Finished task 184.0 in stage 1.0 (TID 370). 1843 bytes result sent to driver
[2021-05-16 20:12:18,395] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 371) in 181 ms on 764b1db4ecfd (executor driver) (185/186)
[2021-05-16 20:12:18,396] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 370) in 186 ms on 764b1db4ecfd (executor driver) (186/186)
[2021-05-16 20:12:18,396] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-16 20:12:18,397] {docker.py:276} INFO - 21/05/16 23:12:18 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 8.813 s
[2021-05-16 20:12:18,398] {docker.py:276} INFO - 21/05/16 23:12:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 20:12:18,398] {docker.py:276} INFO - 21/05/16 23:12:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-16 20:12:18,406] {docker.py:276} INFO - 21/05/16 23:12:18 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 8.835900 s
[2021-05-16 20:12:18,411] {docker.py:276} INFO - 21/05/16 23:12:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 764b1db4ecfd:38389 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-16 20:12:18,445] {docker.py:276} INFO - 21/05/16 23:12:18 INFO InMemoryFileIndex: It took 8940 ms to list leaf files for 186 paths.
[2021-05-16 20:12:21,412] {docker.py:276} INFO - 21/05/16 23:12:21 INFO FileSourceStrategy: Pushed Filters:
[2021-05-16 20:12:21,418] {docker.py:276} INFO - 21/05/16 23:12:21 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-16 20:12:21,424] {docker.py:276} INFO - 21/05/16 23:12:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 20:12:21,942] {docker.py:276} INFO - 21/05/16 23:12:21 INFO CodeGenerator: Code generated in 284.7452 ms
[2021-05-16 20:12:21,956] {docker.py:276} INFO - 21/05/16 23:12:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.6 KiB, free 934.2 MiB)
[2021-05-16 20:12:21,974] {docker.py:276} INFO - 21/05/16 23:12:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-16 20:12:21,975] {docker.py:276} INFO - 21/05/16 23:12:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 764b1db4ecfd:38389 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 20:12:21,977] {docker.py:276} INFO - 21/05/16 23:12:21 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:21,997] {docker.py:276} INFO - 21/05/16 23:12:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 20:12:22,110] {docker.py:276} INFO - 21/05/16 23:12:22 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:22,112] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/16 23:12:22 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
21/05/16 23:12:22 INFO DAGScheduler: Parents of final stage: List()
[2021-05-16 20:12:22,112] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Missing parents: List()
[2021-05-16 20:12:22,113] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 20:12:22,138] {docker.py:276} INFO - 21/05/16 23:12:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-16 20:12:22,155] {docker.py:276} INFO - 21/05/16 23:12:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-16 20:12:22,156] {docker.py:276} INFO - 21/05/16 23:12:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 764b1db4ecfd:38389 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-16 20:12:22,157] {docker.py:276} INFO - 21/05/16 23:12:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-16 20:12:22,158] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/16 23:12:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-16 20:12:22,162] {docker.py:276} INFO - 21/05/16 23:12:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 372) (764b1db4ecfd, executor driver, partition 0, PROCESS_LOCAL, 8315 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:22,162] {docker.py:276} INFO - 21/05/16 23:12:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 372)
[2021-05-16 20:12:22,285] {docker.py:276} INFO - 21/05/16 23:12:22 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:22,313] {docker.py:276} INFO - 21/05/16 23:12:22 INFO CodeGenerator: Code generated in 20.4435 ms
[2021-05-16 20:12:22,728] {docker.py:276} INFO - 21/05/16 23:12:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 372). 1564 bytes result sent to driver
[2021-05-16 20:12:22,729] {docker.py:276} INFO - 21/05/16 23:12:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 372) in 570 ms on 764b1db4ecfd (executor driver) (1/1)
[2021-05-16 20:12:22,729] {docker.py:276} INFO - 21/05/16 23:12:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-16 20:12:22,730] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.614 s
[2021-05-16 20:12:22,731] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 20:12:22,732] {docker.py:276} INFO - 21/05/16 23:12:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-16 20:12:22,732] {docker.py:276} INFO - 21/05/16 23:12:22 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.622765 s
[2021-05-16 20:12:22,766] {docker.py:276} INFO - 21/05/16 23:12:22 INFO CodeGenerator: Code generated in 14.8233 ms
[2021-05-16 20:12:22,844] {docker.py:276} INFO - 21/05/16 23:12:22 INFO FileSourceStrategy: Pushed Filters: 
21/05/16 23:12:22 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 20:12:22,844] {docker.py:276} INFO - 21/05/16 23:12:22 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-16 20:12:22,851] {docker.py:276} INFO - 21/05/16 23:12:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.6 KiB, free 934.0 MiB)
[2021-05-16 20:12:22,875] {docker.py:276} INFO - 21/05/16 23:12:22 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 764b1db4ecfd:38389 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-16 20:12:22,877] {docker.py:276} INFO - 21/05/16 23:12:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-16 20:12:22,878] {docker.py:276} INFO - 21/05/16 23:12:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 764b1db4ecfd:38389 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 20:12:22,879] {docker.py:276} INFO - 21/05/16 23:12:22 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:22,884] {docker.py:276} INFO - 21/05/16 23:12:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 20:12:23,409] {docker.py:276} INFO - 21/05/16 23:12:23 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 764b1db4ecfd:38389 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 20:12:23,535] {docker.py:276} INFO - 21/05/16 23:12:23 INFO FileSourceStrategy: Pushed Filters: 
21/05/16 23:12:23 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-16 20:12:23,535] {docker.py:276} INFO - 21/05/16 23:12:23 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-16 20:12:24,197] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:24,202] {docker.py:276} INFO - 21/05/16 23:12:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:24,203] {docker.py:276} INFO - 21/05/16 23:12:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244394340570080125175_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244394340570080125175_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244394340570080125175_0000}; taskId=attempt_202105162312244394340570080125175_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50596997}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:24,203] {docker.py:276} INFO - 21/05/16 23:12:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:24,242] {docker.py:276} INFO - 21/05/16 23:12:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 20:12:24,343] {docker.py:276} INFO - 21/05/16 23:12:24 INFO CodeGenerator: Code generated in 66.5893 ms
[2021-05-16 20:12:24,345] {docker.py:276} INFO - 21/05/16 23:12:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-16 20:12:24,396] {docker.py:276} INFO - 21/05/16 23:12:24 INFO CodeGenerator: Code generated in 41.6338 ms
[2021-05-16 20:12:24,400] {docker.py:276} INFO - 21/05/16 23:12:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.5 KiB, free 934.0 MiB)
[2021-05-16 20:12:24,432] {docker.py:276} INFO - 21/05/16 23:12:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-16 20:12:24,434] {docker.py:276} INFO - 21/05/16 23:12:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 764b1db4ecfd:38389 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-16 20:12:24,435] {docker.py:276} INFO - 21/05/16 23:12:24 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:24,446] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-16 20:12:24,607] {docker.py:276} INFO - 21/05/16 23:12:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-16 20:12:24,611] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-16 20:12:24,614] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
21/05/16 23:12:24 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-16 20:12:24,614] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-16 20:12:24,617] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-16 20:12:24,619] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 20:12:24,633] {docker.py:276} INFO - 21/05/16 23:12:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 28.0 KiB, free 934.0 MiB)
[2021-05-16 20:12:24,647] {docker.py:276} INFO - 21/05/16 23:12:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.0 MiB)
[2021-05-16 20:12:24,648] {docker.py:276} INFO - 21/05/16 23:12:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 764b1db4ecfd:38389 (size: 12.0 KiB, free: 934.3 MiB)
[2021-05-16 20:12:24,650] {docker.py:276} INFO - 21/05/16 23:12:24 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-16 20:12:24,652] {docker.py:276} INFO - 21/05/16 23:12:24 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
21/05/16 23:12:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 6 tasks resource profile 0
[2021-05-16 20:12:24,655] {docker.py:276} INFO - 21/05/16 23:12:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 373) (764b1db4ecfd, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:24,656] {docker.py:276} INFO - 21/05/16 23:12:24 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 374) (764b1db4ecfd, executor driver, partition 1, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:24,657] {docker.py:276} INFO - 21/05/16 23:12:24 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 375) (764b1db4ecfd, executor driver, partition 2, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:24,658] {docker.py:276} INFO - 21/05/16 23:12:24 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 376) (764b1db4ecfd, executor driver, partition 3, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:24,659] {docker.py:276} INFO - 21/05/16 23:12:24 INFO Executor: Running task 1.0 in stage 3.0 (TID 374)
21/05/16 23:12:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 373)
[2021-05-16 20:12:24,660] {docker.py:276} INFO - 21/05/16 23:12:24 INFO Executor: Running task 2.0 in stage 3.0 (TID 375)
[2021-05-16 20:12:24,662] {docker.py:276} INFO - 21/05/16 23:12:24 INFO Executor: Running task 3.0 in stage 3.0 (TID 376)
[2021-05-16 20:12:24,781] {docker.py:276} INFO - 21/05/16 23:12:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 764b1db4ecfd:38389 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-16 20:12:24,805] {docker.py:276} INFO - 21/05/16 23:12:24 INFO CodeGenerator: Code generated in 52.326 ms
[2021-05-16 20:12:24,842] {docker.py:276} INFO - 21/05/16 23:12:24 INFO CodeGenerator: Code generated in 14.9577 ms
[2021-05-16 20:12:24,869] {docker.py:276} INFO - 21/05/16 23:12:24 INFO CodeGenerator: Code generated in 17.6682 ms
[2021-05-16 20:12:24,887] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621154445_to_1621156245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:24,888] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621096845_to_1621098645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:24,891] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:24,895] {docker.py:276} INFO - 21/05/16 23:12:24 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621150845_to_1621152645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:25,759] {docker.py:276} INFO - 21/05/16 23:12:25 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621095045_to_1621096845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:26,222] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621156245_to_1621158045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:26,230] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621098645_to_1621100445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:26,265] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621096845_to_1621098645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:26,272] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621152645_to_1621154445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:26,577] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621100445_to_1621102245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:26,658] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621098645_to_1621100445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:26,723] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621158045_to_1621159845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:26,862] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621154445_to_1621156245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:26,935] {docker.py:276} INFO - 21/05/16 23:12:26 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621102245_to_1621104045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,022] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621100445_to_1621102245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:27,072] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621159845_to_1621161645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,222] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621156245_to_1621158045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:27,306] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621104045_to_1621105845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,401] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621102245_to_1621104045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:27,412] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621161645_to_1621163445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,577] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621158045_to_1621159845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:27,694] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621105845_to_1621107645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,759] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621163445_to_1621165245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:27,802] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621104045_to_1621105845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:27,947] {docker.py:276} INFO - 21/05/16 23:12:27 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621159845_to_1621161645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:28,043] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621107645_to_1621109445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,146] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621165245_to_1621167045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,196] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621105845_to_1621107645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:28,304] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621161645_to_1621163445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:28,388] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621109445_to_1621111245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,485] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621167045_to_1621168845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,559] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621107645_to_1621109445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:28,704] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621163445_to_1621165245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:28,753] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621111245_to_1621113045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,828] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621168845_to_1621170645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:28,937] {docker.py:276} INFO - 21/05/16 23:12:28 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621109445_to_1621111245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,065] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621165245_to_1621167045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,118] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621113045_to_1621114845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:29,159] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621170645_to_1621172445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:29,309] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621111245_to_1621113045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,453] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621167045_to_1621168845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,470] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621114845_to_1621116645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:29,512] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621172445_to_1621174245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:29,686] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621113045_to_1621114845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,807] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621168845_to_1621170645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:29,824] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621116645_to_1621118445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:29,869] {docker.py:276} INFO - 21/05/16 23:12:29 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621174245_to_1621176045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,075] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621114845_to_1621116645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:30,159] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621118445_to_1621120245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,188] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621170645_to_1621172445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:30,210] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621176045_to_1621177845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,463] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621116645_to_1621118445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:30,497] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621120245_to_1621122045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,556] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621172445_to_1621174245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:30,578] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621177845_to_1621179645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,827] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621118445_to_1621120245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:30,865] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621122045_to_1621123845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:30,916] {docker.py:276} INFO - 21/05/16 23:12:30 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621179645_to_1621181445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,136] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621174245_to_1621176045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:31,189] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621120245_to_1621122045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:31,202] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621123845_to_1621125645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,266] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621181445_to_1621183245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,497] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621176045_to_1621177845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:31,575] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621125645_to_1621127445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,601] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621183245_to_1621185045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,706] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621122045_to_1621123845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:31,871] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621177845_to_1621179645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:31,926] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621127445_to_1621129245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:31,948] {docker.py:276} INFO - 21/05/16 23:12:31 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621185045_to_1621186845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:32,082] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621123845_to_1621125645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:32,223] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621179645_to_1621181445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:32,287] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621186845_to_1621188645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:32,293] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621129245_to_1621131045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:32,454] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621125645_to_1621127445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:32,562] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621181445_to_1621183245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:32,653] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621188645_to_1621190445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:32,804] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621131045_to_1621132845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:32,819] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621127445_to_1621129245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:32,903] {docker.py:276} INFO - 21/05/16 23:12:32 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621183245_to_1621185045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,013] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621190445_to_1621192245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,168] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621132845_to_1621134645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,202] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621129245_to_1621131045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,244] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621185045_to_1621186845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,369] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621192245_to_1621194045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,529] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621134645_to_1621136445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,554] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621131045_to_1621132845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,583] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621186845_to_1621188645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,738] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621194045_to_1621195845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,908] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621136445_to_1621138245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:33,961] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621132845_to_1621134645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:33,966] {docker.py:276} INFO - 21/05/16 23:12:33 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621188645_to_1621190445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:34,117] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621195845_to_1621197645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:34,288] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621138245_to_1621140045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:34,317] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621134645_to_1621136445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:34,326] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621190445_to_1621192245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:34,529] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:34,674] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621140045_to_1621141845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:34,675] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621136445_to_1621138245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:34,690] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621192245_to_1621194045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:34,916] {docker.py:276} INFO - 21/05/16 23:12:34 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621095045_to_1621096845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,036] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621138245_to_1621140045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,046] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621141845_to_1621143645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,056] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621194045_to_1621195845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,280] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621096845_to_1621098645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,421] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621143645_to_1621145445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,444] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621140045_to_1621141845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,447] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621195845_to_1621197645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,618] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621098645_to_1621100445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,784] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621141845_to_1621143645.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,798] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621145445_to_1621147245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:35,806] {docker.py:276} INFO - 21/05/16 23:12:35 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621199445_to_1621201245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:35,983] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621100445_to_1621102245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:36,139] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621143645_to_1621145445.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:36,157] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621197645_to_1621199445.csv, range: 0-111709, partition values: [empty row]
[2021-05-16 20:12:36,161] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621147245_to_1621149045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:36,324] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621102245_to_1621104045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:36,483] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621145445_to_1621147245.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:36,520] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621203045_to_1621204845.csv, range: 0-111631, partition values: [empty row]
[2021-05-16 20:12:36,681] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621149045_to_1621150845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:36,693] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621104045_to_1621105845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:36,824] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621147245_to_1621149045.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:36,854] {docker.py:276} INFO - 21/05/16 23:12:36 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621201245_to_1621203045.csv, range: 0-111433, partition values: [empty row]
[2021-05-16 20:12:37,039] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621150845_to_1621152645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,042] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621105845_to_1621107645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,191] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-16_20_10_45/from_1621149045_to_1621150845.csv, range: 0-111710, partition values: [empty row]
[2021-05-16 20:12:37,207] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621093245_to_1621095045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,395] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621152645_to_1621154445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,588] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621095045_to_1621096845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,896] {docker.py:276} INFO - 21/05/16 23:12:37 INFO Executor: Finished task 3.0 in stage 3.0 (TID 376). 2722 bytes result sent to driver
[2021-05-16 20:12:37,898] {docker.py:276} INFO - 21/05/16 23:12:37 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 377) (764b1db4ecfd, executor driver, partition 4, PROCESS_LOCAL, 8304 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:37,900] {docker.py:276} INFO - 21/05/16 23:12:37 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 376) in 13222 ms on 764b1db4ecfd (executor driver) (1/6)
[2021-05-16 20:12:37,904] {docker.py:276} INFO - 21/05/16 23:12:37 INFO Executor: Running task 4.0 in stage 3.0 (TID 377)
[2021-05-16 20:12:37,923] {docker.py:276} INFO - 21/05/16 23:12:37 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621107645_to_1621109445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:37,957] {docker.py:276} INFO - 21/05/16 23:12:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 373). 2679 bytes result sent to driver
[2021-05-16 20:12:37,958] {docker.py:276} INFO - 21/05/16 23:12:37 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 378) (764b1db4ecfd, executor driver, partition 5, PROCESS_LOCAL, 7644 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:37,959] {docker.py:276} INFO - 21/05/16 23:12:37 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 373) in 13285 ms on 764b1db4ecfd (executor driver) (2/6)
[2021-05-16 20:12:37,959] {docker.py:276} INFO - 21/05/16 23:12:37 INFO Executor: Running task 5.0 in stage 3.0 (TID 378)
[2021-05-16 20:12:37,999] {docker.py:276} INFO - 21/05/16 23:12:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621165245_to_1621167045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:38,136] {docker.py:276} INFO - 21/05/16 23:12:38 INFO Executor: Finished task 2.0 in stage 3.0 (TID 375). 2679 bytes result sent to driver
[2021-05-16 20:12:38,138] {docker.py:276} INFO - 21/05/16 23:12:38 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 375) in 13461 ms on 764b1db4ecfd (executor driver) (3/6)
[2021-05-16 20:12:38,225] {docker.py:276} INFO - 21/05/16 23:12:38 INFO Executor: Finished task 1.0 in stage 3.0 (TID 374). 2679 bytes result sent to driver
[2021-05-16 20:12:38,226] {docker.py:276} INFO - 21/05/16 23:12:38 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 374) in 13550 ms on 764b1db4ecfd (executor driver) (4/6)
[2021-05-16 20:12:38,270] {docker.py:276} INFO - 21/05/16 23:12:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621109445_to_1621111245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:38,358] {docker.py:276} INFO - 21/05/16 23:12:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621167045_to_1621168845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:38,681] {docker.py:276} INFO - 21/05/16 23:12:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621111245_to_1621113045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:38,698] {docker.py:276} INFO - 21/05/16 23:12:38 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621168845_to_1621170645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,022] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621113045_to_1621114845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,035] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621170645_to_1621172445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,368] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621114845_to_1621116645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,396] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621172445_to_1621174245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,719] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621116645_to_1621118445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:39,772] {docker.py:276} INFO - 21/05/16 23:12:39 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621174245_to_1621176045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,055] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621118445_to_1621120245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,151] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621176045_to_1621177845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,411] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621120245_to_1621122045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,494] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621177845_to_1621179645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,749] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621122045_to_1621123845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:40,865] {docker.py:276} INFO - 21/05/16 23:12:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621179645_to_1621181445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,164] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621123845_to_1621125645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,263] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621181445_to_1621183245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,516] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621125645_to_1621127445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,622] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621183245_to_1621185045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,855] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621127445_to_1621129245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:41,990] {docker.py:276} INFO - 21/05/16 23:12:41 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621185045_to_1621186845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:42,225] {docker.py:276} INFO - 21/05/16 23:12:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621129245_to_1621131045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:42,342] {docker.py:276} INFO - 21/05/16 23:12:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621186845_to_1621188645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:42,566] {docker.py:276} INFO - 21/05/16 23:12:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621131045_to_1621132845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:42,705] {docker.py:276} INFO - 21/05/16 23:12:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621188645_to_1621190445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:42,911] {docker.py:276} INFO - 21/05/16 23:12:42 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621132845_to_1621134645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:43,048] {docker.py:276} INFO - 21/05/16 23:12:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621190445_to_1621192245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:43,287] {docker.py:276} INFO - 21/05/16 23:12:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621134645_to_1621136445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:43,434] {docker.py:276} INFO - 21/05/16 23:12:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621192245_to_1621194045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:43,649] {docker.py:276} INFO - 21/05/16 23:12:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621136445_to_1621138245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:43,799] {docker.py:276} INFO - 21/05/16 23:12:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621194045_to_1621195845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:44,005] {docker.py:276} INFO - 21/05/16 23:12:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621138245_to_1621140045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:45,107] {docker.py:276} INFO - 21/05/16 23:12:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621195845_to_1621197645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:45,325] {docker.py:276} INFO - 21/05/16 23:12:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621140045_to_1621141845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:45,638] {docker.py:276} INFO - 21/05/16 23:12:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621197645_to_1621199445.csv, range: 0-104392, partition values: [empty row]
[2021-05-16 20:12:45,708] {docker.py:276} INFO - 21/05/16 23:12:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621141845_to_1621143645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:46,006] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621201245_to_1621203045.csv, range: 0-104247, partition values: [empty row]
[2021-05-16 20:12:46,091] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621143645_to_1621145445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:46,344] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621197645_to_1621199445.csv, range: 0-104209, partition values: [empty row]
[2021-05-16 20:12:46,443] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621145445_to_1621147245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:46,698] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621199445_to_1621201245.csv, range: 0-104199, partition values: [empty row]
[2021-05-16 20:12:46,787] {docker.py:276} INFO - 21/05/16 23:12:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621147245_to_1621149045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:47,055] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-16_20_10_45/from_1621203045_to_1621204845.csv, range: 0-104171, partition values: [empty row]
[2021-05-16 20:12:47,146] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621149045_to_1621150845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:47,427] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621201245_to_1621203045.csv, range: 0-103922, partition values: [empty row]
[2021-05-16 20:12:47,501] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621150845_to_1621152645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:47,794] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621203045_to_1621204845.csv, range: 0-103738, partition values: [empty row]
[2021-05-16 20:12:47,864] {docker.py:276} INFO - 21/05/16 23:12:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621152645_to_1621154445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:48,151] {docker.py:276} INFO - 21/05/16 23:12:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621199445_to_1621201245.csv, range: 0-103656, partition values: [empty row]
[2021-05-16 20:12:48,239] {docker.py:276} INFO - 21/05/16 23:12:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621154445_to_1621156245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:48,621] {docker.py:276} INFO - 21/05/16 23:12:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621156245_to_1621158045.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:48,703] {docker.py:276} INFO - 21/05/16 23:12:48 INFO Executor: Finished task 5.0 in stage 3.0 (TID 378). 2679 bytes result sent to driver
[2021-05-16 20:12:48,704] {docker.py:276} INFO - 21/05/16 23:12:48 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 378) in 10759 ms on 764b1db4ecfd (executor driver) (5/6)
[2021-05-16 20:12:48,996] {docker.py:276} INFO - 21/05/16 23:12:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621158045_to_1621159845.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:49,397] {docker.py:276} INFO - 21/05/16 23:12:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621159845_to_1621161645.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:49,745] {docker.py:276} INFO - 21/05/16 23:12:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621161645_to_1621163445.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:50,097] {docker.py:276} INFO - 21/05/16 23:12:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-16_20_10_45/from_1621163445_to_1621165245.csv, range: 0-104506, partition values: [empty row]
[2021-05-16 20:12:50,583] {docker.py:276} INFO - 21/05/16 23:12:50 INFO Executor: Finished task 4.0 in stage 3.0 (TID 377). 2679 bytes result sent to driver
[2021-05-16 20:12:50,584] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 377) in 12703 ms on 764b1db4ecfd (executor driver) (6/6)
[2021-05-16 20:12:50,585] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-16 20:12:50,585] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 25.955 s
[2021-05-16 20:12:50,588] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: looking for newly runnable stages
[2021-05-16 20:12:50,588] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: running: Set()
[2021-05-16 20:12:50,589] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-16 20:12:50,590] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: failed: Set()
[2021-05-16 20:12:50,594] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-16 20:12:50,638] {docker.py:276} INFO - 21/05/16 23:12:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 934.0 MiB)
[2021-05-16 20:12:50,643] {docker.py:276} INFO - 21/05/16 23:12:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 73.8 KiB, free 933.9 MiB)
[2021-05-16 20:12:50,644] {docker.py:276} INFO - 21/05/16 23:12:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 764b1db4ecfd:38389 (size: 73.8 KiB, free: 934.3 MiB)
[2021-05-16 20:12:50,646] {docker.py:276} INFO - 21/05/16 23:12:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-16 20:12:50,647] {docker.py:276} INFO - 21/05/16 23:12:50 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/16 23:12:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-16 20:12:50,656] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 379) (764b1db4ecfd, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:50,656] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 380) (764b1db4ecfd, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:50,657] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 381) (764b1db4ecfd, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:50,658] {docker.py:276} INFO - 21/05/16 23:12:50 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 382) (764b1db4ecfd, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:50,658] {docker.py:276} INFO - 21/05/16 23:12:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 379)
[2021-05-16 20:12:50,660] {docker.py:276} INFO - 21/05/16 23:12:50 INFO Executor: Running task 1.0 in stage 4.0 (TID 380)
[2021-05-16 20:12:50,661] {docker.py:276} INFO - 21/05/16 23:12:50 INFO Executor: Running task 2.0 in stage 4.0 (TID 381)
[2021-05-16 20:12:50,669] {docker.py:276} INFO - 21/05/16 23:12:50 INFO Executor: Running task 3.0 in stage 4.0 (TID 382)
[2021-05-16 20:12:50,754] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:50,756] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Getting 6 (26.8 KiB) non-empty blocks including 6 (26.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:50,757] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Getting 6 (26.3 KiB) non-empty blocks including 6 (26.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:50,760] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2021-05-16 20:12:50,761] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2021-05-16 20:12:50,761] {docker.py:276} INFO - 21/05/16 23:12:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
[2021-05-16 20:12:50,784] {docker.py:276} INFO - 21/05/16 23:12:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:50,785] {docker.py:276} INFO - 21/05/16 23:12:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:50,786] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,787] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244331669808879446250_0004_m_000000_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244331669808879446250_0004_m_000000_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244331669808879446250_0004}; taskId=attempt_202105162312244331669808879446250_0004_m_000000_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@287d6f49}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:50,788] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,789] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245065810444623071473_0004_m_000001_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245065810444623071473_0004_m_000001_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245065810444623071473_0004}; taskId=attempt_202105162312245065810444623071473_0004_m_000001_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45c80e83}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,790] {docker.py:276} INFO - 21/05/16 23:12:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:12:50,791] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312249048771502065054860_0004_m_000002_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249048771502065054860_0004_m_000002_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312249048771502065054860_0004}; taskId=attempt_202105162312249048771502065054860_0004_m_000002_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bb5458a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,791] {docker.py:276} INFO - 21/05/16 23:12:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:50,792] {docker.py:276} INFO - 21/05/16 23:12:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:50,793] {docker.py:276} INFO - 21/05/16 23:12:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:50,794] {docker.py:276} INFO - 21/05/16 23:12:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:50,794] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,795] {docker.py:276} INFO - 21/05/16 23:12:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241846781122005478459_0004_m_000003_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241846781122005478459_0004_m_000003_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241846781122005478459_0004}; taskId=attempt_202105162312241846781122005478459_0004_m_000003_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30cb63f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:50,796] {docker.py:276} INFO - 21/05/16 23:12:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:50,798] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312249048771502065054860_0004_m_000002_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249048771502065054860_0004_m_000002_381
[2021-05-16 20:12:50,798] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312241846781122005478459_0004_m_000003_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241846781122005478459_0004_m_000003_382
[2021-05-16 20:12:50,799] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312244331669808879446250_0004_m_000000_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244331669808879446250_0004_m_000000_379
[2021-05-16 20:12:50,799] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312245065810444623071473_0004_m_000001_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245065810444623071473_0004_m_000001_380
[2021-05-16 20:12:50,823] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Task committer attempt_202105162312249048771502065054860_0004_m_000002_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249048771502065054860_0004_m_000002_381 : duration 0:00.027s
[2021-05-16 20:12:50,829] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Task committer attempt_202105162312241846781122005478459_0004_m_000003_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241846781122005478459_0004_m_000003_382 : duration 0:00.032s
[2021-05-16 20:12:50,839] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Task committer attempt_202105162312244331669808879446250_0004_m_000000_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244331669808879446250_0004_m_000000_379 : duration 0:00.042s
[2021-05-16 20:12:50,840] {docker.py:276} INFO - 21/05/16 23:12:50 INFO StagingCommitter: Task committer attempt_202105162312245065810444623071473_0004_m_000001_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245065810444623071473_0004_m_000001_380 : duration 0:00.044s
[2021-05-16 20:12:53,100] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312244331669808879446250_0004_m_000000_379: needsTaskCommit() Task attempt_202105162312244331669808879446250_0004_m_000000_379
[2021-05-16 20:12:53,101] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Task committer attempt_202105162312244331669808879446250_0004_m_000000_379: needsTaskCommit() Task attempt_202105162312244331669808879446250_0004_m_000000_379: duration 0:00.001s
[2021-05-16 20:12:53,102] {docker.py:276} INFO - 21/05/16 23:12:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244331669808879446250_0004_m_000000_379
[2021-05-16 20:12:53,108] {docker.py:276} INFO - 21/05/16 23:12:53 INFO Executor: Finished task 0.0 in stage 4.0 (TID 379). 4587 bytes result sent to driver
[2021-05-16 20:12:53,109] {docker.py:276} INFO - 21/05/16 23:12:53 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 383) (764b1db4ecfd, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:53,110] {docker.py:276} INFO - 21/05/16 23:12:53 INFO Executor: Running task 4.0 in stage 4.0 (TID 383)
[2021-05-16 20:12:53,111] {docker.py:276} INFO - 21/05/16 23:12:53 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 379) in 2462 ms on 764b1db4ecfd (executor driver) (1/200)
[2021-05-16 20:12:53,113] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312241846781122005478459_0004_m_000003_382: needsTaskCommit() Task attempt_202105162312241846781122005478459_0004_m_000003_382
[2021-05-16 20:12:53,114] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Task committer attempt_202105162312241846781122005478459_0004_m_000003_382: needsTaskCommit() Task attempt_202105162312241846781122005478459_0004_m_000003_382: duration 0:00.001s
[2021-05-16 20:12:53,115] {docker.py:276} INFO - 21/05/16 23:12:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241846781122005478459_0004_m_000003_382
[2021-05-16 20:12:53,116] {docker.py:276} INFO - 21/05/16 23:12:53 INFO Executor: Finished task 3.0 in stage 4.0 (TID 382). 4544 bytes result sent to driver
[2021-05-16 20:12:53,117] {docker.py:276} INFO - 21/05/16 23:12:53 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 384) (764b1db4ecfd, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:53,118] {docker.py:276} INFO - 21/05/16 23:12:53 INFO Executor: Running task 5.0 in stage 4.0 (TID 384)
21/05/16 23:12:53 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 382) in 2464 ms on 764b1db4ecfd (executor driver) (2/200)
[2021-05-16 20:12:53,126] {docker.py:276} INFO - 21/05/16 23:12:53 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:53,126] {docker.py:276} INFO - 21/05/16 23:12:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:53,127] {docker.py:276} INFO - 21/05/16 23:12:53 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:53,127] {docker.py:276} INFO - 21/05/16 23:12:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:53,129] {docker.py:276} INFO - 21/05/16 23:12:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:53,129] {docker.py:276} INFO - 21/05/16 23:12:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:53,129] {docker.py:276} INFO - 21/05/16 23:12:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248267110619559981940_0004_m_000005_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248267110619559981940_0004_m_000005_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248267110619559981940_0004}; taskId=attempt_202105162312248267110619559981940_0004_m_000005_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43983b40}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:53,130] {docker.py:276} INFO - 21/05/16 23:12:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:12:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312248267110619559981940_0004_m_000005_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248267110619559981940_0004_m_000005_384
[2021-05-16 20:12:53,131] {docker.py:276} INFO - 21/05/16 23:12:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242524058979952124688_0004_m_000004_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242524058979952124688_0004_m_000004_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242524058979952124688_0004}; taskId=attempt_202105162312242524058979952124688_0004_m_000004_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ca7758d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:53,131] {docker.py:276} INFO - 21/05/16 23:12:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:53,132] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312242524058979952124688_0004_m_000004_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242524058979952124688_0004_m_000004_383
[2021-05-16 20:12:53,143] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Task committer attempt_202105162312248267110619559981940_0004_m_000005_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248267110619559981940_0004_m_000005_384 : duration 0:00.009s
[2021-05-16 20:12:53,143] {docker.py:276} INFO - 21/05/16 23:12:53 INFO StagingCommitter: Task committer attempt_202105162312242524058979952124688_0004_m_000004_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242524058979952124688_0004_m_000004_383 : duration 0:00.010s
[2021-05-16 20:12:54,065] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312249048771502065054860_0004_m_000002_381: needsTaskCommit() Task attempt_202105162312249048771502065054860_0004_m_000002_381
[2021-05-16 20:12:54,066] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Task committer attempt_202105162312249048771502065054860_0004_m_000002_381: needsTaskCommit() Task attempt_202105162312249048771502065054860_0004_m_000002_381: duration 0:00.001s
[2021-05-16 20:12:54,067] {docker.py:276} INFO - 21/05/16 23:12:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312249048771502065054860_0004_m_000002_381
[2021-05-16 20:12:54,070] {docker.py:276} INFO - 21/05/16 23:12:54 INFO Executor: Finished task 2.0 in stage 4.0 (TID 381). 4587 bytes result sent to driver
[2021-05-16 20:12:54,071] {docker.py:276} INFO - 21/05/16 23:12:54 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 385) (764b1db4ecfd, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:54,072] {docker.py:276} INFO - 21/05/16 23:12:54 INFO Executor: Running task 6.0 in stage 4.0 (TID 385)
[2021-05-16 20:12:54,073] {docker.py:276} INFO - 21/05/16 23:12:54 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 381) in 3422 ms on 764b1db4ecfd (executor driver) (3/200)
[2021-05-16 20:12:54,084] {docker.py:276} INFO - 21/05/16 23:12:54 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:54,087] {docker.py:276} INFO - 21/05/16 23:12:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:54,087] {docker.py:276} INFO - 21/05/16 23:12:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245950360652103423899_0004_m_000006_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245950360652103423899_0004_m_000006_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245950360652103423899_0004}; taskId=attempt_202105162312245950360652103423899_0004_m_000006_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6593870e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:12:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312245950360652103423899_0004_m_000006_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245950360652103423899_0004_m_000006_385
[2021-05-16 20:12:54,095] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Task committer attempt_202105162312245950360652103423899_0004_m_000006_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245950360652103423899_0004_m_000006_385 : duration 0:00.008s
[2021-05-16 20:12:54,186] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312245065810444623071473_0004_m_000001_380: needsTaskCommit() Task attempt_202105162312245065810444623071473_0004_m_000001_380
[2021-05-16 20:12:54,186] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Task committer attempt_202105162312245065810444623071473_0004_m_000001_380: needsTaskCommit() Task attempt_202105162312245065810444623071473_0004_m_000001_380: duration 0:00.001s
[2021-05-16 20:12:54,187] {docker.py:276} INFO - 21/05/16 23:12:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245065810444623071473_0004_m_000001_380
[2021-05-16 20:12:54,187] {docker.py:276} INFO - 21/05/16 23:12:54 INFO Executor: Finished task 1.0 in stage 4.0 (TID 380). 4544 bytes result sent to driver
[2021-05-16 20:12:54,189] {docker.py:276} INFO - 21/05/16 23:12:54 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 386) (764b1db4ecfd, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:54,190] {docker.py:276} INFO - 21/05/16 23:12:54 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 380) in 3539 ms on 764b1db4ecfd (executor driver) (4/200)
[2021-05-16 20:12:54,191] {docker.py:276} INFO - 21/05/16 23:12:54 INFO Executor: Running task 7.0 in stage 4.0 (TID 386)
[2021-05-16 20:12:54,203] {docker.py:276} INFO - 21/05/16 23:12:54 INFO ShuffleBlockFetcherIterator: Getting 6 (27.0 KiB) non-empty blocks including 6 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:54,204] {docker.py:276} INFO - 21/05/16 23:12:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:12:54,206] {docker.py:276} INFO - 21/05/16 23:12:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:12:54,207] {docker.py:276} INFO - 21/05/16 23:12:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:54,207] {docker.py:276} INFO - 21/05/16 23:12:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:54,208] {docker.py:276} INFO - 21/05/16 23:12:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224389409020209986303_0004_m_000007_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224389409020209986303_0004_m_000007_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224389409020209986303_0004}; taskId=attempt_20210516231224389409020209986303_0004_m_000007_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b937c72}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:54,208] {docker.py:276} INFO - 21/05/16 23:12:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:54,208] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Starting: Task committer attempt_20210516231224389409020209986303_0004_m_000007_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224389409020209986303_0004_m_000007_386
[2021-05-16 20:12:54,211] {docker.py:276} INFO - 21/05/16 23:12:54 INFO StagingCommitter: Task committer attempt_20210516231224389409020209986303_0004_m_000007_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224389409020209986303_0004_m_000007_386 : duration 0:00.004s
[2021-05-16 20:12:55,930] {docker.py:276} INFO - 21/05/16 23:12:55 INFO StagingCommitter: Starting: Task committer attempt_202105162312248267110619559981940_0004_m_000005_384: needsTaskCommit() Task attempt_202105162312248267110619559981940_0004_m_000005_384
[2021-05-16 20:12:55,931] {docker.py:276} INFO - 21/05/16 23:12:55 INFO StagingCommitter: Task committer attempt_202105162312248267110619559981940_0004_m_000005_384: needsTaskCommit() Task attempt_202105162312248267110619559981940_0004_m_000005_384: duration 0:00.002s
[2021-05-16 20:12:55,931] {docker.py:276} INFO - 21/05/16 23:12:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248267110619559981940_0004_m_000005_384
[2021-05-16 20:12:55,934] {docker.py:276} INFO - 21/05/16 23:12:55 INFO Executor: Finished task 5.0 in stage 4.0 (TID 384). 4544 bytes result sent to driver
[2021-05-16 20:12:55,936] {docker.py:276} INFO - 21/05/16 23:12:55 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 387) (764b1db4ecfd, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:55,937] {docker.py:276} INFO - 21/05/16 23:12:55 INFO Executor: Running task 8.0 in stage 4.0 (TID 387)
[2021-05-16 20:12:55,938] {docker.py:276} INFO - 21/05/16 23:12:55 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 384) in 2824 ms on 764b1db4ecfd (executor driver) (5/200)
[2021-05-16 20:12:55,952] {docker.py:276} INFO - 21/05/16 23:12:55 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:12:55,956] {docker.py:276} INFO - 21/05/16 23:12:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:12:55,956] {docker.py:276} INFO - 21/05/16 23:12:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:55,957] {docker.py:276} INFO - 21/05/16 23:12:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:55,957] {docker.py:276} INFO - 21/05/16 23:12:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242457453869859799603_0004_m_000008_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242457453869859799603_0004_m_000008_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242457453869859799603_0004}; taskId=attempt_202105162312242457453869859799603_0004_m_000008_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43f751fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:55,957] {docker.py:276} INFO - 21/05/16 23:12:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:55,958] {docker.py:276} INFO - 21/05/16 23:12:55 INFO StagingCommitter: Starting: Task committer attempt_202105162312242457453869859799603_0004_m_000008_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242457453869859799603_0004_m_000008_387
[2021-05-16 20:12:55,961] {docker.py:276} INFO - 21/05/16 23:12:55 INFO StagingCommitter: Task committer attempt_202105162312242457453869859799603_0004_m_000008_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242457453869859799603_0004_m_000008_387 : duration 0:00.005s
[2021-05-16 20:12:56,096] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Starting: Task committer attempt_202105162312242524058979952124688_0004_m_000004_383: needsTaskCommit() Task attempt_202105162312242524058979952124688_0004_m_000004_383
[2021-05-16 20:12:56,097] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Task committer attempt_202105162312242524058979952124688_0004_m_000004_383: needsTaskCommit() Task attempt_202105162312242524058979952124688_0004_m_000004_383: duration 0:00.002s
21/05/16 23:12:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242524058979952124688_0004_m_000004_383
[2021-05-16 20:12:56,098] {docker.py:276} INFO - 21/05/16 23:12:56 INFO Executor: Finished task 4.0 in stage 4.0 (TID 383). 4544 bytes result sent to driver
[2021-05-16 20:12:56,099] {docker.py:276} INFO - 21/05/16 23:12:56 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 388) (764b1db4ecfd, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:56,100] {docker.py:276} INFO - 21/05/16 23:12:56 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 383) in 2995 ms on 764b1db4ecfd (executor driver) (6/200)
[2021-05-16 20:12:56,102] {docker.py:276} INFO - 21/05/16 23:12:56 INFO Executor: Running task 9.0 in stage 4.0 (TID 388)
[2021-05-16 20:12:56,116] {docker.py:276} INFO - 21/05/16 23:12:56 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:56,120] {docker.py:276} INFO - 21/05/16 23:12:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:56,121] {docker.py:276} INFO - 21/05/16 23:12:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243015262097150705515_0004_m_000009_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243015262097150705515_0004_m_000009_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243015262097150705515_0004}; taskId=attempt_202105162312243015262097150705515_0004_m_000009_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@737ae43f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:56,121] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Starting: Task committer attempt_202105162312243015262097150705515_0004_m_000009_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243015262097150705515_0004_m_000009_388
[2021-05-16 20:12:56,129] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Task committer attempt_202105162312243015262097150705515_0004_m_000009_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243015262097150705515_0004_m_000009_388 : duration 0:00.007s
[2021-05-16 20:12:56,868] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Starting: Task committer attempt_202105162312245950360652103423899_0004_m_000006_385: needsTaskCommit() Task attempt_202105162312245950360652103423899_0004_m_000006_385
[2021-05-16 20:12:56,870] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Task committer attempt_202105162312245950360652103423899_0004_m_000006_385: needsTaskCommit() Task attempt_202105162312245950360652103423899_0004_m_000006_385: duration 0:00.001s
[2021-05-16 20:12:56,870] {docker.py:276} INFO - 21/05/16 23:12:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245950360652103423899_0004_m_000006_385
[2021-05-16 20:12:56,872] {docker.py:276} INFO - 21/05/16 23:12:56 INFO Executor: Finished task 6.0 in stage 4.0 (TID 385). 4544 bytes result sent to driver
[2021-05-16 20:12:56,873] {docker.py:276} INFO - 21/05/16 23:12:56 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 389) (764b1db4ecfd, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:56,874] {docker.py:276} INFO - 21/05/16 23:12:56 INFO Executor: Running task 10.0 in stage 4.0 (TID 389)
21/05/16 23:12:56 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 385) in 2808 ms on 764b1db4ecfd (executor driver) (7/200)
[2021-05-16 20:12:56,889] {docker.py:276} INFO - 21/05/16 23:12:56 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:56,892] {docker.py:276} INFO - 21/05/16 23:12:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:56,892] {docker.py:276} INFO - 21/05/16 23:12:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247712952961912295809_0004_m_000010_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247712952961912295809_0004_m_000010_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247712952961912295809_0004}; taskId=attempt_202105162312247712952961912295809_0004_m_000010_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ee7ef24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:56,892] {docker.py:276} INFO - 21/05/16 23:12:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:12:56 INFO StagingCommitter: Starting: Task committer attempt_202105162312247712952961912295809_0004_m_000010_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247712952961912295809_0004_m_000010_389
[2021-05-16 20:12:56,897] {docker.py:276} INFO - 21/05/16 23:12:56 INFO StagingCommitter: Task committer attempt_202105162312247712952961912295809_0004_m_000010_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247712952961912295809_0004_m_000010_389 : duration 0:00.006s
[2021-05-16 20:12:56,984] {docker.py:276} INFO - 21/05/16 23:12:57 INFO StagingCommitter: Starting: Task committer attempt_20210516231224389409020209986303_0004_m_000007_386: needsTaskCommit() Task attempt_20210516231224389409020209986303_0004_m_000007_386
[2021-05-16 20:12:56,985] {docker.py:276} INFO - 21/05/16 23:12:57 INFO StagingCommitter: Task committer attempt_20210516231224389409020209986303_0004_m_000007_386: needsTaskCommit() Task attempt_20210516231224389409020209986303_0004_m_000007_386: duration 0:00.001s
21/05/16 23:12:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224389409020209986303_0004_m_000007_386
[2021-05-16 20:12:56,985] {docker.py:276} INFO - 21/05/16 23:12:57 INFO Executor: Finished task 7.0 in stage 4.0 (TID 386). 4544 bytes result sent to driver
[2021-05-16 20:12:56,986] {docker.py:276} INFO - 21/05/16 23:12:57 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 390) (764b1db4ecfd, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:56,987] {docker.py:276} INFO - 21/05/16 23:12:57 INFO Executor: Running task 11.0 in stage 4.0 (TID 390)
[2021-05-16 20:12:56,987] {docker.py:276} INFO - 21/05/16 23:12:57 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 386) in 2803 ms on 764b1db4ecfd (executor driver) (8/200)
[2021-05-16 20:12:57,026] {docker.py:276} INFO - 21/05/16 23:12:57 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:12:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:57,029] {docker.py:276} INFO - 21/05/16 23:12:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:12:57,029] {docker.py:276} INFO - 21/05/16 23:12:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:57,030] {docker.py:276} INFO - 21/05/16 23:12:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:57,030] {docker.py:276} INFO - 21/05/16 23:12:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247537410637597301138_0004_m_000011_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247537410637597301138_0004_m_000011_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247537410637597301138_0004}; taskId=attempt_202105162312247537410637597301138_0004_m_000011_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f19b085}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:57,030] {docker.py:276} INFO - 21/05/16 23:12:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:12:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312247537410637597301138_0004_m_000011_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247537410637597301138_0004_m_000011_390
[2021-05-16 20:12:57,041] {docker.py:276} INFO - 21/05/16 23:12:57 INFO StagingCommitter: Task committer attempt_202105162312247537410637597301138_0004_m_000011_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247537410637597301138_0004_m_000011_390 : duration 0:00.011s
[2021-05-16 20:12:58,776] {docker.py:276} INFO - 21/05/16 23:12:58 INFO StagingCommitter: Starting: Task committer attempt_202105162312242457453869859799603_0004_m_000008_387: needsTaskCommit() Task attempt_202105162312242457453869859799603_0004_m_000008_387
[2021-05-16 20:12:58,778] {docker.py:276} INFO - 21/05/16 23:12:58 INFO StagingCommitter: Task committer attempt_202105162312242457453869859799603_0004_m_000008_387: needsTaskCommit() Task attempt_202105162312242457453869859799603_0004_m_000008_387: duration 0:00.003s
21/05/16 23:12:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242457453869859799603_0004_m_000008_387
[2021-05-16 20:12:58,780] {docker.py:276} INFO - 21/05/16 23:12:58 INFO Executor: Finished task 8.0 in stage 4.0 (TID 387). 4587 bytes result sent to driver
[2021-05-16 20:12:58,781] {docker.py:276} INFO - 21/05/16 23:12:58 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 391) (764b1db4ecfd, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:58,782] {docker.py:276} INFO - 21/05/16 23:12:58 INFO Executor: Running task 12.0 in stage 4.0 (TID 391)
[2021-05-16 20:12:58,783] {docker.py:276} INFO - 21/05/16 23:12:58 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 387) in 2851 ms on 764b1db4ecfd (executor driver) (9/200)
[2021-05-16 20:12:58,793] {docker.py:276} INFO - 21/05/16 23:12:58 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:58,794] {docker.py:276} INFO - 21/05/16 23:12:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:12:58,796] {docker.py:276} INFO - 21/05/16 23:12:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:12:58,797] {docker.py:276} INFO - 21/05/16 23:12:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:12:58,797] {docker.py:276} INFO - 21/05/16 23:12:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:58,798] {docker.py:276} INFO - 21/05/16 23:12:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224471841312659784698_0004_m_000012_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224471841312659784698_0004_m_000012_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224471841312659784698_0004}; taskId=attempt_20210516231224471841312659784698_0004_m_000012_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ad76069}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:58,798] {docker.py:276} INFO - 21/05/16 23:12:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:58,798] {docker.py:276} INFO - 21/05/16 23:12:58 INFO StagingCommitter: Starting: Task committer attempt_20210516231224471841312659784698_0004_m_000012_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224471841312659784698_0004_m_000012_391
[2021-05-16 20:12:58,802] {docker.py:276} INFO - 21/05/16 23:12:58 INFO StagingCommitter: Task committer attempt_20210516231224471841312659784698_0004_m_000012_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224471841312659784698_0004_m_000012_391 : duration 0:00.005s
[2021-05-16 20:12:58,982] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312243015262097150705515_0004_m_000009_388: needsTaskCommit() Task attempt_202105162312243015262097150705515_0004_m_000009_388
[2021-05-16 20:12:58,983] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Task committer attempt_202105162312243015262097150705515_0004_m_000009_388: needsTaskCommit() Task attempt_202105162312243015262097150705515_0004_m_000009_388: duration 0:00.002s
21/05/16 23:12:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243015262097150705515_0004_m_000009_388
[2021-05-16 20:12:58,985] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Finished task 9.0 in stage 4.0 (TID 388). 4587 bytes result sent to driver
[2021-05-16 20:12:58,986] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 392) (764b1db4ecfd, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:58,988] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 388) in 2892 ms on 764b1db4ecfd (executor driver) (10/200)
[2021-05-16 20:12:58,988] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Running task 13.0 in stage 4.0 (TID 392)
[2021-05-16 20:12:58,998] {docker.py:276} INFO - 21/05/16 23:12:59 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:58,998] {docker.py:276} INFO - 21/05/16 23:12:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:59,000] {docker.py:276} INFO - 21/05/16 23:12:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:59,001] {docker.py:276} INFO - 21/05/16 23:12:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224519726859031777741_0004_m_000013_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224519726859031777741_0004_m_000013_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224519726859031777741_0004}; taskId=attempt_20210516231224519726859031777741_0004_m_000013_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c08f2bb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:59,001] {docker.py:276} INFO - 21/05/16 23:12:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:12:59,001] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Starting: Task committer attempt_20210516231224519726859031777741_0004_m_000013_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224519726859031777741_0004_m_000013_392
[2021-05-16 20:12:59,006] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Task committer attempt_20210516231224519726859031777741_0004_m_000013_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224519726859031777741_0004_m_000013_392 : duration 0:00.004s
[2021-05-16 20:12:59,758] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312247712952961912295809_0004_m_000010_389: needsTaskCommit() Task attempt_202105162312247712952961912295809_0004_m_000010_389
[2021-05-16 20:12:59,760] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Task committer attempt_202105162312247712952961912295809_0004_m_000010_389: needsTaskCommit() Task attempt_202105162312247712952961912295809_0004_m_000010_389: duration 0:00.004s
21/05/16 23:12:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247712952961912295809_0004_m_000010_389
[2021-05-16 20:12:59,761] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Finished task 10.0 in stage 4.0 (TID 389). 4587 bytes result sent to driver
[2021-05-16 20:12:59,763] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 393) (764b1db4ecfd, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:59,764] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 389) in 2895 ms on 764b1db4ecfd (executor driver) (11/200)
[2021-05-16 20:12:59,765] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Running task 14.0 in stage 4.0 (TID 393)
[2021-05-16 20:12:59,776] {docker.py:276} INFO - 21/05/16 23:12:59 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:59,777] {docker.py:276} INFO - 21/05/16 23:12:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2021-05-16 20:12:59,780] {docker.py:276} INFO - 21/05/16 23:12:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:12:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:12:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:59,780] {docker.py:276} INFO - 21/05/16 23:12:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246103523526661475799_0004_m_000014_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246103523526661475799_0004_m_000014_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246103523526661475799_0004}; taskId=attempt_202105162312246103523526661475799_0004_m_000014_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67722995}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:12:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:12:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312246103523526661475799_0004_m_000014_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246103523526661475799_0004_m_000014_393
[2021-05-16 20:12:59,784] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Task committer attempt_202105162312246103523526661475799_0004_m_000014_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246103523526661475799_0004_m_000014_393 : duration 0:00.004s
[2021-05-16 20:12:59,963] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312247537410637597301138_0004_m_000011_390: needsTaskCommit() Task attempt_202105162312247537410637597301138_0004_m_000011_390
[2021-05-16 20:12:59,964] {docker.py:276} INFO - 21/05/16 23:12:59 INFO StagingCommitter: Task committer attempt_202105162312247537410637597301138_0004_m_000011_390: needsTaskCommit() Task attempt_202105162312247537410637597301138_0004_m_000011_390: duration 0:00.003s
[2021-05-16 20:12:59,965] {docker.py:276} INFO - 21/05/16 23:12:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247537410637597301138_0004_m_000011_390
[2021-05-16 20:12:59,967] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Finished task 11.0 in stage 4.0 (TID 390). 4587 bytes result sent to driver
[2021-05-16 20:12:59,968] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 394) (764b1db4ecfd, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:12:59,969] {docker.py:276} INFO - 21/05/16 23:12:59 INFO Executor: Running task 15.0 in stage 4.0 (TID 394)
[2021-05-16 20:12:59,970] {docker.py:276} INFO - 21/05/16 23:12:59 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 390) in 2987 ms on 764b1db4ecfd (executor driver) (12/200)
[2021-05-16 20:12:59,981] {docker.py:276} INFO - 21/05/16 23:13:00 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:12:59,981] {docker.py:276} INFO - 21/05/16 23:13:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:12:59,983] {docker.py:276} INFO - 21/05/16 23:13:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:12:59,984] {docker.py:276} INFO - 21/05/16 23:13:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245496105734490472743_0004_m_000015_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245496105734490472743_0004_m_000015_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245496105734490472743_0004}; taskId=attempt_202105162312245496105734490472743_0004_m_000015_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@542356a3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:00 INFO StagingCommitter: Starting: Task committer attempt_202105162312245496105734490472743_0004_m_000015_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245496105734490472743_0004_m_000015_394
[2021-05-16 20:12:59,987] {docker.py:276} INFO - 21/05/16 23:13:00 INFO StagingCommitter: Task committer attempt_202105162312245496105734490472743_0004_m_000015_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245496105734490472743_0004_m_000015_394 : duration 0:00.004s
[2021-05-16 20:13:01,638] {docker.py:276} INFO - 21/05/16 23:13:01 INFO StagingCommitter: Starting: Task committer attempt_20210516231224471841312659784698_0004_m_000012_391: needsTaskCommit() Task attempt_20210516231224471841312659784698_0004_m_000012_391
21/05/16 23:13:01 INFO StagingCommitter: Task committer attempt_20210516231224471841312659784698_0004_m_000012_391: needsTaskCommit() Task attempt_20210516231224471841312659784698_0004_m_000012_391: duration 0:00.003s
21/05/16 23:13:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224471841312659784698_0004_m_000012_391
21/05/16 23:13:01 INFO Executor: Finished task 12.0 in stage 4.0 (TID 391). 4544 bytes result sent to driver
[2021-05-16 20:13:01,639] {docker.py:276} INFO - 21/05/16 23:13:01 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 395) (764b1db4ecfd, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:01,641] {docker.py:276} INFO - 21/05/16 23:13:01 INFO Executor: Running task 16.0 in stage 4.0 (TID 395)
[2021-05-16 20:13:01,642] {docker.py:276} INFO - 21/05/16 23:13:01 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 391) in 2865 ms on 764b1db4ecfd (executor driver) (13/200)
[2021-05-16 20:13:01,651] {docker.py:276} INFO - 21/05/16 23:13:01 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:01,651] {docker.py:276} INFO - 21/05/16 23:13:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:01,653] {docker.py:276} INFO - 21/05/16 23:13:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:01,653] {docker.py:276} INFO - 21/05/16 23:13:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243537693058324638183_0004_m_000016_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243537693058324638183_0004_m_000016_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243537693058324638183_0004}; taskId=attempt_202105162312243537693058324638183_0004_m_000016_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4262dfab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:01,654] {docker.py:276} INFO - 21/05/16 23:13:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:01,654] {docker.py:276} INFO - 21/05/16 23:13:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312243537693058324638183_0004_m_000016_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243537693058324638183_0004_m_000016_395
[2021-05-16 20:13:01,659] {docker.py:276} INFO - 21/05/16 23:13:01 INFO StagingCommitter: Task committer attempt_202105162312243537693058324638183_0004_m_000016_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243537693058324638183_0004_m_000016_395 : duration 0:00.005s
[2021-05-16 20:13:01,978] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_20210516231224519726859031777741_0004_m_000013_392: needsTaskCommit() Task attempt_20210516231224519726859031777741_0004_m_000013_392
[2021-05-16 20:13:01,978] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_20210516231224519726859031777741_0004_m_000013_392: needsTaskCommit() Task attempt_20210516231224519726859031777741_0004_m_000013_392: duration 0:00.001s
21/05/16 23:13:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224519726859031777741_0004_m_000013_392
[2021-05-16 20:13:01,979] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Finished task 13.0 in stage 4.0 (TID 392). 4544 bytes result sent to driver
[2021-05-16 20:13:01,979] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 396) (764b1db4ecfd, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:01,980] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Running task 17.0 in stage 4.0 (TID 396)
[2021-05-16 20:13:01,982] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 392) in 3000 ms on 764b1db4ecfd (executor driver) (14/200)
[2021-05-16 20:13:01,997] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Getting 6 (27.5 KiB) non-empty blocks including 6 (27.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:02,089] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:02,090] {docker.py:276} INFO - 21/05/16 23:13:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243252694696671868221_0004_m_000017_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243252694696671868221_0004_m_000017_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243252694696671868221_0004}; taskId=attempt_202105162312243252694696671868221_0004_m_000017_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e1439ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:02,090] {docker.py:276} INFO - 21/05/16 23:13:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312243252694696671868221_0004_m_000017_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243252694696671868221_0004_m_000017_396
[2021-05-16 20:13:02,090] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_202105162312243252694696671868221_0004_m_000017_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243252694696671868221_0004_m_000017_396 : duration 0:00.005s
[2021-05-16 20:13:02,157] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312246103523526661475799_0004_m_000014_393: needsTaskCommit() Task attempt_202105162312246103523526661475799_0004_m_000014_393
[2021-05-16 20:13:02,158] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_202105162312246103523526661475799_0004_m_000014_393: needsTaskCommit() Task attempt_202105162312246103523526661475799_0004_m_000014_393: duration 0:00.002s
[2021-05-16 20:13:02,158] {docker.py:276} INFO - 21/05/16 23:13:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246103523526661475799_0004_m_000014_393
[2021-05-16 20:13:02,160] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Finished task 14.0 in stage 4.0 (TID 393). 4544 bytes result sent to driver
[2021-05-16 20:13:02,161] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 393) in 2402 ms on 764b1db4ecfd (executor driver) (15/200)
[2021-05-16 20:13:02,162] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 397) (764b1db4ecfd, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:02,163] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Running task 18.0 in stage 4.0 (TID 397)
[2021-05-16 20:13:02,178] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:02,179] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:13:02,182] {docker.py:276} INFO - 21/05/16 23:13:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:02,182] {docker.py:276} INFO - 21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:02,183] {docker.py:276} INFO - 21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247176986989617516484_0004_m_000018_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247176986989617516484_0004_m_000018_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247176986989617516484_0004}; taskId=attempt_202105162312247176986989617516484_0004_m_000018_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c240ef7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:02,183] {docker.py:276} INFO - 21/05/16 23:13:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:02,183] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312247176986989617516484_0004_m_000018_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247176986989617516484_0004_m_000018_397
[2021-05-16 20:13:02,189] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_202105162312247176986989617516484_0004_m_000018_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247176986989617516484_0004_m_000018_397 : duration 0:00.005s
[2021-05-16 20:13:02,334] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312245496105734490472743_0004_m_000015_394: needsTaskCommit() Task attempt_202105162312245496105734490472743_0004_m_000015_394
[2021-05-16 20:13:02,334] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_202105162312245496105734490472743_0004_m_000015_394: needsTaskCommit() Task attempt_202105162312245496105734490472743_0004_m_000015_394: duration 0:00.002s
21/05/16 23:13:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245496105734490472743_0004_m_000015_394
[2021-05-16 20:13:02,335] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Finished task 15.0 in stage 4.0 (TID 394). 4544 bytes result sent to driver
[2021-05-16 20:13:02,336] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 398) (764b1db4ecfd, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:02,370] {docker.py:276} INFO - 21/05/16 23:13:02 INFO Executor: Running task 19.0 in stage 4.0 (TID 398)
[2021-05-16 20:13:02,371] {docker.py:276} INFO - 21/05/16 23:13:02 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 394) in 2373 ms on 764b1db4ecfd (executor driver) (16/200)
[2021-05-16 20:13:02,371] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:02,371] {docker.py:276} INFO - 21/05/16 23:13:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:02,371] {docker.py:276} INFO - 21/05/16 23:13:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246818182113453731286_0004_m_000019_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246818182113453731286_0004_m_000019_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246818182113453731286_0004}; taskId=attempt_202105162312246818182113453731286_0004_m_000019_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18be32ef}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:02,372] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312246818182113453731286_0004_m_000019_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246818182113453731286_0004_m_000019_398
[2021-05-16 20:13:02,372] {docker.py:276} INFO - 21/05/16 23:13:02 INFO StagingCommitter: Task committer attempt_202105162312246818182113453731286_0004_m_000019_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246818182113453731286_0004_m_000019_398 : duration 0:00.004s
[2021-05-16 20:13:04,509] {docker.py:276} INFO - 21/05/16 23:13:04 INFO StagingCommitter: Starting: Task committer attempt_202105162312243537693058324638183_0004_m_000016_395: needsTaskCommit() Task attempt_202105162312243537693058324638183_0004_m_000016_395
[2021-05-16 20:13:04,510] {docker.py:276} INFO - 21/05/16 23:13:04 INFO StagingCommitter: Task committer attempt_202105162312243537693058324638183_0004_m_000016_395: needsTaskCommit() Task attempt_202105162312243537693058324638183_0004_m_000016_395: duration 0:00.002s
21/05/16 23:13:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243537693058324638183_0004_m_000016_395
[2021-05-16 20:13:04,514] {docker.py:276} INFO - 21/05/16 23:13:04 INFO Executor: Finished task 16.0 in stage 4.0 (TID 395). 4544 bytes result sent to driver
21/05/16 23:13:04 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 399) (764b1db4ecfd, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:13:04 INFO Executor: Running task 20.0 in stage 4.0 (TID 399)
21/05/16 23:13:04 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 395) in 2879 ms on 764b1db4ecfd (executor driver) (17/200)
[2021-05-16 20:13:04,524] {docker.py:276} INFO - 21/05/16 23:13:04 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:04,527] {docker.py:276} INFO - 21/05/16 23:13:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:04,528] {docker.py:276} INFO - 21/05/16 23:13:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:04,528] {docker.py:276} INFO - 21/05/16 23:13:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:04,529] {docker.py:276} INFO - 21/05/16 23:13:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244803057056249308407_0004_m_000020_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244803057056249308407_0004_m_000020_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244803057056249308407_0004}; taskId=attempt_202105162312244803057056249308407_0004_m_000020_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e2a4484}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:04,529] {docker.py:276} INFO - 21/05/16 23:13:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:04 INFO StagingCommitter: Starting: Task committer attempt_202105162312244803057056249308407_0004_m_000020_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244803057056249308407_0004_m_000020_399
[2021-05-16 20:13:04,532] {docker.py:276} INFO - 21/05/16 23:13:04 INFO StagingCommitter: Task committer attempt_202105162312244803057056249308407_0004_m_000020_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244803057056249308407_0004_m_000020_399 : duration 0:00.004s
[2021-05-16 20:13:05,106] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312246818182113453731286_0004_m_000019_398: needsTaskCommit() Task attempt_202105162312246818182113453731286_0004_m_000019_398
[2021-05-16 20:13:05,106] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312246818182113453731286_0004_m_000019_398: needsTaskCommit() Task attempt_202105162312246818182113453731286_0004_m_000019_398: duration 0:00.001s
21/05/16 23:13:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246818182113453731286_0004_m_000019_398
[2021-05-16 20:13:05,107] {docker.py:276} INFO - 21/05/16 23:13:05 INFO Executor: Finished task 19.0 in stage 4.0 (TID 398). 4544 bytes result sent to driver
[2021-05-16 20:13:05,109] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 400) (764b1db4ecfd, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:05,110] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 398) in 2777 ms on 764b1db4ecfd (executor driver) (18/200)
[2021-05-16 20:13:05,111] {docker.py:276} INFO - 21/05/16 23:13:05 INFO Executor: Running task 21.0 in stage 4.0 (TID 400)
[2021-05-16 20:13:05,119] {docker.py:276} INFO - 21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:05,122] {docker.py:276} INFO - 21/05/16 23:13:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:05,123] {docker.py:276} INFO - 21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244757242078694773185_0004_m_000021_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244757242078694773185_0004_m_000021_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244757242078694773185_0004}; taskId=attempt_202105162312244757242078694773185_0004_m_000021_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e3d808e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:05,123] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312244757242078694773185_0004_m_000021_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244757242078694773185_0004_m_000021_400
[2021-05-16 20:13:05,127] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312244757242078694773185_0004_m_000021_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244757242078694773185_0004_m_000021_400 : duration 0:00.004s
[2021-05-16 20:13:05,364] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312243252694696671868221_0004_m_000017_396: needsTaskCommit() Task attempt_202105162312243252694696671868221_0004_m_000017_396
[2021-05-16 20:13:05,364] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312243252694696671868221_0004_m_000017_396: needsTaskCommit() Task attempt_202105162312243252694696671868221_0004_m_000017_396: duration 0:00.002s
21/05/16 23:13:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243252694696671868221_0004_m_000017_396
[2021-05-16 20:13:05,370] {docker.py:276} INFO - 21/05/16 23:13:05 INFO Executor: Finished task 17.0 in stage 4.0 (TID 396). 4544 bytes result sent to driver
[2021-05-16 20:13:05,371] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 401) (764b1db4ecfd, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:05,373] {docker.py:276} INFO - 21/05/16 23:13:05 INFO Executor: Running task 22.0 in stage 4.0 (TID 401)
[2021-05-16 20:13:05,374] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 396) in 3396 ms on 764b1db4ecfd (executor driver) (19/200)
[2021-05-16 20:13:05,383] {docker.py:276} INFO - 21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:05,386] {docker.py:276} INFO - 21/05/16 23:13:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:05,387] {docker.py:276} INFO - 21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:05,387] {docker.py:276} INFO - 21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248669840163384778503_0004_m_000022_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248669840163384778503_0004_m_000022_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248669840163384778503_0004}; taskId=attempt_202105162312248669840163384778503_0004_m_000022_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4df5e038}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:05,387] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312248669840163384778503_0004_m_000022_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248669840163384778503_0004_m_000022_401
[2021-05-16 20:13:05,393] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312248669840163384778503_0004_m_000022_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248669840163384778503_0004_m_000022_401 : duration 0:00.006s
[2021-05-16 20:13:05,420] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312247176986989617516484_0004_m_000018_397: needsTaskCommit() Task attempt_202105162312247176986989617516484_0004_m_000018_397
[2021-05-16 20:13:05,420] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312247176986989617516484_0004_m_000018_397: needsTaskCommit() Task attempt_202105162312247176986989617516484_0004_m_000018_397: duration 0:00.001s
21/05/16 23:13:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247176986989617516484_0004_m_000018_397
[2021-05-16 20:13:05,422] {docker.py:276} INFO - 21/05/16 23:13:05 INFO Executor: Finished task 18.0 in stage 4.0 (TID 397). 4544 bytes result sent to driver
[2021-05-16 20:13:05,423] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 402) (764b1db4ecfd, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:05,424] {docker.py:276} INFO - 21/05/16 23:13:05 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 397) in 3266 ms on 764b1db4ecfd (executor driver) (20/200)
21/05/16 23:13:05 INFO Executor: Running task 23.0 in stage 4.0 (TID 402)
[2021-05-16 20:13:05,433] {docker.py:276} INFO - 21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:05,435] {docker.py:276} INFO - 21/05/16 23:13:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:05,436] {docker.py:276} INFO - 21/05/16 23:13:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247668944637872132817_0004_m_000023_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247668944637872132817_0004_m_000023_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247668944637872132817_0004}; taskId=attempt_202105162312247668944637872132817_0004_m_000023_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@51ea3705}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312247668944637872132817_0004_m_000023_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247668944637872132817_0004_m_000023_402
[2021-05-16 20:13:05,440] {docker.py:276} INFO - 21/05/16 23:13:05 INFO StagingCommitter: Task committer attempt_202105162312247668944637872132817_0004_m_000023_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247668944637872132817_0004_m_000023_402 : duration 0:00.004s
[2021-05-16 20:13:07,316] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Starting: Task committer attempt_202105162312244803057056249308407_0004_m_000020_399: needsTaskCommit() Task attempt_202105162312244803057056249308407_0004_m_000020_399
[2021-05-16 20:13:07,317] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Task committer attempt_202105162312244803057056249308407_0004_m_000020_399: needsTaskCommit() Task attempt_202105162312244803057056249308407_0004_m_000020_399: duration 0:00.002s
21/05/16 23:13:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244803057056249308407_0004_m_000020_399
[2021-05-16 20:13:07,323] {docker.py:276} INFO - 21/05/16 23:13:07 INFO Executor: Finished task 20.0 in stage 4.0 (TID 399). 4544 bytes result sent to driver
[2021-05-16 20:13:07,324] {docker.py:276} INFO - 21/05/16 23:13:07 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 403) (764b1db4ecfd, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:07,326] {docker.py:276} INFO - 21/05/16 23:13:07 INFO Executor: Running task 24.0 in stage 4.0 (TID 403)
21/05/16 23:13:07 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 399) in 2780 ms on 764b1db4ecfd (executor driver) (21/200)
[2021-05-16 20:13:07,336] {docker.py:276} INFO - 21/05/16 23:13:07 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:07,337] {docker.py:276} INFO - 21/05/16 23:13:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:07,339] {docker.py:276} INFO - 21/05/16 23:13:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:07,340] {docker.py:276} INFO - 21/05/16 23:13:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:07,340] {docker.py:276} INFO - 21/05/16 23:13:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245949189931577135193_0004_m_000024_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245949189931577135193_0004_m_000024_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245949189931577135193_0004}; taskId=attempt_202105162312245949189931577135193_0004_m_000024_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@719ec989}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:07,340] {docker.py:276} INFO - 21/05/16 23:13:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:07,341] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Starting: Task committer attempt_202105162312245949189931577135193_0004_m_000024_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245949189931577135193_0004_m_000024_403
[2021-05-16 20:13:07,345] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Task committer attempt_202105162312245949189931577135193_0004_m_000024_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245949189931577135193_0004_m_000024_403 : duration 0:00.005s
[2021-05-16 20:13:07,917] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Starting: Task committer attempt_202105162312244757242078694773185_0004_m_000021_400: needsTaskCommit() Task attempt_202105162312244757242078694773185_0004_m_000021_400
21/05/16 23:13:07 INFO StagingCommitter: Task committer attempt_202105162312244757242078694773185_0004_m_000021_400: needsTaskCommit() Task attempt_202105162312244757242078694773185_0004_m_000021_400: duration 0:00.002s
[2021-05-16 20:13:07,917] {docker.py:276} INFO - 21/05/16 23:13:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244757242078694773185_0004_m_000021_400
[2021-05-16 20:13:07,919] {docker.py:276} INFO - 21/05/16 23:13:07 INFO Executor: Finished task 21.0 in stage 4.0 (TID 400). 4544 bytes result sent to driver
[2021-05-16 20:13:07,920] {docker.py:276} INFO - 21/05/16 23:13:07 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 404) (764b1db4ecfd, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:07,921] {docker.py:276} INFO - 21/05/16 23:13:07 INFO Executor: Running task 25.0 in stage 4.0 (TID 404)
[2021-05-16 20:13:07,922] {docker.py:276} INFO - 21/05/16 23:13:07 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 400) in 2781 ms on 764b1db4ecfd (executor driver) (22/200)
[2021-05-16 20:13:07,931] {docker.py:276} INFO - 21/05/16 23:13:07 INFO ShuffleBlockFetcherIterator: Getting 6 (25.5 KiB) non-empty blocks including 6 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:07,933] {docker.py:276} INFO - 21/05/16 23:13:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244665149966913322954_0004_m_000025_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244665149966913322954_0004_m_000025_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244665149966913322954_0004}; taskId=attempt_202105162312244665149966913322954_0004_m_000025_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1367d8f4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:07,934] {docker.py:276} INFO - 21/05/16 23:13:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:07 INFO StagingCommitter: Starting: Task committer attempt_202105162312244665149966913322954_0004_m_000025_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244665149966913322954_0004_m_000025_404
[2021-05-16 20:13:07,937] {docker.py:276} INFO - 21/05/16 23:13:07 INFO StagingCommitter: Task committer attempt_202105162312244665149966913322954_0004_m_000025_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244665149966913322954_0004_m_000025_404 : duration 0:00.003s
[2021-05-16 20:13:08,265] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312248669840163384778503_0004_m_000022_401: needsTaskCommit() Task attempt_202105162312248669840163384778503_0004_m_000022_401
[2021-05-16 20:13:08,266] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Task committer attempt_202105162312248669840163384778503_0004_m_000022_401: needsTaskCommit() Task attempt_202105162312248669840163384778503_0004_m_000022_401: duration 0:00.001s
21/05/16 23:13:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248669840163384778503_0004_m_000022_401
[2021-05-16 20:13:08,267] {docker.py:276} INFO - 21/05/16 23:13:08 INFO Executor: Finished task 22.0 in stage 4.0 (TID 401). 4544 bytes result sent to driver
[2021-05-16 20:13:08,268] {docker.py:276} INFO - 21/05/16 23:13:08 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 405) (764b1db4ecfd, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:08,269] {docker.py:276} INFO - 21/05/16 23:13:08 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 401) in 2869 ms on 764b1db4ecfd (executor driver) (23/200)
[2021-05-16 20:13:08,269] {docker.py:276} INFO - 21/05/16 23:13:08 INFO Executor: Running task 26.0 in stage 4.0 (TID 405)
[2021-05-16 20:13:08,278] {docker.py:276} INFO - 21/05/16 23:13:08 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/05/16 23:13:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312247668944637872132817_0004_m_000023_402: needsTaskCommit() Task attempt_202105162312247668944637872132817_0004_m_000023_402
[2021-05-16 20:13:08,279] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Task committer attempt_202105162312247668944637872132817_0004_m_000023_402: needsTaskCommit() Task attempt_202105162312247668944637872132817_0004_m_000023_402: duration 0:00.000s
[2021-05-16 20:13:08,279] {docker.py:276} INFO - 21/05/16 23:13:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247668944637872132817_0004_m_000023_402
[2021-05-16 20:13:08,280] {docker.py:276} INFO - 21/05/16 23:13:08 INFO Executor: Finished task 23.0 in stage 4.0 (TID 402). 4544 bytes result sent to driver
[2021-05-16 20:13:08,281] {docker.py:276} INFO - 21/05/16 23:13:08 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 406) (764b1db4ecfd, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:08,281] {docker.py:276} INFO - 21/05/16 23:13:08 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 402) in 2826 ms on 764b1db4ecfd (executor driver) (24/200)
[2021-05-16 20:13:08,283] {docker.py:276} INFO - 21/05/16 23:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:08,284] {docker.py:276} INFO - 21/05/16 23:13:08 INFO Executor: Running task 27.0 in stage 4.0 (TID 406)
[2021-05-16 20:13:08,284] {docker.py:276} INFO - 21/05/16 23:13:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246495169577494307122_0004_m_000026_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246495169577494307122_0004_m_000026_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246495169577494307122_0004}; taskId=attempt_202105162312246495169577494307122_0004_m_000026_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@574006ce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:08,284] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312246495169577494307122_0004_m_000026_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246495169577494307122_0004_m_000026_405
[2021-05-16 20:13:08,289] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Task committer attempt_202105162312246495169577494307122_0004_m_000026_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246495169577494307122_0004_m_000026_405 : duration 0:00.005s
[2021-05-16 20:13:08,298] {docker.py:276} INFO - 21/05/16 23:13:08 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:08,299] {docker.py:276} INFO - 21/05/16 23:13:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2021-05-16 20:13:08,304] {docker.py:276} INFO - 21/05/16 23:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:08,305] {docker.py:276} INFO - 21/05/16 23:13:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246361923491085738474_0004_m_000027_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246361923491085738474_0004_m_000027_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246361923491085738474_0004}; taskId=attempt_202105162312246361923491085738474_0004_m_000027_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@213304f0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:08,306] {docker.py:276} INFO - 21/05/16 23:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312246361923491085738474_0004_m_000027_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246361923491085738474_0004_m_000027_406
[2021-05-16 20:13:08,309] {docker.py:276} INFO - 21/05/16 23:13:08 INFO StagingCommitter: Task committer attempt_202105162312246361923491085738474_0004_m_000027_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246361923491085738474_0004_m_000027_406 : duration 0:00.003s
[2021-05-16 20:13:10,249] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312245949189931577135193_0004_m_000024_403: needsTaskCommit() Task attempt_202105162312245949189931577135193_0004_m_000024_403
[2021-05-16 20:13:10,250] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_202105162312245949189931577135193_0004_m_000024_403: needsTaskCommit() Task attempt_202105162312245949189931577135193_0004_m_000024_403: duration 0:00.003s
21/05/16 23:13:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245949189931577135193_0004_m_000024_403
[2021-05-16 20:13:10,252] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Finished task 24.0 in stage 4.0 (TID 403). 4544 bytes result sent to driver
[2021-05-16 20:13:10,253] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 407) (764b1db4ecfd, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:10,254] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Running task 28.0 in stage 4.0 (TID 407)
[2021-05-16 20:13:10,255] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 403) in 2937 ms on 764b1db4ecfd (executor driver) (25/200)
[2021-05-16 20:13:10,265] {docker.py:276} INFO - 21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:10,268] {docker.py:276} INFO - 21/05/16 23:13:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224511959722980606763_0004_m_000028_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224511959722980606763_0004_m_000028_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224511959722980606763_0004}; taskId=attempt_20210516231224511959722980606763_0004_m_000028_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f0bb9cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_20210516231224511959722980606763_0004_m_000028_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224511959722980606763_0004_m_000028_407
[2021-05-16 20:13:10,272] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_20210516231224511959722980606763_0004_m_000028_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224511959722980606763_0004_m_000028_407 : duration 0:00.004s
[2021-05-16 20:13:10,803] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312244665149966913322954_0004_m_000025_404: needsTaskCommit() Task attempt_202105162312244665149966913322954_0004_m_000025_404
[2021-05-16 20:13:10,804] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_202105162312244665149966913322954_0004_m_000025_404: needsTaskCommit() Task attempt_202105162312244665149966913322954_0004_m_000025_404: duration 0:00.001s
21/05/16 23:13:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244665149966913322954_0004_m_000025_404
[2021-05-16 20:13:10,805] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Finished task 25.0 in stage 4.0 (TID 404). 4544 bytes result sent to driver
[2021-05-16 20:13:10,806] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 408) (764b1db4ecfd, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:10,807] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 404) in 2891 ms on 764b1db4ecfd (executor driver) (26/200)
[2021-05-16 20:13:10,808] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Running task 29.0 in stage 4.0 (TID 408)
[2021-05-16 20:13:10,820] {docker.py:276} INFO - 21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:10,822] {docker.py:276} INFO - 21/05/16 23:13:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242843474503719019315_0004_m_000029_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242843474503719019315_0004_m_000029_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242843474503719019315_0004}; taskId=attempt_202105162312242843474503719019315_0004_m_000029_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@589b091d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:10,823] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312242843474503719019315_0004_m_000029_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242843474503719019315_0004_m_000029_408
[2021-05-16 20:13:10,826] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312246495169577494307122_0004_m_000026_405: needsTaskCommit() Task attempt_202105162312246495169577494307122_0004_m_000026_405
21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_202105162312242843474503719019315_0004_m_000029_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242843474503719019315_0004_m_000029_408 : duration 0:00.004s
[2021-05-16 20:13:10,827] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_202105162312246495169577494307122_0004_m_000026_405: needsTaskCommit() Task attempt_202105162312246495169577494307122_0004_m_000026_405: duration 0:00.002s
21/05/16 23:13:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246495169577494307122_0004_m_000026_405
[2021-05-16 20:13:10,829] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Finished task 26.0 in stage 4.0 (TID 405). 4544 bytes result sent to driver
[2021-05-16 20:13:10,830] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 409) (764b1db4ecfd, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:10,831] {docker.py:276} INFO - 21/05/16 23:13:10 INFO Executor: Running task 30.0 in stage 4.0 (TID 409)
[2021-05-16 20:13:10,831] {docker.py:276} INFO - 21/05/16 23:13:10 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 405) in 2566 ms on 764b1db4ecfd (executor driver) (27/200)
[2021-05-16 20:13:10,842] {docker.py:276} INFO - 21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:10,844] {docker.py:276} INFO - 21/05/16 23:13:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243342287912639078944_0004_m_000030_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243342287912639078944_0004_m_000030_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243342287912639078944_0004}; taskId=attempt_202105162312243342287912639078944_0004_m_000030_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e1f5f2a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312243342287912639078944_0004_m_000030_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243342287912639078944_0004_m_000030_409
[2021-05-16 20:13:10,847] {docker.py:276} INFO - 21/05/16 23:13:10 INFO StagingCommitter: Task committer attempt_202105162312243342287912639078944_0004_m_000030_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243342287912639078944_0004_m_000030_409 : duration 0:00.003s
[2021-05-16 20:13:11,129] {docker.py:276} INFO - 21/05/16 23:13:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312246361923491085738474_0004_m_000027_406: needsTaskCommit() Task attempt_202105162312246361923491085738474_0004_m_000027_406
[2021-05-16 20:13:11,131] {docker.py:276} INFO - 21/05/16 23:13:11 INFO StagingCommitter: Task committer attempt_202105162312246361923491085738474_0004_m_000027_406: needsTaskCommit() Task attempt_202105162312246361923491085738474_0004_m_000027_406: duration 0:00.002s
21/05/16 23:13:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246361923491085738474_0004_m_000027_406
[2021-05-16 20:13:11,133] {docker.py:276} INFO - 21/05/16 23:13:11 INFO Executor: Finished task 27.0 in stage 4.0 (TID 406). 4544 bytes result sent to driver
[2021-05-16 20:13:11,134] {docker.py:276} INFO - 21/05/16 23:13:11 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 410) (764b1db4ecfd, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:11,136] {docker.py:276} INFO - 21/05/16 23:13:11 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 406) in 2858 ms on 764b1db4ecfd (executor driver) (28/200)
[2021-05-16 20:13:11,137] {docker.py:276} INFO - 21/05/16 23:13:11 INFO Executor: Running task 31.0 in stage 4.0 (TID 410)
[2021-05-16 20:13:11,147] {docker.py:276} INFO - 21/05/16 23:13:11 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:11,150] {docker.py:276} INFO - 21/05/16 23:13:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242921151449775620875_0004_m_000031_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242921151449775620875_0004_m_000031_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242921151449775620875_0004}; taskId=attempt_202105162312242921151449775620875_0004_m_000031_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c7bc877}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312242921151449775620875_0004_m_000031_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242921151449775620875_0004_m_000031_410
[2021-05-16 20:13:11,154] {docker.py:276} INFO - 21/05/16 23:13:11 INFO StagingCommitter: Task committer attempt_202105162312242921151449775620875_0004_m_000031_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242921151449775620875_0004_m_000031_410 : duration 0:00.004s
[2021-05-16 20:13:13,221] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_20210516231224511959722980606763_0004_m_000028_407: needsTaskCommit() Task attempt_20210516231224511959722980606763_0004_m_000028_407
[2021-05-16 20:13:13,221] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_20210516231224511959722980606763_0004_m_000028_407: needsTaskCommit() Task attempt_20210516231224511959722980606763_0004_m_000028_407: duration 0:00.001s
21/05/16 23:13:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224511959722980606763_0004_m_000028_407
[2021-05-16 20:13:13,223] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Finished task 28.0 in stage 4.0 (TID 407). 4544 bytes result sent to driver
[2021-05-16 20:13:13,224] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 411) (764b1db4ecfd, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:13,225] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 407) in 2976 ms on 764b1db4ecfd (executor driver) (29/200)
[2021-05-16 20:13:13,225] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Running task 32.0 in stage 4.0 (TID 411)
[2021-05-16 20:13:13,234] {docker.py:276} INFO - 21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:13,236] {docker.py:276} INFO - 21/05/16 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:13,237] {docker.py:276} INFO - 21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245061582243861465598_0004_m_000032_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245061582243861465598_0004_m_000032_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245061582243861465598_0004}; taskId=attempt_202105162312245061582243861465598_0004_m_000032_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ce77e8d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:13,237] {docker.py:276} INFO - 21/05/16 23:13:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:13,238] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312245061582243861465598_0004_m_000032_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245061582243861465598_0004_m_000032_411
[2021-05-16 20:13:13,241] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312245061582243861465598_0004_m_000032_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245061582243861465598_0004_m_000032_411 : duration 0:00.003s
[2021-05-16 20:13:13,538] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312243342287912639078944_0004_m_000030_409: needsTaskCommit() Task attempt_202105162312243342287912639078944_0004_m_000030_409
[2021-05-16 20:13:13,540] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312243342287912639078944_0004_m_000030_409: needsTaskCommit() Task attempt_202105162312243342287912639078944_0004_m_000030_409: duration 0:00.002s
21/05/16 23:13:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243342287912639078944_0004_m_000030_409
[2021-05-16 20:13:13,541] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Finished task 30.0 in stage 4.0 (TID 409). 4544 bytes result sent to driver
[2021-05-16 20:13:13,543] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 412) (764b1db4ecfd, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:13,544] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Running task 33.0 in stage 4.0 (TID 412)
21/05/16 23:13:13 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 409) in 2716 ms on 764b1db4ecfd (executor driver) (30/200)
[2021-05-16 20:13:13,548] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312242843474503719019315_0004_m_000029_408: needsTaskCommit() Task attempt_202105162312242843474503719019315_0004_m_000029_408
[2021-05-16 20:13:13,549] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312242843474503719019315_0004_m_000029_408: needsTaskCommit() Task attempt_202105162312242843474503719019315_0004_m_000029_408: duration 0:00.001s
21/05/16 23:13:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242843474503719019315_0004_m_000029_408
[2021-05-16 20:13:13,550] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Finished task 29.0 in stage 4.0 (TID 408). 4544 bytes result sent to driver
[2021-05-16 20:13:13,552] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 413) (764b1db4ecfd, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:13,553] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 408) in 2751 ms on 764b1db4ecfd (executor driver) (31/200)
[2021-05-16 20:13:13,553] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Running task 34.0 in stage 4.0 (TID 413)
[2021-05-16 20:13:13,559] {docker.py:276} INFO - 21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:13,561] {docker.py:276} INFO - 21/05/16 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:13,562] {docker.py:276} INFO - 21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246192739457167150063_0004_m_000033_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246192739457167150063_0004_m_000033_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246192739457167150063_0004}; taskId=attempt_202105162312246192739457167150063_0004_m_000033_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@14e3007e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:13,562] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312246192739457167150063_0004_m_000033_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246192739457167150063_0004_m_000033_412
[2021-05-16 20:13:13,566] {docker.py:276} INFO - 21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Getting 6 (25.3 KiB) non-empty blocks including 6 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312246192739457167150063_0004_m_000033_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246192739457167150063_0004_m_000033_412 : duration 0:00.004s
21/05/16 23:13:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:13:13,568] {docker.py:276} INFO - 21/05/16 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:13,569] {docker.py:276} INFO - 21/05/16 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:13,570] {docker.py:276} INFO - 21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:13,571] {docker.py:276} INFO - 21/05/16 23:13:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243025605759508361165_0004_m_000034_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243025605759508361165_0004_m_000034_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243025605759508361165_0004}; taskId=attempt_202105162312243025605759508361165_0004_m_000034_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6571fa52}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:13,571] {docker.py:276} INFO - 21/05/16 23:13:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:13,572] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312243025605759508361165_0004_m_000034_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243025605759508361165_0004_m_000034_413
[2021-05-16 20:13:13,575] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312243025605759508361165_0004_m_000034_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243025605759508361165_0004_m_000034_413 : duration 0:00.004s
[2021-05-16 20:13:13,979] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312242921151449775620875_0004_m_000031_410: needsTaskCommit() Task attempt_202105162312242921151449775620875_0004_m_000031_410
[2021-05-16 20:13:13,980] {docker.py:276} INFO - 21/05/16 23:13:13 INFO StagingCommitter: Task committer attempt_202105162312242921151449775620875_0004_m_000031_410: needsTaskCommit() Task attempt_202105162312242921151449775620875_0004_m_000031_410: duration 0:00.003s
21/05/16 23:13:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242921151449775620875_0004_m_000031_410
[2021-05-16 20:13:13,981] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Finished task 31.0 in stage 4.0 (TID 410). 4544 bytes result sent to driver
[2021-05-16 20:13:13,982] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 414) (764b1db4ecfd, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:13,982] {docker.py:276} INFO - 21/05/16 23:13:13 INFO Executor: Running task 35.0 in stage 4.0 (TID 414)
[2021-05-16 20:13:13,983] {docker.py:276} INFO - 21/05/16 23:13:13 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 410) in 2853 ms on 764b1db4ecfd (executor driver) (32/200)
[2021-05-16 20:13:13,991] {docker.py:276} INFO - 21/05/16 23:13:14 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:13,994] {docker.py:276} INFO - 21/05/16 23:13:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224516866204468773955_0004_m_000035_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224516866204468773955_0004_m_000035_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224516866204468773955_0004}; taskId=attempt_20210516231224516866204468773955_0004_m_000035_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f025f74}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:13,994] {docker.py:276} INFO - 21/05/16 23:13:14 INFO StagingCommitter: Starting: Task committer attempt_20210516231224516866204468773955_0004_m_000035_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224516866204468773955_0004_m_000035_414
[2021-05-16 20:13:13,997] {docker.py:276} INFO - 21/05/16 23:13:14 INFO StagingCommitter: Task committer attempt_20210516231224516866204468773955_0004_m_000035_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224516866204468773955_0004_m_000035_414 : duration 0:00.003s
[2021-05-16 20:13:16,068] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312246192739457167150063_0004_m_000033_412: needsTaskCommit() Task attempt_202105162312246192739457167150063_0004_m_000033_412
[2021-05-16 20:13:16,069] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312246192739457167150063_0004_m_000033_412: needsTaskCommit() Task attempt_202105162312246192739457167150063_0004_m_000033_412: duration 0:00.001s
21/05/16 23:13:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246192739457167150063_0004_m_000033_412
[2021-05-16 20:13:16,072] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Finished task 33.0 in stage 4.0 (TID 412). 4587 bytes result sent to driver
[2021-05-16 20:13:16,074] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 415) (764b1db4ecfd, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:16,075] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Running task 36.0 in stage 4.0 (TID 415)
[2021-05-16 20:13:16,075] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 412) in 2536 ms on 764b1db4ecfd (executor driver) (33/200)
[2021-05-16 20:13:16,084] {docker.py:276} INFO - 21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:16,087] {docker.py:276} INFO - 21/05/16 23:13:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:16,087] {docker.py:276} INFO - 21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244300356318578336857_0004_m_000036_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244300356318578336857_0004_m_000036_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244300356318578336857_0004}; taskId=attempt_202105162312244300356318578336857_0004_m_000036_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@595cef78}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:16,087] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312244300356318578336857_0004_m_000036_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244300356318578336857_0004_m_000036_415
[2021-05-16 20:13:16,090] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312244300356318578336857_0004_m_000036_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244300356318578336857_0004_m_000036_415 : duration 0:00.003s
[2021-05-16 20:13:16,153] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312245061582243861465598_0004_m_000032_411: needsTaskCommit() Task attempt_202105162312245061582243861465598_0004_m_000032_411
[2021-05-16 20:13:16,154] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312245061582243861465598_0004_m_000032_411: needsTaskCommit() Task attempt_202105162312245061582243861465598_0004_m_000032_411: duration 0:00.001s
[2021-05-16 20:13:16,155] {docker.py:276} INFO - 21/05/16 23:13:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245061582243861465598_0004_m_000032_411
[2021-05-16 20:13:16,156] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Finished task 32.0 in stage 4.0 (TID 411). 4587 bytes result sent to driver
[2021-05-16 20:13:16,157] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 416) (764b1db4ecfd, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:16,158] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Running task 37.0 in stage 4.0 (TID 416)
[2021-05-16 20:13:16,158] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 411) in 2938 ms on 764b1db4ecfd (executor driver) (34/200)
[2021-05-16 20:13:16,166] {docker.py:276} INFO - 21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Getting 6 (23.8 KiB) non-empty blocks including 6 (23.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:16,168] {docker.py:276} INFO - 21/05/16 23:13:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:16,169] {docker.py:276} INFO - 21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246944605114364284861_0004_m_000037_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246944605114364284861_0004_m_000037_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246944605114364284861_0004}; taskId=attempt_202105162312246944605114364284861_0004_m_000037_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a0d78a2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:16,169] {docker.py:276} INFO - 21/05/16 23:13:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:16,169] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312246944605114364284861_0004_m_000037_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246944605114364284861_0004_m_000037_416
[2021-05-16 20:13:16,175] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312246944605114364284861_0004_m_000037_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246944605114364284861_0004_m_000037_416 : duration 0:00.006s
[2021-05-16 20:13:16,344] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_20210516231224516866204468773955_0004_m_000035_414: needsTaskCommit() Task attempt_20210516231224516866204468773955_0004_m_000035_414
[2021-05-16 20:13:16,345] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_20210516231224516866204468773955_0004_m_000035_414: needsTaskCommit() Task attempt_20210516231224516866204468773955_0004_m_000035_414: duration 0:00.000s
21/05/16 23:13:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224516866204468773955_0004_m_000035_414
[2021-05-16 20:13:16,346] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Finished task 35.0 in stage 4.0 (TID 414). 4587 bytes result sent to driver
21/05/16 23:13:16 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 417) (764b1db4ecfd, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:16,347] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Running task 38.0 in stage 4.0 (TID 417)
[2021-05-16 20:13:16,347] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 414) in 2369 ms on 764b1db4ecfd (executor driver) (35/200)
[2021-05-16 20:13:16,354] {docker.py:276} INFO - 21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:16,356] {docker.py:276} INFO - 21/05/16 23:13:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224559299272965650651_0004_m_000038_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224559299272965650651_0004_m_000038_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224559299272965650651_0004}; taskId=attempt_20210516231224559299272965650651_0004_m_000038_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f7a8d89}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:16,357] {docker.py:276} INFO - 21/05/16 23:13:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_20210516231224559299272965650651_0004_m_000038_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224559299272965650651_0004_m_000038_417
[2021-05-16 20:13:16,360] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_20210516231224559299272965650651_0004_m_000038_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224559299272965650651_0004_m_000038_417 : duration 0:00.003s
[2021-05-16 20:13:16,371] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312243025605759508361165_0004_m_000034_413: needsTaskCommit() Task attempt_202105162312243025605759508361165_0004_m_000034_413
21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312243025605759508361165_0004_m_000034_413: needsTaskCommit() Task attempt_202105162312243025605759508361165_0004_m_000034_413: duration 0:00.000s
21/05/16 23:13:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243025605759508361165_0004_m_000034_413
[2021-05-16 20:13:16,371] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Finished task 34.0 in stage 4.0 (TID 413). 4587 bytes result sent to driver
[2021-05-16 20:13:16,372] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 418) (764b1db4ecfd, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:16,373] {docker.py:276} INFO - 21/05/16 23:13:16 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 413) in 2826 ms on 764b1db4ecfd (executor driver) (36/200)
[2021-05-16 20:13:16,373] {docker.py:276} INFO - 21/05/16 23:13:16 INFO Executor: Running task 39.0 in stage 4.0 (TID 418)
[2021-05-16 20:13:16,381] {docker.py:276} INFO - 21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:16,382] {docker.py:276} INFO - 21/05/16 23:13:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:16,383] {docker.py:276} INFO - 21/05/16 23:13:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:16,384] {docker.py:276} INFO - 21/05/16 23:13:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:16,384] {docker.py:276} INFO - 21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:16,385] {docker.py:276} INFO - 21/05/16 23:13:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245841137345453098660_0004_m_000039_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245841137345453098660_0004_m_000039_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245841137345453098660_0004}; taskId=attempt_202105162312245841137345453098660_0004_m_000039_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d279e0d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:16,385] {docker.py:276} INFO - 21/05/16 23:13:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:16,385] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Starting: Task committer attempt_202105162312245841137345453098660_0004_m_000039_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245841137345453098660_0004_m_000039_418
[2021-05-16 20:13:16,388] {docker.py:276} INFO - 21/05/16 23:13:16 INFO StagingCommitter: Task committer attempt_202105162312245841137345453098660_0004_m_000039_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245841137345453098660_0004_m_000039_418 : duration 0:00.003s
[2021-05-16 20:13:18,800] {docker.py:276} INFO - 21/05/16 23:13:18 INFO StagingCommitter: Starting: Task committer attempt_202105162312244300356318578336857_0004_m_000036_415: needsTaskCommit() Task attempt_202105162312244300356318578336857_0004_m_000036_415
[2021-05-16 20:13:18,801] {docker.py:276} INFO - 21/05/16 23:13:18 INFO StagingCommitter: Task committer attempt_202105162312244300356318578336857_0004_m_000036_415: needsTaskCommit() Task attempt_202105162312244300356318578336857_0004_m_000036_415: duration 0:00.001s
21/05/16 23:13:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244300356318578336857_0004_m_000036_415
[2021-05-16 20:13:18,803] {docker.py:276} INFO - 21/05/16 23:13:18 INFO Executor: Finished task 36.0 in stage 4.0 (TID 415). 4544 bytes result sent to driver
[2021-05-16 20:13:18,805] {docker.py:276} INFO - 21/05/16 23:13:18 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 419) (764b1db4ecfd, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:18,806] {docker.py:276} INFO - 21/05/16 23:13:18 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 415) in 2736 ms on 764b1db4ecfd (executor driver) (37/200)
[2021-05-16 20:13:18,807] {docker.py:276} INFO - 21/05/16 23:13:18 INFO Executor: Running task 40.0 in stage 4.0 (TID 419)
[2021-05-16 20:13:18,817] {docker.py:276} INFO - 21/05/16 23:13:18 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:18,818] {docker.py:276} INFO - 21/05/16 23:13:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:18,820] {docker.py:276} INFO - 21/05/16 23:13:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:18,820] {docker.py:276} INFO - 21/05/16 23:13:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244163141882255759342_0004_m_000040_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244163141882255759342_0004_m_000040_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244163141882255759342_0004}; taskId=attempt_202105162312244163141882255759342_0004_m_000040_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@695d387f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:18,820] {docker.py:276} INFO - 21/05/16 23:13:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:18 INFO StagingCommitter: Starting: Task committer attempt_202105162312244163141882255759342_0004_m_000040_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244163141882255759342_0004_m_000040_419
[2021-05-16 20:13:18,823] {docker.py:276} INFO - 21/05/16 23:13:18 INFO StagingCommitter: Task committer attempt_202105162312244163141882255759342_0004_m_000040_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244163141882255759342_0004_m_000040_419 : duration 0:00.003s
[2021-05-16 20:13:19,069] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_202105162312245841137345453098660_0004_m_000039_418: needsTaskCommit() Task attempt_202105162312245841137345453098660_0004_m_000039_418
[2021-05-16 20:13:19,070] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_202105162312245841137345453098660_0004_m_000039_418: needsTaskCommit() Task attempt_202105162312245841137345453098660_0004_m_000039_418: duration 0:00.000s
21/05/16 23:13:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245841137345453098660_0004_m_000039_418
[2021-05-16 20:13:19,071] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Finished task 39.0 in stage 4.0 (TID 418). 4544 bytes result sent to driver
[2021-05-16 20:13:19,074] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 420) (764b1db4ecfd, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:19,075] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 418) in 2705 ms on 764b1db4ecfd (executor driver) (38/200)
[2021-05-16 20:13:19,076] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Running task 41.0 in stage 4.0 (TID 420)
[2021-05-16 20:13:19,084] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_20210516231224559299272965650651_0004_m_000038_417: needsTaskCommit() Task attempt_20210516231224559299272965650651_0004_m_000038_417
[2021-05-16 20:13:19,085] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_20210516231224559299272965650651_0004_m_000038_417: needsTaskCommit() Task attempt_20210516231224559299272965650651_0004_m_000038_417: duration 0:00.000s
21/05/16 23:13:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224559299272965650651_0004_m_000038_417
[2021-05-16 20:13:19,086] {docker.py:276} INFO - 21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:19,086] {docker.py:276} INFO - 21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:13:19,086] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Finished task 38.0 in stage 4.0 (TID 417). 4544 bytes result sent to driver
[2021-05-16 20:13:19,087] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 421) (764b1db4ecfd, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:19,088] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 417) in 2745 ms on 764b1db4ecfd (executor driver) (39/200)
[2021-05-16 20:13:19,088] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Running task 42.0 in stage 4.0 (TID 421)
[2021-05-16 20:13:19,093] {docker.py:276} INFO - 21/05/16 23:13:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:19,093] {docker.py:276} INFO - 21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244883041336130954449_0004_m_000041_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244883041336130954449_0004_m_000041_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244883041336130954449_0004}; taskId=attempt_202105162312244883041336130954449_0004_m_000041_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6962301d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:19,094] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_202105162312244883041336130954449_0004_m_000041_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244883041336130954449_0004_m_000041_420
[2021-05-16 20:13:19,094] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_202105162312244883041336130954449_0004_m_000041_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244883041336130954449_0004_m_000041_420 : duration 0:00.004s
[2021-05-16 20:13:19,097] {docker.py:276} INFO - 21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:19,098] {docker.py:276} INFO - 21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:19,099] {docker.py:276} INFO - 21/05/16 23:13:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:19,100] {docker.py:276} INFO - 21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:19,101] {docker.py:276} INFO - 21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248834956201830730128_0004_m_000042_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248834956201830730128_0004_m_000042_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248834956201830730128_0004}; taskId=attempt_202105162312248834956201830730128_0004_m_000042_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3584c147}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:19,101] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_202105162312248834956201830730128_0004_m_000042_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248834956201830730128_0004_m_000042_421
[2021-05-16 20:13:19,104] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_202105162312248834956201830730128_0004_m_000042_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248834956201830730128_0004_m_000042_421 : duration 0:00.003s
[2021-05-16 20:13:19,376] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_202105162312246944605114364284861_0004_m_000037_416: needsTaskCommit() Task attempt_202105162312246944605114364284861_0004_m_000037_416
[2021-05-16 20:13:19,377] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_202105162312246944605114364284861_0004_m_000037_416: needsTaskCommit() Task attempt_202105162312246944605114364284861_0004_m_000037_416: duration 0:00.001s
[2021-05-16 20:13:19,378] {docker.py:276} INFO - 21/05/16 23:13:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246944605114364284861_0004_m_000037_416
[2021-05-16 20:13:19,379] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Finished task 37.0 in stage 4.0 (TID 416). 4544 bytes result sent to driver
[2021-05-16 20:13:19,381] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 422) (764b1db4ecfd, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:19,382] {docker.py:276} INFO - 21/05/16 23:13:19 INFO Executor: Running task 43.0 in stage 4.0 (TID 422)
[2021-05-16 20:13:19,383] {docker.py:276} INFO - 21/05/16 23:13:19 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 416) in 3230 ms on 764b1db4ecfd (executor driver) (40/200)
[2021-05-16 20:13:19,392] {docker.py:276} INFO - 21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:19,395] {docker.py:276} INFO - 21/05/16 23:13:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248371608103861017344_0004_m_000043_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248371608103861017344_0004_m_000043_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248371608103861017344_0004}; taskId=attempt_202105162312248371608103861017344_0004_m_000043_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b4ba485}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:19,396] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Starting: Task committer attempt_202105162312248371608103861017344_0004_m_000043_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248371608103861017344_0004_m_000043_422
[2021-05-16 20:13:19,398] {docker.py:276} INFO - 21/05/16 23:13:19 INFO StagingCommitter: Task committer attempt_202105162312248371608103861017344_0004_m_000043_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248371608103861017344_0004_m_000043_422 : duration 0:00.003s
[2021-05-16 20:13:21,569] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312244163141882255759342_0004_m_000040_419: needsTaskCommit() Task attempt_202105162312244163141882255759342_0004_m_000040_419
[2021-05-16 20:13:21,570] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Task committer attempt_202105162312244163141882255759342_0004_m_000040_419: needsTaskCommit() Task attempt_202105162312244163141882255759342_0004_m_000040_419: duration 0:00.000s
21/05/16 23:13:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244163141882255759342_0004_m_000040_419
[2021-05-16 20:13:21,570] {docker.py:276} INFO - 21/05/16 23:13:21 INFO Executor: Finished task 40.0 in stage 4.0 (TID 419). 4544 bytes result sent to driver
[2021-05-16 20:13:21,571] {docker.py:276} INFO - 21/05/16 23:13:21 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 423) (764b1db4ecfd, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:21,572] {docker.py:276} INFO - 21/05/16 23:13:21 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 419) in 2771 ms on 764b1db4ecfd (executor driver) (41/200)
[2021-05-16 20:13:21,573] {docker.py:276} INFO - 21/05/16 23:13:21 INFO Executor: Running task 44.0 in stage 4.0 (TID 423)
[2021-05-16 20:13:21,581] {docker.py:276} INFO - 21/05/16 23:13:21 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:21,583] {docker.py:276} INFO - 21/05/16 23:13:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242553326997764570316_0004_m_000044_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242553326997764570316_0004_m_000044_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242553326997764570316_0004}; taskId=attempt_202105162312242553326997764570316_0004_m_000044_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5695503b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312242553326997764570316_0004_m_000044_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242553326997764570316_0004_m_000044_423
[2021-05-16 20:13:21,586] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Task committer attempt_202105162312242553326997764570316_0004_m_000044_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242553326997764570316_0004_m_000044_423 : duration 0:00.002s
[2021-05-16 20:13:21,894] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312244883041336130954449_0004_m_000041_420: needsTaskCommit() Task attempt_202105162312244883041336130954449_0004_m_000041_420
[2021-05-16 20:13:21,896] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Task committer attempt_202105162312244883041336130954449_0004_m_000041_420: needsTaskCommit() Task attempt_202105162312244883041336130954449_0004_m_000041_420: duration 0:00.002s
21/05/16 23:13:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244883041336130954449_0004_m_000041_420
[2021-05-16 20:13:21,897] {docker.py:276} INFO - 21/05/16 23:13:21 INFO Executor: Finished task 41.0 in stage 4.0 (TID 420). 4544 bytes result sent to driver
[2021-05-16 20:13:21,899] {docker.py:276} INFO - 21/05/16 23:13:21 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 424) (764b1db4ecfd, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:21,900] {docker.py:276} INFO - 21/05/16 23:13:21 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 420) in 2830 ms on 764b1db4ecfd (executor driver) (42/200)
[2021-05-16 20:13:21,901] {docker.py:276} INFO - 21/05/16 23:13:21 INFO Executor: Running task 45.0 in stage 4.0 (TID 424)
[2021-05-16 20:13:21,910] {docker.py:276} INFO - 21/05/16 23:13:21 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:21,911] {docker.py:276} INFO - 21/05/16 23:13:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:21,913] {docker.py:276} INFO - 21/05/16 23:13:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:21,913] {docker.py:276} INFO - 21/05/16 23:13:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:21,913] {docker.py:276} INFO - 21/05/16 23:13:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:21,914] {docker.py:276} INFO - 21/05/16 23:13:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243484237928182977197_0004_m_000045_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243484237928182977197_0004_m_000045_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243484237928182977197_0004}; taskId=attempt_202105162312243484237928182977197_0004_m_000045_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f383042}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:21,914] {docker.py:276} INFO - 21/05/16 23:13:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:21,914] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312243484237928182977197_0004_m_000045_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243484237928182977197_0004_m_000045_424
[2021-05-16 20:13:21,917] {docker.py:276} INFO - 21/05/16 23:13:21 INFO StagingCommitter: Task committer attempt_202105162312243484237928182977197_0004_m_000045_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243484237928182977197_0004_m_000045_424 : duration 0:00.002s
[2021-05-16 20:13:21,987] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Starting: Task committer attempt_202105162312248834956201830730128_0004_m_000042_421: needsTaskCommit() Task attempt_202105162312248834956201830730128_0004_m_000042_421
[2021-05-16 20:13:21,988] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Task committer attempt_202105162312248834956201830730128_0004_m_000042_421: needsTaskCommit() Task attempt_202105162312248834956201830730128_0004_m_000042_421: duration 0:00.001s
21/05/16 23:13:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248834956201830730128_0004_m_000042_421
[2021-05-16 20:13:21,989] {docker.py:276} INFO - 21/05/16 23:13:22 INFO Executor: Finished task 42.0 in stage 4.0 (TID 421). 4544 bytes result sent to driver
[2021-05-16 20:13:21,990] {docker.py:276} INFO - 21/05/16 23:13:22 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 425) (764b1db4ecfd, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:21,991] {docker.py:276} INFO - 21/05/16 23:13:22 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 421) in 2907 ms on 764b1db4ecfd (executor driver) (43/200)
[2021-05-16 20:13:21,992] {docker.py:276} INFO - 21/05/16 23:13:22 INFO Executor: Running task 46.0 in stage 4.0 (TID 425)
[2021-05-16 20:13:22,000] {docker.py:276} INFO - 21/05/16 23:13:22 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:22,002] {docker.py:276} INFO - 21/05/16 23:13:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224168160422805318189_0004_m_000046_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224168160422805318189_0004_m_000046_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224168160422805318189_0004}; taskId=attempt_20210516231224168160422805318189_0004_m_000046_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ef9735e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:22,002] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Starting: Task committer attempt_20210516231224168160422805318189_0004_m_000046_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224168160422805318189_0004_m_000046_425
[2021-05-16 20:13:22,005] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Task committer attempt_20210516231224168160422805318189_0004_m_000046_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224168160422805318189_0004_m_000046_425 : duration 0:00.003s
[2021-05-16 20:13:22,379] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Starting: Task committer attempt_202105162312248371608103861017344_0004_m_000043_422: needsTaskCommit() Task attempt_202105162312248371608103861017344_0004_m_000043_422
21/05/16 23:13:22 INFO StagingCommitter: Task committer attempt_202105162312248371608103861017344_0004_m_000043_422: needsTaskCommit() Task attempt_202105162312248371608103861017344_0004_m_000043_422: duration 0:00.001s
[2021-05-16 20:13:22,380] {docker.py:276} INFO - 21/05/16 23:13:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248371608103861017344_0004_m_000043_422
[2021-05-16 20:13:22,380] {docker.py:276} INFO - 21/05/16 23:13:22 INFO Executor: Finished task 43.0 in stage 4.0 (TID 422). 4544 bytes result sent to driver
[2021-05-16 20:13:22,382] {docker.py:276} INFO - 21/05/16 23:13:22 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 426) (764b1db4ecfd, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:22,383] {docker.py:276} INFO - 21/05/16 23:13:22 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 422) in 3006 ms on 764b1db4ecfd (executor driver) (44/200)
[2021-05-16 20:13:22,384] {docker.py:276} INFO - 21/05/16 23:13:22 INFO Executor: Running task 47.0 in stage 4.0 (TID 426)
[2021-05-16 20:13:22,394] {docker.py:276} INFO - 21/05/16 23:13:22 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:22,396] {docker.py:276} INFO - 21/05/16 23:13:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245328219098609869868_0004_m_000047_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245328219098609869868_0004_m_000047_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245328219098609869868_0004}; taskId=attempt_202105162312245328219098609869868_0004_m_000047_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@164034c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:22,396] {docker.py:276} INFO - 21/05/16 23:13:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:22 INFO StagingCommitter: Starting: Task committer attempt_202105162312245328219098609869868_0004_m_000047_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245328219098609869868_0004_m_000047_426
[2021-05-16 20:13:22,401] {docker.py:276} INFO - 21/05/16 23:13:22 INFO StagingCommitter: Task committer attempt_202105162312245328219098609869868_0004_m_000047_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245328219098609869868_0004_m_000047_426 : duration 0:00.005s
[2021-05-16 20:13:24,638] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312242553326997764570316_0004_m_000044_423: needsTaskCommit() Task attempt_202105162312242553326997764570316_0004_m_000044_423
[2021-05-16 20:13:24,639] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_202105162312242553326997764570316_0004_m_000044_423: needsTaskCommit() Task attempt_202105162312242553326997764570316_0004_m_000044_423: duration 0:00.001s
21/05/16 23:13:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242553326997764570316_0004_m_000044_423
[2021-05-16 20:13:24,641] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Finished task 44.0 in stage 4.0 (TID 423). 4544 bytes result sent to driver
[2021-05-16 20:13:24,642] {docker.py:276} INFO - 21/05/16 23:13:24 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 427) (764b1db4ecfd, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:24,643] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Running task 48.0 in stage 4.0 (TID 427)
21/05/16 23:13:24 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 423) in 3076 ms on 764b1db4ecfd (executor driver) (45/200)
[2021-05-16 20:13:24,654] {docker.py:276} INFO - 21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Getting 6 (26.3 KiB) non-empty blocks including 6 (26.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:24,656] {docker.py:276} INFO - 21/05/16 23:13:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:24,657] {docker.py:276} INFO - 21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246923312006738981763_0004_m_000048_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246923312006738981763_0004_m_000048_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246923312006738981763_0004}; taskId=attempt_202105162312246923312006738981763_0004_m_000048_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a819970}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312246923312006738981763_0004_m_000048_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246923312006738981763_0004_m_000048_427
[2021-05-16 20:13:24,659] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_202105162312246923312006738981763_0004_m_000048_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246923312006738981763_0004_m_000048_427 : duration 0:00.003s
[2021-05-16 20:13:24,712] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_20210516231224168160422805318189_0004_m_000046_425: needsTaskCommit() Task attempt_20210516231224168160422805318189_0004_m_000046_425
[2021-05-16 20:13:24,713] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_20210516231224168160422805318189_0004_m_000046_425: needsTaskCommit() Task attempt_20210516231224168160422805318189_0004_m_000046_425: duration 0:00.001s
21/05/16 23:13:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224168160422805318189_0004_m_000046_425
[2021-05-16 20:13:24,715] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Finished task 46.0 in stage 4.0 (TID 425). 4544 bytes result sent to driver
[2021-05-16 20:13:24,716] {docker.py:276} INFO - 21/05/16 23:13:24 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 428) (764b1db4ecfd, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:24,718] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Running task 49.0 in stage 4.0 (TID 428)
[2021-05-16 20:13:24,718] {docker.py:276} INFO - 21/05/16 23:13:24 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 425) in 2731 ms on 764b1db4ecfd (executor driver) (46/200)
[2021-05-16 20:13:24,729] {docker.py:276} INFO - 21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:24,731] {docker.py:276} INFO - 21/05/16 23:13:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246858854845803532937_0004_m_000049_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246858854845803532937_0004_m_000049_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246858854845803532937_0004}; taskId=attempt_202105162312246858854845803532937_0004_m_000049_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c330e3f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:24,732] {docker.py:276} INFO - 21/05/16 23:13:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:24,732] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312246858854845803532937_0004_m_000049_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246858854845803532937_0004_m_000049_428
[2021-05-16 20:13:24,735] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_202105162312246858854845803532937_0004_m_000049_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246858854845803532937_0004_m_000049_428 : duration 0:00.003s
[2021-05-16 20:13:24,739] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312243484237928182977197_0004_m_000045_424: needsTaskCommit() Task attempt_202105162312243484237928182977197_0004_m_000045_424
[2021-05-16 20:13:24,739] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_202105162312243484237928182977197_0004_m_000045_424: needsTaskCommit() Task attempt_202105162312243484237928182977197_0004_m_000045_424: duration 0:00.000s
21/05/16 23:13:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243484237928182977197_0004_m_000045_424
[2021-05-16 20:13:24,740] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Finished task 45.0 in stage 4.0 (TID 424). 4544 bytes result sent to driver
[2021-05-16 20:13:24,741] {docker.py:276} INFO - 21/05/16 23:13:24 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 429) (764b1db4ecfd, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:24,741] {docker.py:276} INFO - 21/05/16 23:13:24 INFO Executor: Running task 50.0 in stage 4.0 (TID 429)
21/05/16 23:13:24 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 424) in 2847 ms on 764b1db4ecfd (executor driver) (47/200)
[2021-05-16 20:13:24,749] {docker.py:276} INFO - 21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Getting 6 (27.5 KiB) non-empty blocks including 6 (27.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:24,751] {docker.py:276} INFO - 21/05/16 23:13:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243433583391435827665_0004_m_000050_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243433583391435827665_0004_m_000050_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243433583391435827665_0004}; taskId=attempt_202105162312243433583391435827665_0004_m_000050_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7becdfe6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:24,751] {docker.py:276} INFO - 21/05/16 23:13:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312243433583391435827665_0004_m_000050_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243433583391435827665_0004_m_000050_429
[2021-05-16 20:13:24,754] {docker.py:276} INFO - 21/05/16 23:13:24 INFO StagingCommitter: Task committer attempt_202105162312243433583391435827665_0004_m_000050_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243433583391435827665_0004_m_000050_429 : duration 0:00.003s
[2021-05-16 20:13:25,214] {docker.py:276} INFO - 21/05/16 23:13:25 INFO StagingCommitter: Starting: Task committer attempt_202105162312245328219098609869868_0004_m_000047_426: needsTaskCommit() Task attempt_202105162312245328219098609869868_0004_m_000047_426
[2021-05-16 20:13:25,215] {docker.py:276} INFO - 21/05/16 23:13:25 INFO StagingCommitter: Task committer attempt_202105162312245328219098609869868_0004_m_000047_426: needsTaskCommit() Task attempt_202105162312245328219098609869868_0004_m_000047_426: duration 0:00.000s
21/05/16 23:13:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245328219098609869868_0004_m_000047_426
[2021-05-16 20:13:25,215] {docker.py:276} INFO - 21/05/16 23:13:25 INFO Executor: Finished task 47.0 in stage 4.0 (TID 426). 4544 bytes result sent to driver
[2021-05-16 20:13:25,217] {docker.py:276} INFO - 21/05/16 23:13:25 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 430) (764b1db4ecfd, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:25,217] {docker.py:276} INFO - 21/05/16 23:13:25 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 426) in 2839 ms on 764b1db4ecfd (executor driver) (48/200)
[2021-05-16 20:13:25,218] {docker.py:276} INFO - 21/05/16 23:13:25 INFO Executor: Running task 51.0 in stage 4.0 (TID 430)
[2021-05-16 20:13:25,226] {docker.py:276} INFO - 21/05/16 23:13:25 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:25,228] {docker.py:276} INFO - 21/05/16 23:13:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:25,229] {docker.py:276} INFO - 21/05/16 23:13:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246359071281561376479_0004_m_000051_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246359071281561376479_0004_m_000051_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246359071281561376479_0004}; taskId=attempt_202105162312246359071281561376479_0004_m_000051_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1258e2b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:25,229] {docker.py:276} INFO - 21/05/16 23:13:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:25,230] {docker.py:276} INFO - 21/05/16 23:13:25 INFO StagingCommitter: Starting: Task committer attempt_202105162312246359071281561376479_0004_m_000051_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246359071281561376479_0004_m_000051_430
[2021-05-16 20:13:25,233] {docker.py:276} INFO - 21/05/16 23:13:25 INFO StagingCommitter: Task committer attempt_202105162312246359071281561376479_0004_m_000051_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246359071281561376479_0004_m_000051_430 : duration 0:00.004s
[2021-05-16 20:13:27,484] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312246923312006738981763_0004_m_000048_427: needsTaskCommit() Task attempt_202105162312246923312006738981763_0004_m_000048_427
[2021-05-16 20:13:27,485] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312246923312006738981763_0004_m_000048_427: needsTaskCommit() Task attempt_202105162312246923312006738981763_0004_m_000048_427: duration 0:00.001s
21/05/16 23:13:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246923312006738981763_0004_m_000048_427
[2021-05-16 20:13:27,488] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Finished task 48.0 in stage 4.0 (TID 427). 4544 bytes result sent to driver
[2021-05-16 20:13:27,490] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 431) (764b1db4ecfd, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:27,491] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Running task 52.0 in stage 4.0 (TID 431)
[2021-05-16 20:13:27,492] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 427) in 2853 ms on 764b1db4ecfd (executor driver) (49/200)
[2021-05-16 20:13:27,502] {docker.py:276} INFO - 21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:27,505] {docker.py:276} INFO - 21/05/16 23:13:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:27,506] {docker.py:276} INFO - 21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:27,506] {docker.py:276} INFO - 21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241203319447341878872_0004_m_000052_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241203319447341878872_0004_m_000052_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241203319447341878872_0004}; taskId=attempt_202105162312241203319447341878872_0004_m_000052_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a953324}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:27,506] {docker.py:276} INFO - 21/05/16 23:13:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:27,507] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312241203319447341878872_0004_m_000052_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241203319447341878872_0004_m_000052_431
[2021-05-16 20:13:27,511] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312241203319447341878872_0004_m_000052_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241203319447341878872_0004_m_000052_431 : duration 0:00.004s
[2021-05-16 20:13:27,644] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312243433583391435827665_0004_m_000050_429: needsTaskCommit() Task attempt_202105162312243433583391435827665_0004_m_000050_429
[2021-05-16 20:13:27,645] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312243433583391435827665_0004_m_000050_429: needsTaskCommit() Task attempt_202105162312243433583391435827665_0004_m_000050_429: duration 0:00.000s
21/05/16 23:13:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243433583391435827665_0004_m_000050_429
[2021-05-16 20:13:27,647] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Finished task 50.0 in stage 4.0 (TID 429). 4544 bytes result sent to driver
[2021-05-16 20:13:27,649] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 432) (764b1db4ecfd, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:27,650] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Running task 53.0 in stage 4.0 (TID 432)
[2021-05-16 20:13:27,650] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 429) in 2911 ms on 764b1db4ecfd (executor driver) (50/200)
[2021-05-16 20:13:27,660] {docker.py:276} INFO - 21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Getting 6 (27.5 KiB) non-empty blocks including 6 (27.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:27,662] {docker.py:276} INFO - 21/05/16 23:13:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248146982704418232061_0004_m_000053_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248146982704418232061_0004_m_000053_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248146982704418232061_0004}; taskId=attempt_202105162312248146982704418232061_0004_m_000053_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@648801bd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:27,662] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312248146982704418232061_0004_m_000053_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248146982704418232061_0004_m_000053_432
[2021-05-16 20:13:27,665] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312248146982704418232061_0004_m_000053_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248146982704418232061_0004_m_000053_432 : duration 0:00.003s
[2021-05-16 20:13:27,682] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312246858854845803532937_0004_m_000049_428: needsTaskCommit() Task attempt_202105162312246858854845803532937_0004_m_000049_428
[2021-05-16 20:13:27,683] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312246858854845803532937_0004_m_000049_428: needsTaskCommit() Task attempt_202105162312246858854845803532937_0004_m_000049_428: duration 0:00.001s
[2021-05-16 20:13:27,684] {docker.py:276} INFO - 21/05/16 23:13:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246858854845803532937_0004_m_000049_428
[2021-05-16 20:13:27,685] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Finished task 49.0 in stage 4.0 (TID 428). 4544 bytes result sent to driver
[2021-05-16 20:13:27,686] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 433) (764b1db4ecfd, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:27,687] {docker.py:276} INFO - 21/05/16 23:13:27 INFO Executor: Running task 54.0 in stage 4.0 (TID 433)
[2021-05-16 20:13:27,687] {docker.py:276} INFO - 21/05/16 23:13:27 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 428) in 2975 ms on 764b1db4ecfd (executor driver) (51/200)
[2021-05-16 20:13:27,694] {docker.py:276} INFO - 21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:27,696] {docker.py:276} INFO - 21/05/16 23:13:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247515982027990669361_0004_m_000054_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247515982027990669361_0004_m_000054_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247515982027990669361_0004}; taskId=attempt_202105162312247515982027990669361_0004_m_000054_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6399a553}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:27,696] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Starting: Task committer attempt_202105162312247515982027990669361_0004_m_000054_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247515982027990669361_0004_m_000054_433
[2021-05-16 20:13:27,699] {docker.py:276} INFO - 21/05/16 23:13:27 INFO StagingCommitter: Task committer attempt_202105162312247515982027990669361_0004_m_000054_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247515982027990669361_0004_m_000054_433 : duration 0:00.002s
[2021-05-16 20:13:28,179] {docker.py:276} INFO - 21/05/16 23:13:28 INFO StagingCommitter: Starting: Task committer attempt_202105162312246359071281561376479_0004_m_000051_430: needsTaskCommit() Task attempt_202105162312246359071281561376479_0004_m_000051_430
[2021-05-16 20:13:28,180] {docker.py:276} INFO - 21/05/16 23:13:28 INFO StagingCommitter: Task committer attempt_202105162312246359071281561376479_0004_m_000051_430: needsTaskCommit() Task attempt_202105162312246359071281561376479_0004_m_000051_430: duration 0:00.002s
[2021-05-16 20:13:28,181] {docker.py:276} INFO - 21/05/16 23:13:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246359071281561376479_0004_m_000051_430
[2021-05-16 20:13:28,182] {docker.py:276} INFO - 21/05/16 23:13:28 INFO Executor: Finished task 51.0 in stage 4.0 (TID 430). 4544 bytes result sent to driver
[2021-05-16 20:13:28,183] {docker.py:276} INFO - 21/05/16 23:13:28 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 434) (764b1db4ecfd, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:28,184] {docker.py:276} INFO - 21/05/16 23:13:28 INFO Executor: Running task 55.0 in stage 4.0 (TID 434)
[2021-05-16 20:13:28,185] {docker.py:276} INFO - 21/05/16 23:13:28 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 430) in 2972 ms on 764b1db4ecfd (executor driver) (52/200)
[2021-05-16 20:13:28,193] {docker.py:276} INFO - 21/05/16 23:13:28 INFO ShuffleBlockFetcherIterator: Getting 6 (27.0 KiB) non-empty blocks including 6 (27.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:28,193] {docker.py:276} INFO - 21/05/16 23:13:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:28,195] {docker.py:276} INFO - 21/05/16 23:13:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:28,196] {docker.py:276} INFO - 21/05/16 23:13:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:28,196] {docker.py:276} INFO - 21/05/16 23:13:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:28,196] {docker.py:276} INFO - 21/05/16 23:13:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245125490709335499761_0004_m_000055_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245125490709335499761_0004_m_000055_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245125490709335499761_0004}; taskId=attempt_202105162312245125490709335499761_0004_m_000055_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cd5da10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:28,197] {docker.py:276} INFO - 21/05/16 23:13:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:28,197] {docker.py:276} INFO - 21/05/16 23:13:28 INFO StagingCommitter: Starting: Task committer attempt_202105162312245125490709335499761_0004_m_000055_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245125490709335499761_0004_m_000055_434
[2021-05-16 20:13:28,200] {docker.py:276} INFO - 21/05/16 23:13:28 INFO StagingCommitter: Task committer attempt_202105162312245125490709335499761_0004_m_000055_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245125490709335499761_0004_m_000055_434 : duration 0:00.003s
[2021-05-16 20:13:30,257] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312241203319447341878872_0004_m_000052_431: needsTaskCommit() Task attempt_202105162312241203319447341878872_0004_m_000052_431
21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312241203319447341878872_0004_m_000052_431: needsTaskCommit() Task attempt_202105162312241203319447341878872_0004_m_000052_431: duration 0:00.000s
21/05/16 23:13:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241203319447341878872_0004_m_000052_431
[2021-05-16 20:13:30,258] {docker.py:276} INFO - 21/05/16 23:13:30 INFO Executor: Finished task 52.0 in stage 4.0 (TID 431). 4544 bytes result sent to driver
[2021-05-16 20:13:30,259] {docker.py:276} INFO - 21/05/16 23:13:30 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 435) (764b1db4ecfd, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:30,260] {docker.py:276} INFO - 21/05/16 23:13:30 INFO Executor: Running task 56.0 in stage 4.0 (TID 435)
21/05/16 23:13:30 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 431) in 2773 ms on 764b1db4ecfd (executor driver) (53/200)
[2021-05-16 20:13:30,270] {docker.py:276} INFO - 21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:30,272] {docker.py:276} INFO - 21/05/16 23:13:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242976350391972568661_0004_m_000056_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242976350391972568661_0004_m_000056_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242976350391972568661_0004}; taskId=attempt_202105162312242976350391972568661_0004_m_000056_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@314f2689}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312242976350391972568661_0004_m_000056_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242976350391972568661_0004_m_000056_435
[2021-05-16 20:13:30,275] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312242976350391972568661_0004_m_000056_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242976350391972568661_0004_m_000056_435 : duration 0:00.003s
[2021-05-16 20:13:30,538] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312247515982027990669361_0004_m_000054_433: needsTaskCommit() Task attempt_202105162312247515982027990669361_0004_m_000054_433
[2021-05-16 20:13:30,539] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312247515982027990669361_0004_m_000054_433: needsTaskCommit() Task attempt_202105162312247515982027990669361_0004_m_000054_433: duration 0:00.001s
21/05/16 23:13:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247515982027990669361_0004_m_000054_433
[2021-05-16 20:13:30,540] {docker.py:276} INFO - 21/05/16 23:13:30 INFO Executor: Finished task 54.0 in stage 4.0 (TID 433). 4544 bytes result sent to driver
[2021-05-16 20:13:30,541] {docker.py:276} INFO - 21/05/16 23:13:30 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 436) (764b1db4ecfd, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:30,542] {docker.py:276} INFO - 21/05/16 23:13:30 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 433) in 2860 ms on 764b1db4ecfd (executor driver) (54/200)
21/05/16 23:13:30 INFO Executor: Running task 57.0 in stage 4.0 (TID 436)
[2021-05-16 20:13:30,553] {docker.py:276} INFO - 21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:30,554] {docker.py:276} INFO - 21/05/16 23:13:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312249166465581145299833_0004_m_000057_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249166465581145299833_0004_m_000057_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312249166465581145299833_0004}; taskId=attempt_202105162312249166465581145299833_0004_m_000057_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c48b250}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312249166465581145299833_0004_m_000057_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249166465581145299833_0004_m_000057_436
[2021-05-16 20:13:30,557] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312249166465581145299833_0004_m_000057_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249166465581145299833_0004_m_000057_436 : duration 0:00.003s
[2021-05-16 20:13:30,657] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312248146982704418232061_0004_m_000053_432: needsTaskCommit() Task attempt_202105162312248146982704418232061_0004_m_000053_432
[2021-05-16 20:13:30,658] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312248146982704418232061_0004_m_000053_432: needsTaskCommit() Task attempt_202105162312248146982704418232061_0004_m_000053_432: duration 0:00.001s
21/05/16 23:13:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248146982704418232061_0004_m_000053_432
[2021-05-16 20:13:30,659] {docker.py:276} INFO - 21/05/16 23:13:30 INFO Executor: Finished task 53.0 in stage 4.0 (TID 432). 4544 bytes result sent to driver
[2021-05-16 20:13:30,660] {docker.py:276} INFO - 21/05/16 23:13:30 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 437) (764b1db4ecfd, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:30,661] {docker.py:276} INFO - 21/05/16 23:13:30 INFO Executor: Running task 58.0 in stage 4.0 (TID 437)
[2021-05-16 20:13:30,661] {docker.py:276} INFO - 21/05/16 23:13:30 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 432) in 3017 ms on 764b1db4ecfd (executor driver) (55/200)
[2021-05-16 20:13:30,673] {docker.py:276} INFO - 21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:30,675] {docker.py:276} INFO - 21/05/16 23:13:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:30,675] {docker.py:276} INFO - 21/05/16 23:13:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243789079751953347457_0004_m_000058_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243789079751953347457_0004_m_000058_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243789079751953347457_0004}; taskId=attempt_202105162312243789079751953347457_0004_m_000058_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d91b43c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:30 INFO StagingCommitter: Starting: Task committer attempt_202105162312243789079751953347457_0004_m_000058_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243789079751953347457_0004_m_000058_437
[2021-05-16 20:13:30,680] {docker.py:276} INFO - 21/05/16 23:13:30 INFO StagingCommitter: Task committer attempt_202105162312243789079751953347457_0004_m_000058_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243789079751953347457_0004_m_000058_437 : duration 0:00.004s
[2021-05-16 20:13:31,093] {docker.py:276} INFO - 21/05/16 23:13:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312245125490709335499761_0004_m_000055_434: needsTaskCommit() Task attempt_202105162312245125490709335499761_0004_m_000055_434
[2021-05-16 20:13:31,093] {docker.py:276} INFO - 21/05/16 23:13:31 INFO StagingCommitter: Task committer attempt_202105162312245125490709335499761_0004_m_000055_434: needsTaskCommit() Task attempt_202105162312245125490709335499761_0004_m_000055_434: duration 0:00.000s
21/05/16 23:13:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245125490709335499761_0004_m_000055_434
[2021-05-16 20:13:31,095] {docker.py:276} INFO - 21/05/16 23:13:31 INFO Executor: Finished task 55.0 in stage 4.0 (TID 434). 4544 bytes result sent to driver
[2021-05-16 20:13:31,095] {docker.py:276} INFO - 21/05/16 23:13:31 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 438) (764b1db4ecfd, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:31,096] {docker.py:276} INFO - 21/05/16 23:13:31 INFO Executor: Running task 59.0 in stage 4.0 (TID 438)
21/05/16 23:13:31 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 434) in 2918 ms on 764b1db4ecfd (executor driver) (56/200)
[2021-05-16 20:13:31,106] {docker.py:276} INFO - 21/05/16 23:13:31 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:31,106] {docker.py:276} INFO - 21/05/16 23:13:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:31,108] {docker.py:276} INFO - 21/05/16 23:13:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:31,108] {docker.py:276} INFO - 21/05/16 23:13:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:31,109] {docker.py:276} INFO - 21/05/16 23:13:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247390847995711010482_0004_m_000059_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247390847995711010482_0004_m_000059_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247390847995711010482_0004}; taskId=attempt_202105162312247390847995711010482_0004_m_000059_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c328e6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:31,109] {docker.py:276} INFO - 21/05/16 23:13:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:31,109] {docker.py:276} INFO - 21/05/16 23:13:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312247390847995711010482_0004_m_000059_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247390847995711010482_0004_m_000059_438
[2021-05-16 20:13:31,113] {docker.py:276} INFO - 21/05/16 23:13:31 INFO StagingCommitter: Task committer attempt_202105162312247390847995711010482_0004_m_000059_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247390847995711010482_0004_m_000059_438 : duration 0:00.003s
[2021-05-16 20:13:32,453] {docker.py:276} INFO - 21/05/16 23:13:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312242976350391972568661_0004_m_000056_435: needsTaskCommit() Task attempt_202105162312242976350391972568661_0004_m_000056_435
[2021-05-16 20:13:32,455] {docker.py:276} INFO - 21/05/16 23:13:32 INFO StagingCommitter: Task committer attempt_202105162312242976350391972568661_0004_m_000056_435: needsTaskCommit() Task attempt_202105162312242976350391972568661_0004_m_000056_435: duration 0:00.001s
21/05/16 23:13:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242976350391972568661_0004_m_000056_435
[2021-05-16 20:13:32,456] {docker.py:276} INFO - 21/05/16 23:13:32 INFO Executor: Finished task 56.0 in stage 4.0 (TID 435). 4544 bytes result sent to driver
[2021-05-16 20:13:32,457] {docker.py:276} INFO - 21/05/16 23:13:32 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 439) (764b1db4ecfd, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:32,458] {docker.py:276} INFO - 21/05/16 23:13:32 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 435) in 2202 ms on 764b1db4ecfd (executor driver) (57/200)
[2021-05-16 20:13:32,459] {docker.py:276} INFO - 21/05/16 23:13:32 INFO Executor: Running task 60.0 in stage 4.0 (TID 439)
[2021-05-16 20:13:32,480] {docker.py:276} INFO - 21/05/16 23:13:32 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:32,483] {docker.py:276} INFO - 21/05/16 23:13:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:32,483] {docker.py:276} INFO - 21/05/16 23:13:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241786482287555591846_0004_m_000060_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241786482287555591846_0004_m_000060_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241786482287555591846_0004}; taskId=attempt_202105162312241786482287555591846_0004_m_000060_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16f1e590}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312241786482287555591846_0004_m_000060_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241786482287555591846_0004_m_000060_439
[2021-05-16 20:13:32,486] {docker.py:276} INFO - 21/05/16 23:13:32 INFO StagingCommitter: Task committer attempt_202105162312241786482287555591846_0004_m_000060_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241786482287555591846_0004_m_000060_439 : duration 0:00.002s
[2021-05-16 20:13:33,480] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Starting: Task committer attempt_202105162312249166465581145299833_0004_m_000057_436: needsTaskCommit() Task attempt_202105162312249166465581145299833_0004_m_000057_436
[2021-05-16 20:13:33,480] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Task committer attempt_202105162312249166465581145299833_0004_m_000057_436: needsTaskCommit() Task attempt_202105162312249166465581145299833_0004_m_000057_436: duration 0:00.000s
21/05/16 23:13:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312249166465581145299833_0004_m_000057_436
[2021-05-16 20:13:33,482] {docker.py:276} INFO - 21/05/16 23:13:33 INFO Executor: Finished task 57.0 in stage 4.0 (TID 436). 4587 bytes result sent to driver
[2021-05-16 20:13:33,483] {docker.py:276} INFO - 21/05/16 23:13:33 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 440) (764b1db4ecfd, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:33,484] {docker.py:276} INFO - 21/05/16 23:13:33 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 436) in 2946 ms on 764b1db4ecfd (executor driver) (58/200)
21/05/16 23:13:33 INFO Executor: Running task 61.0 in stage 4.0 (TID 440)
[2021-05-16 20:13:33,494] {docker.py:276} INFO - 21/05/16 23:13:33 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:33,497] {docker.py:276} INFO - 21/05/16 23:13:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:33,497] {docker.py:276} INFO - 21/05/16 23:13:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:33,498] {docker.py:276} INFO - 21/05/16 23:13:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242496189154396709835_0004_m_000061_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242496189154396709835_0004_m_000061_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242496189154396709835_0004}; taskId=attempt_202105162312242496189154396709835_0004_m_000061_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11da6dbc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:33,498] {docker.py:276} INFO - 21/05/16 23:13:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:33,498] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Starting: Task committer attempt_202105162312242496189154396709835_0004_m_000061_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242496189154396709835_0004_m_000061_440
[2021-05-16 20:13:33,501] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Task committer attempt_202105162312242496189154396709835_0004_m_000061_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242496189154396709835_0004_m_000061_440 : duration 0:00.003s
[2021-05-16 20:13:33,629] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Starting: Task committer attempt_202105162312243789079751953347457_0004_m_000058_437: needsTaskCommit() Task attempt_202105162312243789079751953347457_0004_m_000058_437
21/05/16 23:13:33 INFO StagingCommitter: Task committer attempt_202105162312243789079751953347457_0004_m_000058_437: needsTaskCommit() Task attempt_202105162312243789079751953347457_0004_m_000058_437: duration 0:00.000s
21/05/16 23:13:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243789079751953347457_0004_m_000058_437
[2021-05-16 20:13:33,630] {docker.py:276} INFO - 21/05/16 23:13:33 INFO Executor: Finished task 58.0 in stage 4.0 (TID 437). 4587 bytes result sent to driver
[2021-05-16 20:13:33,631] {docker.py:276} INFO - 21/05/16 23:13:33 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 441) (764b1db4ecfd, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:33,632] {docker.py:276} INFO - 21/05/16 23:13:33 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 437) in 2976 ms on 764b1db4ecfd (executor driver) (59/200)
[2021-05-16 20:13:33,633] {docker.py:276} INFO - 21/05/16 23:13:33 INFO Executor: Running task 62.0 in stage 4.0 (TID 441)
[2021-05-16 20:13:33,641] {docker.py:276} INFO - 21/05/16 23:13:33 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:33,647] {docker.py:276} INFO - 21/05/16 23:13:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:33,647] {docker.py:276} INFO - 21/05/16 23:13:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245712813250529268598_0004_m_000062_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245712813250529268598_0004_m_000062_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245712813250529268598_0004}; taskId=attempt_202105162312245712813250529268598_0004_m_000062_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@72da8ef2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:33 INFO StagingCommitter: Starting: Task committer attempt_202105162312245712813250529268598_0004_m_000062_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245712813250529268598_0004_m_000062_441
[2021-05-16 20:13:33,650] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Task committer attempt_202105162312245712813250529268598_0004_m_000062_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245712813250529268598_0004_m_000062_441 : duration 0:00.003s
[2021-05-16 20:13:33,968] {docker.py:276} INFO - 21/05/16 23:13:33 INFO StagingCommitter: Starting: Task committer attempt_202105162312247390847995711010482_0004_m_000059_438: needsTaskCommit() Task attempt_202105162312247390847995711010482_0004_m_000059_438
[2021-05-16 20:13:33,969] {docker.py:276} INFO - 21/05/16 23:13:34 INFO StagingCommitter: Task committer attempt_202105162312247390847995711010482_0004_m_000059_438: needsTaskCommit() Task attempt_202105162312247390847995711010482_0004_m_000059_438: duration 0:00.002s
21/05/16 23:13:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247390847995711010482_0004_m_000059_438
[2021-05-16 20:13:33,971] {docker.py:276} INFO - 21/05/16 23:13:34 INFO Executor: Finished task 59.0 in stage 4.0 (TID 438). 4587 bytes result sent to driver
[2021-05-16 20:13:33,972] {docker.py:276} INFO - 21/05/16 23:13:34 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 442) (764b1db4ecfd, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:33,973] {docker.py:276} INFO - 21/05/16 23:13:34 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 438) in 2881 ms on 764b1db4ecfd (executor driver) (60/200)
21/05/16 23:13:34 INFO Executor: Running task 63.0 in stage 4.0 (TID 442)
[2021-05-16 20:13:33,983] {docker.py:276} INFO - 21/05/16 23:13:34 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:33,985] {docker.py:276} INFO - 21/05/16 23:13:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242243158528710098941_0004_m_000063_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242243158528710098941_0004_m_000063_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242243158528710098941_0004}; taskId=attempt_202105162312242243158528710098941_0004_m_000063_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fa336f3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:33,986] {docker.py:276} INFO - 21/05/16 23:13:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:33,986] {docker.py:276} INFO - 21/05/16 23:13:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312242243158528710098941_0004_m_000063_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242243158528710098941_0004_m_000063_442
[2021-05-16 20:13:33,989] {docker.py:276} INFO - 21/05/16 23:13:34 INFO StagingCommitter: Task committer attempt_202105162312242243158528710098941_0004_m_000063_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242243158528710098941_0004_m_000063_442 : duration 0:00.004s
[2021-05-16 20:13:35,284] {docker.py:276} INFO - 21/05/16 23:13:35 INFO StagingCommitter: Starting: Task committer attempt_202105162312241786482287555591846_0004_m_000060_439: needsTaskCommit() Task attempt_202105162312241786482287555591846_0004_m_000060_439
[2021-05-16 20:13:35,285] {docker.py:276} INFO - 21/05/16 23:13:35 INFO StagingCommitter: Task committer attempt_202105162312241786482287555591846_0004_m_000060_439: needsTaskCommit() Task attempt_202105162312241786482287555591846_0004_m_000060_439: duration 0:00.001s
21/05/16 23:13:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241786482287555591846_0004_m_000060_439
[2021-05-16 20:13:35,287] {docker.py:276} INFO - 21/05/16 23:13:35 INFO Executor: Finished task 60.0 in stage 4.0 (TID 439). 4587 bytes result sent to driver
[2021-05-16 20:13:35,288] {docker.py:276} INFO - 21/05/16 23:13:35 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 443) (764b1db4ecfd, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:35,289] {docker.py:276} INFO - 21/05/16 23:13:35 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 439) in 2835 ms on 764b1db4ecfd (executor driver) (61/200)
[2021-05-16 20:13:35,289] {docker.py:276} INFO - 21/05/16 23:13:35 INFO Executor: Running task 64.0 in stage 4.0 (TID 443)
[2021-05-16 20:13:35,300] {docker.py:276} INFO - 21/05/16 23:13:35 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:35,302] {docker.py:276} INFO - 21/05/16 23:13:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:35,303] {docker.py:276} INFO - 21/05/16 23:13:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247776455337715322307_0004_m_000064_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247776455337715322307_0004_m_000064_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247776455337715322307_0004}; taskId=attempt_202105162312247776455337715322307_0004_m_000064_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@301b5e1f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:35,303] {docker.py:276} INFO - 21/05/16 23:13:35 INFO StagingCommitter: Starting: Task committer attempt_202105162312247776455337715322307_0004_m_000064_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247776455337715322307_0004_m_000064_443
[2021-05-16 20:13:35,306] {docker.py:276} INFO - 21/05/16 23:13:35 INFO StagingCommitter: Task committer attempt_202105162312247776455337715322307_0004_m_000064_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247776455337715322307_0004_m_000064_443 : duration 0:00.004s
[2021-05-16 20:13:36,293] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312242496189154396709835_0004_m_000061_440: needsTaskCommit() Task attempt_202105162312242496189154396709835_0004_m_000061_440
21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312242496189154396709835_0004_m_000061_440: needsTaskCommit() Task attempt_202105162312242496189154396709835_0004_m_000061_440: duration 0:00.001s
21/05/16 23:13:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242496189154396709835_0004_m_000061_440
[2021-05-16 20:13:36,295] {docker.py:276} INFO - 21/05/16 23:13:36 INFO Executor: Finished task 61.0 in stage 4.0 (TID 440). 4544 bytes result sent to driver
[2021-05-16 20:13:36,298] {docker.py:276} INFO - 21/05/16 23:13:36 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 444) (764b1db4ecfd, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:13:36 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 440) in 2781 ms on 764b1db4ecfd (executor driver) (62/200)
21/05/16 23:13:36 INFO Executor: Running task 65.0 in stage 4.0 (TID 444)
[2021-05-16 20:13:36,306] {docker.py:276} INFO - 21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Getting 6 (25.1 KiB) non-empty blocks including 6 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:36,325] {docker.py:276} INFO - 21/05/16 23:13:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247477642289361054622_0004_m_000065_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247477642289361054622_0004_m_000065_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247477642289361054622_0004}; taskId=attempt_202105162312247477642289361054622_0004_m_000065_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1d8b48f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312247477642289361054622_0004_m_000065_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247477642289361054622_0004_m_000065_444
[2021-05-16 20:13:36,325] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312247477642289361054622_0004_m_000065_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247477642289361054622_0004_m_000065_444 : duration 0:00.003s
[2021-05-16 20:13:36,630] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312245712813250529268598_0004_m_000062_441: needsTaskCommit() Task attempt_202105162312245712813250529268598_0004_m_000062_441
[2021-05-16 20:13:36,631] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312245712813250529268598_0004_m_000062_441: needsTaskCommit() Task attempt_202105162312245712813250529268598_0004_m_000062_441: duration 0:00.001s
21/05/16 23:13:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245712813250529268598_0004_m_000062_441
[2021-05-16 20:13:36,633] {docker.py:276} INFO - 21/05/16 23:13:36 INFO Executor: Finished task 62.0 in stage 4.0 (TID 441). 4544 bytes result sent to driver
[2021-05-16 20:13:36,634] {docker.py:276} INFO - 21/05/16 23:13:36 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 445) (764b1db4ecfd, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:36,635] {docker.py:276} INFO - 21/05/16 23:13:36 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 441) in 2970 ms on 764b1db4ecfd (executor driver) (63/200)
[2021-05-16 20:13:36,636] {docker.py:276} INFO - 21/05/16 23:13:36 INFO Executor: Running task 66.0 in stage 4.0 (TID 445)
[2021-05-16 20:13:36,646] {docker.py:276} INFO - 21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:36,648] {docker.py:276} INFO - 21/05/16 23:13:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248011673917050264804_0004_m_000066_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248011673917050264804_0004_m_000066_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248011673917050264804_0004}; taskId=attempt_202105162312248011673917050264804_0004_m_000066_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b1ef4c9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:36,649] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312248011673917050264804_0004_m_000066_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248011673917050264804_0004_m_000066_445
[2021-05-16 20:13:36,652] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312248011673917050264804_0004_m_000066_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248011673917050264804_0004_m_000066_445 : duration 0:00.003s
[2021-05-16 20:13:36,734] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312242243158528710098941_0004_m_000063_442: needsTaskCommit() Task attempt_202105162312242243158528710098941_0004_m_000063_442
[2021-05-16 20:13:36,735] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312242243158528710098941_0004_m_000063_442: needsTaskCommit() Task attempt_202105162312242243158528710098941_0004_m_000063_442: duration 0:00.001s
21/05/16 23:13:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242243158528710098941_0004_m_000063_442
[2021-05-16 20:13:36,736] {docker.py:276} INFO - 21/05/16 23:13:36 INFO Executor: Finished task 63.0 in stage 4.0 (TID 442). 4544 bytes result sent to driver
[2021-05-16 20:13:36,737] {docker.py:276} INFO - 21/05/16 23:13:36 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 446) (764b1db4ecfd, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:36,738] {docker.py:276} INFO - 21/05/16 23:13:36 INFO Executor: Running task 67.0 in stage 4.0 (TID 446)
[2021-05-16 20:13:36,738] {docker.py:276} INFO - 21/05/16 23:13:36 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 442) in 2735 ms on 764b1db4ecfd (executor driver) (64/200)
[2021-05-16 20:13:36,745] {docker.py:276} INFO - 21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:36,746] {docker.py:276} INFO - 21/05/16 23:13:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:36,749] {docker.py:276} INFO - 21/05/16 23:13:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:36,749] {docker.py:276} INFO - 21/05/16 23:13:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248651922696359294857_0004_m_000067_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248651922696359294857_0004_m_000067_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248651922696359294857_0004}; taskId=attempt_202105162312248651922696359294857_0004_m_000067_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@434f2428}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:36,750] {docker.py:276} INFO - 21/05/16 23:13:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:36,750] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Starting: Task committer attempt_202105162312248651922696359294857_0004_m_000067_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248651922696359294857_0004_m_000067_446
[2021-05-16 20:13:36,752] {docker.py:276} INFO - 21/05/16 23:13:36 INFO StagingCommitter: Task committer attempt_202105162312248651922696359294857_0004_m_000067_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248651922696359294857_0004_m_000067_446 : duration 0:00.004s
[2021-05-16 20:13:38,300] {docker.py:276} INFO - 21/05/16 23:13:38 INFO StagingCommitter: Starting: Task committer attempt_202105162312247776455337715322307_0004_m_000064_443: needsTaskCommit() Task attempt_202105162312247776455337715322307_0004_m_000064_443
[2021-05-16 20:13:38,301] {docker.py:276} INFO - 21/05/16 23:13:38 INFO StagingCommitter: Task committer attempt_202105162312247776455337715322307_0004_m_000064_443: needsTaskCommit() Task attempt_202105162312247776455337715322307_0004_m_000064_443: duration 0:00.001s
21/05/16 23:13:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247776455337715322307_0004_m_000064_443
[2021-05-16 20:13:38,303] {docker.py:276} INFO - 21/05/16 23:13:38 INFO Executor: Finished task 64.0 in stage 4.0 (TID 443). 4544 bytes result sent to driver
[2021-05-16 20:13:38,304] {docker.py:276} INFO - 21/05/16 23:13:38 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 447) (764b1db4ecfd, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:38,305] {docker.py:276} INFO - 21/05/16 23:13:38 INFO Executor: Running task 68.0 in stage 4.0 (TID 447)
[2021-05-16 20:13:38,306] {docker.py:276} INFO - 21/05/16 23:13:38 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 443) in 2985 ms on 764b1db4ecfd (executor driver) (65/200)
[2021-05-16 20:13:38,314] {docker.py:276} INFO - 21/05/16 23:13:38 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:38,315] {docker.py:276} INFO - 21/05/16 23:13:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:38,317] {docker.py:276} INFO - 21/05/16 23:13:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:38,318] {docker.py:276} INFO - 21/05/16 23:13:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246672500791579393982_0004_m_000068_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246672500791579393982_0004_m_000068_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246672500791579393982_0004}; taskId=attempt_202105162312246672500791579393982_0004_m_000068_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60bdcb81}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:38,318] {docker.py:276} INFO - 21/05/16 23:13:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:38 INFO StagingCommitter: Starting: Task committer attempt_202105162312246672500791579393982_0004_m_000068_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246672500791579393982_0004_m_000068_447
[2021-05-16 20:13:38,320] {docker.py:276} INFO - 21/05/16 23:13:38 INFO StagingCommitter: Task committer attempt_202105162312246672500791579393982_0004_m_000068_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246672500791579393982_0004_m_000068_447 : duration 0:00.003s
[2021-05-16 20:13:39,008] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_202105162312247477642289361054622_0004_m_000065_444: needsTaskCommit() Task attempt_202105162312247477642289361054622_0004_m_000065_444
[2021-05-16 20:13:39,010] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_202105162312247477642289361054622_0004_m_000065_444: needsTaskCommit() Task attempt_202105162312247477642289361054622_0004_m_000065_444: duration 0:00.000s
21/05/16 23:13:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247477642289361054622_0004_m_000065_444
[2021-05-16 20:13:39,011] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Finished task 65.0 in stage 4.0 (TID 444). 4544 bytes result sent to driver
[2021-05-16 20:13:39,012] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 448) (764b1db4ecfd, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:39,013] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Running task 69.0 in stage 4.0 (TID 448)
[2021-05-16 20:13:39,014] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 444) in 2721 ms on 764b1db4ecfd (executor driver) (66/200)
[2021-05-16 20:13:39,025] {docker.py:276} INFO - 21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:39,027] {docker.py:276} INFO - 21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:39,029] {docker.py:276} INFO - 21/05/16 23:13:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:39,031] {docker.py:276} INFO - 21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312249176387132656318985_0004_m_000069_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249176387132656318985_0004_m_000069_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312249176387132656318985_0004}; taskId=attempt_202105162312249176387132656318985_0004_m_000069_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ad6d292}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:39,032] {docker.py:276} INFO - 21/05/16 23:13:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_202105162312249176387132656318985_0004_m_000069_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249176387132656318985_0004_m_000069_448
[2021-05-16 20:13:39,035] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_202105162312249176387132656318985_0004_m_000069_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249176387132656318985_0004_m_000069_448 : duration 0:00.005s
[2021-05-16 20:13:39,601] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_202105162312248011673917050264804_0004_m_000066_445: needsTaskCommit() Task attempt_202105162312248011673917050264804_0004_m_000066_445
[2021-05-16 20:13:39,601] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_202105162312248011673917050264804_0004_m_000066_445: needsTaskCommit() Task attempt_202105162312248011673917050264804_0004_m_000066_445: duration 0:00.001s
[2021-05-16 20:13:39,602] {docker.py:276} INFO - 21/05/16 23:13:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248011673917050264804_0004_m_000066_445
[2021-05-16 20:13:39,603] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Finished task 66.0 in stage 4.0 (TID 445). 4544 bytes result sent to driver
[2021-05-16 20:13:39,604] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 449) (764b1db4ecfd, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:39,605] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 445) in 2975 ms on 764b1db4ecfd (executor driver) (67/200)
[2021-05-16 20:13:39,606] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Running task 70.0 in stage 4.0 (TID 449)
[2021-05-16 20:13:39,616] {docker.py:276} INFO - 21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Getting 6 (24.6 KiB) non-empty blocks including 6 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:39,619] {docker.py:276} INFO - 21/05/16 23:13:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224779227862806293147_0004_m_000070_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224779227862806293147_0004_m_000070_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224779227862806293147_0004}; taskId=attempt_20210516231224779227862806293147_0004_m_000070_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47f99787}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:39,619] {docker.py:276} INFO - 21/05/16 23:13:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:39,620] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_20210516231224779227862806293147_0004_m_000070_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224779227862806293147_0004_m_000070_449
[2021-05-16 20:13:39,622] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_20210516231224779227862806293147_0004_m_000070_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224779227862806293147_0004_m_000070_449 : duration 0:00.004s
[2021-05-16 20:13:39,705] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_202105162312248651922696359294857_0004_m_000067_446: needsTaskCommit() Task attempt_202105162312248651922696359294857_0004_m_000067_446
[2021-05-16 20:13:39,707] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_202105162312248651922696359294857_0004_m_000067_446: needsTaskCommit() Task attempt_202105162312248651922696359294857_0004_m_000067_446: duration 0:00.001s
21/05/16 23:13:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248651922696359294857_0004_m_000067_446
[2021-05-16 20:13:39,708] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Finished task 67.0 in stage 4.0 (TID 446). 4544 bytes result sent to driver
[2021-05-16 20:13:39,710] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 450) (764b1db4ecfd, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:39,711] {docker.py:276} INFO - 21/05/16 23:13:39 INFO Executor: Running task 71.0 in stage 4.0 (TID 450)
[2021-05-16 20:13:39,712] {docker.py:276} INFO - 21/05/16 23:13:39 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 446) in 2978 ms on 764b1db4ecfd (executor driver) (68/200)
[2021-05-16 20:13:39,725] {docker.py:276} INFO - 21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:39,726] {docker.py:276} INFO - 21/05/16 23:13:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:39,735] {docker.py:276} INFO - 21/05/16 23:13:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:39,736] {docker.py:276} INFO - 21/05/16 23:13:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:39,737] {docker.py:276} INFO - 21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:39,738] {docker.py:276} INFO - 21/05/16 23:13:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247435409576257268688_0004_m_000071_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247435409576257268688_0004_m_000071_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247435409576257268688_0004}; taskId=attempt_202105162312247435409576257268688_0004_m_000071_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d3e5845}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:39,739] {docker.py:276} INFO - 21/05/16 23:13:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:39,739] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Starting: Task committer attempt_202105162312247435409576257268688_0004_m_000071_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247435409576257268688_0004_m_000071_450
[2021-05-16 20:13:39,743] {docker.py:276} INFO - 21/05/16 23:13:39 INFO StagingCommitter: Task committer attempt_202105162312247435409576257268688_0004_m_000071_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247435409576257268688_0004_m_000071_450 : duration 0:00.005s
[2021-05-16 20:13:41,187] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312246672500791579393982_0004_m_000068_447: needsTaskCommit() Task attempt_202105162312246672500791579393982_0004_m_000068_447
[2021-05-16 20:13:41,188] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Task committer attempt_202105162312246672500791579393982_0004_m_000068_447: needsTaskCommit() Task attempt_202105162312246672500791579393982_0004_m_000068_447: duration 0:00.001s
21/05/16 23:13:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246672500791579393982_0004_m_000068_447
[2021-05-16 20:13:41,190] {docker.py:276} INFO - 21/05/16 23:13:41 INFO Executor: Finished task 68.0 in stage 4.0 (TID 447). 4544 bytes result sent to driver
[2021-05-16 20:13:41,191] {docker.py:276} INFO - 21/05/16 23:13:41 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 451) (764b1db4ecfd, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:41,192] {docker.py:276} INFO - 21/05/16 23:13:41 INFO Executor: Running task 72.0 in stage 4.0 (TID 451)
[2021-05-16 20:13:41,193] {docker.py:276} INFO - 21/05/16 23:13:41 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 447) in 2893 ms on 764b1db4ecfd (executor driver) (69/200)
[2021-05-16 20:13:41,203] {docker.py:276} INFO - 21/05/16 23:13:41 INFO ShuffleBlockFetcherIterator: Getting 6 (27.4 KiB) non-empty blocks including 6 (27.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:41,205] {docker.py:276} INFO - 21/05/16 23:13:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247702405597461466796_0004_m_000072_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247702405597461466796_0004_m_000072_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247702405597461466796_0004}; taskId=attempt_202105162312247702405597461466796_0004_m_000072_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a29ba99}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:41,206] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312247702405597461466796_0004_m_000072_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247702405597461466796_0004_m_000072_451
[2021-05-16 20:13:41,209] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Task committer attempt_202105162312247702405597461466796_0004_m_000072_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247702405597461466796_0004_m_000072_451 : duration 0:00.003s
[2021-05-16 20:13:41,889] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312249176387132656318985_0004_m_000069_448: needsTaskCommit() Task attempt_202105162312249176387132656318985_0004_m_000069_448
[2021-05-16 20:13:41,891] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Task committer attempt_202105162312249176387132656318985_0004_m_000069_448: needsTaskCommit() Task attempt_202105162312249176387132656318985_0004_m_000069_448: duration 0:00.001s
21/05/16 23:13:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312249176387132656318985_0004_m_000069_448
[2021-05-16 20:13:41,893] {docker.py:276} INFO - 21/05/16 23:13:41 INFO Executor: Finished task 69.0 in stage 4.0 (TID 448). 4544 bytes result sent to driver
[2021-05-16 20:13:41,895] {docker.py:276} INFO - 21/05/16 23:13:41 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 452) (764b1db4ecfd, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:41,896] {docker.py:276} INFO - 21/05/16 23:13:41 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 448) in 2888 ms on 764b1db4ecfd (executor driver) (70/200)
[2021-05-16 20:13:41,896] {docker.py:276} INFO - 21/05/16 23:13:41 INFO Executor: Running task 73.0 in stage 4.0 (TID 452)
[2021-05-16 20:13:41,906] {docker.py:276} INFO - 21/05/16 23:13:41 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:41,909] {docker.py:276} INFO - 21/05/16 23:13:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244837365190611899207_0004_m_000073_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244837365190611899207_0004_m_000073_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244837365190611899207_0004}; taskId=attempt_202105162312244837365190611899207_0004_m_000073_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c10c24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:41,910] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312244837365190611899207_0004_m_000073_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244837365190611899207_0004_m_000073_452
[2021-05-16 20:13:41,914] {docker.py:276} INFO - 21/05/16 23:13:41 INFO StagingCommitter: Task committer attempt_202105162312244837365190611899207_0004_m_000073_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244837365190611899207_0004_m_000073_452 : duration 0:00.005s
[2021-05-16 20:13:42,399] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Starting: Task committer attempt_20210516231224779227862806293147_0004_m_000070_449: needsTaskCommit() Task attempt_20210516231224779227862806293147_0004_m_000070_449
[2021-05-16 20:13:42,400] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Task committer attempt_20210516231224779227862806293147_0004_m_000070_449: needsTaskCommit() Task attempt_20210516231224779227862806293147_0004_m_000070_449: duration 0:00.000s
[2021-05-16 20:13:42,401] {docker.py:276} INFO - 21/05/16 23:13:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224779227862806293147_0004_m_000070_449
[2021-05-16 20:13:42,403] {docker.py:276} INFO - 21/05/16 23:13:42 INFO Executor: Finished task 70.0 in stage 4.0 (TID 449). 4544 bytes result sent to driver
[2021-05-16 20:13:42,404] {docker.py:276} INFO - 21/05/16 23:13:42 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 453) (764b1db4ecfd, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:42,404] {docker.py:276} INFO - 21/05/16 23:13:42 INFO Executor: Running task 74.0 in stage 4.0 (TID 453)
[2021-05-16 20:13:42,406] {docker.py:276} INFO - 21/05/16 23:13:42 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 449) in 2805 ms on 764b1db4ecfd (executor driver) (71/200)
[2021-05-16 20:13:42,414] {docker.py:276} INFO - 21/05/16 23:13:42 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:42,416] {docker.py:276} INFO - 21/05/16 23:13:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241619182160969955927_0004_m_000074_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241619182160969955927_0004_m_000074_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241619182160969955927_0004}; taskId=attempt_202105162312241619182160969955927_0004_m_000074_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@9cc6c2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:42,417] {docker.py:276} INFO - 21/05/16 23:13:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:42 INFO StagingCommitter: Starting: Task committer attempt_202105162312241619182160969955927_0004_m_000074_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241619182160969955927_0004_m_000074_453
[2021-05-16 20:13:42,420] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Task committer attempt_202105162312241619182160969955927_0004_m_000074_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241619182160969955927_0004_m_000074_453 : duration 0:00.003s
[2021-05-16 20:13:42,611] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Starting: Task committer attempt_202105162312247435409576257268688_0004_m_000071_450: needsTaskCommit() Task attempt_202105162312247435409576257268688_0004_m_000071_450
[2021-05-16 20:13:42,612] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Task committer attempt_202105162312247435409576257268688_0004_m_000071_450: needsTaskCommit() Task attempt_202105162312247435409576257268688_0004_m_000071_450: duration 0:00.001s
21/05/16 23:13:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247435409576257268688_0004_m_000071_450
[2021-05-16 20:13:42,614] {docker.py:276} INFO - 21/05/16 23:13:42 INFO Executor: Finished task 71.0 in stage 4.0 (TID 450). 4544 bytes result sent to driver
[2021-05-16 20:13:42,615] {docker.py:276} INFO - 21/05/16 23:13:42 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 454) (764b1db4ecfd, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:42,616] {docker.py:276} INFO - 21/05/16 23:13:42 INFO Executor: Running task 75.0 in stage 4.0 (TID 454)
[2021-05-16 20:13:42,617] {docker.py:276} INFO - 21/05/16 23:13:42 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 450) in 2911 ms on 764b1db4ecfd (executor driver) (72/200)
[2021-05-16 20:13:42,627] {docker.py:276} INFO - 21/05/16 23:13:42 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:42,629] {docker.py:276} INFO - 21/05/16 23:13:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248603234421084802380_0004_m_000075_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248603234421084802380_0004_m_000075_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248603234421084802380_0004}; taskId=attempt_202105162312248603234421084802380_0004_m_000075_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@424ca64c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:42,629] {docker.py:276} INFO - 21/05/16 23:13:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:42 INFO StagingCommitter: Starting: Task committer attempt_202105162312248603234421084802380_0004_m_000075_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248603234421084802380_0004_m_000075_454
[2021-05-16 20:13:42,632] {docker.py:276} INFO - 21/05/16 23:13:42 INFO StagingCommitter: Task committer attempt_202105162312248603234421084802380_0004_m_000075_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248603234421084802380_0004_m_000075_454 : duration 0:00.003s
[2021-05-16 20:13:44,010] {docker.py:276} INFO - 21/05/16 23:13:44 INFO StagingCommitter: Starting: Task committer attempt_202105162312247702405597461466796_0004_m_000072_451: needsTaskCommit() Task attempt_202105162312247702405597461466796_0004_m_000072_451
21/05/16 23:13:44 INFO StagingCommitter: Task committer attempt_202105162312247702405597461466796_0004_m_000072_451: needsTaskCommit() Task attempt_202105162312247702405597461466796_0004_m_000072_451: duration 0:00.001s
21/05/16 23:13:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247702405597461466796_0004_m_000072_451
[2021-05-16 20:13:44,012] {docker.py:276} INFO - 21/05/16 23:13:44 INFO Executor: Finished task 72.0 in stage 4.0 (TID 451). 4544 bytes result sent to driver
[2021-05-16 20:13:44,014] {docker.py:276} INFO - 21/05/16 23:13:44 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 455) (764b1db4ecfd, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:44,015] {docker.py:276} INFO - 21/05/16 23:13:44 INFO Executor: Running task 76.0 in stage 4.0 (TID 455)
[2021-05-16 20:13:44,017] {docker.py:276} INFO - 21/05/16 23:13:44 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 451) in 2829 ms on 764b1db4ecfd (executor driver) (73/200)
[2021-05-16 20:13:44,032] {docker.py:276} INFO - 21/05/16 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:44,033] {docker.py:276} INFO - 21/05/16 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:44,037] {docker.py:276} INFO - 21/05/16 23:13:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:44,038] {docker.py:276} INFO - 21/05/16 23:13:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243443317855588892374_0004_m_000076_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243443317855588892374_0004_m_000076_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243443317855588892374_0004}; taskId=attempt_202105162312243443317855588892374_0004_m_000076_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37699d50}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:44 INFO StagingCommitter: Starting: Task committer attempt_202105162312243443317855588892374_0004_m_000076_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243443317855588892374_0004_m_000076_455
[2021-05-16 20:13:44,042] {docker.py:276} INFO - 21/05/16 23:13:44 INFO StagingCommitter: Task committer attempt_202105162312243443317855588892374_0004_m_000076_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243443317855588892374_0004_m_000076_455 : duration 0:00.006s
[2021-05-16 20:13:44,606] {docker.py:276} INFO - 21/05/16 23:13:44 INFO StagingCommitter: Starting: Task committer attempt_202105162312244837365190611899207_0004_m_000073_452: needsTaskCommit() Task attempt_202105162312244837365190611899207_0004_m_000073_452
[2021-05-16 20:13:44,607] {docker.py:276} INFO - 21/05/16 23:13:44 INFO StagingCommitter: Task committer attempt_202105162312244837365190611899207_0004_m_000073_452: needsTaskCommit() Task attempt_202105162312244837365190611899207_0004_m_000073_452: duration 0:00.000s
21/05/16 23:13:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244837365190611899207_0004_m_000073_452
[2021-05-16 20:13:44,608] {docker.py:276} INFO - 21/05/16 23:13:44 INFO Executor: Finished task 73.0 in stage 4.0 (TID 452). 4544 bytes result sent to driver
[2021-05-16 20:13:44,609] {docker.py:276} INFO - 21/05/16 23:13:44 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 456) (764b1db4ecfd, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:44,610] {docker.py:276} INFO - 21/05/16 23:13:44 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 452) in 2719 ms on 764b1db4ecfd (executor driver) (74/200)
[2021-05-16 20:13:44,611] {docker.py:276} INFO - 21/05/16 23:13:44 INFO Executor: Running task 77.0 in stage 4.0 (TID 456)
[2021-05-16 20:13:44,622] {docker.py:276} INFO - 21/05/16 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:44,625] {docker.py:276} INFO - 21/05/16 23:13:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242666829747607076832_0004_m_000077_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242666829747607076832_0004_m_000077_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242666829747607076832_0004}; taskId=attempt_202105162312242666829747607076832_0004_m_000077_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32fa9b45}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:44 INFO StagingCommitter: Starting: Task committer attempt_202105162312242666829747607076832_0004_m_000077_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242666829747607076832_0004_m_000077_456
[2021-05-16 20:13:44,629] {docker.py:276} INFO - 21/05/16 23:13:44 INFO StagingCommitter: Task committer attempt_202105162312242666829747607076832_0004_m_000077_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242666829747607076832_0004_m_000077_456 : duration 0:00.004s
[2021-05-16 20:13:45,173] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312241619182160969955927_0004_m_000074_453: needsTaskCommit() Task attempt_202105162312241619182160969955927_0004_m_000074_453
[2021-05-16 20:13:45,174] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Task committer attempt_202105162312241619182160969955927_0004_m_000074_453: needsTaskCommit() Task attempt_202105162312241619182160969955927_0004_m_000074_453: duration 0:00.001s
[2021-05-16 20:13:45,174] {docker.py:276} INFO - 21/05/16 23:13:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241619182160969955927_0004_m_000074_453
[2021-05-16 20:13:45,176] {docker.py:276} INFO - 21/05/16 23:13:45 INFO Executor: Finished task 74.0 in stage 4.0 (TID 453). 4544 bytes result sent to driver
[2021-05-16 20:13:45,176] {docker.py:276} INFO - 21/05/16 23:13:45 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 457) (764b1db4ecfd, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:45,178] {docker.py:276} INFO - 21/05/16 23:13:45 INFO Executor: Running task 78.0 in stage 4.0 (TID 457)
21/05/16 23:13:45 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 453) in 2778 ms on 764b1db4ecfd (executor driver) (75/200)
[2021-05-16 20:13:45,190] {docker.py:276} INFO - 21/05/16 23:13:45 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:45,192] {docker.py:276} INFO - 21/05/16 23:13:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242985530924802635379_0004_m_000078_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242985530924802635379_0004_m_000078_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242985530924802635379_0004}; taskId=attempt_202105162312242985530924802635379_0004_m_000078_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3359c6cf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312242985530924802635379_0004_m_000078_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242985530924802635379_0004_m_000078_457
[2021-05-16 20:13:45,196] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Task committer attempt_202105162312242985530924802635379_0004_m_000078_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242985530924802635379_0004_m_000078_457 : duration 0:00.004s
[2021-05-16 20:13:45,558] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312248603234421084802380_0004_m_000075_454: needsTaskCommit() Task attempt_202105162312248603234421084802380_0004_m_000075_454
[2021-05-16 20:13:45,560] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Task committer attempt_202105162312248603234421084802380_0004_m_000075_454: needsTaskCommit() Task attempt_202105162312248603234421084802380_0004_m_000075_454: duration 0:00.003s
21/05/16 23:13:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248603234421084802380_0004_m_000075_454
[2021-05-16 20:13:45,563] {docker.py:276} INFO - 21/05/16 23:13:45 INFO Executor: Finished task 75.0 in stage 4.0 (TID 454). 4544 bytes result sent to driver
[2021-05-16 20:13:45,565] {docker.py:276} INFO - 21/05/16 23:13:45 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 458) (764b1db4ecfd, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:45,565] {docker.py:276} INFO - 21/05/16 23:13:45 INFO Executor: Running task 79.0 in stage 4.0 (TID 458)
[2021-05-16 20:13:45,566] {docker.py:276} INFO - 21/05/16 23:13:45 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 454) in 2955 ms on 764b1db4ecfd (executor driver) (76/200)
[2021-05-16 20:13:45,576] {docker.py:276} INFO - 21/05/16 23:13:45 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:45,578] {docker.py:276} INFO - 21/05/16 23:13:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247428407641102283257_0004_m_000079_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247428407641102283257_0004_m_000079_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247428407641102283257_0004}; taskId=attempt_202105162312247428407641102283257_0004_m_000079_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e3e5444}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:45,578] {docker.py:276} INFO - 21/05/16 23:13:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312247428407641102283257_0004_m_000079_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247428407641102283257_0004_m_000079_458
[2021-05-16 20:13:45,581] {docker.py:276} INFO - 21/05/16 23:13:45 INFO StagingCommitter: Task committer attempt_202105162312247428407641102283257_0004_m_000079_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247428407641102283257_0004_m_000079_458 : duration 0:00.003s
[2021-05-16 20:13:46,277] {docker.py:276} INFO - 21/05/16 23:13:46 INFO StagingCommitter: Starting: Task committer attempt_202105162312243443317855588892374_0004_m_000076_455: needsTaskCommit() Task attempt_202105162312243443317855588892374_0004_m_000076_455
[2021-05-16 20:13:46,278] {docker.py:276} INFO - 21/05/16 23:13:46 INFO StagingCommitter: Task committer attempt_202105162312243443317855588892374_0004_m_000076_455: needsTaskCommit() Task attempt_202105162312243443317855588892374_0004_m_000076_455: duration 0:00.001s
21/05/16 23:13:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243443317855588892374_0004_m_000076_455
[2021-05-16 20:13:46,280] {docker.py:276} INFO - 21/05/16 23:13:46 INFO Executor: Finished task 76.0 in stage 4.0 (TID 455). 4544 bytes result sent to driver
[2021-05-16 20:13:46,281] {docker.py:276} INFO - 21/05/16 23:13:46 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 459) (764b1db4ecfd, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:46,282] {docker.py:276} INFO - 21/05/16 23:13:46 INFO Executor: Running task 80.0 in stage 4.0 (TID 459)
[2021-05-16 20:13:46,282] {docker.py:276} INFO - 21/05/16 23:13:46 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 455) in 2272 ms on 764b1db4ecfd (executor driver) (77/200)
[2021-05-16 20:13:46,306] {docker.py:276} INFO - 21/05/16 23:13:46 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:46,309] {docker.py:276} INFO - 21/05/16 23:13:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:13:46,316] {docker.py:276} INFO - 21/05/16 23:13:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:46,319] {docker.py:276} INFO - 21/05/16 23:13:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243308814680438729229_0004_m_000080_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243308814680438729229_0004_m_000080_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243308814680438729229_0004}; taskId=attempt_202105162312243308814680438729229_0004_m_000080_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5e67d0db}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:46 INFO StagingCommitter: Starting: Task committer attempt_202105162312243308814680438729229_0004_m_000080_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243308814680438729229_0004_m_000080_459
[2021-05-16 20:13:46,329] {docker.py:276} INFO - 21/05/16 23:13:46 INFO StagingCommitter: Task committer attempt_202105162312243308814680438729229_0004_m_000080_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243308814680438729229_0004_m_000080_459 : duration 0:00.013s
[2021-05-16 20:13:47,443] {docker.py:276} INFO - 21/05/16 23:13:47 INFO StagingCommitter: Starting: Task committer attempt_202105162312242666829747607076832_0004_m_000077_456: needsTaskCommit() Task attempt_202105162312242666829747607076832_0004_m_000077_456
[2021-05-16 20:13:47,444] {docker.py:276} INFO - 21/05/16 23:13:47 INFO StagingCommitter: Task committer attempt_202105162312242666829747607076832_0004_m_000077_456: needsTaskCommit() Task attempt_202105162312242666829747607076832_0004_m_000077_456: duration 0:00.002s
21/05/16 23:13:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242666829747607076832_0004_m_000077_456
[2021-05-16 20:13:47,447] {docker.py:276} INFO - 21/05/16 23:13:47 INFO Executor: Finished task 77.0 in stage 4.0 (TID 456). 4544 bytes result sent to driver
[2021-05-16 20:13:47,448] {docker.py:276} INFO - 21/05/16 23:13:47 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 460) (764b1db4ecfd, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:47,449] {docker.py:276} INFO - 21/05/16 23:13:47 INFO Executor: Running task 81.0 in stage 4.0 (TID 460)
[2021-05-16 20:13:47,450] {docker.py:276} INFO - 21/05/16 23:13:47 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 456) in 2845 ms on 764b1db4ecfd (executor driver) (78/200)
[2021-05-16 20:13:47,460] {docker.py:276} INFO - 21/05/16 23:13:47 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:47,460] {docker.py:276} INFO - 21/05/16 23:13:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:47,462] {docker.py:276} INFO - 21/05/16 23:13:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:47,463] {docker.py:276} INFO - 21/05/16 23:13:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:47,463] {docker.py:276} INFO - 21/05/16 23:13:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243178749485307033159_0004_m_000081_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243178749485307033159_0004_m_000081_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243178749485307033159_0004}; taskId=attempt_202105162312243178749485307033159_0004_m_000081_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33f40f10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:47,463] {docker.py:276} INFO - 21/05/16 23:13:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:47,464] {docker.py:276} INFO - 21/05/16 23:13:47 INFO StagingCommitter: Starting: Task committer attempt_202105162312243178749485307033159_0004_m_000081_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243178749485307033159_0004_m_000081_460
[2021-05-16 20:13:47,466] {docker.py:276} INFO - 21/05/16 23:13:47 INFO StagingCommitter: Task committer attempt_202105162312243178749485307033159_0004_m_000081_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243178749485307033159_0004_m_000081_460 : duration 0:00.002s
[2021-05-16 20:13:48,218] {docker.py:276} INFO - 21/05/16 23:13:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312242985530924802635379_0004_m_000078_457: needsTaskCommit() Task attempt_202105162312242985530924802635379_0004_m_000078_457
[2021-05-16 20:13:48,219] {docker.py:276} INFO - 21/05/16 23:13:48 INFO StagingCommitter: Task committer attempt_202105162312242985530924802635379_0004_m_000078_457: needsTaskCommit() Task attempt_202105162312242985530924802635379_0004_m_000078_457: duration 0:00.001s
[2021-05-16 20:13:48,219] {docker.py:276} INFO - 21/05/16 23:13:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242985530924802635379_0004_m_000078_457
[2021-05-16 20:13:48,220] {docker.py:276} INFO - 21/05/16 23:13:48 INFO Executor: Finished task 78.0 in stage 4.0 (TID 457). 4544 bytes result sent to driver
[2021-05-16 20:13:48,221] {docker.py:276} INFO - 21/05/16 23:13:48 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 461) (764b1db4ecfd, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:48,223] {docker.py:276} INFO - 21/05/16 23:13:48 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 457) in 3050 ms on 764b1db4ecfd (executor driver) (79/200)
[2021-05-16 20:13:48,223] {docker.py:276} INFO - 21/05/16 23:13:48 INFO Executor: Running task 82.0 in stage 4.0 (TID 461)
[2021-05-16 20:13:48,232] {docker.py:276} INFO - 21/05/16 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:48,234] {docker.py:276} INFO - 21/05/16 23:13:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:48,234] {docker.py:276} INFO - 21/05/16 23:13:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:48,234] {docker.py:276} INFO - 21/05/16 23:13:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241000251271213804981_0004_m_000082_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241000251271213804981_0004_m_000082_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241000251271213804981_0004}; taskId=attempt_202105162312241000251271213804981_0004_m_000082_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b254c9c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312241000251271213804981_0004_m_000082_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241000251271213804981_0004_m_000082_461
[2021-05-16 20:13:48,237] {docker.py:276} INFO - 21/05/16 23:13:48 INFO StagingCommitter: Task committer attempt_202105162312241000251271213804981_0004_m_000082_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241000251271213804981_0004_m_000082_461 : duration 0:00.003s
[2021-05-16 20:13:49,080] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312247428407641102283257_0004_m_000079_458: needsTaskCommit() Task attempt_202105162312247428407641102283257_0004_m_000079_458
[2021-05-16 20:13:49,081] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Task committer attempt_202105162312247428407641102283257_0004_m_000079_458: needsTaskCommit() Task attempt_202105162312247428407641102283257_0004_m_000079_458: duration 0:00.001s
21/05/16 23:13:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247428407641102283257_0004_m_000079_458
[2021-05-16 20:13:49,083] {docker.py:276} INFO - 21/05/16 23:13:49 INFO Executor: Finished task 79.0 in stage 4.0 (TID 458). 4544 bytes result sent to driver
[2021-05-16 20:13:49,087] {docker.py:276} INFO - 21/05/16 23:13:49 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 462) (764b1db4ecfd, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:49,088] {docker.py:276} INFO - 21/05/16 23:13:49 INFO Executor: Running task 83.0 in stage 4.0 (TID 462)
21/05/16 23:13:49 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 458) in 3526 ms on 764b1db4ecfd (executor driver) (80/200)
[2021-05-16 20:13:49,103] {docker.py:276} INFO - 21/05/16 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:49,106] {docker.py:276} INFO - 21/05/16 23:13:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:49,107] {docker.py:276} INFO - 21/05/16 23:13:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:49,107] {docker.py:276} INFO - 21/05/16 23:13:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241921931332531632070_0004_m_000083_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241921931332531632070_0004_m_000083_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241921931332531632070_0004}; taskId=attempt_202105162312241921931332531632070_0004_m_000083_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54d0a992}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:49,108] {docker.py:276} INFO - 21/05/16 23:13:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312241921931332531632070_0004_m_000083_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241921931332531632070_0004_m_000083_462
[2021-05-16 20:13:49,111] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Task committer attempt_202105162312241921931332531632070_0004_m_000083_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241921931332531632070_0004_m_000083_462 : duration 0:00.004s
[2021-05-16 20:13:49,193] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312243308814680438729229_0004_m_000080_459: needsTaskCommit() Task attempt_202105162312243308814680438729229_0004_m_000080_459
[2021-05-16 20:13:49,194] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Task committer attempt_202105162312243308814680438729229_0004_m_000080_459: needsTaskCommit() Task attempt_202105162312243308814680438729229_0004_m_000080_459: duration 0:00.001s
[2021-05-16 20:13:49,195] {docker.py:276} INFO - 21/05/16 23:13:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243308814680438729229_0004_m_000080_459
[2021-05-16 20:13:49,195] {docker.py:276} INFO - 21/05/16 23:13:49 INFO Executor: Finished task 80.0 in stage 4.0 (TID 459). 4544 bytes result sent to driver
[2021-05-16 20:13:49,196] {docker.py:276} INFO - 21/05/16 23:13:49 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 463) (764b1db4ecfd, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:49,197] {docker.py:276} INFO - 21/05/16 23:13:49 INFO Executor: Running task 84.0 in stage 4.0 (TID 463)
[2021-05-16 20:13:49,197] {docker.py:276} INFO - 21/05/16 23:13:49 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 459) in 2920 ms on 764b1db4ecfd (executor driver) (81/200)
[2021-05-16 20:13:49,205] {docker.py:276} INFO - 21/05/16 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:49,207] {docker.py:276} INFO - 21/05/16 23:13:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242205795671778930845_0004_m_000084_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242205795671778930845_0004_m_000084_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242205795671778930845_0004}; taskId=attempt_202105162312242205795671778930845_0004_m_000084_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@49d9dd5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:49,208] {docker.py:276} INFO - 21/05/16 23:13:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:49,208] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312242205795671778930845_0004_m_000084_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242205795671778930845_0004_m_000084_463
[2021-05-16 20:13:49,211] {docker.py:276} INFO - 21/05/16 23:13:49 INFO StagingCommitter: Task committer attempt_202105162312242205795671778930845_0004_m_000084_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242205795671778930845_0004_m_000084_463 : duration 0:00.003s
[2021-05-16 20:13:50,318] {docker.py:276} INFO - 21/05/16 23:13:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312243178749485307033159_0004_m_000081_460: needsTaskCommit() Task attempt_202105162312243178749485307033159_0004_m_000081_460
[2021-05-16 20:13:50,319] {docker.py:276} INFO - 21/05/16 23:13:50 INFO StagingCommitter: Task committer attempt_202105162312243178749485307033159_0004_m_000081_460: needsTaskCommit() Task attempt_202105162312243178749485307033159_0004_m_000081_460: duration 0:00.001s
21/05/16 23:13:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243178749485307033159_0004_m_000081_460
[2021-05-16 20:13:50,321] {docker.py:276} INFO - 21/05/16 23:13:50 INFO Executor: Finished task 81.0 in stage 4.0 (TID 460). 4544 bytes result sent to driver
[2021-05-16 20:13:50,322] {docker.py:276} INFO - 21/05/16 23:13:50 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 464) (764b1db4ecfd, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:50,324] {docker.py:276} INFO - 21/05/16 23:13:50 INFO Executor: Running task 85.0 in stage 4.0 (TID 464)
21/05/16 23:13:50 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 460) in 2879 ms on 764b1db4ecfd (executor driver) (82/200)
[2021-05-16 20:13:50,340] {docker.py:276} INFO - 21/05/16 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:50,343] {docker.py:276} INFO - 21/05/16 23:13:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:50,344] {docker.py:276} INFO - 21/05/16 23:13:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244529074373915622052_0004_m_000085_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244529074373915622052_0004_m_000085_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244529074373915622052_0004}; taskId=attempt_202105162312244529074373915622052_0004_m_000085_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c8c6152}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:50 INFO StagingCommitter: Starting: Task committer attempt_202105162312244529074373915622052_0004_m_000085_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244529074373915622052_0004_m_000085_464
[2021-05-16 20:13:50,348] {docker.py:276} INFO - 21/05/16 23:13:50 INFO StagingCommitter: Task committer attempt_202105162312244529074373915622052_0004_m_000085_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244529074373915622052_0004_m_000085_464 : duration 0:00.004s
[2021-05-16 20:13:51,212] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312241000251271213804981_0004_m_000082_461: needsTaskCommit() Task attempt_202105162312241000251271213804981_0004_m_000082_461
[2021-05-16 20:13:51,214] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Task committer attempt_202105162312241000251271213804981_0004_m_000082_461: needsTaskCommit() Task attempt_202105162312241000251271213804981_0004_m_000082_461: duration 0:00.001s
21/05/16 23:13:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241000251271213804981_0004_m_000082_461
[2021-05-16 20:13:51,215] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Finished task 82.0 in stage 4.0 (TID 461). 4587 bytes result sent to driver
[2021-05-16 20:13:51,217] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 465) (764b1db4ecfd, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:51,218] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 461) in 3000 ms on 764b1db4ecfd (executor driver) (83/200)
[2021-05-16 20:13:51,218] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Running task 86.0 in stage 4.0 (TID 465)
[2021-05-16 20:13:51,228] {docker.py:276} INFO - 21/05/16 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:51,230] {docker.py:276} INFO - 21/05/16 23:13:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:51,231] {docker.py:276} INFO - 21/05/16 23:13:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248628930600791178231_0004_m_000086_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248628930600791178231_0004_m_000086_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248628930600791178231_0004}; taskId=attempt_202105162312248628930600791178231_0004_m_000086_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c09cc16}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312248628930600791178231_0004_m_000086_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248628930600791178231_0004_m_000086_465
[2021-05-16 20:13:51,234] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Task committer attempt_202105162312248628930600791178231_0004_m_000086_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248628930600791178231_0004_m_000086_465 : duration 0:00.003s
[2021-05-16 20:13:51,952] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312242205795671778930845_0004_m_000084_463: needsTaskCommit() Task attempt_202105162312242205795671778930845_0004_m_000084_463
[2021-05-16 20:13:51,953] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Task committer attempt_202105162312242205795671778930845_0004_m_000084_463: needsTaskCommit() Task attempt_202105162312242205795671778930845_0004_m_000084_463: duration 0:00.001s
[2021-05-16 20:13:51,953] {docker.py:276} INFO - 21/05/16 23:13:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242205795671778930845_0004_m_000084_463
[2021-05-16 20:13:51,954] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Finished task 84.0 in stage 4.0 (TID 463). 4587 bytes result sent to driver
[2021-05-16 20:13:51,955] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 466) (764b1db4ecfd, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:51,955] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Running task 87.0 in stage 4.0 (TID 466)
[2021-05-16 20:13:51,956] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 463) in 2763 ms on 764b1db4ecfd (executor driver) (84/200)
[2021-05-16 20:13:51,965] {docker.py:276} INFO - 21/05/16 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:51,967] {docker.py:276} INFO - 21/05/16 23:13:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245252574804781507368_0004_m_000087_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245252574804781507368_0004_m_000087_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245252574804781507368_0004}; taskId=attempt_202105162312245252574804781507368_0004_m_000087_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1aaf18de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:51,967] {docker.py:276} INFO - 21/05/16 23:13:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:51,968] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312245252574804781507368_0004_m_000087_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245252574804781507368_0004_m_000087_466
[2021-05-16 20:13:51,971] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Task committer attempt_202105162312245252574804781507368_0004_m_000087_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245252574804781507368_0004_m_000087_466 : duration 0:00.004s
[2021-05-16 20:13:51,972] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312241921931332531632070_0004_m_000083_462: needsTaskCommit() Task attempt_202105162312241921931332531632070_0004_m_000083_462
[2021-05-16 20:13:51,973] {docker.py:276} INFO - 21/05/16 23:13:51 INFO StagingCommitter: Task committer attempt_202105162312241921931332531632070_0004_m_000083_462: needsTaskCommit() Task attempt_202105162312241921931332531632070_0004_m_000083_462: duration 0:00.000s
21/05/16 23:13:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241921931332531632070_0004_m_000083_462
[2021-05-16 20:13:51,973] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Finished task 83.0 in stage 4.0 (TID 462). 4587 bytes result sent to driver
[2021-05-16 20:13:51,974] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 467) (764b1db4ecfd, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:51,975] {docker.py:276} INFO - 21/05/16 23:13:51 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 462) in 2895 ms on 764b1db4ecfd (executor driver) (85/200)
[2021-05-16 20:13:51,976] {docker.py:276} INFO - 21/05/16 23:13:51 INFO Executor: Running task 88.0 in stage 4.0 (TID 467)
[2021-05-16 20:13:51,984] {docker.py:276} INFO - 21/05/16 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:51,986] {docker.py:276} INFO - 21/05/16 23:13:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:51,986] {docker.py:276} INFO - 21/05/16 23:13:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:51,987] {docker.py:276} INFO - 21/05/16 23:13:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243237418975994225332_0004_m_000088_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243237418975994225332_0004_m_000088_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243237418975994225332_0004}; taskId=attempt_202105162312243237418975994225332_0004_m_000088_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42184b09}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:51,987] {docker.py:276} INFO - 21/05/16 23:13:52 INFO StagingCommitter: Starting: Task committer attempt_202105162312243237418975994225332_0004_m_000088_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243237418975994225332_0004_m_000088_467
[2021-05-16 20:13:51,989] {docker.py:276} INFO - 21/05/16 23:13:52 INFO StagingCommitter: Task committer attempt_202105162312243237418975994225332_0004_m_000088_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243237418975994225332_0004_m_000088_467 : duration 0:00.003s
[2021-05-16 20:13:53,291] {docker.py:276} INFO - 21/05/16 23:13:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312244529074373915622052_0004_m_000085_464: needsTaskCommit() Task attempt_202105162312244529074373915622052_0004_m_000085_464
[2021-05-16 20:13:53,291] {docker.py:276} INFO - 21/05/16 23:13:53 INFO StagingCommitter: Task committer attempt_202105162312244529074373915622052_0004_m_000085_464: needsTaskCommit() Task attempt_202105162312244529074373915622052_0004_m_000085_464: duration 0:00.000s
[2021-05-16 20:13:53,292] {docker.py:276} INFO - 21/05/16 23:13:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244529074373915622052_0004_m_000085_464
[2021-05-16 20:13:53,293] {docker.py:276} INFO - 21/05/16 23:13:53 INFO Executor: Finished task 85.0 in stage 4.0 (TID 464). 4587 bytes result sent to driver
[2021-05-16 20:13:53,295] {docker.py:276} INFO - 21/05/16 23:13:53 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 468) (764b1db4ecfd, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:53,295] {docker.py:276} INFO - 21/05/16 23:13:53 INFO Executor: Running task 89.0 in stage 4.0 (TID 468)
[2021-05-16 20:13:53,296] {docker.py:276} INFO - 21/05/16 23:13:53 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 464) in 2977 ms on 764b1db4ecfd (executor driver) (86/200)
[2021-05-16 20:13:53,306] {docker.py:276} INFO - 21/05/16 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:53,306] {docker.py:276} INFO - 21/05/16 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:53,308] {docker.py:276} INFO - 21/05/16 23:13:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:13:53,308] {docker.py:276} INFO - 21/05/16 23:13:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:53,309] {docker.py:276} INFO - 21/05/16 23:13:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:53,309] {docker.py:276} INFO - 21/05/16 23:13:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242603647501404586615_0004_m_000089_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242603647501404586615_0004_m_000089_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242603647501404586615_0004}; taskId=attempt_202105162312242603647501404586615_0004_m_000089_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@555b29fe}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:53,309] {docker.py:276} INFO - 21/05/16 23:13:53 INFO StagingCommitter: Starting: Task committer attempt_202105162312242603647501404586615_0004_m_000089_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242603647501404586615_0004_m_000089_468
[2021-05-16 20:13:53,312] {docker.py:276} INFO - 21/05/16 23:13:53 INFO StagingCommitter: Task committer attempt_202105162312242603647501404586615_0004_m_000089_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242603647501404586615_0004_m_000089_468 : duration 0:00.003s
[2021-05-16 20:13:54,036] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312248628930600791178231_0004_m_000086_465: needsTaskCommit() Task attempt_202105162312248628930600791178231_0004_m_000086_465
[2021-05-16 20:13:54,038] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_202105162312248628930600791178231_0004_m_000086_465: needsTaskCommit() Task attempt_202105162312248628930600791178231_0004_m_000086_465: duration 0:00.001s
21/05/16 23:13:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248628930600791178231_0004_m_000086_465
[2021-05-16 20:13:54,039] {docker.py:276} INFO - 21/05/16 23:13:54 INFO Executor: Finished task 86.0 in stage 4.0 (TID 465). 4544 bytes result sent to driver
[2021-05-16 20:13:54,041] {docker.py:276} INFO - 21/05/16 23:13:54 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 469) (764b1db4ecfd, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:13:54 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 465) in 2828 ms on 764b1db4ecfd (executor driver) (87/200)
21/05/16 23:13:54 INFO Executor: Running task 90.0 in stage 4.0 (TID 469)
[2021-05-16 20:13:54,056] {docker.py:276} INFO - 21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:54,059] {docker.py:276} INFO - 21/05/16 23:13:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224207926056117504052_0004_m_000090_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224207926056117504052_0004_m_000090_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224207926056117504052_0004}; taskId=attempt_20210516231224207926056117504052_0004_m_000090_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79fba280}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:54,059] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_20210516231224207926056117504052_0004_m_000090_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224207926056117504052_0004_m_000090_469
[2021-05-16 20:13:54,063] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_20210516231224207926056117504052_0004_m_000090_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224207926056117504052_0004_m_000090_469 : duration 0:00.004s
[2021-05-16 20:13:54,724] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312243237418975994225332_0004_m_000088_467: needsTaskCommit() Task attempt_202105162312243237418975994225332_0004_m_000088_467
21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_202105162312243237418975994225332_0004_m_000088_467: needsTaskCommit() Task attempt_202105162312243237418975994225332_0004_m_000088_467: duration 0:00.001s
[2021-05-16 20:13:54,725] {docker.py:276} INFO - 21/05/16 23:13:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243237418975994225332_0004_m_000088_467
[2021-05-16 20:13:54,726] {docker.py:276} INFO - 21/05/16 23:13:54 INFO Executor: Finished task 88.0 in stage 4.0 (TID 467). 4544 bytes result sent to driver
[2021-05-16 20:13:54,729] {docker.py:276} INFO - 21/05/16 23:13:54 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 470) (764b1db4ecfd, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:13:54 INFO Executor: Running task 91.0 in stage 4.0 (TID 470)
21/05/16 23:13:54 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 467) in 2757 ms on 764b1db4ecfd (executor driver) (88/200)
[2021-05-16 20:13:54,740] {docker.py:276} INFO - 21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 6 (25.3 KiB) non-empty blocks including 6 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:54,741] {docker.py:276} INFO - 21/05/16 23:13:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241067599640695710576_0004_m_000091_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241067599640695710576_0004_m_000091_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241067599640695710576_0004}; taskId=attempt_202105162312241067599640695710576_0004_m_000091_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7dc59a48}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312241067599640695710576_0004_m_000091_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241067599640695710576_0004_m_000091_470
[2021-05-16 20:13:54,744] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_202105162312241067599640695710576_0004_m_000091_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241067599640695710576_0004_m_000091_470 : duration 0:00.003s
[2021-05-16 20:13:54,825] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312245252574804781507368_0004_m_000087_466: needsTaskCommit() Task attempt_202105162312245252574804781507368_0004_m_000087_466
21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_202105162312245252574804781507368_0004_m_000087_466: needsTaskCommit() Task attempt_202105162312245252574804781507368_0004_m_000087_466: duration 0:00.000s
21/05/16 23:13:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245252574804781507368_0004_m_000087_466
[2021-05-16 20:13:54,827] {docker.py:276} INFO - 21/05/16 23:13:54 INFO Executor: Finished task 87.0 in stage 4.0 (TID 466). 4544 bytes result sent to driver
[2021-05-16 20:13:54,828] {docker.py:276} INFO - 21/05/16 23:13:54 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 471) (764b1db4ecfd, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:54,829] {docker.py:276} INFO - 21/05/16 23:13:54 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 466) in 2878 ms on 764b1db4ecfd (executor driver) (89/200)
[2021-05-16 20:13:54,829] {docker.py:276} INFO - 21/05/16 23:13:54 INFO Executor: Running task 92.0 in stage 4.0 (TID 471)
[2021-05-16 20:13:54,840] {docker.py:276} INFO - 21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:54,842] {docker.py:276} INFO - 21/05/16 23:13:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:54,842] {docker.py:276} INFO - 21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246659341393126328098_0004_m_000092_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246659341393126328098_0004_m_000092_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246659341393126328098_0004}; taskId=attempt_202105162312246659341393126328098_0004_m_000092_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2eee36de}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:54,842] {docker.py:276} INFO - 21/05/16 23:13:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:54,843] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312246659341393126328098_0004_m_000092_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246659341393126328098_0004_m_000092_471
[2021-05-16 20:13:54,845] {docker.py:276} INFO - 21/05/16 23:13:54 INFO StagingCommitter: Task committer attempt_202105162312246659341393126328098_0004_m_000092_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246659341393126328098_0004_m_000092_471 : duration 0:00.003s
[2021-05-16 20:13:56,102] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Starting: Task committer attempt_202105162312242603647501404586615_0004_m_000089_468: needsTaskCommit() Task attempt_202105162312242603647501404586615_0004_m_000089_468
[2021-05-16 20:13:56,103] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Task committer attempt_202105162312242603647501404586615_0004_m_000089_468: needsTaskCommit() Task attempt_202105162312242603647501404586615_0004_m_000089_468: duration 0:00.002s
[2021-05-16 20:13:56,104] {docker.py:276} INFO - 21/05/16 23:13:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242603647501404586615_0004_m_000089_468
[2021-05-16 20:13:56,106] {docker.py:276} INFO - 21/05/16 23:13:56 INFO Executor: Finished task 89.0 in stage 4.0 (TID 468). 4544 bytes result sent to driver
[2021-05-16 20:13:56,106] {docker.py:276} INFO - 21/05/16 23:13:56 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 472) (764b1db4ecfd, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:56,107] {docker.py:276} INFO - 21/05/16 23:13:56 INFO Executor: Running task 93.0 in stage 4.0 (TID 472)
[2021-05-16 20:13:56,108] {docker.py:276} INFO - 21/05/16 23:13:56 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 468) in 2817 ms on 764b1db4ecfd (executor driver) (90/200)
[2021-05-16 20:13:56,118] {docker.py:276} INFO - 21/05/16 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:56,119] {docker.py:276} INFO - 21/05/16 23:13:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:56,120] {docker.py:276} INFO - 21/05/16 23:13:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224774603492083184547_0004_m_000093_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224774603492083184547_0004_m_000093_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224774603492083184547_0004}; taskId=attempt_20210516231224774603492083184547_0004_m_000093_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a52dcb6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:56 INFO StagingCommitter: Starting: Task committer attempt_20210516231224774603492083184547_0004_m_000093_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224774603492083184547_0004_m_000093_472
[2021-05-16 20:13:56,122] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Task committer attempt_20210516231224774603492083184547_0004_m_000093_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224774603492083184547_0004_m_000093_472 : duration 0:00.003s
[2021-05-16 20:13:56,905] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Starting: Task committer attempt_20210516231224207926056117504052_0004_m_000090_469: needsTaskCommit() Task attempt_20210516231224207926056117504052_0004_m_000090_469
[2021-05-16 20:13:56,906] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Task committer attempt_20210516231224207926056117504052_0004_m_000090_469: needsTaskCommit() Task attempt_20210516231224207926056117504052_0004_m_000090_469: duration 0:00.000s
21/05/16 23:13:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224207926056117504052_0004_m_000090_469
[2021-05-16 20:13:56,908] {docker.py:276} INFO - 21/05/16 23:13:56 INFO Executor: Finished task 90.0 in stage 4.0 (TID 469). 4544 bytes result sent to driver
[2021-05-16 20:13:56,910] {docker.py:276} INFO - 21/05/16 23:13:56 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 473) (764b1db4ecfd, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:56,911] {docker.py:276} INFO - 21/05/16 23:13:56 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 469) in 2873 ms on 764b1db4ecfd (executor driver) (91/200)
[2021-05-16 20:13:56,911] {docker.py:276} INFO - 21/05/16 23:13:56 INFO Executor: Running task 94.0 in stage 4.0 (TID 473)
[2021-05-16 20:13:56,920] {docker.py:276} INFO - 21/05/16 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:13:56,921] {docker.py:276} INFO - 21/05/16 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:56,923] {docker.py:276} INFO - 21/05/16 23:13:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:56,923] {docker.py:276} INFO - 21/05/16 23:13:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:56,924] {docker.py:276} INFO - 21/05/16 23:13:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_2021051623122412082390383858724_0004_m_000094_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_2021051623122412082390383858724_0004_m_000094_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2021051623122412082390383858724_0004}; taskId=attempt_2021051623122412082390383858724_0004_m_000094_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@527ecf9e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:13:56,924] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Starting: Task committer attempt_2021051623122412082390383858724_0004_m_000094_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_2021051623122412082390383858724_0004_m_000094_473
[2021-05-16 20:13:56,927] {docker.py:276} INFO - 21/05/16 23:13:56 INFO StagingCommitter: Task committer attempt_2021051623122412082390383858724_0004_m_000094_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_2021051623122412082390383858724_0004_m_000094_473 : duration 0:00.004s
[2021-05-16 20:13:57,524] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312241067599640695710576_0004_m_000091_470: needsTaskCommit() Task attempt_202105162312241067599640695710576_0004_m_000091_470
[2021-05-16 20:13:57,525] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Task committer attempt_202105162312241067599640695710576_0004_m_000091_470: needsTaskCommit() Task attempt_202105162312241067599640695710576_0004_m_000091_470: duration 0:00.001s
21/05/16 23:13:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241067599640695710576_0004_m_000091_470
[2021-05-16 20:13:57,527] {docker.py:276} INFO - 21/05/16 23:13:57 INFO Executor: Finished task 91.0 in stage 4.0 (TID 470). 4544 bytes result sent to driver
[2021-05-16 20:13:57,529] {docker.py:276} INFO - 21/05/16 23:13:57 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 474) (764b1db4ecfd, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:57,530] {docker.py:276} INFO - 21/05/16 23:13:57 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 470) in 2805 ms on 764b1db4ecfd (executor driver) (92/200)
21/05/16 23:13:57 INFO Executor: Running task 95.0 in stage 4.0 (TID 474)
[2021-05-16 20:13:57,539] {docker.py:276} INFO - 21/05/16 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:57,541] {docker.py:276} INFO - 21/05/16 23:13:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:13:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248587137877756206672_0004_m_000095_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248587137877756206672_0004_m_000095_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248587137877756206672_0004}; taskId=attempt_202105162312248587137877756206672_0004_m_000095_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d5d4c1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:57,541] {docker.py:276} INFO - 21/05/16 23:13:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312248587137877756206672_0004_m_000095_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248587137877756206672_0004_m_000095_474
[2021-05-16 20:13:57,544] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Task committer attempt_202105162312248587137877756206672_0004_m_000095_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248587137877756206672_0004_m_000095_474 : duration 0:00.002s
[2021-05-16 20:13:57,638] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312246659341393126328098_0004_m_000092_471: needsTaskCommit() Task attempt_202105162312246659341393126328098_0004_m_000092_471
[2021-05-16 20:13:57,640] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Task committer attempt_202105162312246659341393126328098_0004_m_000092_471: needsTaskCommit() Task attempt_202105162312246659341393126328098_0004_m_000092_471: duration 0:00.002s
21/05/16 23:13:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246659341393126328098_0004_m_000092_471
[2021-05-16 20:13:57,643] {docker.py:276} INFO - 21/05/16 23:13:57 INFO Executor: Finished task 92.0 in stage 4.0 (TID 471). 4544 bytes result sent to driver
[2021-05-16 20:13:57,644] {docker.py:276} INFO - 21/05/16 23:13:57 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 475) (764b1db4ecfd, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:57,645] {docker.py:276} INFO - 21/05/16 23:13:57 INFO Executor: Running task 96.0 in stage 4.0 (TID 475)
21/05/16 23:13:57 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 471) in 2821 ms on 764b1db4ecfd (executor driver) (93/200)
[2021-05-16 20:13:57,654] {docker.py:276} INFO - 21/05/16 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:57,656] {docker.py:276} INFO - 21/05/16 23:13:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:57,656] {docker.py:276} INFO - 21/05/16 23:13:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242566224933750377873_0004_m_000096_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242566224933750377873_0004_m_000096_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242566224933750377873_0004}; taskId=attempt_202105162312242566224933750377873_0004_m_000096_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30fa8482}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312242566224933750377873_0004_m_000096_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242566224933750377873_0004_m_000096_475
[2021-05-16 20:13:57,659] {docker.py:276} INFO - 21/05/16 23:13:57 INFO StagingCommitter: Task committer attempt_202105162312242566224933750377873_0004_m_000096_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242566224933750377873_0004_m_000096_475 : duration 0:00.003s
[2021-05-16 20:13:58,274] {docker.py:276} INFO - 21/05/16 23:13:58 INFO StagingCommitter: Starting: Task committer attempt_20210516231224774603492083184547_0004_m_000093_472: needsTaskCommit() Task attempt_20210516231224774603492083184547_0004_m_000093_472
[2021-05-16 20:13:58,275] {docker.py:276} INFO - 21/05/16 23:13:58 INFO StagingCommitter: Task committer attempt_20210516231224774603492083184547_0004_m_000093_472: needsTaskCommit() Task attempt_20210516231224774603492083184547_0004_m_000093_472: duration 0:00.001s
21/05/16 23:13:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224774603492083184547_0004_m_000093_472
[2021-05-16 20:13:58,276] {docker.py:276} INFO - 21/05/16 23:13:58 INFO Executor: Finished task 93.0 in stage 4.0 (TID 472). 4544 bytes result sent to driver
[2021-05-16 20:13:58,277] {docker.py:276} INFO - 21/05/16 23:13:58 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 476) (764b1db4ecfd, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:58,278] {docker.py:276} INFO - 21/05/16 23:13:58 INFO Executor: Running task 97.0 in stage 4.0 (TID 476)
[2021-05-16 20:13:58,279] {docker.py:276} INFO - 21/05/16 23:13:58 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 472) in 2175 ms on 764b1db4ecfd (executor driver) (94/200)
[2021-05-16 20:13:58,288] {docker.py:276} INFO - 21/05/16 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:58,290] {docker.py:276} INFO - 21/05/16 23:13:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:13:58,291] {docker.py:276} INFO - 21/05/16 23:13:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:58,291] {docker.py:276} INFO - 21/05/16 23:13:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247286477506524494467_0004_m_000097_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247286477506524494467_0004_m_000097_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247286477506524494467_0004}; taskId=attempt_202105162312247286477506524494467_0004_m_000097_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a1ee123}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:58 INFO StagingCommitter: Starting: Task committer attempt_202105162312247286477506524494467_0004_m_000097_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247286477506524494467_0004_m_000097_476
[2021-05-16 20:13:58,293] {docker.py:276} INFO - 21/05/16 23:13:58 INFO StagingCommitter: Task committer attempt_202105162312247286477506524494467_0004_m_000097_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247286477506524494467_0004_m_000097_476 : duration 0:00.003s
[2021-05-16 20:13:59,636] {docker.py:276} INFO - 21/05/16 23:13:59 INFO StagingCommitter: Starting: Task committer attempt_2021051623122412082390383858724_0004_m_000094_473: needsTaskCommit() Task attempt_2021051623122412082390383858724_0004_m_000094_473
[2021-05-16 20:13:59,637] {docker.py:276} INFO - 21/05/16 23:13:59 INFO StagingCommitter: Task committer attempt_2021051623122412082390383858724_0004_m_000094_473: needsTaskCommit() Task attempt_2021051623122412082390383858724_0004_m_000094_473: duration 0:00.002s
21/05/16 23:13:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2021051623122412082390383858724_0004_m_000094_473
[2021-05-16 20:13:59,639] {docker.py:276} INFO - 21/05/16 23:13:59 INFO Executor: Finished task 94.0 in stage 4.0 (TID 473). 4544 bytes result sent to driver
[2021-05-16 20:13:59,640] {docker.py:276} INFO - 21/05/16 23:13:59 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 477) (764b1db4ecfd, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:13:59,641] {docker.py:276} INFO - 21/05/16 23:13:59 INFO Executor: Running task 98.0 in stage 4.0 (TID 477)
[2021-05-16 20:13:59,642] {docker.py:276} INFO - 21/05/16 23:13:59 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 473) in 2737 ms on 764b1db4ecfd (executor driver) (95/200)
[2021-05-16 20:13:59,653] {docker.py:276} INFO - 21/05/16 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:13:59,655] {docker.py:276} INFO - 21/05/16 23:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:13:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:13:59,655] {docker.py:276} INFO - 21/05/16 23:13:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242258250715701617805_0004_m_000098_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242258250715701617805_0004_m_000098_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242258250715701617805_0004}; taskId=attempt_202105162312242258250715701617805_0004_m_000098_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@741904f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:13:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312242258250715701617805_0004_m_000098_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242258250715701617805_0004_m_000098_477
[2021-05-16 20:13:59,658] {docker.py:276} INFO - 21/05/16 23:13:59 INFO StagingCommitter: Task committer attempt_202105162312242258250715701617805_0004_m_000098_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242258250715701617805_0004_m_000098_477 : duration 0:00.002s
[2021-05-16 20:14:00,464] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Starting: Task committer attempt_202105162312248587137877756206672_0004_m_000095_474: needsTaskCommit() Task attempt_202105162312248587137877756206672_0004_m_000095_474
[2021-05-16 20:14:00,465] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Task committer attempt_202105162312248587137877756206672_0004_m_000095_474: needsTaskCommit() Task attempt_202105162312248587137877756206672_0004_m_000095_474: duration 0:00.001s
21/05/16 23:14:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248587137877756206672_0004_m_000095_474
[2021-05-16 20:14:00,466] {docker.py:276} INFO - 21/05/16 23:14:00 INFO Executor: Finished task 95.0 in stage 4.0 (TID 474). 4544 bytes result sent to driver
[2021-05-16 20:14:00,467] {docker.py:276} INFO - 21/05/16 23:14:00 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 478) (764b1db4ecfd, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:00,468] {docker.py:276} INFO - 21/05/16 23:14:00 INFO Executor: Running task 99.0 in stage 4.0 (TID 478)
[2021-05-16 20:14:00,469] {docker.py:276} INFO - 21/05/16 23:14:00 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 474) in 2944 ms on 764b1db4ecfd (executor driver) (96/200)
[2021-05-16 20:14:00,478] {docker.py:276} INFO - 21/05/16 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:00,480] {docker.py:276} INFO - 21/05/16 23:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224862291811224014113_0004_m_000099_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224862291811224014113_0004_m_000099_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224862291811224014113_0004}; taskId=attempt_20210516231224862291811224014113_0004_m_000099_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ed16dc3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:00 INFO StagingCommitter: Starting: Task committer attempt_20210516231224862291811224014113_0004_m_000099_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224862291811224014113_0004_m_000099_478
[2021-05-16 20:14:00,483] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Task committer attempt_20210516231224862291811224014113_0004_m_000099_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224862291811224014113_0004_m_000099_478 : duration 0:00.003s
[2021-05-16 20:14:00,569] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Starting: Task committer attempt_202105162312242566224933750377873_0004_m_000096_475: needsTaskCommit() Task attempt_202105162312242566224933750377873_0004_m_000096_475
[2021-05-16 20:14:00,570] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Task committer attempt_202105162312242566224933750377873_0004_m_000096_475: needsTaskCommit() Task attempt_202105162312242566224933750377873_0004_m_000096_475: duration 0:00.000s
21/05/16 23:14:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242566224933750377873_0004_m_000096_475
[2021-05-16 20:14:00,571] {docker.py:276} INFO - 21/05/16 23:14:00 INFO Executor: Finished task 96.0 in stage 4.0 (TID 475). 4544 bytes result sent to driver
[2021-05-16 20:14:00,573] {docker.py:276} INFO - 21/05/16 23:14:00 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 479) (764b1db4ecfd, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:00,574] {docker.py:276} INFO - 21/05/16 23:14:00 INFO Executor: Running task 100.0 in stage 4.0 (TID 479)
[2021-05-16 20:14:00,574] {docker.py:276} INFO - 21/05/16 23:14:00 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 475) in 2933 ms on 764b1db4ecfd (executor driver) (97/200)
[2021-05-16 20:14:00,584] {docker.py:276} INFO - 21/05/16 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:00,586] {docker.py:276} INFO - 21/05/16 23:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243008321577808648864_0004_m_000100_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243008321577808648864_0004_m_000100_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243008321577808648864_0004}; taskId=attempt_202105162312243008321577808648864_0004_m_000100_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@351fb11f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:00 INFO StagingCommitter: Starting: Task committer attempt_202105162312243008321577808648864_0004_m_000100_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243008321577808648864_0004_m_000100_479
[2021-05-16 20:14:00,589] {docker.py:276} INFO - 21/05/16 23:14:00 INFO StagingCommitter: Task committer attempt_202105162312243008321577808648864_0004_m_000100_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243008321577808648864_0004_m_000100_479 : duration 0:00.003s
[2021-05-16 20:14:01,015] {docker.py:276} INFO - 21/05/16 23:14:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312247286477506524494467_0004_m_000097_476: needsTaskCommit() Task attempt_202105162312247286477506524494467_0004_m_000097_476
[2021-05-16 20:14:01,016] {docker.py:276} INFO - 21/05/16 23:14:01 INFO StagingCommitter: Task committer attempt_202105162312247286477506524494467_0004_m_000097_476: needsTaskCommit() Task attempt_202105162312247286477506524494467_0004_m_000097_476: duration 0:00.000s
[2021-05-16 20:14:01,016] {docker.py:276} INFO - 21/05/16 23:14:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247286477506524494467_0004_m_000097_476
[2021-05-16 20:14:01,017] {docker.py:276} INFO - 21/05/16 23:14:01 INFO Executor: Finished task 97.0 in stage 4.0 (TID 476). 4544 bytes result sent to driver
[2021-05-16 20:14:01,018] {docker.py:276} INFO - 21/05/16 23:14:01 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 480) (764b1db4ecfd, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:01,019] {docker.py:276} INFO - 21/05/16 23:14:01 INFO Executor: Running task 101.0 in stage 4.0 (TID 480)
[2021-05-16 20:14:01,019] {docker.py:276} INFO - 21/05/16 23:14:01 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 476) in 2746 ms on 764b1db4ecfd (executor driver) (98/200)
[2021-05-16 20:14:01,027] {docker.py:276} INFO - 21/05/16 23:14:01 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:01,029] {docker.py:276} INFO - 21/05/16 23:14:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245228162762811125500_0004_m_000101_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245228162762811125500_0004_m_000101_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245228162762811125500_0004}; taskId=attempt_202105162312245228162762811125500_0004_m_000101_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bc955b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312245228162762811125500_0004_m_000101_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245228162762811125500_0004_m_000101_480
[2021-05-16 20:14:01,031] {docker.py:276} INFO - 21/05/16 23:14:01 INFO StagingCommitter: Task committer attempt_202105162312245228162762811125500_0004_m_000101_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245228162762811125500_0004_m_000101_480 : duration 0:00.002s
[2021-05-16 20:14:02,522] {docker.py:276} INFO - 21/05/16 23:14:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312242258250715701617805_0004_m_000098_477: needsTaskCommit() Task attempt_202105162312242258250715701617805_0004_m_000098_477
[2021-05-16 20:14:02,523] {docker.py:276} INFO - 21/05/16 23:14:02 INFO StagingCommitter: Task committer attempt_202105162312242258250715701617805_0004_m_000098_477: needsTaskCommit() Task attempt_202105162312242258250715701617805_0004_m_000098_477: duration 0:00.001s
[2021-05-16 20:14:02,523] {docker.py:276} INFO - 21/05/16 23:14:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242258250715701617805_0004_m_000098_477
[2021-05-16 20:14:02,525] {docker.py:276} INFO - 21/05/16 23:14:02 INFO Executor: Finished task 98.0 in stage 4.0 (TID 477). 4544 bytes result sent to driver
[2021-05-16 20:14:02,527] {docker.py:276} INFO - 21/05/16 23:14:02 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 481) (764b1db4ecfd, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:02,528] {docker.py:276} INFO - 21/05/16 23:14:02 INFO Executor: Running task 102.0 in stage 4.0 (TID 481)
21/05/16 23:14:02 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 477) in 2892 ms on 764b1db4ecfd (executor driver) (99/200)
[2021-05-16 20:14:02,537] {docker.py:276} INFO - 21/05/16 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:02,539] {docker.py:276} INFO - 21/05/16 23:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248351967395527437670_0004_m_000102_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248351967395527437670_0004_m_000102_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248351967395527437670_0004}; taskId=attempt_202105162312248351967395527437670_0004_m_000102_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@352f71bc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:02 INFO StagingCommitter: Starting: Task committer attempt_202105162312248351967395527437670_0004_m_000102_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248351967395527437670_0004_m_000102_481
[2021-05-16 20:14:02,542] {docker.py:276} INFO - 21/05/16 23:14:02 INFO StagingCommitter: Task committer attempt_202105162312248351967395527437670_0004_m_000102_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248351967395527437670_0004_m_000102_481 : duration 0:00.003s
[2021-05-16 20:14:03,285] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Starting: Task committer attempt_20210516231224862291811224014113_0004_m_000099_478: needsTaskCommit() Task attempt_20210516231224862291811224014113_0004_m_000099_478
[2021-05-16 20:14:03,286] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Task committer attempt_20210516231224862291811224014113_0004_m_000099_478: needsTaskCommit() Task attempt_20210516231224862291811224014113_0004_m_000099_478: duration 0:00.001s
[2021-05-16 20:14:03,287] {docker.py:276} INFO - 21/05/16 23:14:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224862291811224014113_0004_m_000099_478
[2021-05-16 20:14:03,290] {docker.py:276} INFO - 21/05/16 23:14:03 INFO Executor: Finished task 99.0 in stage 4.0 (TID 478). 4544 bytes result sent to driver
[2021-05-16 20:14:03,291] {docker.py:276} INFO - 21/05/16 23:14:03 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 482) (764b1db4ecfd, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:03,293] {docker.py:276} INFO - 21/05/16 23:14:03 INFO Executor: Running task 103.0 in stage 4.0 (TID 482)
[2021-05-16 20:14:03,294] {docker.py:276} INFO - 21/05/16 23:14:03 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 478) in 2831 ms on 764b1db4ecfd (executor driver) (100/200)
[2021-05-16 20:14:03,303] {docker.py:276} INFO - 21/05/16 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:03,306] {docker.py:276} INFO - 21/05/16 23:14:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:03,306] {docker.py:276} INFO - 21/05/16 23:14:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:03,307] {docker.py:276} INFO - 21/05/16 23:14:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241245378282921140065_0004_m_000103_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241245378282921140065_0004_m_000103_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241245378282921140065_0004}; taskId=attempt_202105162312241245378282921140065_0004_m_000103_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41e026a9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:03,307] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312241245378282921140065_0004_m_000103_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241245378282921140065_0004_m_000103_482
[2021-05-16 20:14:03,309] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Task committer attempt_202105162312241245378282921140065_0004_m_000103_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241245378282921140065_0004_m_000103_482 : duration 0:00.003s
[2021-05-16 20:14:03,408] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312243008321577808648864_0004_m_000100_479: needsTaskCommit() Task attempt_202105162312243008321577808648864_0004_m_000100_479
[2021-05-16 20:14:03,408] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Task committer attempt_202105162312243008321577808648864_0004_m_000100_479: needsTaskCommit() Task attempt_202105162312243008321577808648864_0004_m_000100_479: duration 0:00.000s
21/05/16 23:14:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243008321577808648864_0004_m_000100_479
[2021-05-16 20:14:03,409] {docker.py:276} INFO - 21/05/16 23:14:03 INFO Executor: Finished task 100.0 in stage 4.0 (TID 479). 4544 bytes result sent to driver
[2021-05-16 20:14:03,410] {docker.py:276} INFO - 21/05/16 23:14:03 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 483) (764b1db4ecfd, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:03,411] {docker.py:276} INFO - 21/05/16 23:14:03 INFO Executor: Running task 104.0 in stage 4.0 (TID 483)
21/05/16 23:14:03 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 479) in 2842 ms on 764b1db4ecfd (executor driver) (101/200)
[2021-05-16 20:14:03,419] {docker.py:276} INFO - 21/05/16 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:03,419] {docker.py:276} INFO - 21/05/16 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:03,421] {docker.py:276} INFO - 21/05/16 23:14:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241073897713137387581_0004_m_000104_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241073897713137387581_0004_m_000104_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241073897713137387581_0004}; taskId=attempt_202105162312241073897713137387581_0004_m_000104_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@364f85f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312241073897713137387581_0004_m_000104_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241073897713137387581_0004_m_000104_483
[2021-05-16 20:14:03,424] {docker.py:276} INFO - 21/05/16 23:14:03 INFO StagingCommitter: Task committer attempt_202105162312241073897713137387581_0004_m_000104_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241073897713137387581_0004_m_000104_483 : duration 0:00.002s
[2021-05-16 20:14:04,062] {docker.py:276} INFO - 21/05/16 23:14:04 INFO StagingCommitter: Starting: Task committer attempt_202105162312245228162762811125500_0004_m_000101_480: needsTaskCommit() Task attempt_202105162312245228162762811125500_0004_m_000101_480
[2021-05-16 20:14:04,063] {docker.py:276} INFO - 21/05/16 23:14:04 INFO StagingCommitter: Task committer attempt_202105162312245228162762811125500_0004_m_000101_480: needsTaskCommit() Task attempt_202105162312245228162762811125500_0004_m_000101_480: duration 0:00.001s
21/05/16 23:14:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245228162762811125500_0004_m_000101_480
[2021-05-16 20:14:04,064] {docker.py:276} INFO - 21/05/16 23:14:04 INFO Executor: Finished task 101.0 in stage 4.0 (TID 480). 4544 bytes result sent to driver
[2021-05-16 20:14:04,066] {docker.py:276} INFO - 21/05/16 23:14:04 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 484) (764b1db4ecfd, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:04,067] {docker.py:276} INFO - 21/05/16 23:14:04 INFO Executor: Running task 105.0 in stage 4.0 (TID 484)
[2021-05-16 20:14:04,067] {docker.py:276} INFO - 21/05/16 23:14:04 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 480) in 3051 ms on 764b1db4ecfd (executor driver) (102/200)
[2021-05-16 20:14:04,077] {docker.py:276} INFO - 21/05/16 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:04,079] {docker.py:276} INFO - 21/05/16 23:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224829125155127119586_0004_m_000105_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224829125155127119586_0004_m_000105_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224829125155127119586_0004}; taskId=attempt_20210516231224829125155127119586_0004_m_000105_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d07e65c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:04 INFO StagingCommitter: Starting: Task committer attempt_20210516231224829125155127119586_0004_m_000105_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224829125155127119586_0004_m_000105_484
[2021-05-16 20:14:04,083] {docker.py:276} INFO - 21/05/16 23:14:04 INFO StagingCommitter: Task committer attempt_20210516231224829125155127119586_0004_m_000105_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224829125155127119586_0004_m_000105_484 : duration 0:00.003s
[2021-05-16 20:14:05,456] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312248351967395527437670_0004_m_000102_481: needsTaskCommit() Task attempt_202105162312248351967395527437670_0004_m_000102_481
[2021-05-16 20:14:05,456] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Task committer attempt_202105162312248351967395527437670_0004_m_000102_481: needsTaskCommit() Task attempt_202105162312248351967395527437670_0004_m_000102_481: duration 0:00.001s
21/05/16 23:14:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248351967395527437670_0004_m_000102_481
[2021-05-16 20:14:05,460] {docker.py:276} INFO - 21/05/16 23:14:05 INFO Executor: Finished task 102.0 in stage 4.0 (TID 481). 4544 bytes result sent to driver
[2021-05-16 20:14:05,461] {docker.py:276} INFO - 21/05/16 23:14:05 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 485) (764b1db4ecfd, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:05,461] {docker.py:276} INFO - 21/05/16 23:14:05 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 481) in 2938 ms on 764b1db4ecfd (executor driver) (103/200)
[2021-05-16 20:14:05,462] {docker.py:276} INFO - 21/05/16 23:14:05 INFO Executor: Running task 106.0 in stage 4.0 (TID 485)
[2021-05-16 20:14:05,472] {docker.py:276} INFO - 21/05/16 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:05,475] {docker.py:276} INFO - 21/05/16 23:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:05,476] {docker.py:276} INFO - 21/05/16 23:14:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242500929538711699050_0004_m_000106_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242500929538711699050_0004_m_000106_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242500929538711699050_0004}; taskId=attempt_202105162312242500929538711699050_0004_m_000106_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5adee74f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:05,476] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312242500929538711699050_0004_m_000106_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242500929538711699050_0004_m_000106_485
[2021-05-16 20:14:05,480] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Task committer attempt_202105162312242500929538711699050_0004_m_000106_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242500929538711699050_0004_m_000106_485 : duration 0:00.004s
[2021-05-16 20:14:05,642] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Starting: Task committer attempt_202105162312241245378282921140065_0004_m_000103_482: needsTaskCommit() Task attempt_202105162312241245378282921140065_0004_m_000103_482
[2021-05-16 20:14:05,643] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Task committer attempt_202105162312241245378282921140065_0004_m_000103_482: needsTaskCommit() Task attempt_202105162312241245378282921140065_0004_m_000103_482: duration 0:00.000s
[2021-05-16 20:14:05,644] {docker.py:276} INFO - 21/05/16 23:14:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241245378282921140065_0004_m_000103_482
[2021-05-16 20:14:05,646] {docker.py:276} INFO - 21/05/16 23:14:05 INFO Executor: Finished task 103.0 in stage 4.0 (TID 482). 4544 bytes result sent to driver
[2021-05-16 20:14:05,647] {docker.py:276} INFO - 21/05/16 23:14:05 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 486) (764b1db4ecfd, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:05,648] {docker.py:276} INFO - 21/05/16 23:14:05 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 482) in 2360 ms on 764b1db4ecfd (executor driver) (104/200)
[2021-05-16 20:14:05,649] {docker.py:276} INFO - 21/05/16 23:14:05 INFO Executor: Running task 107.0 in stage 4.0 (TID 486)
[2021-05-16 20:14:05,659] {docker.py:276} INFO - 21/05/16 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 6 (25.3 KiB) non-empty blocks including 6 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:05,660] {docker.py:276} INFO - 21/05/16 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:05,662] {docker.py:276} INFO - 21/05/16 23:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:05,663] {docker.py:276} INFO - 21/05/16 23:14:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:05,663] {docker.py:276} INFO - 21/05/16 23:14:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224292420089300427829_0004_m_000107_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224292420089300427829_0004_m_000107_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224292420089300427829_0004}; taskId=attempt_20210516231224292420089300427829_0004_m_000107_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48d6f755}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:05,664] {docker.py:276} INFO - 21/05/16 23:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:05,664] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Starting: Task committer attempt_20210516231224292420089300427829_0004_m_000107_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224292420089300427829_0004_m_000107_486
[2021-05-16 20:14:05,668] {docker.py:276} INFO - 21/05/16 23:14:05 INFO StagingCommitter: Task committer attempt_20210516231224292420089300427829_0004_m_000107_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224292420089300427829_0004_m_000107_486 : duration 0:00.005s
[2021-05-16 20:14:06,148] {docker.py:276} INFO - 21/05/16 23:14:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312241073897713137387581_0004_m_000104_483: needsTaskCommit() Task attempt_202105162312241073897713137387581_0004_m_000104_483
[2021-05-16 20:14:06,148] {docker.py:276} INFO - 21/05/16 23:14:06 INFO StagingCommitter: Task committer attempt_202105162312241073897713137387581_0004_m_000104_483: needsTaskCommit() Task attempt_202105162312241073897713137387581_0004_m_000104_483: duration 0:00.001s
21/05/16 23:14:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241073897713137387581_0004_m_000104_483
[2021-05-16 20:14:06,150] {docker.py:276} INFO - 21/05/16 23:14:06 INFO Executor: Finished task 104.0 in stage 4.0 (TID 483). 4544 bytes result sent to driver
[2021-05-16 20:14:06,151] {docker.py:276} INFO - 21/05/16 23:14:06 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 487) (764b1db4ecfd, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:06,152] {docker.py:276} INFO - 21/05/16 23:14:06 INFO Executor: Running task 108.0 in stage 4.0 (TID 487)
[2021-05-16 20:14:06,153] {docker.py:276} INFO - 21/05/16 23:14:06 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 483) in 2745 ms on 764b1db4ecfd (executor driver) (105/200)
[2021-05-16 20:14:06,164] {docker.py:276} INFO - 21/05/16 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:06,167] {docker.py:276} INFO - 21/05/16 23:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241688975498447198037_0004_m_000108_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241688975498447198037_0004_m_000108_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241688975498447198037_0004}; taskId=attempt_202105162312241688975498447198037_0004_m_000108_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c4fb257}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:06,167] {docker.py:276} INFO - 21/05/16 23:14:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312241688975498447198037_0004_m_000108_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241688975498447198037_0004_m_000108_487
[2021-05-16 20:14:06,171] {docker.py:276} INFO - 21/05/16 23:14:06 INFO StagingCommitter: Task committer attempt_202105162312241688975498447198037_0004_m_000108_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241688975498447198037_0004_m_000108_487 : duration 0:00.004s
[2021-05-16 20:14:07,090] {docker.py:276} INFO - 21/05/16 23:14:07 INFO StagingCommitter: Starting: Task committer attempt_20210516231224829125155127119586_0004_m_000105_484: needsTaskCommit() Task attempt_20210516231224829125155127119586_0004_m_000105_484
[2021-05-16 20:14:07,091] {docker.py:276} INFO - 21/05/16 23:14:07 INFO StagingCommitter: Task committer attempt_20210516231224829125155127119586_0004_m_000105_484: needsTaskCommit() Task attempt_20210516231224829125155127119586_0004_m_000105_484: duration 0:00.000s
21/05/16 23:14:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224829125155127119586_0004_m_000105_484
[2021-05-16 20:14:07,094] {docker.py:276} INFO - 21/05/16 23:14:07 INFO Executor: Finished task 105.0 in stage 4.0 (TID 484). 4544 bytes result sent to driver
[2021-05-16 20:14:07,095] {docker.py:276} INFO - 21/05/16 23:14:07 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 488) (764b1db4ecfd, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:07,096] {docker.py:276} INFO - 21/05/16 23:14:07 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 484) in 2998 ms on 764b1db4ecfd (executor driver) (106/200)
21/05/16 23:14:07 INFO Executor: Running task 109.0 in stage 4.0 (TID 488)
[2021-05-16 20:14:07,105] {docker.py:276} INFO - 21/05/16 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:07,106] {docker.py:276} INFO - 21/05/16 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:07,109] {docker.py:276} INFO - 21/05/16 23:14:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:07,110] {docker.py:276} INFO - 21/05/16 23:14:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:07,110] {docker.py:276} INFO - 21/05/16 23:14:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:07,110] {docker.py:276} INFO - 21/05/16 23:14:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247538589873812907597_0004_m_000109_488, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247538589873812907597_0004_m_000109_488}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247538589873812907597_0004}; taskId=attempt_202105162312247538589873812907597_0004_m_000109_488, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3767ac67}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:07,111] {docker.py:276} INFO - 21/05/16 23:14:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:07,111] {docker.py:276} INFO - 21/05/16 23:14:07 INFO StagingCommitter: Starting: Task committer attempt_202105162312247538589873812907597_0004_m_000109_488: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247538589873812907597_0004_m_000109_488
[2021-05-16 20:14:07,113] {docker.py:276} INFO - 21/05/16 23:14:07 INFO StagingCommitter: Task committer attempt_202105162312247538589873812907597_0004_m_000109_488: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247538589873812907597_0004_m_000109_488 : duration 0:00.002s
[2021-05-16 20:14:08,360] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Starting: Task committer attempt_20210516231224292420089300427829_0004_m_000107_486: needsTaskCommit() Task attempt_20210516231224292420089300427829_0004_m_000107_486
[2021-05-16 20:14:08,361] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Task committer attempt_20210516231224292420089300427829_0004_m_000107_486: needsTaskCommit() Task attempt_20210516231224292420089300427829_0004_m_000107_486: duration 0:00.001s
21/05/16 23:14:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224292420089300427829_0004_m_000107_486
[2021-05-16 20:14:08,362] {docker.py:276} INFO - 21/05/16 23:14:08 INFO Executor: Finished task 107.0 in stage 4.0 (TID 486). 4544 bytes result sent to driver
[2021-05-16 20:14:08,365] {docker.py:276} INFO - 21/05/16 23:14:08 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 489) (764b1db4ecfd, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:08 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 486) in 2683 ms on 764b1db4ecfd (executor driver) (107/200)
21/05/16 23:14:08 INFO Executor: Running task 110.0 in stage 4.0 (TID 489)
[2021-05-16 20:14:08,375] {docker.py:276} INFO - 21/05/16 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:08,377] {docker.py:276} INFO - 21/05/16 23:14:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248079059370185574505_0004_m_000110_489, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248079059370185574505_0004_m_000110_489}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248079059370185574505_0004}; taskId=attempt_202105162312248079059370185574505_0004_m_000110_489, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@23690570}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312248079059370185574505_0004_m_000110_489: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248079059370185574505_0004_m_000110_489
[2021-05-16 20:14:08,379] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Task committer attempt_202105162312248079059370185574505_0004_m_000110_489: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248079059370185574505_0004_m_000110_489 : duration 0:00.003s
[2021-05-16 20:14:08,642] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312241688975498447198037_0004_m_000108_487: needsTaskCommit() Task attempt_202105162312241688975498447198037_0004_m_000108_487
[2021-05-16 20:14:08,643] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Task committer attempt_202105162312241688975498447198037_0004_m_000108_487: needsTaskCommit() Task attempt_202105162312241688975498447198037_0004_m_000108_487: duration 0:00.000s
21/05/16 23:14:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241688975498447198037_0004_m_000108_487
[2021-05-16 20:14:08,644] {docker.py:276} INFO - 21/05/16 23:14:08 INFO Executor: Finished task 108.0 in stage 4.0 (TID 487). 4544 bytes result sent to driver
[2021-05-16 20:14:08,645] {docker.py:276} INFO - 21/05/16 23:14:08 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 490) (764b1db4ecfd, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:08,646] {docker.py:276} INFO - 21/05/16 23:14:08 INFO Executor: Running task 111.0 in stage 4.0 (TID 490)
21/05/16 23:14:08 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 487) in 2461 ms on 764b1db4ecfd (executor driver) (108/200)
[2021-05-16 20:14:08,656] {docker.py:276} INFO - 21/05/16 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:08,658] {docker.py:276} INFO - 21/05/16 23:14:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246162802823893117559_0004_m_000111_490, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246162802823893117559_0004_m_000111_490}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246162802823893117559_0004}; taskId=attempt_202105162312246162802823893117559_0004_m_000111_490, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66ca72fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:08 INFO StagingCommitter: Starting: Task committer attempt_202105162312246162802823893117559_0004_m_000111_490: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246162802823893117559_0004_m_000111_490
[2021-05-16 20:14:08,660] {docker.py:276} INFO - 21/05/16 23:14:08 INFO StagingCommitter: Task committer attempt_202105162312246162802823893117559_0004_m_000111_490: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246162802823893117559_0004_m_000111_490 : duration 0:00.002s
[2021-05-16 20:14:09,235] {docker.py:276} INFO - 21/05/16 23:14:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312242500929538711699050_0004_m_000106_485: needsTaskCommit() Task attempt_202105162312242500929538711699050_0004_m_000106_485
[2021-05-16 20:14:09,236] {docker.py:276} INFO - 21/05/16 23:14:09 INFO StagingCommitter: Task committer attempt_202105162312242500929538711699050_0004_m_000106_485: needsTaskCommit() Task attempt_202105162312242500929538711699050_0004_m_000106_485: duration 0:00.001s
21/05/16 23:14:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242500929538711699050_0004_m_000106_485
[2021-05-16 20:14:09,238] {docker.py:276} INFO - 21/05/16 23:14:09 INFO Executor: Finished task 106.0 in stage 4.0 (TID 485). 4587 bytes result sent to driver
[2021-05-16 20:14:09,240] {docker.py:276} INFO - 21/05/16 23:14:09 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 491) (764b1db4ecfd, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:09,241] {docker.py:276} INFO - 21/05/16 23:14:09 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 485) in 3749 ms on 764b1db4ecfd (executor driver) (109/200)
[2021-05-16 20:14:09,242] {docker.py:276} INFO - 21/05/16 23:14:09 INFO Executor: Running task 112.0 in stage 4.0 (TID 491)
[2021-05-16 20:14:09,251] {docker.py:276} INFO - 21/05/16 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:09,253] {docker.py:276} INFO - 21/05/16 23:14:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246115278400226844536_0004_m_000112_491, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246115278400226844536_0004_m_000112_491}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246115278400226844536_0004}; taskId=attempt_202105162312246115278400226844536_0004_m_000112_491, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@11ed4930}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:09,253] {docker.py:276} INFO - 21/05/16 23:14:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312246115278400226844536_0004_m_000112_491: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246115278400226844536_0004_m_000112_491
[2021-05-16 20:14:09,256] {docker.py:276} INFO - 21/05/16 23:14:09 INFO StagingCommitter: Task committer attempt_202105162312246115278400226844536_0004_m_000112_491: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246115278400226844536_0004_m_000112_491 : duration 0:00.003s
[2021-05-16 20:14:10,026] {docker.py:276} INFO - 21/05/16 23:14:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312247538589873812907597_0004_m_000109_488: needsTaskCommit() Task attempt_202105162312247538589873812907597_0004_m_000109_488
[2021-05-16 20:14:10,028] {docker.py:276} INFO - 21/05/16 23:14:10 INFO StagingCommitter: Task committer attempt_202105162312247538589873812907597_0004_m_000109_488: needsTaskCommit() Task attempt_202105162312247538589873812907597_0004_m_000109_488: duration 0:00.000s
21/05/16 23:14:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247538589873812907597_0004_m_000109_488
[2021-05-16 20:14:10,029] {docker.py:276} INFO - 21/05/16 23:14:10 INFO Executor: Finished task 109.0 in stage 4.0 (TID 488). 4587 bytes result sent to driver
[2021-05-16 20:14:10,029] {docker.py:276} INFO - 21/05/16 23:14:10 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 492) (764b1db4ecfd, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:10 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 488) in 2937 ms on 764b1db4ecfd (executor driver) (110/200)
[2021-05-16 20:14:10,030] {docker.py:276} INFO - 21/05/16 23:14:10 INFO Executor: Running task 113.0 in stage 4.0 (TID 492)
[2021-05-16 20:14:10,039] {docker.py:276} INFO - 21/05/16 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:10,041] {docker.py:276} INFO - 21/05/16 23:14:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244218537634616129481_0004_m_000113_492, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244218537634616129481_0004_m_000113_492}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244218537634616129481_0004}; taskId=attempt_202105162312244218537634616129481_0004_m_000113_492, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29f1fa85}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:10,041] {docker.py:276} INFO - 21/05/16 23:14:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312244218537634616129481_0004_m_000113_492: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244218537634616129481_0004_m_000113_492
[2021-05-16 20:14:10,046] {docker.py:276} INFO - 21/05/16 23:14:10 INFO StagingCommitter: Task committer attempt_202105162312244218537634616129481_0004_m_000113_492: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244218537634616129481_0004_m_000113_492 : duration 0:00.005s
[2021-05-16 20:14:11,365] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312248079059370185574505_0004_m_000110_489: needsTaskCommit() Task attempt_202105162312248079059370185574505_0004_m_000110_489
[2021-05-16 20:14:11,366] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312248079059370185574505_0004_m_000110_489: needsTaskCommit() Task attempt_202105162312248079059370185574505_0004_m_000110_489: duration 0:00.000s
21/05/16 23:14:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248079059370185574505_0004_m_000110_489
[2021-05-16 20:14:11,367] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Finished task 110.0 in stage 4.0 (TID 489). 4587 bytes result sent to driver
[2021-05-16 20:14:11,368] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 493) (764b1db4ecfd, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:11,368] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Running task 114.0 in stage 4.0 (TID 493)
[2021-05-16 20:14:11,369] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 489) in 3010 ms on 764b1db4ecfd (executor driver) (111/200)
[2021-05-16 20:14:11,377] {docker.py:276} INFO - 21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:11,380] {docker.py:276} INFO - 21/05/16 23:14:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:11,381] {docker.py:276} INFO - 21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243011062204717560397_0004_m_000114_493, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243011062204717560397_0004_m_000114_493}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243011062204717560397_0004}; taskId=attempt_202105162312243011062204717560397_0004_m_000114_493, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36eae9d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312243011062204717560397_0004_m_000114_493: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243011062204717560397_0004_m_000114_493
[2021-05-16 20:14:11,383] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312243011062204717560397_0004_m_000114_493: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243011062204717560397_0004_m_000114_493 : duration 0:00.003s
[2021-05-16 20:14:11,543] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312246115278400226844536_0004_m_000112_491: needsTaskCommit() Task attempt_202105162312246115278400226844536_0004_m_000112_491
[2021-05-16 20:14:11,544] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312246115278400226844536_0004_m_000112_491: needsTaskCommit() Task attempt_202105162312246115278400226844536_0004_m_000112_491: duration 0:00.001s
21/05/16 23:14:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246115278400226844536_0004_m_000112_491
[2021-05-16 20:14:11,546] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Finished task 112.0 in stage 4.0 (TID 491). 4587 bytes result sent to driver
[2021-05-16 20:14:11,548] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 494) (764b1db4ecfd, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:11,549] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 491) in 2312 ms on 764b1db4ecfd (executor driver) (112/200)
[2021-05-16 20:14:11,549] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Running task 115.0 in stage 4.0 (TID 494)
[2021-05-16 20:14:11,560] {docker.py:276} INFO - 21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:11,561] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312246162802823893117559_0004_m_000111_490: needsTaskCommit() Task attempt_202105162312246162802823893117559_0004_m_000111_490
[2021-05-16 20:14:11,562] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312246162802823893117559_0004_m_000111_490: needsTaskCommit() Task attempt_202105162312246162802823893117559_0004_m_000111_490: duration 0:00.001s
21/05/16 23:14:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246162802823893117559_0004_m_000111_490
[2021-05-16 20:14:11,563] {docker.py:276} INFO - 21/05/16 23:14:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242016153675375800032_0004_m_000115_494, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242016153675375800032_0004_m_000115_494}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242016153675375800032_0004}; taskId=attempt_202105162312242016153675375800032_0004_m_000115_494, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26f7348d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:11,564] {docker.py:276} INFO - 21/05/16 23:14:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:11,564] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312242016153675375800032_0004_m_000115_494: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242016153675375800032_0004_m_000115_494
[2021-05-16 20:14:11,565] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Finished task 111.0 in stage 4.0 (TID 490). 4587 bytes result sent to driver
[2021-05-16 20:14:11,565] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 495) (764b1db4ecfd, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:11,566] {docker.py:276} INFO - 21/05/16 23:14:11 INFO Executor: Running task 116.0 in stage 4.0 (TID 495)
[2021-05-16 20:14:11,566] {docker.py:276} INFO - 21/05/16 23:14:11 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 490) in 2926 ms on 764b1db4ecfd (executor driver) (113/200)
[2021-05-16 20:14:11,570] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312242016153675375800032_0004_m_000115_494: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242016153675375800032_0004_m_000115_494 : duration 0:00.007s
[2021-05-16 20:14:11,575] {docker.py:276} INFO - 21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:11,576] {docker.py:276} INFO - 21/05/16 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:11,577] {docker.py:276} INFO - 21/05/16 23:14:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:11,577] {docker.py:276} INFO - 21/05/16 23:14:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:11,578] {docker.py:276} INFO - 21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:11,578] {docker.py:276} INFO - 21/05/16 23:14:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247270247795252824699_0004_m_000116_495, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247270247795252824699_0004_m_000116_495}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247270247795252824699_0004}; taskId=attempt_202105162312247270247795252824699_0004_m_000116_495, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7815888d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:11,578] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Starting: Task committer attempt_202105162312247270247795252824699_0004_m_000116_495: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247270247795252824699_0004_m_000116_495
[2021-05-16 20:14:11,582] {docker.py:276} INFO - 21/05/16 23:14:11 INFO StagingCommitter: Task committer attempt_202105162312247270247795252824699_0004_m_000116_495: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247270247795252824699_0004_m_000116_495 : duration 0:00.003s
[2021-05-16 20:14:12,865] {docker.py:276} INFO - 21/05/16 23:14:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312244218537634616129481_0004_m_000113_492: needsTaskCommit() Task attempt_202105162312244218537634616129481_0004_m_000113_492
[2021-05-16 20:14:12,865] {docker.py:276} INFO - 21/05/16 23:14:12 INFO StagingCommitter: Task committer attempt_202105162312244218537634616129481_0004_m_000113_492: needsTaskCommit() Task attempt_202105162312244218537634616129481_0004_m_000113_492: duration 0:00.000s
[2021-05-16 20:14:12,866] {docker.py:276} INFO - 21/05/16 23:14:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244218537634616129481_0004_m_000113_492
[2021-05-16 20:14:12,867] {docker.py:276} INFO - 21/05/16 23:14:12 INFO Executor: Finished task 113.0 in stage 4.0 (TID 492). 4544 bytes result sent to driver
[2021-05-16 20:14:12,868] {docker.py:276} INFO - 21/05/16 23:14:12 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 496) (764b1db4ecfd, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:12,869] {docker.py:276} INFO - 21/05/16 23:14:12 INFO Executor: Running task 117.0 in stage 4.0 (TID 496)
21/05/16 23:14:12 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 492) in 2845 ms on 764b1db4ecfd (executor driver) (114/200)
[2021-05-16 20:14:12,877] {docker.py:276} INFO - 21/05/16 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:12,881] {docker.py:276} INFO - 21/05/16 23:14:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:12,881] {docker.py:276} INFO - 21/05/16 23:14:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245122357586179966730_0004_m_000117_496, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245122357586179966730_0004_m_000117_496}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245122357586179966730_0004}; taskId=attempt_202105162312245122357586179966730_0004_m_000117_496, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18e1c22c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:12,882] {docker.py:276} INFO - 21/05/16 23:14:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312245122357586179966730_0004_m_000117_496: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245122357586179966730_0004_m_000117_496
[2021-05-16 20:14:12,885] {docker.py:276} INFO - 21/05/16 23:14:12 INFO StagingCommitter: Task committer attempt_202105162312245122357586179966730_0004_m_000117_496: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245122357586179966730_0004_m_000117_496 : duration 0:00.004s
[2021-05-16 20:14:14,370] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312247270247795252824699_0004_m_000116_495: needsTaskCommit() Task attempt_202105162312247270247795252824699_0004_m_000116_495
[2021-05-16 20:14:14,371] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312247270247795252824699_0004_m_000116_495: needsTaskCommit() Task attempt_202105162312247270247795252824699_0004_m_000116_495: duration 0:00.002s
21/05/16 23:14:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247270247795252824699_0004_m_000116_495
[2021-05-16 20:14:14,373] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Finished task 116.0 in stage 4.0 (TID 495). 4544 bytes result sent to driver
[2021-05-16 20:14:14,374] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 497) (764b1db4ecfd, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:14,376] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Running task 118.0 in stage 4.0 (TID 497)
[2021-05-16 20:14:14,377] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 495) in 2815 ms on 764b1db4ecfd (executor driver) (115/200)
[2021-05-16 20:14:14,386] {docker.py:276} INFO - 21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:14,388] {docker.py:276} INFO - 21/05/16 23:14:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241555575924122361538_0004_m_000118_497, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241555575924122361538_0004_m_000118_497}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241555575924122361538_0004}; taskId=attempt_202105162312241555575924122361538_0004_m_000118_497, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6912763c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312241555575924122361538_0004_m_000118_497: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241555575924122361538_0004_m_000118_497
[2021-05-16 20:14:14,391] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312241555575924122361538_0004_m_000118_497: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241555575924122361538_0004_m_000118_497 : duration 0:00.004s
[2021-05-16 20:14:14,494] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312242016153675375800032_0004_m_000115_494: needsTaskCommit() Task attempt_202105162312242016153675375800032_0004_m_000115_494
[2021-05-16 20:14:14,495] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312242016153675375800032_0004_m_000115_494: needsTaskCommit() Task attempt_202105162312242016153675375800032_0004_m_000115_494: duration 0:00.000s
[2021-05-16 20:14:14,496] {docker.py:276} INFO - 21/05/16 23:14:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242016153675375800032_0004_m_000115_494
[2021-05-16 20:14:14,497] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Finished task 115.0 in stage 4.0 (TID 494). 4544 bytes result sent to driver
[2021-05-16 20:14:14,498] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 498) (764b1db4ecfd, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:14,499] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 494) in 2955 ms on 764b1db4ecfd (executor driver) (116/200)
[2021-05-16 20:14:14,500] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Running task 119.0 in stage 4.0 (TID 498)
[2021-05-16 20:14:14,508] {docker.py:276} INFO - 21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:14,508] {docker.py:276} INFO - 21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:14,510] {docker.py:276} INFO - 21/05/16 23:14:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:14,511] {docker.py:276} INFO - 21/05/16 23:14:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:14,511] {docker.py:276} INFO - 21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:14,511] {docker.py:276} INFO - 21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242208065136242009789_0004_m_000119_498, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242208065136242009789_0004_m_000119_498}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242208065136242009789_0004}; taskId=attempt_202105162312242208065136242009789_0004_m_000119_498, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36829777}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312242208065136242009789_0004_m_000119_498: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242208065136242009789_0004_m_000119_498
[2021-05-16 20:14:14,515] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312242208065136242009789_0004_m_000119_498: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242208065136242009789_0004_m_000119_498 : duration 0:00.004s
[2021-05-16 20:14:14,803] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312243011062204717560397_0004_m_000114_493: needsTaskCommit() Task attempt_202105162312243011062204717560397_0004_m_000114_493
[2021-05-16 20:14:14,803] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312243011062204717560397_0004_m_000114_493: needsTaskCommit() Task attempt_202105162312243011062204717560397_0004_m_000114_493: duration 0:00.001s
21/05/16 23:14:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243011062204717560397_0004_m_000114_493
[2021-05-16 20:14:14,805] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Finished task 114.0 in stage 4.0 (TID 493). 4544 bytes result sent to driver
[2021-05-16 20:14:14,806] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 499) (764b1db4ecfd, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:14,807] {docker.py:276} INFO - 21/05/16 23:14:14 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 493) in 3444 ms on 764b1db4ecfd (executor driver) (117/200)
[2021-05-16 20:14:14,808] {docker.py:276} INFO - 21/05/16 23:14:14 INFO Executor: Running task 120.0 in stage 4.0 (TID 499)
[2021-05-16 20:14:14,817] {docker.py:276} INFO - 21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 6 (25.7 KiB) non-empty blocks including 6 (25.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:14,819] {docker.py:276} INFO - 21/05/16 23:14:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245704219537726176184_0004_m_000120_499, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245704219537726176184_0004_m_000120_499}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245704219537726176184_0004}; taskId=attempt_202105162312245704219537726176184_0004_m_000120_499, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@40b84065}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:14,819] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312245704219537726176184_0004_m_000120_499: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245704219537726176184_0004_m_000120_499
[2021-05-16 20:14:14,822] {docker.py:276} INFO - 21/05/16 23:14:14 INFO StagingCommitter: Task committer attempt_202105162312245704219537726176184_0004_m_000120_499: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245704219537726176184_0004_m_000120_499 : duration 0:00.002s
[2021-05-16 20:14:15,755] {docker.py:276} INFO - 21/05/16 23:14:15 INFO StagingCommitter: Starting: Task committer attempt_202105162312245122357586179966730_0004_m_000117_496: needsTaskCommit() Task attempt_202105162312245122357586179966730_0004_m_000117_496
[2021-05-16 20:14:15,756] {docker.py:276} INFO - 21/05/16 23:14:15 INFO StagingCommitter: Task committer attempt_202105162312245122357586179966730_0004_m_000117_496: needsTaskCommit() Task attempt_202105162312245122357586179966730_0004_m_000117_496: duration 0:00.001s
21/05/16 23:14:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245122357586179966730_0004_m_000117_496
[2021-05-16 20:14:15,758] {docker.py:276} INFO - 21/05/16 23:14:15 INFO Executor: Finished task 117.0 in stage 4.0 (TID 496). 4544 bytes result sent to driver
[2021-05-16 20:14:15,759] {docker.py:276} INFO - 21/05/16 23:14:15 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 500) (764b1db4ecfd, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:15,759] {docker.py:276} INFO - 21/05/16 23:14:15 INFO Executor: Running task 121.0 in stage 4.0 (TID 500)
[2021-05-16 20:14:15,760] {docker.py:276} INFO - 21/05/16 23:14:15 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 496) in 2894 ms on 764b1db4ecfd (executor driver) (118/200)
[2021-05-16 20:14:15,770] {docker.py:276} INFO - 21/05/16 23:14:15 INFO ShuffleBlockFetcherIterator: Getting 6 (27.5 KiB) non-empty blocks including 6 (27.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:15,772] {docker.py:276} INFO - 21/05/16 23:14:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242960986430875418043_0004_m_000121_500, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242960986430875418043_0004_m_000121_500}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242960986430875418043_0004}; taskId=attempt_202105162312242960986430875418043_0004_m_000121_500, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65271e0a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:15 INFO StagingCommitter: Starting: Task committer attempt_202105162312242960986430875418043_0004_m_000121_500: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242960986430875418043_0004_m_000121_500
[2021-05-16 20:14:15,774] {docker.py:276} INFO - 21/05/16 23:14:15 INFO StagingCommitter: Task committer attempt_202105162312242960986430875418043_0004_m_000121_500: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242960986430875418043_0004_m_000121_500 : duration 0:00.002s
[2021-05-16 20:14:17,232] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312242208065136242009789_0004_m_000119_498: needsTaskCommit() Task attempt_202105162312242208065136242009789_0004_m_000119_498
[2021-05-16 20:14:17,233] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312242208065136242009789_0004_m_000119_498: needsTaskCommit() Task attempt_202105162312242208065136242009789_0004_m_000119_498: duration 0:00.000s
21/05/16 23:14:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242208065136242009789_0004_m_000119_498
[2021-05-16 20:14:17,234] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Finished task 119.0 in stage 4.0 (TID 498). 4544 bytes result sent to driver
[2021-05-16 20:14:17,235] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 501) (764b1db4ecfd, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:17,236] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Running task 122.0 in stage 4.0 (TID 501)
[2021-05-16 20:14:17,237] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 498) in 2742 ms on 764b1db4ecfd (executor driver) (119/200)
[2021-05-16 20:14:17,248] {docker.py:276} INFO - 21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:17,249] {docker.py:276} INFO - 21/05/16 23:14:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:17,250] {docker.py:276} INFO - 21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247170579843182532241_0004_m_000122_501, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170579843182532241_0004_m_000122_501}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247170579843182532241_0004}; taskId=attempt_202105162312247170579843182532241_0004_m_000122_501, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b608f5a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312247170579843182532241_0004_m_000122_501: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170579843182532241_0004_m_000122_501
[2021-05-16 20:14:17,254] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312247170579843182532241_0004_m_000122_501: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170579843182532241_0004_m_000122_501 : duration 0:00.004s
[2021-05-16 20:14:17,514] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312245704219537726176184_0004_m_000120_499: needsTaskCommit() Task attempt_202105162312245704219537726176184_0004_m_000120_499
[2021-05-16 20:14:17,515] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312245704219537726176184_0004_m_000120_499: needsTaskCommit() Task attempt_202105162312245704219537726176184_0004_m_000120_499: duration 0:00.000s
21/05/16 23:14:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245704219537726176184_0004_m_000120_499
[2021-05-16 20:14:17,516] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Finished task 120.0 in stage 4.0 (TID 499). 4544 bytes result sent to driver
[2021-05-16 20:14:17,517] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 502) (764b1db4ecfd, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:17,518] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Running task 123.0 in stage 4.0 (TID 502)
[2021-05-16 20:14:17,519] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 499) in 2716 ms on 764b1db4ecfd (executor driver) (120/200)
[2021-05-16 20:14:17,527] {docker.py:276} INFO - 21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:17,529] {docker.py:276} INFO - 21/05/16 23:14:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:17,530] {docker.py:276} INFO - 21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244921790507008443409_0004_m_000123_502, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244921790507008443409_0004_m_000123_502}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244921790507008443409_0004}; taskId=attempt_202105162312244921790507008443409_0004_m_000123_502, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b082896}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312244921790507008443409_0004_m_000123_502: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244921790507008443409_0004_m_000123_502
[2021-05-16 20:14:17,532] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312244921790507008443409_0004_m_000123_502: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244921790507008443409_0004_m_000123_502 : duration 0:00.002s
[2021-05-16 20:14:17,939] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312241555575924122361538_0004_m_000118_497: needsTaskCommit() Task attempt_202105162312241555575924122361538_0004_m_000118_497
[2021-05-16 20:14:17,940] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312241555575924122361538_0004_m_000118_497: needsTaskCommit() Task attempt_202105162312241555575924122361538_0004_m_000118_497: duration 0:00.000s
21/05/16 23:14:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241555575924122361538_0004_m_000118_497
[2021-05-16 20:14:17,942] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Finished task 118.0 in stage 4.0 (TID 497). 4544 bytes result sent to driver
[2021-05-16 20:14:17,944] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 503) (764b1db4ecfd, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:17,945] {docker.py:276} INFO - 21/05/16 23:14:17 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 497) in 3575 ms on 764b1db4ecfd (executor driver) (121/200)
[2021-05-16 20:14:17,946] {docker.py:276} INFO - 21/05/16 23:14:17 INFO Executor: Running task 124.0 in stage 4.0 (TID 503)
[2021-05-16 20:14:17,958] {docker.py:276} INFO - 21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:17,960] {docker.py:276} INFO - 21/05/16 23:14:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:17,961] {docker.py:276} INFO - 21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:17,961] {docker.py:276} INFO - 21/05/16 23:14:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244359874534377390965_0004_m_000124_503, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244359874534377390965_0004_m_000124_503}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244359874534377390965_0004}; taskId=attempt_202105162312244359874534377390965_0004_m_000124_503, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34f21ecb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:17,961] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Starting: Task committer attempt_202105162312244359874534377390965_0004_m_000124_503: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244359874534377390965_0004_m_000124_503
[2021-05-16 20:14:17,964] {docker.py:276} INFO - 21/05/16 23:14:17 INFO StagingCommitter: Task committer attempt_202105162312244359874534377390965_0004_m_000124_503: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244359874534377390965_0004_m_000124_503 : duration 0:00.003s
[2021-05-16 20:14:18,121] {docker.py:276} INFO - 21/05/16 23:14:18 INFO StagingCommitter: Starting: Task committer attempt_202105162312242960986430875418043_0004_m_000121_500: needsTaskCommit() Task attempt_202105162312242960986430875418043_0004_m_000121_500
[2021-05-16 20:14:18,122] {docker.py:276} INFO - 21/05/16 23:14:18 INFO StagingCommitter: Task committer attempt_202105162312242960986430875418043_0004_m_000121_500: needsTaskCommit() Task attempt_202105162312242960986430875418043_0004_m_000121_500: duration 0:00.002s
[2021-05-16 20:14:18,123] {docker.py:276} INFO - 21/05/16 23:14:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242960986430875418043_0004_m_000121_500
[2021-05-16 20:14:18,123] {docker.py:276} INFO - 21/05/16 23:14:18 INFO Executor: Finished task 121.0 in stage 4.0 (TID 500). 4544 bytes result sent to driver
[2021-05-16 20:14:18,126] {docker.py:276} INFO - 21/05/16 23:14:18 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 504) (764b1db4ecfd, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:18,127] {docker.py:276} INFO - 21/05/16 23:14:18 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 500) in 2372 ms on 764b1db4ecfd (executor driver) (122/200)
[2021-05-16 20:14:18,127] {docker.py:276} INFO - 21/05/16 23:14:18 INFO Executor: Running task 125.0 in stage 4.0 (TID 504)
[2021-05-16 20:14:18,138] {docker.py:276} INFO - 21/05/16 23:14:18 INFO ShuffleBlockFetcherIterator: Getting 6 (26.3 KiB) non-empty blocks including 6 (26.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:18,140] {docker.py:276} INFO - 21/05/16 23:14:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245531754437628847083_0004_m_000125_504, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245531754437628847083_0004_m_000125_504}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245531754437628847083_0004}; taskId=attempt_202105162312245531754437628847083_0004_m_000125_504, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4dad3f2c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:18 INFO StagingCommitter: Starting: Task committer attempt_202105162312245531754437628847083_0004_m_000125_504: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245531754437628847083_0004_m_000125_504
[2021-05-16 20:14:18,143] {docker.py:276} INFO - 21/05/16 23:14:18 INFO StagingCommitter: Task committer attempt_202105162312245531754437628847083_0004_m_000125_504: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245531754437628847083_0004_m_000125_504 : duration 0:00.004s
[2021-05-16 20:14:20,028] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_202105162312247170579843182532241_0004_m_000122_501: needsTaskCommit() Task attempt_202105162312247170579843182532241_0004_m_000122_501
21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_202105162312247170579843182532241_0004_m_000122_501: needsTaskCommit() Task attempt_202105162312247170579843182532241_0004_m_000122_501: duration 0:00.000s
21/05/16 23:14:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247170579843182532241_0004_m_000122_501
21/05/16 23:14:20 INFO Executor: Finished task 122.0 in stage 4.0 (TID 501). 4544 bytes result sent to driver
21/05/16 23:14:20 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 505) (764b1db4ecfd, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:20 INFO Executor: Running task 126.0 in stage 4.0 (TID 505)
21/05/16 23:14:20 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 501) in 2796 ms on 764b1db4ecfd (executor driver) (123/200)
[2021-05-16 20:14:20,036] {docker.py:276} INFO - 21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:20,043] {docker.py:276} INFO - 21/05/16 23:14:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:20,044] {docker.py:276} INFO - 21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243606544196207058129_0004_m_000126_505, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243606544196207058129_0004_m_000126_505}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243606544196207058129_0004}; taskId=attempt_202105162312243606544196207058129_0004_m_000126_505, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33b56214}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:20,044] {docker.py:276} INFO - 21/05/16 23:14:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:20,046] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_202105162312243606544196207058129_0004_m_000126_505: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243606544196207058129_0004_m_000126_505
[2021-05-16 20:14:20,048] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_202105162312243606544196207058129_0004_m_000126_505: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243606544196207058129_0004_m_000126_505 : duration 0:00.004s
[2021-05-16 20:14:20,244] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_202105162312244921790507008443409_0004_m_000123_502: needsTaskCommit() Task attempt_202105162312244921790507008443409_0004_m_000123_502
[2021-05-16 20:14:20,244] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_202105162312244921790507008443409_0004_m_000123_502: needsTaskCommit() Task attempt_202105162312244921790507008443409_0004_m_000123_502: duration 0:00.000s
21/05/16 23:14:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244921790507008443409_0004_m_000123_502
[2021-05-16 20:14:20,244] {docker.py:276} INFO - 21/05/16 23:14:20 INFO Executor: Finished task 123.0 in stage 4.0 (TID 502). 4544 bytes result sent to driver
[2021-05-16 20:14:20,245] {docker.py:276} INFO - 21/05/16 23:14:20 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 506) (764b1db4ecfd, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:20,246] {docker.py:276} INFO - 21/05/16 23:14:20 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 502) in 2732 ms on 764b1db4ecfd (executor driver) (124/200)
[2021-05-16 20:14:20,246] {docker.py:276} INFO - 21/05/16 23:14:20 INFO Executor: Running task 127.0 in stage 4.0 (TID 506)
[2021-05-16 20:14:20,255] {docker.py:276} INFO - 21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:20,258] {docker.py:276} INFO - 21/05/16 23:14:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:20,259] {docker.py:276} INFO - 21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224575862286700606420_0004_m_000127_506, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224575862286700606420_0004_m_000127_506}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224575862286700606420_0004}; taskId=attempt_20210516231224575862286700606420_0004_m_000127_506, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55851ec4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:20,259] {docker.py:276} INFO - 21/05/16 23:14:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_20210516231224575862286700606420_0004_m_000127_506: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224575862286700606420_0004_m_000127_506
[2021-05-16 20:14:20,262] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_20210516231224575862286700606420_0004_m_000127_506: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224575862286700606420_0004_m_000127_506 : duration 0:00.003s
[2021-05-16 20:14:20,799] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_202105162312245531754437628847083_0004_m_000125_504: needsTaskCommit() Task attempt_202105162312245531754437628847083_0004_m_000125_504
[2021-05-16 20:14:20,800] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_202105162312245531754437628847083_0004_m_000125_504: needsTaskCommit() Task attempt_202105162312245531754437628847083_0004_m_000125_504: duration 0:00.000s
21/05/16 23:14:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245531754437628847083_0004_m_000125_504
[2021-05-16 20:14:20,802] {docker.py:276} INFO - 21/05/16 23:14:20 INFO Executor: Finished task 125.0 in stage 4.0 (TID 504). 4544 bytes result sent to driver
[2021-05-16 20:14:20,802] {docker.py:276} INFO - 21/05/16 23:14:20 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 507) (764b1db4ecfd, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:20,804] {docker.py:276} INFO - 21/05/16 23:14:20 INFO Executor: Running task 128.0 in stage 4.0 (TID 507)
[2021-05-16 20:14:20,804] {docker.py:276} INFO - 21/05/16 23:14:20 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 504) in 2683 ms on 764b1db4ecfd (executor driver) (125/200)
[2021-05-16 20:14:20,815] {docker.py:276} INFO - 21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:20,817] {docker.py:276} INFO - 21/05/16 23:14:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:20,817] {docker.py:276} INFO - 21/05/16 23:14:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246849755494125664470_0004_m_000128_507, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246849755494125664470_0004_m_000128_507}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246849755494125664470_0004}; taskId=attempt_202105162312246849755494125664470_0004_m_000128_507, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@171fcc70}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:20,817] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Starting: Task committer attempt_202105162312246849755494125664470_0004_m_000128_507: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246849755494125664470_0004_m_000128_507
[2021-05-16 20:14:20,821] {docker.py:276} INFO - 21/05/16 23:14:20 INFO StagingCommitter: Task committer attempt_202105162312246849755494125664470_0004_m_000128_507: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246849755494125664470_0004_m_000128_507 : duration 0:00.004s
[2021-05-16 20:14:21,310] {docker.py:276} INFO - 21/05/16 23:14:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312244359874534377390965_0004_m_000124_503: needsTaskCommit() Task attempt_202105162312244359874534377390965_0004_m_000124_503
[2021-05-16 20:14:21,310] {docker.py:276} INFO - 21/05/16 23:14:21 INFO StagingCommitter: Task committer attempt_202105162312244359874534377390965_0004_m_000124_503: needsTaskCommit() Task attempt_202105162312244359874534377390965_0004_m_000124_503: duration 0:00.001s
[2021-05-16 20:14:21,311] {docker.py:276} INFO - 21/05/16 23:14:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244359874534377390965_0004_m_000124_503
[2021-05-16 20:14:21,312] {docker.py:276} INFO - 21/05/16 23:14:21 INFO Executor: Finished task 124.0 in stage 4.0 (TID 503). 4544 bytes result sent to driver
[2021-05-16 20:14:21,314] {docker.py:276} INFO - 21/05/16 23:14:21 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 508) (764b1db4ecfd, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:21,315] {docker.py:276} INFO - 21/05/16 23:14:21 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 503) in 3377 ms on 764b1db4ecfd (executor driver) (126/200)
[2021-05-16 20:14:21,320] {docker.py:276} INFO - 21/05/16 23:14:21 INFO Executor: Running task 129.0 in stage 4.0 (TID 508)
[2021-05-16 20:14:21,330] {docker.py:276} INFO - 21/05/16 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:21,331] {docker.py:276} INFO - 21/05/16 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:14:21,335] {docker.py:276} INFO - 21/05/16 23:14:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:21,344] {docker.py:276} INFO - 21/05/16 23:14:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246865514254646856045_0004_m_000129_508, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246865514254646856045_0004_m_000129_508}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246865514254646856045_0004}; taskId=attempt_202105162312246865514254646856045_0004_m_000129_508, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ded051c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:21,344] {docker.py:276} INFO - 21/05/16 23:14:21 INFO StagingCommitter: Starting: Task committer attempt_202105162312246865514254646856045_0004_m_000129_508: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246865514254646856045_0004_m_000129_508
[2021-05-16 20:14:21,351] {docker.py:276} INFO - 21/05/16 23:14:21 INFO StagingCommitter: Task committer attempt_202105162312246865514254646856045_0004_m_000129_508: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246865514254646856045_0004_m_000129_508 : duration 0:00.009s
[2021-05-16 20:14:22,730] {docker.py:276} INFO - 21/05/16 23:14:22 INFO StagingCommitter: Starting: Task committer attempt_202105162312243606544196207058129_0004_m_000126_505: needsTaskCommit() Task attempt_202105162312243606544196207058129_0004_m_000126_505
[2021-05-16 20:14:22,731] {docker.py:276} INFO - 21/05/16 23:14:22 INFO StagingCommitter: Task committer attempt_202105162312243606544196207058129_0004_m_000126_505: needsTaskCommit() Task attempt_202105162312243606544196207058129_0004_m_000126_505: duration 0:00.000s
21/05/16 23:14:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243606544196207058129_0004_m_000126_505
[2021-05-16 20:14:22,733] {docker.py:276} INFO - 21/05/16 23:14:22 INFO Executor: Finished task 126.0 in stage 4.0 (TID 505). 4544 bytes result sent to driver
[2021-05-16 20:14:22,736] {docker.py:276} INFO - 21/05/16 23:14:22 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 509) (764b1db4ecfd, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:22,736] {docker.py:276} INFO - 21/05/16 23:14:22 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 505) in 2712 ms on 764b1db4ecfd (executor driver) (127/200)
[2021-05-16 20:14:22,736] {docker.py:276} INFO - 21/05/16 23:14:22 INFO Executor: Running task 130.0 in stage 4.0 (TID 509)
[2021-05-16 20:14:22,744] {docker.py:276} INFO - 21/05/16 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:22,745] {docker.py:276} INFO - 21/05/16 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:22,748] {docker.py:276} INFO - 21/05/16 23:14:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:22,748] {docker.py:276} INFO - 21/05/16 23:14:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:22,749] {docker.py:276} INFO - 21/05/16 23:14:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241301457733846910610_0004_m_000130_509, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241301457733846910610_0004_m_000130_509}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241301457733846910610_0004}; taskId=attempt_202105162312241301457733846910610_0004_m_000130_509, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6887015b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:22,749] {docker.py:276} INFO - 21/05/16 23:14:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:22,749] {docker.py:276} INFO - 21/05/16 23:14:22 INFO StagingCommitter: Starting: Task committer attempt_202105162312241301457733846910610_0004_m_000130_509: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241301457733846910610_0004_m_000130_509
[2021-05-16 20:14:22,753] {docker.py:276} INFO - 21/05/16 23:14:22 INFO StagingCommitter: Task committer attempt_202105162312241301457733846910610_0004_m_000130_509: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241301457733846910610_0004_m_000130_509 : duration 0:00.004s
[2021-05-16 20:14:23,052] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Starting: Task committer attempt_20210516231224575862286700606420_0004_m_000127_506: needsTaskCommit() Task attempt_20210516231224575862286700606420_0004_m_000127_506
[2021-05-16 20:14:23,053] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Task committer attempt_20210516231224575862286700606420_0004_m_000127_506: needsTaskCommit() Task attempt_20210516231224575862286700606420_0004_m_000127_506: duration 0:00.001s
[2021-05-16 20:14:23,053] {docker.py:276} INFO - 21/05/16 23:14:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224575862286700606420_0004_m_000127_506
[2021-05-16 20:14:23,054] {docker.py:276} INFO - 21/05/16 23:14:23 INFO Executor: Finished task 127.0 in stage 4.0 (TID 506). 4544 bytes result sent to driver
[2021-05-16 20:14:23,056] {docker.py:276} INFO - 21/05/16 23:14:23 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 510) (764b1db4ecfd, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:23,057] {docker.py:276} INFO - 21/05/16 23:14:23 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 506) in 2815 ms on 764b1db4ecfd (executor driver) (128/200)
[2021-05-16 20:14:23,058] {docker.py:276} INFO - 21/05/16 23:14:23 INFO Executor: Running task 131.0 in stage 4.0 (TID 510)
[2021-05-16 20:14:23,066] {docker.py:276} INFO - 21/05/16 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:23,068] {docker.py:276} INFO - 21/05/16 23:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246316069488380870325_0004_m_000131_510, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246316069488380870325_0004_m_000131_510}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246316069488380870325_0004}; taskId=attempt_202105162312246316069488380870325_0004_m_000131_510, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66c8ea59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:23 INFO StagingCommitter: Starting: Task committer attempt_202105162312246316069488380870325_0004_m_000131_510: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246316069488380870325_0004_m_000131_510
[2021-05-16 20:14:23,072] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Task committer attempt_202105162312246316069488380870325_0004_m_000131_510: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246316069488380870325_0004_m_000131_510 : duration 0:00.004s
[2021-05-16 20:14:23,547] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Starting: Task committer attempt_202105162312246849755494125664470_0004_m_000128_507: needsTaskCommit() Task attempt_202105162312246849755494125664470_0004_m_000128_507
[2021-05-16 20:14:23,548] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Task committer attempt_202105162312246849755494125664470_0004_m_000128_507: needsTaskCommit() Task attempt_202105162312246849755494125664470_0004_m_000128_507: duration 0:00.001s
21/05/16 23:14:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246849755494125664470_0004_m_000128_507
[2021-05-16 20:14:23,548] {docker.py:276} INFO - 21/05/16 23:14:23 INFO Executor: Finished task 128.0 in stage 4.0 (TID 507). 4544 bytes result sent to driver
[2021-05-16 20:14:23,551] {docker.py:276} INFO - 21/05/16 23:14:23 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 511) (764b1db4ecfd, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:23,553] {docker.py:276} INFO - 21/05/16 23:14:23 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 507) in 2754 ms on 764b1db4ecfd (executor driver) (129/200)
[2021-05-16 20:14:23,555] {docker.py:276} INFO - 21/05/16 23:14:23 INFO Executor: Running task 132.0 in stage 4.0 (TID 511)
[2021-05-16 20:14:23,564] {docker.py:276} INFO - 21/05/16 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:23,566] {docker.py:276} INFO - 21/05/16 23:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247818103174270858370_0004_m_000132_511, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247818103174270858370_0004_m_000132_511}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247818103174270858370_0004}; taskId=attempt_202105162312247818103174270858370_0004_m_000132_511, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3caf23ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:23,567] {docker.py:276} INFO - 21/05/16 23:14:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:23 INFO StagingCommitter: Starting: Task committer attempt_202105162312247818103174270858370_0004_m_000132_511: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247818103174270858370_0004_m_000132_511
[2021-05-16 20:14:23,569] {docker.py:276} INFO - 21/05/16 23:14:23 INFO StagingCommitter: Task committer attempt_202105162312247818103174270858370_0004_m_000132_511: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247818103174270858370_0004_m_000132_511 : duration 0:00.004s
[2021-05-16 20:14:24,292] {docker.py:276} INFO - 21/05/16 23:14:24 INFO StagingCommitter: Starting: Task committer attempt_202105162312246865514254646856045_0004_m_000129_508: needsTaskCommit() Task attempt_202105162312246865514254646856045_0004_m_000129_508
21/05/16 23:14:24 INFO StagingCommitter: Task committer attempt_202105162312246865514254646856045_0004_m_000129_508: needsTaskCommit() Task attempt_202105162312246865514254646856045_0004_m_000129_508: duration 0:00.000s
[2021-05-16 20:14:24,293] {docker.py:276} INFO - 21/05/16 23:14:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246865514254646856045_0004_m_000129_508
[2021-05-16 20:14:24,295] {docker.py:276} INFO - 21/05/16 23:14:24 INFO Executor: Finished task 129.0 in stage 4.0 (TID 508). 4544 bytes result sent to driver
[2021-05-16 20:14:24,296] {docker.py:276} INFO - 21/05/16 23:14:24 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 512) (764b1db4ecfd, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:24,298] {docker.py:276} INFO - 21/05/16 23:14:24 INFO Executor: Running task 133.0 in stage 4.0 (TID 512)
[2021-05-16 20:14:24,299] {docker.py:276} INFO - 21/05/16 23:14:24 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 508) in 2988 ms on 764b1db4ecfd (executor driver) (130/200)
[2021-05-16 20:14:24,309] {docker.py:276} INFO - 21/05/16 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:24,311] {docker.py:276} INFO - 21/05/16 23:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:24,311] {docker.py:276} INFO - 21/05/16 23:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:24,312] {docker.py:276} INFO - 21/05/16 23:14:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:24,312] {docker.py:276} INFO - 21/05/16 23:14:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224184839187984610678_0004_m_000133_512, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224184839187984610678_0004_m_000133_512}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224184839187984610678_0004}; taskId=attempt_20210516231224184839187984610678_0004_m_000133_512, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ffa3205}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:24,313] {docker.py:276} INFO - 21/05/16 23:14:24 INFO StagingCommitter: Starting: Task committer attempt_20210516231224184839187984610678_0004_m_000133_512: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224184839187984610678_0004_m_000133_512
[2021-05-16 20:14:24,317] {docker.py:276} INFO - 21/05/16 23:14:24 INFO StagingCommitter: Task committer attempt_20210516231224184839187984610678_0004_m_000133_512: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224184839187984610678_0004_m_000133_512 : duration 0:00.005s
[2021-05-16 20:14:25,449] {docker.py:276} INFO - 21/05/16 23:14:25 INFO StagingCommitter: Starting: Task committer attempt_202105162312241301457733846910610_0004_m_000130_509: needsTaskCommit() Task attempt_202105162312241301457733846910610_0004_m_000130_509
[2021-05-16 20:14:25,451] {docker.py:276} INFO - 21/05/16 23:14:25 INFO StagingCommitter: Task committer attempt_202105162312241301457733846910610_0004_m_000130_509: needsTaskCommit() Task attempt_202105162312241301457733846910610_0004_m_000130_509: duration 0:00.001s
[2021-05-16 20:14:25,451] {docker.py:276} INFO - 21/05/16 23:14:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241301457733846910610_0004_m_000130_509
[2021-05-16 20:14:25,453] {docker.py:276} INFO - 21/05/16 23:14:25 INFO Executor: Finished task 130.0 in stage 4.0 (TID 509). 4544 bytes result sent to driver
[2021-05-16 20:14:25,455] {docker.py:276} INFO - 21/05/16 23:14:25 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 513) (764b1db4ecfd, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:25,455] {docker.py:276} INFO - 21/05/16 23:14:25 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 509) in 2726 ms on 764b1db4ecfd (executor driver) (131/200)
[2021-05-16 20:14:25,457] {docker.py:276} INFO - 21/05/16 23:14:25 INFO Executor: Running task 134.0 in stage 4.0 (TID 513)
[2021-05-16 20:14:25,466] {docker.py:276} INFO - 21/05/16 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:25,468] {docker.py:276} INFO - 21/05/16 23:14:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224236469049427318426_0004_m_000134_513, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224236469049427318426_0004_m_000134_513}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224236469049427318426_0004}; taskId=attempt_20210516231224236469049427318426_0004_m_000134_513, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a93d02b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:25,490] {docker.py:276} INFO - 21/05/16 23:14:25 INFO StagingCommitter: Starting: Task committer attempt_20210516231224236469049427318426_0004_m_000134_513: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224236469049427318426_0004_m_000134_513
[2021-05-16 20:14:25,490] {docker.py:276} INFO - 21/05/16 23:14:25 INFO StagingCommitter: Task committer attempt_20210516231224236469049427318426_0004_m_000134_513: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224236469049427318426_0004_m_000134_513 : duration 0:00.003s
[2021-05-16 20:14:25,988] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_202105162312246316069488380870325_0004_m_000131_510: needsTaskCommit() Task attempt_202105162312246316069488380870325_0004_m_000131_510
[2021-05-16 20:14:25,989] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_202105162312246316069488380870325_0004_m_000131_510: needsTaskCommit() Task attempt_202105162312246316069488380870325_0004_m_000131_510: duration 0:00.001s
21/05/16 23:14:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246316069488380870325_0004_m_000131_510
[2021-05-16 20:14:25,991] {docker.py:276} INFO - 21/05/16 23:14:26 INFO Executor: Finished task 131.0 in stage 4.0 (TID 510). 4544 bytes result sent to driver
[2021-05-16 20:14:25,992] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 514) (764b1db4ecfd, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:25,993] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 510) in 2941 ms on 764b1db4ecfd (executor driver) (132/200)
[2021-05-16 20:14:25,994] {docker.py:276} INFO - 21/05/16 23:14:26 INFO Executor: Running task 135.0 in stage 4.0 (TID 514)
[2021-05-16 20:14:26,003] {docker.py:276} INFO - 21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 6 (25.5 KiB) non-empty blocks including 6 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:26,005] {docker.py:276} INFO - 21/05/16 23:14:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:26,006] {docker.py:276} INFO - 21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247749012604356158455_0004_m_000135_514, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247749012604356158455_0004_m_000135_514}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247749012604356158455_0004}; taskId=attempt_202105162312247749012604356158455_0004_m_000135_514, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64d20afb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:26,006] {docker.py:276} INFO - 21/05/16 23:14:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:26,006] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_202105162312247749012604356158455_0004_m_000135_514: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247749012604356158455_0004_m_000135_514
[2021-05-16 20:14:26,009] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_202105162312247749012604356158455_0004_m_000135_514: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247749012604356158455_0004_m_000135_514 : duration 0:00.003s
[2021-05-16 20:14:26,398] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_202105162312247818103174270858370_0004_m_000132_511: needsTaskCommit() Task attempt_202105162312247818103174270858370_0004_m_000132_511
[2021-05-16 20:14:26,399] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_202105162312247818103174270858370_0004_m_000132_511: needsTaskCommit() Task attempt_202105162312247818103174270858370_0004_m_000132_511: duration 0:00.002s
21/05/16 23:14:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247818103174270858370_0004_m_000132_511
[2021-05-16 20:14:26,401] {docker.py:276} INFO - 21/05/16 23:14:26 INFO Executor: Finished task 132.0 in stage 4.0 (TID 511). 4544 bytes result sent to driver
[2021-05-16 20:14:26,402] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 515) (764b1db4ecfd, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:26,404] {docker.py:276} INFO - 21/05/16 23:14:26 INFO Executor: Running task 136.0 in stage 4.0 (TID 515)
[2021-05-16 20:14:26,404] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 511) in 2858 ms on 764b1db4ecfd (executor driver) (133/200)
[2021-05-16 20:14:26,414] {docker.py:276} INFO - 21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:26,415] {docker.py:276} INFO - 21/05/16 23:14:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246083614133510970091_0004_m_000136_515, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246083614133510970091_0004_m_000136_515}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246083614133510970091_0004}; taskId=attempt_202105162312246083614133510970091_0004_m_000136_515, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27eacb5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:26,416] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_202105162312246083614133510970091_0004_m_000136_515: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246083614133510970091_0004_m_000136_515
[2021-05-16 20:14:26,418] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_202105162312246083614133510970091_0004_m_000136_515: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246083614133510970091_0004_m_000136_515 : duration 0:00.003s
[2021-05-16 20:14:26,575] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_20210516231224184839187984610678_0004_m_000133_512: needsTaskCommit() Task attempt_20210516231224184839187984610678_0004_m_000133_512
21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_20210516231224184839187984610678_0004_m_000133_512: needsTaskCommit() Task attempt_20210516231224184839187984610678_0004_m_000133_512: duration 0:00.001s
21/05/16 23:14:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224184839187984610678_0004_m_000133_512
[2021-05-16 20:14:26,578] {docker.py:276} INFO - 21/05/16 23:14:26 INFO Executor: Finished task 133.0 in stage 4.0 (TID 512). 4544 bytes result sent to driver
[2021-05-16 20:14:26,579] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 516) (764b1db4ecfd, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:26,580] {docker.py:276} INFO - 21/05/16 23:14:26 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 512) in 2288 ms on 764b1db4ecfd (executor driver) (134/200)
21/05/16 23:14:26 INFO Executor: Running task 137.0 in stage 4.0 (TID 516)
[2021-05-16 20:14:26,590] {docker.py:276} INFO - 21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:26,592] {docker.py:276} INFO - 21/05/16 23:14:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244556011332823874803_0004_m_000137_516, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244556011332823874803_0004_m_000137_516}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244556011332823874803_0004}; taskId=attempt_202105162312244556011332823874803_0004_m_000137_516, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79c3c1e4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:26 INFO StagingCommitter: Starting: Task committer attempt_202105162312244556011332823874803_0004_m_000137_516: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244556011332823874803_0004_m_000137_516
[2021-05-16 20:14:26,595] {docker.py:276} INFO - 21/05/16 23:14:26 INFO StagingCommitter: Task committer attempt_202105162312244556011332823874803_0004_m_000137_516: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244556011332823874803_0004_m_000137_516 : duration 0:00.003s
[2021-05-16 20:14:28,398] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Starting: Task committer attempt_20210516231224236469049427318426_0004_m_000134_513: needsTaskCommit() Task attempt_20210516231224236469049427318426_0004_m_000134_513
[2021-05-16 20:14:28,399] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Task committer attempt_20210516231224236469049427318426_0004_m_000134_513: needsTaskCommit() Task attempt_20210516231224236469049427318426_0004_m_000134_513: duration 0:00.001s
21/05/16 23:14:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224236469049427318426_0004_m_000134_513
[2021-05-16 20:14:28,401] {docker.py:276} INFO - 21/05/16 23:14:28 INFO Executor: Finished task 134.0 in stage 4.0 (TID 513). 4587 bytes result sent to driver
[2021-05-16 20:14:28,402] {docker.py:276} INFO - 21/05/16 23:14:28 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 517) (764b1db4ecfd, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:28,403] {docker.py:276} INFO - 21/05/16 23:14:28 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 513) in 2952 ms on 764b1db4ecfd (executor driver) (135/200)
[2021-05-16 20:14:28,404] {docker.py:276} INFO - 21/05/16 23:14:28 INFO Executor: Running task 138.0 in stage 4.0 (TID 517)
[2021-05-16 20:14:28,413] {docker.py:276} INFO - 21/05/16 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:28,416] {docker.py:276} INFO - 21/05/16 23:14:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244275654028462211827_0004_m_000138_517, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244275654028462211827_0004_m_000138_517}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244275654028462211827_0004}; taskId=attempt_202105162312244275654028462211827_0004_m_000138_517, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f8d6150}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:28,417] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Starting: Task committer attempt_202105162312244275654028462211827_0004_m_000138_517: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244275654028462211827_0004_m_000138_517
[2021-05-16 20:14:28,419] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Task committer attempt_202105162312244275654028462211827_0004_m_000138_517: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244275654028462211827_0004_m_000138_517 : duration 0:00.005s
[2021-05-16 20:14:28,875] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Starting: Task committer attempt_202105162312247749012604356158455_0004_m_000135_514: needsTaskCommit() Task attempt_202105162312247749012604356158455_0004_m_000135_514
[2021-05-16 20:14:28,876] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Task committer attempt_202105162312247749012604356158455_0004_m_000135_514: needsTaskCommit() Task attempt_202105162312247749012604356158455_0004_m_000135_514: duration 0:00.000s
[2021-05-16 20:14:28,877] {docker.py:276} INFO - 21/05/16 23:14:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247749012604356158455_0004_m_000135_514
[2021-05-16 20:14:28,881] {docker.py:276} INFO - 21/05/16 23:14:28 INFO Executor: Finished task 135.0 in stage 4.0 (TID 514). 4587 bytes result sent to driver
[2021-05-16 20:14:28,882] {docker.py:276} INFO - 21/05/16 23:14:28 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 518) (764b1db4ecfd, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:28,883] {docker.py:276} INFO - 21/05/16 23:14:28 INFO Executor: Running task 139.0 in stage 4.0 (TID 518)
[2021-05-16 20:14:28,883] {docker.py:276} INFO - 21/05/16 23:14:28 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 514) in 2895 ms on 764b1db4ecfd (executor driver) (136/200)
[2021-05-16 20:14:28,893] {docker.py:276} INFO - 21/05/16 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:28,895] {docker.py:276} INFO - 21/05/16 23:14:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:28,896] {docker.py:276} INFO - 21/05/16 23:14:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312249062582932096488316_0004_m_000139_518, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249062582932096488316_0004_m_000139_518}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312249062582932096488316_0004}; taskId=attempt_202105162312249062582932096488316_0004_m_000139_518, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bd90804}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:28 INFO StagingCommitter: Starting: Task committer attempt_202105162312249062582932096488316_0004_m_000139_518: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249062582932096488316_0004_m_000139_518
[2021-05-16 20:14:28,899] {docker.py:276} INFO - 21/05/16 23:14:28 INFO StagingCommitter: Task committer attempt_202105162312249062582932096488316_0004_m_000139_518: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312249062582932096488316_0004_m_000139_518 : duration 0:00.003s
[2021-05-16 20:14:29,382] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Starting: Task committer attempt_202105162312244556011332823874803_0004_m_000137_516: needsTaskCommit() Task attempt_202105162312244556011332823874803_0004_m_000137_516
[2021-05-16 20:14:29,382] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Task committer attempt_202105162312244556011332823874803_0004_m_000137_516: needsTaskCommit() Task attempt_202105162312244556011332823874803_0004_m_000137_516: duration 0:00.000s
21/05/16 23:14:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244556011332823874803_0004_m_000137_516
[2021-05-16 20:14:29,384] {docker.py:276} INFO - 21/05/16 23:14:29 INFO Executor: Finished task 137.0 in stage 4.0 (TID 516). 4587 bytes result sent to driver
[2021-05-16 20:14:29,385] {docker.py:276} INFO - 21/05/16 23:14:29 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 519) (764b1db4ecfd, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:29,386] {docker.py:276} INFO - 21/05/16 23:14:29 INFO Executor: Running task 140.0 in stage 4.0 (TID 519)
[2021-05-16 20:14:29,387] {docker.py:276} INFO - 21/05/16 23:14:29 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 516) in 2812 ms on 764b1db4ecfd (executor driver) (137/200)
[2021-05-16 20:14:29,397] {docker.py:276} INFO - 21/05/16 23:14:29 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:29,399] {docker.py:276} INFO - 21/05/16 23:14:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:29,400] {docker.py:276} INFO - 21/05/16 23:14:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242365068708763813260_0004_m_000140_519, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242365068708763813260_0004_m_000140_519}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242365068708763813260_0004}; taskId=attempt_202105162312242365068708763813260_0004_m_000140_519, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@50d31704}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:29 INFO StagingCommitter: Starting: Task committer attempt_202105162312242365068708763813260_0004_m_000140_519: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242365068708763813260_0004_m_000140_519
[2021-05-16 20:14:29,402] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Task committer attempt_202105162312242365068708763813260_0004_m_000140_519: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242365068708763813260_0004_m_000140_519 : duration 0:00.003s
[2021-05-16 20:14:29,677] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Starting: Task committer attempt_202105162312246083614133510970091_0004_m_000136_515: needsTaskCommit() Task attempt_202105162312246083614133510970091_0004_m_000136_515
[2021-05-16 20:14:29,678] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Task committer attempt_202105162312246083614133510970091_0004_m_000136_515: needsTaskCommit() Task attempt_202105162312246083614133510970091_0004_m_000136_515: duration 0:00.001s
21/05/16 23:14:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246083614133510970091_0004_m_000136_515
[2021-05-16 20:14:29,679] {docker.py:276} INFO - 21/05/16 23:14:29 INFO Executor: Finished task 136.0 in stage 4.0 (TID 515). 4587 bytes result sent to driver
[2021-05-16 20:14:29,680] {docker.py:276} INFO - 21/05/16 23:14:29 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 520) (764b1db4ecfd, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:29,681] {docker.py:276} INFO - 21/05/16 23:14:29 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 515) in 3282 ms on 764b1db4ecfd (executor driver) (138/200)
21/05/16 23:14:29 INFO Executor: Running task 141.0 in stage 4.0 (TID 520)
[2021-05-16 20:14:29,691] {docker.py:276} INFO - 21/05/16 23:14:29 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:29,693] {docker.py:276} INFO - 21/05/16 23:14:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245278812596561431308_0004_m_000141_520, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245278812596561431308_0004_m_000141_520}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245278812596561431308_0004}; taskId=attempt_202105162312245278812596561431308_0004_m_000141_520, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b506ea0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:29,693] {docker.py:276} INFO - 21/05/16 23:14:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:29 INFO StagingCommitter: Starting: Task committer attempt_202105162312245278812596561431308_0004_m_000141_520: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245278812596561431308_0004_m_000141_520
[2021-05-16 20:14:29,696] {docker.py:276} INFO - 21/05/16 23:14:29 INFO StagingCommitter: Task committer attempt_202105162312245278812596561431308_0004_m_000141_520: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245278812596561431308_0004_m_000141_520 : duration 0:00.003s
[2021-05-16 20:14:31,299] {docker.py:276} INFO - 21/05/16 23:14:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312244275654028462211827_0004_m_000138_517: needsTaskCommit() Task attempt_202105162312244275654028462211827_0004_m_000138_517
[2021-05-16 20:14:31,300] {docker.py:276} INFO - 21/05/16 23:14:31 INFO StagingCommitter: Task committer attempt_202105162312244275654028462211827_0004_m_000138_517: needsTaskCommit() Task attempt_202105162312244275654028462211827_0004_m_000138_517: duration 0:00.000s
21/05/16 23:14:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244275654028462211827_0004_m_000138_517
[2021-05-16 20:14:31,301] {docker.py:276} INFO - 21/05/16 23:14:31 INFO Executor: Finished task 138.0 in stage 4.0 (TID 517). 4544 bytes result sent to driver
[2021-05-16 20:14:31,301] {docker.py:276} INFO - 21/05/16 23:14:31 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 521) (764b1db4ecfd, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:31,302] {docker.py:276} INFO - 21/05/16 23:14:31 INFO Executor: Running task 142.0 in stage 4.0 (TID 521)
[2021-05-16 20:14:31,303] {docker.py:276} INFO - 21/05/16 23:14:31 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 517) in 2904 ms on 764b1db4ecfd (executor driver) (139/200)
[2021-05-16 20:14:31,309] {docker.py:276} INFO - 21/05/16 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:31,311] {docker.py:276} INFO - 21/05/16 23:14:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244469344524717405646_0004_m_000142_521, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244469344524717405646_0004_m_000142_521}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244469344524717405646_0004}; taskId=attempt_202105162312244469344524717405646_0004_m_000142_521, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e06098d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312244469344524717405646_0004_m_000142_521: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244469344524717405646_0004_m_000142_521
[2021-05-16 20:14:31,314] {docker.py:276} INFO - 21/05/16 23:14:31 INFO StagingCommitter: Task committer attempt_202105162312244469344524717405646_0004_m_000142_521: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244469344524717405646_0004_m_000142_521 : duration 0:00.003s
[2021-05-16 20:14:31,605] {docker.py:276} INFO - 21/05/16 23:14:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312249062582932096488316_0004_m_000139_518: needsTaskCommit() Task attempt_202105162312249062582932096488316_0004_m_000139_518
21/05/16 23:14:31 INFO StagingCommitter: Task committer attempt_202105162312249062582932096488316_0004_m_000139_518: needsTaskCommit() Task attempt_202105162312249062582932096488316_0004_m_000139_518: duration 0:00.001s
[2021-05-16 20:14:31,606] {docker.py:276} INFO - 21/05/16 23:14:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312249062582932096488316_0004_m_000139_518
[2021-05-16 20:14:31,607] {docker.py:276} INFO - 21/05/16 23:14:31 INFO Executor: Finished task 139.0 in stage 4.0 (TID 518). 4544 bytes result sent to driver
[2021-05-16 20:14:31,608] {docker.py:276} INFO - 21/05/16 23:14:31 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 522) (764b1db4ecfd, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:31,609] {docker.py:276} INFO - 21/05/16 23:14:31 INFO Executor: Running task 143.0 in stage 4.0 (TID 522)
[2021-05-16 20:14:31,611] {docker.py:276} INFO - 21/05/16 23:14:31 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 518) in 2732 ms on 764b1db4ecfd (executor driver) (140/200)
[2021-05-16 20:14:31,619] {docker.py:276} INFO - 21/05/16 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 6 (25.3 KiB) non-empty blocks including 6 (25.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:31,621] {docker.py:276} INFO - 21/05/16 23:14:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241604952174866588990_0004_m_000143_522, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241604952174866588990_0004_m_000143_522}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241604952174866588990_0004}; taskId=attempt_202105162312241604952174866588990_0004_m_000143_522, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@650a111f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:31 INFO StagingCommitter: Starting: Task committer attempt_202105162312241604952174866588990_0004_m_000143_522: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241604952174866588990_0004_m_000143_522
[2021-05-16 20:14:31,624] {docker.py:276} INFO - 21/05/16 23:14:31 INFO StagingCommitter: Task committer attempt_202105162312241604952174866588990_0004_m_000143_522: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241604952174866588990_0004_m_000143_522 : duration 0:00.003s
[2021-05-16 20:14:32,128] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312242365068708763813260_0004_m_000140_519: needsTaskCommit() Task attempt_202105162312242365068708763813260_0004_m_000140_519
[2021-05-16 20:14:32,128] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Task committer attempt_202105162312242365068708763813260_0004_m_000140_519: needsTaskCommit() Task attempt_202105162312242365068708763813260_0004_m_000140_519: duration 0:00.001s
21/05/16 23:14:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242365068708763813260_0004_m_000140_519
[2021-05-16 20:14:32,130] {docker.py:276} INFO - 21/05/16 23:14:32 INFO Executor: Finished task 140.0 in stage 4.0 (TID 519). 4544 bytes result sent to driver
[2021-05-16 20:14:32,130] {docker.py:276} INFO - 21/05/16 23:14:32 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 523) (764b1db4ecfd, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:32,131] {docker.py:276} INFO - 21/05/16 23:14:32 INFO Executor: Running task 144.0 in stage 4.0 (TID 523)
[2021-05-16 20:14:32,132] {docker.py:276} INFO - 21/05/16 23:14:32 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 519) in 2751 ms on 764b1db4ecfd (executor driver) (141/200)
[2021-05-16 20:14:32,140] {docker.py:276} INFO - 21/05/16 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:32,143] {docker.py:276} INFO - 21/05/16 23:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246288708454005333585_0004_m_000144_523, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246288708454005333585_0004_m_000144_523}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246288708454005333585_0004}; taskId=attempt_202105162312246288708454005333585_0004_m_000144_523, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@173526b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:32,143] {docker.py:276} INFO - 21/05/16 23:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312246288708454005333585_0004_m_000144_523: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246288708454005333585_0004_m_000144_523
[2021-05-16 20:14:32,146] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Task committer attempt_202105162312246288708454005333585_0004_m_000144_523: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246288708454005333585_0004_m_000144_523 : duration 0:00.003s
[2021-05-16 20:14:32,406] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312245278812596561431308_0004_m_000141_520: needsTaskCommit() Task attempt_202105162312245278812596561431308_0004_m_000141_520
[2021-05-16 20:14:32,408] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Task committer attempt_202105162312245278812596561431308_0004_m_000141_520: needsTaskCommit() Task attempt_202105162312245278812596561431308_0004_m_000141_520: duration 0:00.001s
21/05/16 23:14:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245278812596561431308_0004_m_000141_520
[2021-05-16 20:14:32,410] {docker.py:276} INFO - 21/05/16 23:14:32 INFO Executor: Finished task 141.0 in stage 4.0 (TID 520). 4544 bytes result sent to driver
[2021-05-16 20:14:32,411] {docker.py:276} INFO - 21/05/16 23:14:32 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 524) (764b1db4ecfd, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:32 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 520) in 2734 ms on 764b1db4ecfd (executor driver) (142/200)
[2021-05-16 20:14:32,412] {docker.py:276} INFO - 21/05/16 23:14:32 INFO Executor: Running task 145.0 in stage 4.0 (TID 524)
[2021-05-16 20:14:32,421] {docker.py:276} INFO - 21/05/16 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:32,423] {docker.py:276} INFO - 21/05/16 23:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246905453579205742568_0004_m_000145_524, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246905453579205742568_0004_m_000145_524}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246905453579205742568_0004}; taskId=attempt_202105162312246905453579205742568_0004_m_000145_524, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@279db61c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:32 INFO StagingCommitter: Starting: Task committer attempt_202105162312246905453579205742568_0004_m_000145_524: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246905453579205742568_0004_m_000145_524
[2021-05-16 20:14:32,426] {docker.py:276} INFO - 21/05/16 23:14:32 INFO StagingCommitter: Task committer attempt_202105162312246905453579205742568_0004_m_000145_524: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246905453579205742568_0004_m_000145_524 : duration 0:00.003s
[2021-05-16 20:14:34,292] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312244469344524717405646_0004_m_000142_521: needsTaskCommit() Task attempt_202105162312244469344524717405646_0004_m_000142_521
[2021-05-16 20:14:34,293] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_202105162312244469344524717405646_0004_m_000142_521: needsTaskCommit() Task attempt_202105162312244469344524717405646_0004_m_000142_521: duration 0:00.001s
21/05/16 23:14:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244469344524717405646_0004_m_000142_521
[2021-05-16 20:14:34,295] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Finished task 142.0 in stage 4.0 (TID 521). 4544 bytes result sent to driver
[2021-05-16 20:14:34,296] {docker.py:276} INFO - 21/05/16 23:14:34 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 525) (764b1db4ecfd, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:34,297] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Running task 146.0 in stage 4.0 (TID 525)
21/05/16 23:14:34 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 521) in 2999 ms on 764b1db4ecfd (executor driver) (143/200)
[2021-05-16 20:14:34,308] {docker.py:276} INFO - 21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:14:34,310] {docker.py:276} INFO - 21/05/16 23:14:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248752244597143043080_0004_m_000146_525, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248752244597143043080_0004_m_000146_525}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248752244597143043080_0004}; taskId=attempt_202105162312248752244597143043080_0004_m_000146_525, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@171792a8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:34,310] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312248752244597143043080_0004_m_000146_525: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248752244597143043080_0004_m_000146_525
[2021-05-16 20:14:34,315] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_202105162312248752244597143043080_0004_m_000146_525: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248752244597143043080_0004_m_000146_525 : duration 0:00.004s
[2021-05-16 20:14:34,482] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312246288708454005333585_0004_m_000144_523: needsTaskCommit() Task attempt_202105162312246288708454005333585_0004_m_000144_523
[2021-05-16 20:14:34,483] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_202105162312246288708454005333585_0004_m_000144_523: needsTaskCommit() Task attempt_202105162312246288708454005333585_0004_m_000144_523: duration 0:00.000s
21/05/16 23:14:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246288708454005333585_0004_m_000144_523
[2021-05-16 20:14:34,484] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Finished task 144.0 in stage 4.0 (TID 523). 4544 bytes result sent to driver
[2021-05-16 20:14:34,485] {docker.py:276} INFO - 21/05/16 23:14:34 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 526) (764b1db4ecfd, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:34,486] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Running task 147.0 in stage 4.0 (TID 526)
21/05/16 23:14:34 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 523) in 2358 ms on 764b1db4ecfd (executor driver) (144/200)
[2021-05-16 20:14:34,495] {docker.py:276} INFO - 21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:34,497] {docker.py:276} INFO - 21/05/16 23:14:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242723991087841181909_0004_m_000147_526, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242723991087841181909_0004_m_000147_526}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242723991087841181909_0004}; taskId=attempt_202105162312242723991087841181909_0004_m_000147_526, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30ab2150}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312242723991087841181909_0004_m_000147_526: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242723991087841181909_0004_m_000147_526
[2021-05-16 20:14:34,500] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_202105162312242723991087841181909_0004_m_000147_526: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242723991087841181909_0004_m_000147_526 : duration 0:00.003s
[2021-05-16 20:14:34,566] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_202105162312241604952174866588990_0004_m_000143_522: needsTaskCommit() Task attempt_202105162312241604952174866588990_0004_m_000143_522
[2021-05-16 20:14:34,567] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_202105162312241604952174866588990_0004_m_000143_522: needsTaskCommit() Task attempt_202105162312241604952174866588990_0004_m_000143_522: duration 0:00.001s
21/05/16 23:14:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241604952174866588990_0004_m_000143_522
[2021-05-16 20:14:34,568] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Finished task 143.0 in stage 4.0 (TID 522). 4544 bytes result sent to driver
[2021-05-16 20:14:34,569] {docker.py:276} INFO - 21/05/16 23:14:34 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 527) (764b1db4ecfd, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:34,571] {docker.py:276} INFO - 21/05/16 23:14:34 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 522) in 2965 ms on 764b1db4ecfd (executor driver) (145/200)
[2021-05-16 20:14:34,571] {docker.py:276} INFO - 21/05/16 23:14:34 INFO Executor: Running task 148.0 in stage 4.0 (TID 527)
[2021-05-16 20:14:34,581] {docker.py:276} INFO - 21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:34,581] {docker.py:276} INFO - 21/05/16 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:34,583] {docker.py:276} INFO - 21/05/16 23:14:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:34,583] {docker.py:276} INFO - 21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:34,583] {docker.py:276} INFO - 21/05/16 23:14:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224366124810093508911_0004_m_000148_527, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224366124810093508911_0004_m_000148_527}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224366124810093508911_0004}; taskId=attempt_20210516231224366124810093508911_0004_m_000148_527, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@763bce64}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:34,584] {docker.py:276} INFO - 21/05/16 23:14:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:34,584] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Starting: Task committer attempt_20210516231224366124810093508911_0004_m_000148_527: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224366124810093508911_0004_m_000148_527
[2021-05-16 20:14:34,586] {docker.py:276} INFO - 21/05/16 23:14:34 INFO StagingCommitter: Task committer attempt_20210516231224366124810093508911_0004_m_000148_527: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224366124810093508911_0004_m_000148_527 : duration 0:00.003s
[2021-05-16 20:14:35,513] {docker.py:276} INFO - 21/05/16 23:14:35 INFO StagingCommitter: Starting: Task committer attempt_202105162312246905453579205742568_0004_m_000145_524: needsTaskCommit() Task attempt_202105162312246905453579205742568_0004_m_000145_524
21/05/16 23:14:35 INFO StagingCommitter: Task committer attempt_202105162312246905453579205742568_0004_m_000145_524: needsTaskCommit() Task attempt_202105162312246905453579205742568_0004_m_000145_524: duration 0:00.000s
21/05/16 23:14:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246905453579205742568_0004_m_000145_524
[2021-05-16 20:14:35,515] {docker.py:276} INFO - 21/05/16 23:14:35 INFO Executor: Finished task 145.0 in stage 4.0 (TID 524). 4544 bytes result sent to driver
[2021-05-16 20:14:35,516] {docker.py:276} INFO - 21/05/16 23:14:35 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 528) (764b1db4ecfd, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:35,517] {docker.py:276} INFO - 21/05/16 23:14:35 INFO Executor: Running task 149.0 in stage 4.0 (TID 528)
[2021-05-16 20:14:35,518] {docker.py:276} INFO - 21/05/16 23:14:35 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 524) in 3111 ms on 764b1db4ecfd (executor driver) (146/200)
[2021-05-16 20:14:35,527] {docker.py:276} INFO - 21/05/16 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 6 (24.9 KiB) non-empty blocks including 6 (24.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:35,529] {docker.py:276} INFO - 21/05/16 23:14:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247859161393948864945_0004_m_000149_528, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247859161393948864945_0004_m_000149_528}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247859161393948864945_0004}; taskId=attempt_202105162312247859161393948864945_0004_m_000149_528, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b37dac3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:35 INFO StagingCommitter: Starting: Task committer attempt_202105162312247859161393948864945_0004_m_000149_528: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247859161393948864945_0004_m_000149_528
[2021-05-16 20:14:35,532] {docker.py:276} INFO - 21/05/16 23:14:35 INFO StagingCommitter: Task committer attempt_202105162312247859161393948864945_0004_m_000149_528: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247859161393948864945_0004_m_000149_528 : duration 0:00.003s
[2021-05-16 20:14:37,217] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_202105162312242723991087841181909_0004_m_000147_526: needsTaskCommit() Task attempt_202105162312242723991087841181909_0004_m_000147_526
21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_202105162312242723991087841181909_0004_m_000147_526: needsTaskCommit() Task attempt_202105162312242723991087841181909_0004_m_000147_526: duration 0:00.000s
[2021-05-16 20:14:37,218] {docker.py:276} INFO - 21/05/16 23:14:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242723991087841181909_0004_m_000147_526
[2021-05-16 20:14:37,219] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Finished task 147.0 in stage 4.0 (TID 526). 4544 bytes result sent to driver
[2021-05-16 20:14:37,220] {docker.py:276} INFO - 21/05/16 23:14:37 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 529) (764b1db4ecfd, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:37,221] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Running task 150.0 in stage 4.0 (TID 529)
21/05/16 23:14:37 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 526) in 2704 ms on 764b1db4ecfd (executor driver) (147/200)
[2021-05-16 20:14:37,230] {docker.py:276} INFO - 21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:37,231] {docker.py:276} INFO - 21/05/16 23:14:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241635375399213982363_0004_m_000150_529, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241635375399213982363_0004_m_000150_529}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241635375399213982363_0004}; taskId=attempt_202105162312241635375399213982363_0004_m_000150_529, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53660c10}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_202105162312241635375399213982363_0004_m_000150_529: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241635375399213982363_0004_m_000150_529
[2021-05-16 20:14:37,235] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_202105162312241635375399213982363_0004_m_000150_529: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241635375399213982363_0004_m_000150_529 : duration 0:00.003s
[2021-05-16 20:14:37,362] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_20210516231224366124810093508911_0004_m_000148_527: needsTaskCommit() Task attempt_20210516231224366124810093508911_0004_m_000148_527
[2021-05-16 20:14:37,362] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_20210516231224366124810093508911_0004_m_000148_527: needsTaskCommit() Task attempt_20210516231224366124810093508911_0004_m_000148_527: duration 0:00.000s
[2021-05-16 20:14:37,364] {docker.py:276} INFO - 21/05/16 23:14:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224366124810093508911_0004_m_000148_527
[2021-05-16 20:14:37,364] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Finished task 148.0 in stage 4.0 (TID 527). 4544 bytes result sent to driver
[2021-05-16 20:14:37,365] {docker.py:276} INFO - 21/05/16 23:14:37 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 530) (764b1db4ecfd, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:37,367] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Running task 151.0 in stage 4.0 (TID 530)
[2021-05-16 20:14:37,367] {docker.py:276} INFO - 21/05/16 23:14:37 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 527) in 2764 ms on 764b1db4ecfd (executor driver) (148/200)
[2021-05-16 20:14:37,376] {docker.py:276} INFO - 21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:37,377] {docker.py:276} INFO - 21/05/16 23:14:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243509335056699503861_0004_m_000151_530, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243509335056699503861_0004_m_000151_530}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243509335056699503861_0004}; taskId=attempt_202105162312243509335056699503861_0004_m_000151_530, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d651d2b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_202105162312243509335056699503861_0004_m_000151_530: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243509335056699503861_0004_m_000151_530
[2021-05-16 20:14:37,380] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_202105162312243509335056699503861_0004_m_000151_530: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243509335056699503861_0004_m_000151_530 : duration 0:00.003s
[2021-05-16 20:14:37,699] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_202105162312248752244597143043080_0004_m_000146_525: needsTaskCommit() Task attempt_202105162312248752244597143043080_0004_m_000146_525
[2021-05-16 20:14:37,700] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_202105162312248752244597143043080_0004_m_000146_525: needsTaskCommit() Task attempt_202105162312248752244597143043080_0004_m_000146_525: duration 0:00.001s
21/05/16 23:14:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248752244597143043080_0004_m_000146_525
[2021-05-16 20:14:37,702] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Finished task 146.0 in stage 4.0 (TID 525). 4544 bytes result sent to driver
[2021-05-16 20:14:37,704] {docker.py:276} INFO - 21/05/16 23:14:37 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 531) (764b1db4ecfd, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:37,705] {docker.py:276} INFO - 21/05/16 23:14:37 INFO Executor: Running task 152.0 in stage 4.0 (TID 531)
21/05/16 23:14:37 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 525) in 3376 ms on 764b1db4ecfd (executor driver) (149/200)
[2021-05-16 20:14:37,714] {docker.py:276} INFO - 21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:37,716] {docker.py:276} INFO - 21/05/16 23:14:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248530362725380225120_0004_m_000152_531, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248530362725380225120_0004_m_000152_531}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248530362725380225120_0004}; taskId=attempt_202105162312248530362725380225120_0004_m_000152_531, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@8b0cc4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:37 INFO StagingCommitter: Starting: Task committer attempt_202105162312248530362725380225120_0004_m_000152_531: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248530362725380225120_0004_m_000152_531
[2021-05-16 20:14:37,720] {docker.py:276} INFO - 21/05/16 23:14:37 INFO StagingCommitter: Task committer attempt_202105162312248530362725380225120_0004_m_000152_531: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248530362725380225120_0004_m_000152_531 : duration 0:00.004s
[2021-05-16 20:14:38,482] {docker.py:276} INFO - 21/05/16 23:14:38 INFO StagingCommitter: Starting: Task committer attempt_202105162312247859161393948864945_0004_m_000149_528: needsTaskCommit() Task attempt_202105162312247859161393948864945_0004_m_000149_528
[2021-05-16 20:14:38,484] {docker.py:276} INFO - 21/05/16 23:14:38 INFO StagingCommitter: Task committer attempt_202105162312247859161393948864945_0004_m_000149_528: needsTaskCommit() Task attempt_202105162312247859161393948864945_0004_m_000149_528: duration 0:00.002s
21/05/16 23:14:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247859161393948864945_0004_m_000149_528
[2021-05-16 20:14:38,485] {docker.py:276} INFO - 21/05/16 23:14:38 INFO Executor: Finished task 149.0 in stage 4.0 (TID 528). 4544 bytes result sent to driver
[2021-05-16 20:14:38,486] {docker.py:276} INFO - 21/05/16 23:14:38 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 532) (764b1db4ecfd, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:38,487] {docker.py:276} INFO - 21/05/16 23:14:38 INFO Executor: Running task 153.0 in stage 4.0 (TID 532)
21/05/16 23:14:38 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 528) in 2939 ms on 764b1db4ecfd (executor driver) (150/200)
[2021-05-16 20:14:38,498] {docker.py:276} INFO - 21/05/16 23:14:38 INFO ShuffleBlockFetcherIterator: Getting 6 (25.1 KiB) non-empty blocks including 6 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-16 20:14:38,499] {docker.py:276} INFO - 21/05/16 23:14:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243557942510886220751_0004_m_000153_532, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243557942510886220751_0004_m_000153_532}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243557942510886220751_0004}; taskId=attempt_202105162312243557942510886220751_0004_m_000153_532, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@477e0410}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:38,500] {docker.py:276} INFO - 21/05/16 23:14:38 INFO StagingCommitter: Starting: Task committer attempt_202105162312243557942510886220751_0004_m_000153_532: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243557942510886220751_0004_m_000153_532
[2021-05-16 20:14:38,503] {docker.py:276} INFO - 21/05/16 23:14:38 INFO StagingCommitter: Task committer attempt_202105162312243557942510886220751_0004_m_000153_532: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243557942510886220751_0004_m_000153_532 : duration 0:00.003s
[2021-05-16 20:14:40,137] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312241635375399213982363_0004_m_000150_529: needsTaskCommit() Task attempt_202105162312241635375399213982363_0004_m_000150_529
[2021-05-16 20:14:40,138] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312241635375399213982363_0004_m_000150_529: needsTaskCommit() Task attempt_202105162312241635375399213982363_0004_m_000150_529: duration 0:00.000s
[2021-05-16 20:14:40,139] {docker.py:276} INFO - 21/05/16 23:14:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241635375399213982363_0004_m_000150_529
[2021-05-16 20:14:40,140] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Finished task 150.0 in stage 4.0 (TID 529). 4544 bytes result sent to driver
[2021-05-16 20:14:40,142] {docker.py:276} INFO - 21/05/16 23:14:40 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 533) (764b1db4ecfd, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:40 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 529) in 2925 ms on 764b1db4ecfd (executor driver) (151/200)
[2021-05-16 20:14:40,142] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Running task 154.0 in stage 4.0 (TID 533)
[2021-05-16 20:14:40,152] {docker.py:276} INFO - 21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:40,154] {docker.py:276} INFO - 21/05/16 23:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247851904440767789526_0004_m_000154_533, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247851904440767789526_0004_m_000154_533}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247851904440767789526_0004}; taskId=attempt_202105162312247851904440767789526_0004_m_000154_533, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@46a155a6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312247851904440767789526_0004_m_000154_533: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247851904440767789526_0004_m_000154_533
[2021-05-16 20:14:40,157] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312247851904440767789526_0004_m_000154_533: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247851904440767789526_0004_m_000154_533 : duration 0:00.003s
[2021-05-16 20:14:40,177] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312243509335056699503861_0004_m_000151_530: needsTaskCommit() Task attempt_202105162312243509335056699503861_0004_m_000151_530
[2021-05-16 20:14:40,177] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312243509335056699503861_0004_m_000151_530: needsTaskCommit() Task attempt_202105162312243509335056699503861_0004_m_000151_530: duration 0:00.000s
21/05/16 23:14:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243509335056699503861_0004_m_000151_530
[2021-05-16 20:14:40,178] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Finished task 151.0 in stage 4.0 (TID 530). 4544 bytes result sent to driver
[2021-05-16 20:14:40,179] {docker.py:276} INFO - 21/05/16 23:14:40 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 534) (764b1db4ecfd, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:40,180] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Running task 155.0 in stage 4.0 (TID 534)
21/05/16 23:14:40 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 530) in 2819 ms on 764b1db4ecfd (executor driver) (152/200)
[2021-05-16 20:14:40,187] {docker.py:276} INFO - 21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:40,189] {docker.py:276} INFO - 21/05/16 23:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241519406125501694226_0004_m_000155_534, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241519406125501694226_0004_m_000155_534}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241519406125501694226_0004}; taskId=attempt_202105162312241519406125501694226_0004_m_000155_534, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3d2d9bfd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:40,190] {docker.py:276} INFO - 21/05/16 23:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312241519406125501694226_0004_m_000155_534: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241519406125501694226_0004_m_000155_534
[2021-05-16 20:14:40,192] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312241519406125501694226_0004_m_000155_534: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241519406125501694226_0004_m_000155_534 : duration 0:00.002s
[2021-05-16 20:14:40,536] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312248530362725380225120_0004_m_000152_531: needsTaskCommit() Task attempt_202105162312248530362725380225120_0004_m_000152_531
[2021-05-16 20:14:40,537] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312248530362725380225120_0004_m_000152_531: needsTaskCommit() Task attempt_202105162312248530362725380225120_0004_m_000152_531: duration 0:00.000s
21/05/16 23:14:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248530362725380225120_0004_m_000152_531
[2021-05-16 20:14:40,538] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Finished task 152.0 in stage 4.0 (TID 531). 4544 bytes result sent to driver
[2021-05-16 20:14:40,538] {docker.py:276} INFO - 21/05/16 23:14:40 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 535) (764b1db4ecfd, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:40,539] {docker.py:276} INFO - 21/05/16 23:14:40 INFO Executor: Running task 156.0 in stage 4.0 (TID 535)
[2021-05-16 20:14:40,540] {docker.py:276} INFO - 21/05/16 23:14:40 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 531) in 2840 ms on 764b1db4ecfd (executor driver) (153/200)
[2021-05-16 20:14:40,547] {docker.py:276} INFO - 21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:40,549] {docker.py:276} INFO - 21/05/16 23:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248685640391135659296_0004_m_000156_535, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248685640391135659296_0004_m_000156_535}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248685640391135659296_0004}; taskId=attempt_202105162312248685640391135659296_0004_m_000156_535, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@28ee7e3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:40 INFO StagingCommitter: Starting: Task committer attempt_202105162312248685640391135659296_0004_m_000156_535: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248685640391135659296_0004_m_000156_535
[2021-05-16 20:14:40,553] {docker.py:276} INFO - 21/05/16 23:14:40 INFO StagingCommitter: Task committer attempt_202105162312248685640391135659296_0004_m_000156_535: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248685640391135659296_0004_m_000156_535 : duration 0:00.003s
[2021-05-16 20:14:41,199] {docker.py:276} INFO - 21/05/16 23:14:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312243557942510886220751_0004_m_000153_532: needsTaskCommit() Task attempt_202105162312243557942510886220751_0004_m_000153_532
[2021-05-16 20:14:41,200] {docker.py:276} INFO - 21/05/16 23:14:41 INFO StagingCommitter: Task committer attempt_202105162312243557942510886220751_0004_m_000153_532: needsTaskCommit() Task attempt_202105162312243557942510886220751_0004_m_000153_532: duration 0:00.001s
21/05/16 23:14:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243557942510886220751_0004_m_000153_532
[2021-05-16 20:14:41,202] {docker.py:276} INFO - 21/05/16 23:14:41 INFO Executor: Finished task 153.0 in stage 4.0 (TID 532). 4544 bytes result sent to driver
[2021-05-16 20:14:41,204] {docker.py:276} INFO - 21/05/16 23:14:41 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 536) (764b1db4ecfd, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:41,205] {docker.py:276} INFO - 21/05/16 23:14:41 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 532) in 2722 ms on 764b1db4ecfd (executor driver) (154/200)
[2021-05-16 20:14:41,206] {docker.py:276} INFO - 21/05/16 23:14:41 INFO Executor: Running task 157.0 in stage 4.0 (TID 536)
[2021-05-16 20:14:41,215] {docker.py:276} INFO - 21/05/16 23:14:41 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:41,218] {docker.py:276} INFO - 21/05/16 23:14:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243068877491932828447_0004_m_000157_536, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243068877491932828447_0004_m_000157_536}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243068877491932828447_0004}; taskId=attempt_202105162312243068877491932828447_0004_m_000157_536, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@34dd4dd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:41,218] {docker.py:276} INFO - 21/05/16 23:14:41 INFO StagingCommitter: Starting: Task committer attempt_202105162312243068877491932828447_0004_m_000157_536: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243068877491932828447_0004_m_000157_536
[2021-05-16 20:14:41,222] {docker.py:276} INFO - 21/05/16 23:14:41 INFO StagingCommitter: Task committer attempt_202105162312243068877491932828447_0004_m_000157_536: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243068877491932828447_0004_m_000157_536 : duration 0:00.004s
[2021-05-16 20:14:42,958] {docker.py:276} INFO - 21/05/16 23:14:42 INFO StagingCommitter: Starting: Task committer attempt_202105162312247851904440767789526_0004_m_000154_533: needsTaskCommit() Task attempt_202105162312247851904440767789526_0004_m_000154_533
[2021-05-16 20:14:42,960] {docker.py:276} INFO - 21/05/16 23:14:42 INFO StagingCommitter: Task committer attempt_202105162312247851904440767789526_0004_m_000154_533: needsTaskCommit() Task attempt_202105162312247851904440767789526_0004_m_000154_533: duration 0:00.000s
21/05/16 23:14:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247851904440767789526_0004_m_000154_533
[2021-05-16 20:14:42,961] {docker.py:276} INFO - 21/05/16 23:14:42 INFO Executor: Finished task 154.0 in stage 4.0 (TID 533). 4544 bytes result sent to driver
[2021-05-16 20:14:42,961] {docker.py:276} INFO - 21/05/16 23:14:42 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 537) (764b1db4ecfd, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:42,962] {docker.py:276} INFO - 21/05/16 23:14:42 INFO Executor: Running task 158.0 in stage 4.0 (TID 537)
[2021-05-16 20:14:42,964] {docker.py:276} INFO - 21/05/16 23:14:42 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 533) in 2826 ms on 764b1db4ecfd (executor driver) (155/200)
[2021-05-16 20:14:42,975] {docker.py:276} INFO - 21/05/16 23:14:42 INFO ShuffleBlockFetcherIterator: Getting 6 (27.4 KiB) non-empty blocks including 6 (27.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:42,978] {docker.py:276} INFO - 21/05/16 23:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:42,979] {docker.py:276} INFO - 21/05/16 23:14:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245865226842736157674_0004_m_000158_537, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245865226842736157674_0004_m_000158_537}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245865226842736157674_0004}; taskId=attempt_202105162312245865226842736157674_0004_m_000158_537, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@609a5619}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:42 INFO StagingCommitter: Starting: Task committer attempt_202105162312245865226842736157674_0004_m_000158_537: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245865226842736157674_0004_m_000158_537
[2021-05-16 20:14:42,983] {docker.py:276} INFO - 21/05/16 23:14:42 INFO StagingCommitter: Task committer attempt_202105162312245865226842736157674_0004_m_000158_537: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245865226842736157674_0004_m_000158_537 : duration 0:00.004s
[2021-05-16 20:14:43,138] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Starting: Task committer attempt_202105162312241519406125501694226_0004_m_000155_534: needsTaskCommit() Task attempt_202105162312241519406125501694226_0004_m_000155_534
[2021-05-16 20:14:43,138] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Task committer attempt_202105162312241519406125501694226_0004_m_000155_534: needsTaskCommit() Task attempt_202105162312241519406125501694226_0004_m_000155_534: duration 0:00.000s
[2021-05-16 20:14:43,139] {docker.py:276} INFO - 21/05/16 23:14:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241519406125501694226_0004_m_000155_534
[2021-05-16 20:14:43,140] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Finished task 155.0 in stage 4.0 (TID 534). 4544 bytes result sent to driver
[2021-05-16 20:14:43,141] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 538) (764b1db4ecfd, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:43,141] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Running task 159.0 in stage 4.0 (TID 538)
[2021-05-16 20:14:43,142] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 534) in 2967 ms on 764b1db4ecfd (executor driver) (156/200)
[2021-05-16 20:14:43,152] {docker.py:276} INFO - 21/05/16 23:14:43 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:43,153] {docker.py:276} INFO - 21/05/16 23:14:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:43,154] {docker.py:276} INFO - 21/05/16 23:14:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:43,155] {docker.py:276} INFO - 21/05/16 23:14:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242264996429700235821_0004_m_000159_538, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242264996429700235821_0004_m_000159_538}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242264996429700235821_0004}; taskId=attempt_202105162312242264996429700235821_0004_m_000159_538, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@144c495a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:43,155] {docker.py:276} INFO - 21/05/16 23:14:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:43,155] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Starting: Task committer attempt_202105162312242264996429700235821_0004_m_000159_538: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242264996429700235821_0004_m_000159_538
[2021-05-16 20:14:43,158] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Task committer attempt_202105162312242264996429700235821_0004_m_000159_538: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242264996429700235821_0004_m_000159_538 : duration 0:00.003s
[2021-05-16 20:14:43,488] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Starting: Task committer attempt_202105162312248685640391135659296_0004_m_000156_535: needsTaskCommit() Task attempt_202105162312248685640391135659296_0004_m_000156_535
[2021-05-16 20:14:43,489] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Task committer attempt_202105162312248685640391135659296_0004_m_000156_535: needsTaskCommit() Task attempt_202105162312248685640391135659296_0004_m_000156_535: duration 0:00.000s
21/05/16 23:14:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248685640391135659296_0004_m_000156_535
[2021-05-16 20:14:43,490] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Finished task 156.0 in stage 4.0 (TID 535). 4544 bytes result sent to driver
[2021-05-16 20:14:43,490] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 539) (764b1db4ecfd, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:43,491] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 535) in 2957 ms on 764b1db4ecfd (executor driver) (157/200)
[2021-05-16 20:14:43,492] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Running task 160.0 in stage 4.0 (TID 539)
[2021-05-16 20:14:43,501] {docker.py:276} INFO - 21/05/16 23:14:43 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:43,502] {docker.py:276} INFO - 21/05/16 23:14:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:43,504] {docker.py:276} INFO - 21/05/16 23:14:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:43,505] {docker.py:276} INFO - 21/05/16 23:14:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:43,505] {docker.py:276} INFO - 21/05/16 23:14:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:43,506] {docker.py:276} INFO - 21/05/16 23:14:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242211288594172851121_0004_m_000160_539, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242211288594172851121_0004_m_000160_539}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242211288594172851121_0004}; taskId=attempt_202105162312242211288594172851121_0004_m_000160_539, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d29af4f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:43,506] {docker.py:276} INFO - 21/05/16 23:14:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:43,507] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Starting: Task committer attempt_202105162312242211288594172851121_0004_m_000160_539: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242211288594172851121_0004_m_000160_539
[2021-05-16 20:14:43,510] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Task committer attempt_202105162312242211288594172851121_0004_m_000160_539: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242211288594172851121_0004_m_000160_539 : duration 0:00.003s
[2021-05-16 20:14:43,984] {docker.py:276} INFO - 21/05/16 23:14:43 INFO StagingCommitter: Starting: Task committer attempt_202105162312243068877491932828447_0004_m_000157_536: needsTaskCommit() Task attempt_202105162312243068877491932828447_0004_m_000157_536
21/05/16 23:14:43 INFO StagingCommitter: Task committer attempt_202105162312243068877491932828447_0004_m_000157_536: needsTaskCommit() Task attempt_202105162312243068877491932828447_0004_m_000157_536: duration 0:00.001s
[2021-05-16 20:14:43,985] {docker.py:276} INFO - 21/05/16 23:14:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243068877491932828447_0004_m_000157_536
[2021-05-16 20:14:43,987] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Finished task 157.0 in stage 4.0 (TID 536). 4544 bytes result sent to driver
[2021-05-16 20:14:43,988] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 540) (764b1db4ecfd, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:43,989] {docker.py:276} INFO - 21/05/16 23:14:43 INFO Executor: Running task 161.0 in stage 4.0 (TID 540)
[2021-05-16 20:14:43,989] {docker.py:276} INFO - 21/05/16 23:14:43 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 536) in 2789 ms on 764b1db4ecfd (executor driver) (158/200)
[2021-05-16 20:14:43,999] {docker.py:276} INFO - 21/05/16 23:14:44 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:44,001] {docker.py:276} INFO - 21/05/16 23:14:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224696623485407608624_0004_m_000161_540, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224696623485407608624_0004_m_000161_540}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224696623485407608624_0004}; taskId=attempt_20210516231224696623485407608624_0004_m_000161_540, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6183d6b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:44,002] {docker.py:276} INFO - 21/05/16 23:14:44 INFO StagingCommitter: Starting: Task committer attempt_20210516231224696623485407608624_0004_m_000161_540: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224696623485407608624_0004_m_000161_540
[2021-05-16 20:14:44,004] {docker.py:276} INFO - 21/05/16 23:14:44 INFO StagingCommitter: Task committer attempt_20210516231224696623485407608624_0004_m_000161_540: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224696623485407608624_0004_m_000161_540 : duration 0:00.003s
[2021-05-16 20:14:45,903] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312245865226842736157674_0004_m_000158_537: needsTaskCommit() Task attempt_202105162312245865226842736157674_0004_m_000158_537
[2021-05-16 20:14:45,904] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Task committer attempt_202105162312245865226842736157674_0004_m_000158_537: needsTaskCommit() Task attempt_202105162312245865226842736157674_0004_m_000158_537: duration 0:00.001s
21/05/16 23:14:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245865226842736157674_0004_m_000158_537
[2021-05-16 20:14:45,906] {docker.py:276} INFO - 21/05/16 23:14:45 INFO Executor: Finished task 158.0 in stage 4.0 (TID 537). 4544 bytes result sent to driver
[2021-05-16 20:14:45,907] {docker.py:276} INFO - 21/05/16 23:14:45 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 541) (764b1db4ecfd, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:45,908] {docker.py:276} INFO - 21/05/16 23:14:45 INFO Executor: Running task 162.0 in stage 4.0 (TID 541)
[2021-05-16 20:14:45,909] {docker.py:276} INFO - 21/05/16 23:14:45 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 537) in 2951 ms on 764b1db4ecfd (executor driver) (159/200)
[2021-05-16 20:14:45,920] {docker.py:276} INFO - 21/05/16 23:14:45 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:45,921] {docker.py:276} INFO - 21/05/16 23:14:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:45,922] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312242264996429700235821_0004_m_000159_538: needsTaskCommit() Task attempt_202105162312242264996429700235821_0004_m_000159_538
[2021-05-16 20:14:45,923] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Task committer attempt_202105162312242264996429700235821_0004_m_000159_538: needsTaskCommit() Task attempt_202105162312242264996429700235821_0004_m_000159_538: duration 0:00.000s
21/05/16 23:14:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242264996429700235821_0004_m_000159_538
[2021-05-16 20:14:45,923] {docker.py:276} INFO - 21/05/16 23:14:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:45,924] {docker.py:276} INFO - 21/05/16 23:14:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:45,924] {docker.py:276} INFO - 21/05/16 23:14:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:45,924] {docker.py:276} INFO - 21/05/16 23:14:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247568648487936266808_0004_m_000162_541, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247568648487936266808_0004_m_000162_541}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247568648487936266808_0004}; taskId=attempt_202105162312247568648487936266808_0004_m_000162_541, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5433ac37}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:45,924] {docker.py:276} INFO - 21/05/16 23:14:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:45,925] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312247568648487936266808_0004_m_000162_541: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247568648487936266808_0004_m_000162_541
[2021-05-16 20:14:45,925] {docker.py:276} INFO - 21/05/16 23:14:45 INFO Executor: Finished task 159.0 in stage 4.0 (TID 538). 4544 bytes result sent to driver
[2021-05-16 20:14:45,926] {docker.py:276} INFO - 21/05/16 23:14:45 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 542) (764b1db4ecfd, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:45,927] {docker.py:276} INFO - 21/05/16 23:14:45 INFO Executor: Running task 163.0 in stage 4.0 (TID 542)
21/05/16 23:14:45 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 538) in 2791 ms on 764b1db4ecfd (executor driver) (160/200)
[2021-05-16 20:14:45,929] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Task committer attempt_202105162312247568648487936266808_0004_m_000162_541: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247568648487936266808_0004_m_000162_541 : duration 0:00.005s
[2021-05-16 20:14:45,937] {docker.py:276} INFO - 21/05/16 23:14:45 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:45,939] {docker.py:276} INFO - 21/05/16 23:14:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248203472082279026760_0004_m_000163_542, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248203472082279026760_0004_m_000163_542}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248203472082279026760_0004}; taskId=attempt_202105162312248203472082279026760_0004_m_000163_542, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6853cfdb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:45,939] {docker.py:276} INFO - 21/05/16 23:14:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:45 INFO StagingCommitter: Starting: Task committer attempt_202105162312248203472082279026760_0004_m_000163_542: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248203472082279026760_0004_m_000163_542
[2021-05-16 20:14:45,942] {docker.py:276} INFO - 21/05/16 23:14:45 INFO StagingCommitter: Task committer attempt_202105162312248203472082279026760_0004_m_000163_542: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248203472082279026760_0004_m_000163_542 : duration 0:00.002s
[2021-05-16 20:14:46,258] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Starting: Task committer attempt_202105162312242211288594172851121_0004_m_000160_539: needsTaskCommit() Task attempt_202105162312242211288594172851121_0004_m_000160_539
[2021-05-16 20:14:46,259] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Task committer attempt_202105162312242211288594172851121_0004_m_000160_539: needsTaskCommit() Task attempt_202105162312242211288594172851121_0004_m_000160_539: duration 0:00.001s
21/05/16 23:14:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242211288594172851121_0004_m_000160_539
[2021-05-16 20:14:46,261] {docker.py:276} INFO - 21/05/16 23:14:46 INFO Executor: Finished task 160.0 in stage 4.0 (TID 539). 4587 bytes result sent to driver
[2021-05-16 20:14:46,262] {docker.py:276} INFO - 21/05/16 23:14:46 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 543) (764b1db4ecfd, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:46,263] {docker.py:276} INFO - 21/05/16 23:14:46 INFO Executor: Running task 164.0 in stage 4.0 (TID 543)
21/05/16 23:14:46 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 539) in 2776 ms on 764b1db4ecfd (executor driver) (161/200)
[2021-05-16 20:14:46,272] {docker.py:276} INFO - 21/05/16 23:14:46 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:46,273] {docker.py:276} INFO - 21/05/16 23:14:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241238051494122575784_0004_m_000164_543, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241238051494122575784_0004_m_000164_543}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241238051494122575784_0004}; taskId=attempt_202105162312241238051494122575784_0004_m_000164_543, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@27f14710}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:46 INFO StagingCommitter: Starting: Task committer attempt_202105162312241238051494122575784_0004_m_000164_543: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241238051494122575784_0004_m_000164_543
[2021-05-16 20:14:46,276] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Task committer attempt_202105162312241238051494122575784_0004_m_000164_543: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241238051494122575784_0004_m_000164_543 : duration 0:00.002s
[2021-05-16 20:14:46,783] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Starting: Task committer attempt_20210516231224696623485407608624_0004_m_000161_540: needsTaskCommit() Task attempt_20210516231224696623485407608624_0004_m_000161_540
[2021-05-16 20:14:46,784] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Task committer attempt_20210516231224696623485407608624_0004_m_000161_540: needsTaskCommit() Task attempt_20210516231224696623485407608624_0004_m_000161_540: duration 0:00.001s
21/05/16 23:14:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224696623485407608624_0004_m_000161_540
[2021-05-16 20:14:46,786] {docker.py:276} INFO - 21/05/16 23:14:46 INFO Executor: Finished task 161.0 in stage 4.0 (TID 540). 4587 bytes result sent to driver
[2021-05-16 20:14:46,789] {docker.py:276} INFO - 21/05/16 23:14:46 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 544) (764b1db4ecfd, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:46,790] {docker.py:276} INFO - 21/05/16 23:14:46 INFO Executor: Running task 165.0 in stage 4.0 (TID 544)
21/05/16 23:14:46 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 540) in 2806 ms on 764b1db4ecfd (executor driver) (162/200)
[2021-05-16 20:14:46,801] {docker.py:276} INFO - 21/05/16 23:14:46 INFO ShuffleBlockFetcherIterator: Getting 6 (24.6 KiB) non-empty blocks including 6 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:46,803] {docker.py:276} INFO - 21/05/16 23:14:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242470375565200037986_0004_m_000165_544, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242470375565200037986_0004_m_000165_544}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242470375565200037986_0004}; taskId=attempt_202105162312242470375565200037986_0004_m_000165_544, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2af1bdf4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:46 INFO StagingCommitter: Starting: Task committer attempt_202105162312242470375565200037986_0004_m_000165_544: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242470375565200037986_0004_m_000165_544
[2021-05-16 20:14:46,807] {docker.py:276} INFO - 21/05/16 23:14:46 INFO StagingCommitter: Task committer attempt_202105162312242470375565200037986_0004_m_000165_544: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242470375565200037986_0004_m_000165_544 : duration 0:00.004s
[2021-05-16 20:14:48,734] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312247568648487936266808_0004_m_000162_541: needsTaskCommit() Task attempt_202105162312247568648487936266808_0004_m_000162_541
[2021-05-16 20:14:48,735] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Task committer attempt_202105162312247568648487936266808_0004_m_000162_541: needsTaskCommit() Task attempt_202105162312247568648487936266808_0004_m_000162_541: duration 0:00.001s
21/05/16 23:14:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247568648487936266808_0004_m_000162_541
[2021-05-16 20:14:48,736] {docker.py:276} INFO - 21/05/16 23:14:48 INFO Executor: Finished task 162.0 in stage 4.0 (TID 541). 4587 bytes result sent to driver
[2021-05-16 20:14:48,737] {docker.py:276} INFO - 21/05/16 23:14:48 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 545) (764b1db4ecfd, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:48,738] {docker.py:276} INFO - 21/05/16 23:14:48 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 541) in 2835 ms on 764b1db4ecfd (executor driver) (163/200)
[2021-05-16 20:14:48,739] {docker.py:276} INFO - 21/05/16 23:14:48 INFO Executor: Running task 166.0 in stage 4.0 (TID 545)
[2021-05-16 20:14:48,747] {docker.py:276} INFO - 21/05/16 23:14:48 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:48,747] {docker.py:276} INFO - 21/05/16 23:14:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:48,749] {docker.py:276} INFO - 21/05/16 23:14:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:48,750] {docker.py:276} INFO - 21/05/16 23:14:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246907808585965085607_0004_m_000166_545, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246907808585965085607_0004_m_000166_545}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246907808585965085607_0004}; taskId=attempt_202105162312246907808585965085607_0004_m_000166_545, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@338263d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:48,750] {docker.py:276} INFO - 21/05/16 23:14:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:48,750] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312246907808585965085607_0004_m_000166_545: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246907808585965085607_0004_m_000166_545
[2021-05-16 20:14:48,752] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Task committer attempt_202105162312246907808585965085607_0004_m_000166_545: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246907808585965085607_0004_m_000166_545 : duration 0:00.003s
[2021-05-16 20:14:48,936] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312248203472082279026760_0004_m_000163_542: needsTaskCommit() Task attempt_202105162312248203472082279026760_0004_m_000163_542
[2021-05-16 20:14:48,936] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Task committer attempt_202105162312248203472082279026760_0004_m_000163_542: needsTaskCommit() Task attempt_202105162312248203472082279026760_0004_m_000163_542: duration 0:00.000s
21/05/16 23:14:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248203472082279026760_0004_m_000163_542
[2021-05-16 20:14:48,938] {docker.py:276} INFO - 21/05/16 23:14:48 INFO Executor: Finished task 163.0 in stage 4.0 (TID 542). 4587 bytes result sent to driver
[2021-05-16 20:14:48,938] {docker.py:276} INFO - 21/05/16 23:14:48 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 546) (764b1db4ecfd, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:48,939] {docker.py:276} INFO - 21/05/16 23:14:48 INFO Executor: Running task 167.0 in stage 4.0 (TID 546)
[2021-05-16 20:14:48,939] {docker.py:276} INFO - 21/05/16 23:14:48 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 542) in 3018 ms on 764b1db4ecfd (executor driver) (164/200)
[2021-05-16 20:14:48,951] {docker.py:276} INFO - 21/05/16 23:14:48 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:48,953] {docker.py:276} INFO - 21/05/16 23:14:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:48,953] {docker.py:276} INFO - 21/05/16 23:14:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247793874968637743692_0004_m_000167_546, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247793874968637743692_0004_m_000167_546}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247793874968637743692_0004}; taskId=attempt_202105162312247793874968637743692_0004_m_000167_546, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dab8240}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:48,953] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Starting: Task committer attempt_202105162312247793874968637743692_0004_m_000167_546: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247793874968637743692_0004_m_000167_546
[2021-05-16 20:14:48,957] {docker.py:276} INFO - 21/05/16 23:14:48 INFO StagingCommitter: Task committer attempt_202105162312247793874968637743692_0004_m_000167_546: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247793874968637743692_0004_m_000167_546 : duration 0:00.004s
[2021-05-16 20:14:49,050] {docker.py:276} INFO - 21/05/16 23:14:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312241238051494122575784_0004_m_000164_543: needsTaskCommit() Task attempt_202105162312241238051494122575784_0004_m_000164_543
[2021-05-16 20:14:49,050] {docker.py:276} INFO - 21/05/16 23:14:49 INFO StagingCommitter: Task committer attempt_202105162312241238051494122575784_0004_m_000164_543: needsTaskCommit() Task attempt_202105162312241238051494122575784_0004_m_000164_543: duration 0:00.000s
[2021-05-16 20:14:49,051] {docker.py:276} INFO - 21/05/16 23:14:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241238051494122575784_0004_m_000164_543
[2021-05-16 20:14:49,052] {docker.py:276} INFO - 21/05/16 23:14:49 INFO Executor: Finished task 164.0 in stage 4.0 (TID 543). 4544 bytes result sent to driver
[2021-05-16 20:14:49,053] {docker.py:276} INFO - 21/05/16 23:14:49 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 547) (764b1db4ecfd, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:49,054] {docker.py:276} INFO - 21/05/16 23:14:49 INFO Executor: Running task 168.0 in stage 4.0 (TID 547)
[2021-05-16 20:14:49,055] {docker.py:276} INFO - 21/05/16 23:14:49 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 543) in 2796 ms on 764b1db4ecfd (executor driver) (165/200)
[2021-05-16 20:14:49,063] {docker.py:276} INFO - 21/05/16 23:14:49 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:49,065] {docker.py:276} INFO - 21/05/16 23:14:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241985276233698744419_0004_m_000168_547, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241985276233698744419_0004_m_000168_547}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241985276233698744419_0004}; taskId=attempt_202105162312241985276233698744419_0004_m_000168_547, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@73e6564}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312241985276233698744419_0004_m_000168_547: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241985276233698744419_0004_m_000168_547
[2021-05-16 20:14:49,068] {docker.py:276} INFO - 21/05/16 23:14:49 INFO StagingCommitter: Task committer attempt_202105162312241985276233698744419_0004_m_000168_547: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241985276233698744419_0004_m_000168_547 : duration 0:00.003s
[2021-05-16 20:14:49,513] {docker.py:276} INFO - 21/05/16 23:14:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312242470375565200037986_0004_m_000165_544: needsTaskCommit() Task attempt_202105162312242470375565200037986_0004_m_000165_544
21/05/16 23:14:49 INFO StagingCommitter: Task committer attempt_202105162312242470375565200037986_0004_m_000165_544: needsTaskCommit() Task attempt_202105162312242470375565200037986_0004_m_000165_544: duration 0:00.000s
21/05/16 23:14:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242470375565200037986_0004_m_000165_544
[2021-05-16 20:14:49,514] {docker.py:276} INFO - 21/05/16 23:14:49 INFO Executor: Finished task 165.0 in stage 4.0 (TID 544). 4544 bytes result sent to driver
[2021-05-16 20:14:49,514] {docker.py:276} INFO - 21/05/16 23:14:49 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 548) (764b1db4ecfd, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:49,515] {docker.py:276} INFO - 21/05/16 23:14:49 INFO Executor: Running task 169.0 in stage 4.0 (TID 548)
[2021-05-16 20:14:49,516] {docker.py:276} INFO - 21/05/16 23:14:49 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 544) in 2732 ms on 764b1db4ecfd (executor driver) (166/200)
[2021-05-16 20:14:49,524] {docker.py:276} INFO - 21/05/16 23:14:49 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:49,527] {docker.py:276} INFO - 21/05/16 23:14:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248304335801455165690_0004_m_000169_548, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248304335801455165690_0004_m_000169_548}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248304335801455165690_0004}; taskId=attempt_202105162312248304335801455165690_0004_m_000169_548, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12250b62}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:49 INFO StagingCommitter: Starting: Task committer attempt_202105162312248304335801455165690_0004_m_000169_548: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248304335801455165690_0004_m_000169_548
[2021-05-16 20:14:49,530] {docker.py:276} INFO - 21/05/16 23:14:49 INFO StagingCommitter: Task committer attempt_202105162312248304335801455165690_0004_m_000169_548: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248304335801455165690_0004_m_000169_548 : duration 0:00.002s
[2021-05-16 20:14:51,479] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312246907808585965085607_0004_m_000166_545: needsTaskCommit() Task attempt_202105162312246907808585965085607_0004_m_000166_545
[2021-05-16 20:14:51,480] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312246907808585965085607_0004_m_000166_545: needsTaskCommit() Task attempt_202105162312246907808585965085607_0004_m_000166_545: duration 0:00.001s
21/05/16 23:14:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246907808585965085607_0004_m_000166_545
[2021-05-16 20:14:51,481] {docker.py:276} INFO - 21/05/16 23:14:51 INFO Executor: Finished task 166.0 in stage 4.0 (TID 545). 4544 bytes result sent to driver
[2021-05-16 20:14:51,483] {docker.py:276} INFO - 21/05/16 23:14:51 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 549) (764b1db4ecfd, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:51 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 545) in 2750 ms on 764b1db4ecfd (executor driver) (167/200)
21/05/16 23:14:51 INFO Executor: Running task 170.0 in stage 4.0 (TID 549)
[2021-05-16 20:14:51,493] {docker.py:276} INFO - 21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Getting 6 (24.6 KiB) non-empty blocks including 6 (24.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:51,495] {docker.py:276} INFO - 21/05/16 23:14:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242173419151071982980_0004_m_000170_549, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242173419151071982980_0004_m_000170_549}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242173419151071982980_0004}; taskId=attempt_202105162312242173419151071982980_0004_m_000170_549, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5198229}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312242173419151071982980_0004_m_000170_549: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242173419151071982980_0004_m_000170_549
[2021-05-16 20:14:51,498] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312242173419151071982980_0004_m_000170_549: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242173419151071982980_0004_m_000170_549 : duration 0:00.003s
[2021-05-16 20:14:51,670] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312247793874968637743692_0004_m_000167_546: needsTaskCommit() Task attempt_202105162312247793874968637743692_0004_m_000167_546
[2021-05-16 20:14:51,671] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312247793874968637743692_0004_m_000167_546: needsTaskCommit() Task attempt_202105162312247793874968637743692_0004_m_000167_546: duration 0:00.001s
21/05/16 23:14:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247793874968637743692_0004_m_000167_546
[2021-05-16 20:14:51,673] {docker.py:276} INFO - 21/05/16 23:14:51 INFO Executor: Finished task 167.0 in stage 4.0 (TID 546). 4544 bytes result sent to driver
[2021-05-16 20:14:51,674] {docker.py:276} INFO - 21/05/16 23:14:51 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 550) (764b1db4ecfd, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:51,675] {docker.py:276} INFO - 21/05/16 23:14:51 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 546) in 2740 ms on 764b1db4ecfd (executor driver) (168/200)
[2021-05-16 20:14:51,676] {docker.py:276} INFO - 21/05/16 23:14:51 INFO Executor: Running task 171.0 in stage 4.0 (TID 550)
[2021-05-16 20:14:51,686] {docker.py:276} INFO - 21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:51,686] {docker.py:276} INFO - 21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:51,688] {docker.py:276} INFO - 21/05/16 23:14:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:51,688] {docker.py:276} INFO - 21/05/16 23:14:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:51,689] {docker.py:276} INFO - 21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:51,689] {docker.py:276} INFO - 21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241105668030561480992_0004_m_000171_550, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241105668030561480992_0004_m_000171_550}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241105668030561480992_0004}; taskId=attempt_202105162312241105668030561480992_0004_m_000171_550, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3afb4587}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:51,690] {docker.py:276} INFO - 21/05/16 23:14:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:51,690] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312241105668030561480992_0004_m_000171_550: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241105668030561480992_0004_m_000171_550
[2021-05-16 20:14:51,693] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312241105668030561480992_0004_m_000171_550: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241105668030561480992_0004_m_000171_550 : duration 0:00.003s
[2021-05-16 20:14:51,897] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312241985276233698744419_0004_m_000168_547: needsTaskCommit() Task attempt_202105162312241985276233698744419_0004_m_000168_547
21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312241985276233698744419_0004_m_000168_547: needsTaskCommit() Task attempt_202105162312241985276233698744419_0004_m_000168_547: duration 0:00.000s
21/05/16 23:14:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241985276233698744419_0004_m_000168_547
[2021-05-16 20:14:51,899] {docker.py:276} INFO - 21/05/16 23:14:51 INFO Executor: Finished task 168.0 in stage 4.0 (TID 547). 4544 bytes result sent to driver
[2021-05-16 20:14:51,900] {docker.py:276} INFO - 21/05/16 23:14:51 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 551) (764b1db4ecfd, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:51,901] {docker.py:276} INFO - 21/05/16 23:14:51 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 547) in 2851 ms on 764b1db4ecfd (executor driver) (169/200)
21/05/16 23:14:51 INFO Executor: Running task 172.0 in stage 4.0 (TID 551)
[2021-05-16 20:14:51,911] {docker.py:276} INFO - 21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:51,913] {docker.py:276} INFO - 21/05/16 23:14:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247170718529649454577_0004_m_000172_551, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170718529649454577_0004_m_000172_551}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247170718529649454577_0004}; taskId=attempt_202105162312247170718529649454577_0004_m_000172_551, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@116248a7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:51 INFO StagingCommitter: Starting: Task committer attempt_202105162312247170718529649454577_0004_m_000172_551: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170718529649454577_0004_m_000172_551
[2021-05-16 20:14:51,916] {docker.py:276} INFO - 21/05/16 23:14:51 INFO StagingCommitter: Task committer attempt_202105162312247170718529649454577_0004_m_000172_551: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247170718529649454577_0004_m_000172_551 : duration 0:00.003s
[2021-05-16 20:14:52,516] {docker.py:276} INFO - 21/05/16 23:14:52 INFO StagingCommitter: Starting: Task committer attempt_202105162312248304335801455165690_0004_m_000169_548: needsTaskCommit() Task attempt_202105162312248304335801455165690_0004_m_000169_548
21/05/16 23:14:52 INFO StagingCommitter: Task committer attempt_202105162312248304335801455165690_0004_m_000169_548: needsTaskCommit() Task attempt_202105162312248304335801455165690_0004_m_000169_548: duration 0:00.001s
21/05/16 23:14:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248304335801455165690_0004_m_000169_548
[2021-05-16 20:14:52,518] {docker.py:276} INFO - 21/05/16 23:14:52 INFO Executor: Finished task 169.0 in stage 4.0 (TID 548). 4544 bytes result sent to driver
[2021-05-16 20:14:52,519] {docker.py:276} INFO - 21/05/16 23:14:52 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 552) (764b1db4ecfd, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:52,520] {docker.py:276} INFO - 21/05/16 23:14:52 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 548) in 3009 ms on 764b1db4ecfd (executor driver) (170/200)
[2021-05-16 20:14:52,522] {docker.py:276} INFO - 21/05/16 23:14:52 INFO Executor: Running task 173.0 in stage 4.0 (TID 552)
[2021-05-16 20:14:52,531] {docker.py:276} INFO - 21/05/16 23:14:52 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:52,533] {docker.py:276} INFO - 21/05/16 23:14:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247840436764088689708_0004_m_000173_552, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247840436764088689708_0004_m_000173_552}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247840436764088689708_0004}; taskId=attempt_202105162312247840436764088689708_0004_m_000173_552, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4457f9c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:52 INFO StagingCommitter: Starting: Task committer attempt_202105162312247840436764088689708_0004_m_000173_552: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247840436764088689708_0004_m_000173_552
[2021-05-16 20:14:52,536] {docker.py:276} INFO - 21/05/16 23:14:52 INFO StagingCommitter: Task committer attempt_202105162312247840436764088689708_0004_m_000173_552: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247840436764088689708_0004_m_000173_552 : duration 0:00.003s
[2021-05-16 20:14:54,290] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312242173419151071982980_0004_m_000170_549: needsTaskCommit() Task attempt_202105162312242173419151071982980_0004_m_000170_549
21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312242173419151071982980_0004_m_000170_549: needsTaskCommit() Task attempt_202105162312242173419151071982980_0004_m_000170_549: duration 0:00.001s
21/05/16 23:14:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242173419151071982980_0004_m_000170_549
[2021-05-16 20:14:54,292] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Finished task 170.0 in stage 4.0 (TID 549). 4544 bytes result sent to driver
[2021-05-16 20:14:54,293] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 553) (764b1db4ecfd, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:54,295] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 549) in 2816 ms on 764b1db4ecfd (executor driver) (171/200)
[2021-05-16 20:14:54,296] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Running task 174.0 in stage 4.0 (TID 553)
[2021-05-16 20:14:54,306] {docker.py:276} INFO - 21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:54,307] {docker.py:276} INFO - 21/05/16 23:14:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242519210137247378333_0004_m_000174_553, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242519210137247378333_0004_m_000174_553}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242519210137247378333_0004}; taskId=attempt_202105162312242519210137247378333_0004_m_000174_553, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71a35a6d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312242519210137247378333_0004_m_000174_553: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242519210137247378333_0004_m_000174_553
[2021-05-16 20:14:54,310] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312242519210137247378333_0004_m_000174_553: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242519210137247378333_0004_m_000174_553 : duration 0:00.002s
[2021-05-16 20:14:54,417] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312241105668030561480992_0004_m_000171_550: needsTaskCommit() Task attempt_202105162312241105668030561480992_0004_m_000171_550
[2021-05-16 20:14:54,418] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312241105668030561480992_0004_m_000171_550: needsTaskCommit() Task attempt_202105162312241105668030561480992_0004_m_000171_550: duration 0:00.001s
21/05/16 23:14:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241105668030561480992_0004_m_000171_550
[2021-05-16 20:14:54,419] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Finished task 171.0 in stage 4.0 (TID 550). 4544 bytes result sent to driver
[2021-05-16 20:14:54,421] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 554) (764b1db4ecfd, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:54,421] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 550) in 2751 ms on 764b1db4ecfd (executor driver) (172/200)
[2021-05-16 20:14:54,422] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Running task 175.0 in stage 4.0 (TID 554)
[2021-05-16 20:14:54,430] {docker.py:276} INFO - 21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:54,432] {docker.py:276} INFO - 21/05/16 23:14:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242701374380861347303_0004_m_000175_554, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242701374380861347303_0004_m_000175_554}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242701374380861347303_0004}; taskId=attempt_202105162312242701374380861347303_0004_m_000175_554, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2a1c8940}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:54,432] {docker.py:276} INFO - 21/05/16 23:14:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312242701374380861347303_0004_m_000175_554: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242701374380861347303_0004_m_000175_554
[2021-05-16 20:14:54,435] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312242701374380861347303_0004_m_000175_554: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242701374380861347303_0004_m_000175_554 : duration 0:00.003s
[2021-05-16 20:14:54,660] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312247170718529649454577_0004_m_000172_551: needsTaskCommit() Task attempt_202105162312247170718529649454577_0004_m_000172_551
[2021-05-16 20:14:54,660] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312247170718529649454577_0004_m_000172_551: needsTaskCommit() Task attempt_202105162312247170718529649454577_0004_m_000172_551: duration 0:00.001s
21/05/16 23:14:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247170718529649454577_0004_m_000172_551
[2021-05-16 20:14:54,661] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Finished task 172.0 in stage 4.0 (TID 551). 4544 bytes result sent to driver
[2021-05-16 20:14:54,662] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 555) (764b1db4ecfd, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:54,663] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 551) in 2768 ms on 764b1db4ecfd (executor driver) (173/200)
21/05/16 23:14:54 INFO Executor: Running task 176.0 in stage 4.0 (TID 555)
[2021-05-16 20:14:54,673] {docker.py:276} INFO - 21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:54,674] {docker.py:276} INFO - 21/05/16 23:14:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248967945558001524926_0004_m_000176_555, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248967945558001524926_0004_m_000176_555}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248967945558001524926_0004}; taskId=attempt_202105162312248967945558001524926_0004_m_000176_555, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bd6105f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312248967945558001524926_0004_m_000176_555: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248967945558001524926_0004_m_000176_555
[2021-05-16 20:14:54,678] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312248967945558001524926_0004_m_000176_555: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248967945558001524926_0004_m_000176_555 : duration 0:00.003s
[2021-05-16 20:14:54,820] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312247840436764088689708_0004_m_000173_552: needsTaskCommit() Task attempt_202105162312247840436764088689708_0004_m_000173_552
[2021-05-16 20:14:54,821] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312247840436764088689708_0004_m_000173_552: needsTaskCommit() Task attempt_202105162312247840436764088689708_0004_m_000173_552: duration 0:00.000s
21/05/16 23:14:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247840436764088689708_0004_m_000173_552
[2021-05-16 20:14:54,823] {docker.py:276} INFO - 21/05/16 23:14:54 INFO Executor: Finished task 173.0 in stage 4.0 (TID 552). 4544 bytes result sent to driver
[2021-05-16 20:14:54,824] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 556) (764b1db4ecfd, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:14:54 INFO Executor: Running task 177.0 in stage 4.0 (TID 556)
[2021-05-16 20:14:54,825] {docker.py:276} INFO - 21/05/16 23:14:54 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 552) in 2309 ms on 764b1db4ecfd (executor driver) (174/200)
[2021-05-16 20:14:54,836] {docker.py:276} INFO - 21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:54,838] {docker.py:276} INFO - 21/05/16 23:14:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248115179637042014804_0004_m_000177_556, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248115179637042014804_0004_m_000177_556}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248115179637042014804_0004}; taskId=attempt_202105162312248115179637042014804_0004_m_000177_556, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f86dd57}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:54 INFO StagingCommitter: Starting: Task committer attempt_202105162312248115179637042014804_0004_m_000177_556: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248115179637042014804_0004_m_000177_556
[2021-05-16 20:14:54,841] {docker.py:276} INFO - 21/05/16 23:14:54 INFO StagingCommitter: Task committer attempt_202105162312248115179637042014804_0004_m_000177_556: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248115179637042014804_0004_m_000177_556 : duration 0:00.003s
[2021-05-16 20:14:57,060] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312242519210137247378333_0004_m_000174_553: needsTaskCommit() Task attempt_202105162312242519210137247378333_0004_m_000174_553
[2021-05-16 20:14:57,060] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Task committer attempt_202105162312242519210137247378333_0004_m_000174_553: needsTaskCommit() Task attempt_202105162312242519210137247378333_0004_m_000174_553: duration 0:00.001s
21/05/16 23:14:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242519210137247378333_0004_m_000174_553
[2021-05-16 20:14:57,062] {docker.py:276} INFO - 21/05/16 23:14:57 INFO Executor: Finished task 174.0 in stage 4.0 (TID 553). 4544 bytes result sent to driver
[2021-05-16 20:14:57,063] {docker.py:276} INFO - 21/05/16 23:14:57 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 557) (764b1db4ecfd, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:57,064] {docker.py:276} INFO - 21/05/16 23:14:57 INFO Executor: Running task 178.0 in stage 4.0 (TID 557)
21/05/16 23:14:57 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 553) in 2775 ms on 764b1db4ecfd (executor driver) (175/200)
[2021-05-16 20:14:57,071] {docker.py:276} INFO - 21/05/16 23:14:57 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:57,073] {docker.py:276} INFO - 21/05/16 23:14:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:57,074] {docker.py:276} INFO - 21/05/16 23:14:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248604502988487529739_0004_m_000178_557, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248604502988487529739_0004_m_000178_557}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248604502988487529739_0004}; taskId=attempt_202105162312248604502988487529739_0004_m_000178_557, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a1cf81a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:14:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:57,074] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312248604502988487529739_0004_m_000178_557: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248604502988487529739_0004_m_000178_557
[2021-05-16 20:14:57,077] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Task committer attempt_202105162312248604502988487529739_0004_m_000178_557: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248604502988487529739_0004_m_000178_557 : duration 0:00.004s
[2021-05-16 20:14:57,229] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312242701374380861347303_0004_m_000175_554: needsTaskCommit() Task attempt_202105162312242701374380861347303_0004_m_000175_554
[2021-05-16 20:14:57,230] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Task committer attempt_202105162312242701374380861347303_0004_m_000175_554: needsTaskCommit() Task attempt_202105162312242701374380861347303_0004_m_000175_554: duration 0:00.001s
21/05/16 23:14:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242701374380861347303_0004_m_000175_554
[2021-05-16 20:14:57,231] {docker.py:276} INFO - 21/05/16 23:14:57 INFO Executor: Finished task 175.0 in stage 4.0 (TID 554). 4544 bytes result sent to driver
[2021-05-16 20:14:57,232] {docker.py:276} INFO - 21/05/16 23:14:57 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 558) (764b1db4ecfd, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:57,233] {docker.py:276} INFO - 21/05/16 23:14:57 INFO Executor: Running task 179.0 in stage 4.0 (TID 558)
[2021-05-16 20:14:57,234] {docker.py:276} INFO - 21/05/16 23:14:57 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 554) in 2817 ms on 764b1db4ecfd (executor driver) (176/200)
[2021-05-16 20:14:57,243] {docker.py:276} INFO - 21/05/16 23:14:57 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:57,243] {docker.py:276} INFO - 21/05/16 23:14:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:57,245] {docker.py:276} INFO - 21/05/16 23:14:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:57,245] {docker.py:276} INFO - 21/05/16 23:14:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245107711435249884713_0004_m_000179_558, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245107711435249884713_0004_m_000179_558}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245107711435249884713_0004}; taskId=attempt_202105162312245107711435249884713_0004_m_000179_558, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74085fc1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:57,246] {docker.py:276} INFO - 21/05/16 23:14:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312245107711435249884713_0004_m_000179_558: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245107711435249884713_0004_m_000179_558
[2021-05-16 20:14:57,249] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Task committer attempt_202105162312245107711435249884713_0004_m_000179_558: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245107711435249884713_0004_m_000179_558 : duration 0:00.004s
[2021-05-16 20:14:57,971] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Starting: Task committer attempt_202105162312248967945558001524926_0004_m_000176_555: needsTaskCommit() Task attempt_202105162312248967945558001524926_0004_m_000176_555
[2021-05-16 20:14:57,972] {docker.py:276} INFO - 21/05/16 23:14:57 INFO StagingCommitter: Task committer attempt_202105162312248967945558001524926_0004_m_000176_555: needsTaskCommit() Task attempt_202105162312248967945558001524926_0004_m_000176_555: duration 0:00.001s
[2021-05-16 20:14:57,972] {docker.py:276} INFO - 21/05/16 23:14:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248967945558001524926_0004_m_000176_555
[2021-05-16 20:14:57,974] {docker.py:276} INFO - 21/05/16 23:14:58 INFO Executor: Finished task 176.0 in stage 4.0 (TID 555). 4544 bytes result sent to driver
[2021-05-16 20:14:57,975] {docker.py:276} INFO - 21/05/16 23:14:58 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 559) (764b1db4ecfd, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:57,976] {docker.py:276} INFO - 21/05/16 23:14:58 INFO Executor: Running task 180.0 in stage 4.0 (TID 559)
[2021-05-16 20:14:57,976] {docker.py:276} INFO - 21/05/16 23:14:58 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 555) in 3319 ms on 764b1db4ecfd (executor driver) (177/200)
[2021-05-16 20:14:57,984] {docker.py:276} INFO - 21/05/16 23:14:58 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:14:57,985] {docker.py:276} INFO - 21/05/16 23:14:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:57,986] {docker.py:276} INFO - 21/05/16 23:14:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:57,987] {docker.py:276} INFO - 21/05/16 23:14:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:57,987] {docker.py:276} INFO - 21/05/16 23:14:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:57,987] {docker.py:276} INFO - 21/05/16 23:14:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244436509685742243393_0004_m_000180_559, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244436509685742243393_0004_m_000180_559}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244436509685742243393_0004}; taskId=attempt_202105162312244436509685742243393_0004_m_000180_559, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3886f061}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:57,988] {docker.py:276} INFO - 21/05/16 23:14:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:57,988] {docker.py:276} INFO - 21/05/16 23:14:58 INFO StagingCommitter: Starting: Task committer attempt_202105162312244436509685742243393_0004_m_000180_559: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244436509685742243393_0004_m_000180_559
[2021-05-16 20:14:57,990] {docker.py:276} INFO - 21/05/16 23:14:58 INFO StagingCommitter: Task committer attempt_202105162312244436509685742243393_0004_m_000180_559: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244436509685742243393_0004_m_000180_559 : duration 0:00.003s
[2021-05-16 20:14:58,224] {docker.py:276} INFO - 21/05/16 23:14:58 INFO StagingCommitter: Starting: Task committer attempt_202105162312248115179637042014804_0004_m_000177_556: needsTaskCommit() Task attempt_202105162312248115179637042014804_0004_m_000177_556
[2021-05-16 20:14:58,226] {docker.py:276} INFO - 21/05/16 23:14:58 INFO StagingCommitter: Task committer attempt_202105162312248115179637042014804_0004_m_000177_556: needsTaskCommit() Task attempt_202105162312248115179637042014804_0004_m_000177_556: duration 0:00.002s
21/05/16 23:14:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248115179637042014804_0004_m_000177_556
[2021-05-16 20:14:58,229] {docker.py:276} INFO - 21/05/16 23:14:58 INFO Executor: Finished task 177.0 in stage 4.0 (TID 556). 4544 bytes result sent to driver
[2021-05-16 20:14:58,230] {docker.py:276} INFO - 21/05/16 23:14:58 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 560) (764b1db4ecfd, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:58,232] {docker.py:276} INFO - 21/05/16 23:14:58 INFO Executor: Running task 181.0 in stage 4.0 (TID 560)
[2021-05-16 20:14:58,233] {docker.py:276} INFO - 21/05/16 23:14:58 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 556) in 3413 ms on 764b1db4ecfd (executor driver) (178/200)
[2021-05-16 20:14:58,243] {docker.py:276} INFO - 21/05/16 23:14:58 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:58,244] {docker.py:276} INFO - 21/05/16 23:14:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:14:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:14:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:14:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247482198501090622314_0004_m_000181_560, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247482198501090622314_0004_m_000181_560}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247482198501090622314_0004}; taskId=attempt_202105162312247482198501090622314_0004_m_000181_560, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79cec35d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:58,245] {docker.py:276} INFO - 21/05/16 23:14:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:14:58 INFO StagingCommitter: Starting: Task committer attempt_202105162312247482198501090622314_0004_m_000181_560: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247482198501090622314_0004_m_000181_560
[2021-05-16 20:14:58,248] {docker.py:276} INFO - 21/05/16 23:14:58 INFO StagingCommitter: Task committer attempt_202105162312247482198501090622314_0004_m_000181_560: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247482198501090622314_0004_m_000181_560 : duration 0:00.003s
[2021-05-16 20:14:59,886] {docker.py:276} INFO - 21/05/16 23:14:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312248604502988487529739_0004_m_000178_557: needsTaskCommit() Task attempt_202105162312248604502988487529739_0004_m_000178_557
[2021-05-16 20:14:59,887] {docker.py:276} INFO - 21/05/16 23:14:59 INFO StagingCommitter: Task committer attempt_202105162312248604502988487529739_0004_m_000178_557: needsTaskCommit() Task attempt_202105162312248604502988487529739_0004_m_000178_557: duration 0:00.001s
21/05/16 23:14:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248604502988487529739_0004_m_000178_557
[2021-05-16 20:14:59,888] {docker.py:276} INFO - 21/05/16 23:14:59 INFO Executor: Finished task 178.0 in stage 4.0 (TID 557). 4544 bytes result sent to driver
[2021-05-16 20:14:59,889] {docker.py:276} INFO - 21/05/16 23:14:59 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 561) (764b1db4ecfd, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:14:59,890] {docker.py:276} INFO - 21/05/16 23:14:59 INFO Executor: Running task 182.0 in stage 4.0 (TID 561)
21/05/16 23:14:59 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 557) in 2831 ms on 764b1db4ecfd (executor driver) (179/200)
[2021-05-16 20:14:59,898] {docker.py:276} INFO - 21/05/16 23:14:59 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:14:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:14:59,901] {docker.py:276} INFO - 21/05/16 23:14:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-16 20:14:59,902] {docker.py:276} INFO - 21/05/16 23:14:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:14:59,903] {docker.py:276} INFO - 21/05/16 23:14:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:59,903] {docker.py:276} INFO - 21/05/16 23:14:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247386008736669529644_0004_m_000182_561, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247386008736669529644_0004_m_000182_561}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247386008736669529644_0004}; taskId=attempt_202105162312247386008736669529644_0004_m_000182_561, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@345aeea2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:14:59,903] {docker.py:276} INFO - 21/05/16 23:14:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:14:59,904] {docker.py:276} INFO - 21/05/16 23:14:59 INFO StagingCommitter: Starting: Task committer attempt_202105162312247386008736669529644_0004_m_000182_561: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247386008736669529644_0004_m_000182_561
[2021-05-16 20:14:59,906] {docker.py:276} INFO - 21/05/16 23:14:59 INFO StagingCommitter: Task committer attempt_202105162312247386008736669529644_0004_m_000182_561: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247386008736669529644_0004_m_000182_561 : duration 0:00.003s
[2021-05-16 20:15:00,950] {docker.py:276} INFO - 21/05/16 23:15:00 INFO StagingCommitter: Starting: Task committer attempt_202105162312245107711435249884713_0004_m_000179_558: needsTaskCommit() Task attempt_202105162312245107711435249884713_0004_m_000179_558
[2021-05-16 20:15:00,952] {docker.py:276} INFO - 21/05/16 23:15:00 INFO StagingCommitter: Task committer attempt_202105162312245107711435249884713_0004_m_000179_558: needsTaskCommit() Task attempt_202105162312245107711435249884713_0004_m_000179_558: duration 0:00.001s
21/05/16 23:15:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245107711435249884713_0004_m_000179_558
[2021-05-16 20:15:00,958] {docker.py:276} INFO - 21/05/16 23:15:00 INFO Executor: Finished task 179.0 in stage 4.0 (TID 558). 4544 bytes result sent to driver
[2021-05-16 20:15:00,960] {docker.py:276} INFO - 21/05/16 23:15:00 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 562) (764b1db4ecfd, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/16 23:15:00 INFO Executor: Running task 183.0 in stage 4.0 (TID 562)
21/05/16 23:15:00 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 558) in 3728 ms on 764b1db4ecfd (executor driver) (180/200)
[2021-05-16 20:15:00,974] {docker.py:276} INFO - 21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:00,976] {docker.py:276} INFO - 21/05/16 23:15:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:00,977] {docker.py:276} INFO - 21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248252328825863736236_0004_m_000183_562, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248252328825863736236_0004_m_000183_562}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248252328825863736236_0004}; taskId=attempt_202105162312248252328825863736236_0004_m_000183_562, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@191f707}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312248252328825863736236_0004_m_000183_562: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248252328825863736236_0004_m_000183_562
[2021-05-16 20:15:00,980] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Task committer attempt_202105162312248252328825863736236_0004_m_000183_562: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248252328825863736236_0004_m_000183_562 : duration 0:00.004s
[2021-05-16 20:15:01,052] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312244436509685742243393_0004_m_000180_559: needsTaskCommit() Task attempt_202105162312244436509685742243393_0004_m_000180_559
[2021-05-16 20:15:01,053] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Task committer attempt_202105162312244436509685742243393_0004_m_000180_559: needsTaskCommit() Task attempt_202105162312244436509685742243393_0004_m_000180_559: duration 0:00.001s
21/05/16 23:15:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244436509685742243393_0004_m_000180_559
[2021-05-16 20:15:01,055] {docker.py:276} INFO - 21/05/16 23:15:01 INFO Executor: Finished task 180.0 in stage 4.0 (TID 559). 4544 bytes result sent to driver
[2021-05-16 20:15:01,056] {docker.py:276} INFO - 21/05/16 23:15:01 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 563) (764b1db4ecfd, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:01,059] {docker.py:276} INFO - 21/05/16 23:15:01 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 559) in 3086 ms on 764b1db4ecfd (executor driver) (181/200)
[2021-05-16 20:15:01,060] {docker.py:276} INFO - 21/05/16 23:15:01 INFO Executor: Running task 184.0 in stage 4.0 (TID 563)
[2021-05-16 20:15:01,069] {docker.py:276} INFO - 21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Getting 6 (25.1 KiB) non-empty blocks including 6 (25.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:01,071] {docker.py:276} INFO - 21/05/16 23:15:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:01,071] {docker.py:276} INFO - 21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244537060852391713620_0004_m_000184_563, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244537060852391713620_0004_m_000184_563}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244537060852391713620_0004}; taskId=attempt_202105162312244537060852391713620_0004_m_000184_563, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1920882f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:01,071] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312244537060852391713620_0004_m_000184_563: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244537060852391713620_0004_m_000184_563
[2021-05-16 20:15:01,073] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Task committer attempt_202105162312244537060852391713620_0004_m_000184_563: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244537060852391713620_0004_m_000184_563 : duration 0:00.003s
[2021-05-16 20:15:01,381] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Starting: Task committer attempt_202105162312247482198501090622314_0004_m_000181_560: needsTaskCommit() Task attempt_202105162312247482198501090622314_0004_m_000181_560
[2021-05-16 20:15:01,383] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Task committer attempt_202105162312247482198501090622314_0004_m_000181_560: needsTaskCommit() Task attempt_202105162312247482198501090622314_0004_m_000181_560: duration 0:00.002s
21/05/16 23:15:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247482198501090622314_0004_m_000181_560
[2021-05-16 20:15:01,384] {docker.py:276} INFO - 21/05/16 23:15:01 INFO Executor: Finished task 181.0 in stage 4.0 (TID 560). 4544 bytes result sent to driver
[2021-05-16 20:15:01,386] {docker.py:276} INFO - 21/05/16 23:15:01 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 564) (764b1db4ecfd, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:01,388] {docker.py:276} INFO - 21/05/16 23:15:01 INFO Executor: Running task 185.0 in stage 4.0 (TID 564)
21/05/16 23:15:01 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 560) in 3162 ms on 764b1db4ecfd (executor driver) (182/200)
[2021-05-16 20:15:01,403] {docker.py:276} INFO - 21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:01,404] {docker.py:276} INFO - 21/05/16 23:15:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224149476764871128965_0004_m_000185_564, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224149476764871128965_0004_m_000185_564}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224149476764871128965_0004}; taskId=attempt_20210516231224149476764871128965_0004_m_000185_564, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3cd00959}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:01,405] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Starting: Task committer attempt_20210516231224149476764871128965_0004_m_000185_564: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224149476764871128965_0004_m_000185_564
[2021-05-16 20:15:01,408] {docker.py:276} INFO - 21/05/16 23:15:01 INFO StagingCommitter: Task committer attempt_20210516231224149476764871128965_0004_m_000185_564: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224149476764871128965_0004_m_000185_564 : duration 0:00.003s
[2021-05-16 20:15:03,163] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312247386008736669529644_0004_m_000182_561: needsTaskCommit() Task attempt_202105162312247386008736669529644_0004_m_000182_561
[2021-05-16 20:15:03,164] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312247386008736669529644_0004_m_000182_561: needsTaskCommit() Task attempt_202105162312247386008736669529644_0004_m_000182_561: duration 0:00.000s
[2021-05-16 20:15:03,164] {docker.py:276} INFO - 21/05/16 23:15:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247386008736669529644_0004_m_000182_561
[2021-05-16 20:15:03,165] {docker.py:276} INFO - 21/05/16 23:15:03 INFO Executor: Finished task 182.0 in stage 4.0 (TID 561). 4544 bytes result sent to driver
[2021-05-16 20:15:03,166] {docker.py:276} INFO - 21/05/16 23:15:03 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 565) (764b1db4ecfd, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:03,166] {docker.py:276} INFO - 21/05/16 23:15:03 INFO Executor: Running task 186.0 in stage 4.0 (TID 565)
21/05/16 23:15:03 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 561) in 3283 ms on 764b1db4ecfd (executor driver) (183/200)
[2021-05-16 20:15:03,176] {docker.py:276} INFO - 21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:03,177] {docker.py:276} INFO - 21/05/16 23:15:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247862794311723928508_0004_m_000186_565, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247862794311723928508_0004_m_000186_565}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247862794311723928508_0004}; taskId=attempt_202105162312247862794311723928508_0004_m_000186_565, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36e327ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312247862794311723928508_0004_m_000186_565: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247862794311723928508_0004_m_000186_565
[2021-05-16 20:15:03,180] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312247862794311723928508_0004_m_000186_565: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247862794311723928508_0004_m_000186_565 : duration 0:00.003s
[2021-05-16 20:15:03,810] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312248252328825863736236_0004_m_000183_562: needsTaskCommit() Task attempt_202105162312248252328825863736236_0004_m_000183_562
[2021-05-16 20:15:03,811] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312248252328825863736236_0004_m_000183_562: needsTaskCommit() Task attempt_202105162312248252328825863736236_0004_m_000183_562: duration 0:00.000s
21/05/16 23:15:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248252328825863736236_0004_m_000183_562
[2021-05-16 20:15:03,812] {docker.py:276} INFO - 21/05/16 23:15:03 INFO Executor: Finished task 183.0 in stage 4.0 (TID 562). 4544 bytes result sent to driver
[2021-05-16 20:15:03,813] {docker.py:276} INFO - 21/05/16 23:15:03 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 566) (764b1db4ecfd, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:03,814] {docker.py:276} INFO - 21/05/16 23:15:03 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 562) in 2863 ms on 764b1db4ecfd (executor driver) (184/200)
[2021-05-16 20:15:03,815] {docker.py:276} INFO - 21/05/16 23:15:03 INFO Executor: Running task 187.0 in stage 4.0 (TID 566)
[2021-05-16 20:15:03,824] {docker.py:276} INFO - 21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Getting 6 (25.5 KiB) non-empty blocks including 6 (25.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:03,826] {docker.py:276} INFO - 21/05/16 23:15:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:03,827] {docker.py:276} INFO - 21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241122377430297208778_0004_m_000187_566, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241122377430297208778_0004_m_000187_566}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241122377430297208778_0004}; taskId=attempt_202105162312241122377430297208778_0004_m_000187_566, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@114adfe7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:03,827] {docker.py:276} INFO - 21/05/16 23:15:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:03,828] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312241122377430297208778_0004_m_000187_566: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241122377430297208778_0004_m_000187_566
[2021-05-16 20:15:03,831] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312241122377430297208778_0004_m_000187_566: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241122377430297208778_0004_m_000187_566 : duration 0:00.004s
[2021-05-16 20:15:03,880] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312244537060852391713620_0004_m_000184_563: needsTaskCommit() Task attempt_202105162312244537060852391713620_0004_m_000184_563
[2021-05-16 20:15:03,881] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312244537060852391713620_0004_m_000184_563: needsTaskCommit() Task attempt_202105162312244537060852391713620_0004_m_000184_563: duration 0:00.001s
21/05/16 23:15:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244537060852391713620_0004_m_000184_563
[2021-05-16 20:15:03,882] {docker.py:276} INFO - 21/05/16 23:15:03 INFO Executor: Finished task 184.0 in stage 4.0 (TID 563). 4544 bytes result sent to driver
[2021-05-16 20:15:03,884] {docker.py:276} INFO - 21/05/16 23:15:03 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 567) (764b1db4ecfd, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:03,886] {docker.py:276} INFO - 21/05/16 23:15:03 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 563) in 2832 ms on 764b1db4ecfd (executor driver) (185/200)
21/05/16 23:15:03 INFO Executor: Running task 188.0 in stage 4.0 (TID 567)
[2021-05-16 20:15:03,896] {docker.py:276} INFO - 21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Getting 6 (25.0 KiB) non-empty blocks including 6 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:03,898] {docker.py:276} INFO - 21/05/16 23:15:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:03,899] {docker.py:276} INFO - 21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:03,899] {docker.py:276} INFO - 21/05/16 23:15:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312242587314598431996127_0004_m_000188_567, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242587314598431996127_0004_m_000188_567}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312242587314598431996127_0004}; taskId=attempt_202105162312242587314598431996127_0004_m_000188_567, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41f7a6e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:03,900] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Starting: Task committer attempt_202105162312242587314598431996127_0004_m_000188_567: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242587314598431996127_0004_m_000188_567
[2021-05-16 20:15:03,903] {docker.py:276} INFO - 21/05/16 23:15:03 INFO StagingCommitter: Task committer attempt_202105162312242587314598431996127_0004_m_000188_567: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312242587314598431996127_0004_m_000188_567 : duration 0:00.003s
[2021-05-16 20:15:04,459] {docker.py:276} INFO - 21/05/16 23:15:04 INFO StagingCommitter: Starting: Task committer attempt_20210516231224149476764871128965_0004_m_000185_564: needsTaskCommit() Task attempt_20210516231224149476764871128965_0004_m_000185_564
21/05/16 23:15:04 INFO StagingCommitter: Task committer attempt_20210516231224149476764871128965_0004_m_000185_564: needsTaskCommit() Task attempt_20210516231224149476764871128965_0004_m_000185_564: duration 0:00.000s
21/05/16 23:15:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224149476764871128965_0004_m_000185_564
[2021-05-16 20:15:04,461] {docker.py:276} INFO - 21/05/16 23:15:04 INFO Executor: Finished task 185.0 in stage 4.0 (TID 564). 4544 bytes result sent to driver
[2021-05-16 20:15:04,462] {docker.py:276} INFO - 21/05/16 23:15:04 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 568) (764b1db4ecfd, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:04,463] {docker.py:276} INFO - 21/05/16 23:15:04 INFO Executor: Running task 189.0 in stage 4.0 (TID 568)
[2021-05-16 20:15:04,464] {docker.py:276} INFO - 21/05/16 23:15:04 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 564) in 3082 ms on 764b1db4ecfd (executor driver) (186/200)
[2021-05-16 20:15:04,473] {docker.py:276} INFO - 21/05/16 23:15:04 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:04,475] {docker.py:276} INFO - 21/05/16 23:15:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:04,475] {docker.py:276} INFO - 21/05/16 23:15:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:04,476] {docker.py:276} INFO - 21/05/16 23:15:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246606401185307953722_0004_m_000189_568, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246606401185307953722_0004_m_000189_568}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246606401185307953722_0004}; taskId=attempt_202105162312246606401185307953722_0004_m_000189_568, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@132895da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:04,476] {docker.py:276} INFO - 21/05/16 23:15:04 INFO StagingCommitter: Starting: Task committer attempt_202105162312246606401185307953722_0004_m_000189_568: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246606401185307953722_0004_m_000189_568
[2021-05-16 20:15:04,478] {docker.py:276} INFO - 21/05/16 23:15:04 INFO StagingCommitter: Task committer attempt_202105162312246606401185307953722_0004_m_000189_568: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246606401185307953722_0004_m_000189_568 : duration 0:00.003s
[2021-05-16 20:15:06,220] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312247862794311723928508_0004_m_000186_565: needsTaskCommit() Task attempt_202105162312247862794311723928508_0004_m_000186_565
[2021-05-16 20:15:06,221] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312247862794311723928508_0004_m_000186_565: needsTaskCommit() Task attempt_202105162312247862794311723928508_0004_m_000186_565: duration 0:00.001s
[2021-05-16 20:15:06,221] {docker.py:276} INFO - 21/05/16 23:15:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247862794311723928508_0004_m_000186_565
[2021-05-16 20:15:06,223] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Finished task 186.0 in stage 4.0 (TID 565). 4587 bytes result sent to driver
[2021-05-16 20:15:06,224] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 569) (764b1db4ecfd, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:06,225] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Running task 190.0 in stage 4.0 (TID 569)
[2021-05-16 20:15:06,225] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 565) in 3025 ms on 764b1db4ecfd (executor driver) (187/200)
[2021-05-16 20:15:06,233] {docker.py:276} INFO - 21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:06,236] {docker.py:276} INFO - 21/05/16 23:15:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312244092381340247269227_0004_m_000190_569, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244092381340247269227_0004_m_000190_569}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312244092381340247269227_0004}; taskId=attempt_202105162312244092381340247269227_0004_m_000190_569, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@629359cd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:06,236] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312244092381340247269227_0004_m_000190_569: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244092381340247269227_0004_m_000190_569
[2021-05-16 20:15:06,239] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312244092381340247269227_0004_m_000190_569: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312244092381340247269227_0004_m_000190_569 : duration 0:00.003s
[2021-05-16 20:15:06,547] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312241122377430297208778_0004_m_000187_566: needsTaskCommit() Task attempt_202105162312241122377430297208778_0004_m_000187_566
[2021-05-16 20:15:06,548] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312241122377430297208778_0004_m_000187_566: needsTaskCommit() Task attempt_202105162312241122377430297208778_0004_m_000187_566: duration 0:00.001s
[2021-05-16 20:15:06,550] {docker.py:276} INFO - 21/05/16 23:15:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241122377430297208778_0004_m_000187_566
[2021-05-16 20:15:06,553] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Finished task 187.0 in stage 4.0 (TID 566). 4587 bytes result sent to driver
[2021-05-16 20:15:06,555] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 570) (764b1db4ecfd, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:06,556] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 566) in 2710 ms on 764b1db4ecfd (executor driver) (188/200)
[2021-05-16 20:15:06,556] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Running task 191.0 in stage 4.0 (TID 570)
[2021-05-16 20:15:06,565] {docker.py:276} INFO - 21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:06,567] {docker.py:276} INFO - 21/05/16 23:15:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245153307625225529220_0004_m_000191_570, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245153307625225529220_0004_m_000191_570}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245153307625225529220_0004}; taskId=attempt_202105162312245153307625225529220_0004_m_000191_570, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a2fa882}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:06,568] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312245153307625225529220_0004_m_000191_570: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245153307625225529220_0004_m_000191_570
[2021-05-16 20:15:06,570] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312245153307625225529220_0004_m_000191_570: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245153307625225529220_0004_m_000191_570 : duration 0:00.003s
[2021-05-16 20:15:06,675] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312242587314598431996127_0004_m_000188_567: needsTaskCommit() Task attempt_202105162312242587314598431996127_0004_m_000188_567
[2021-05-16 20:15:06,676] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312242587314598431996127_0004_m_000188_567: needsTaskCommit() Task attempt_202105162312242587314598431996127_0004_m_000188_567: duration 0:00.001s
21/05/16 23:15:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312242587314598431996127_0004_m_000188_567
[2021-05-16 20:15:06,678] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Finished task 188.0 in stage 4.0 (TID 567). 4587 bytes result sent to driver
[2021-05-16 20:15:06,679] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 571) (764b1db4ecfd, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:06,680] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 567) in 2763 ms on 764b1db4ecfd (executor driver) (189/200)
[2021-05-16 20:15:06,681] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Running task 192.0 in stage 4.0 (TID 571)
[2021-05-16 20:15:06,691] {docker.py:276} INFO - 21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:06,693] {docker.py:276} INFO - 21/05/16 23:15:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:06,693] {docker.py:276} INFO - 21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312241426937454899843392_0004_m_000192_571, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241426937454899843392_0004_m_000192_571}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312241426937454899843392_0004}; taskId=attempt_202105162312241426937454899843392_0004_m_000192_571, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7a96ed9c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312241426937454899843392_0004_m_000192_571: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241426937454899843392_0004_m_000192_571
[2021-05-16 20:15:06,696] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312241426937454899843392_0004_m_000192_571: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312241426937454899843392_0004_m_000192_571 : duration 0:00.003s
[2021-05-16 20:15:06,859] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312246606401185307953722_0004_m_000189_568: needsTaskCommit() Task attempt_202105162312246606401185307953722_0004_m_000189_568
[2021-05-16 20:15:06,861] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312246606401185307953722_0004_m_000189_568: needsTaskCommit() Task attempt_202105162312246606401185307953722_0004_m_000189_568: duration 0:00.001s
21/05/16 23:15:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246606401185307953722_0004_m_000189_568
[2021-05-16 20:15:06,863] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Finished task 189.0 in stage 4.0 (TID 568). 4587 bytes result sent to driver
[2021-05-16 20:15:06,865] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 572) (764b1db4ecfd, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:06,866] {docker.py:276} INFO - 21/05/16 23:15:06 INFO Executor: Running task 193.0 in stage 4.0 (TID 572)
[2021-05-16 20:15:06,867] {docker.py:276} INFO - 21/05/16 23:15:06 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 568) in 2371 ms on 764b1db4ecfd (executor driver) (190/200)
[2021-05-16 20:15:06,877] {docker.py:276} INFO - 21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Getting 6 (26.2 KiB) non-empty blocks including 6 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:06,878] {docker.py:276} INFO - 21/05/16 23:15:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:06,878] {docker.py:276} INFO - 21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312246751984745232191868_0004_m_000193_572, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246751984745232191868_0004_m_000193_572}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312246751984745232191868_0004}; taskId=attempt_202105162312246751984745232191868_0004_m_000193_572, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bfadf06}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:06 INFO StagingCommitter: Starting: Task committer attempt_202105162312246751984745232191868_0004_m_000193_572: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246751984745232191868_0004_m_000193_572
[2021-05-16 20:15:06,881] {docker.py:276} INFO - 21/05/16 23:15:06 INFO StagingCommitter: Task committer attempt_202105162312246751984745232191868_0004_m_000193_572: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312246751984745232191868_0004_m_000193_572 : duration 0:00.003s
[2021-05-16 20:15:09,035] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312244092381340247269227_0004_m_000190_569: needsTaskCommit() Task attempt_202105162312244092381340247269227_0004_m_000190_569
[2021-05-16 20:15:09,036] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_202105162312244092381340247269227_0004_m_000190_569: needsTaskCommit() Task attempt_202105162312244092381340247269227_0004_m_000190_569: duration 0:00.000s
21/05/16 23:15:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312244092381340247269227_0004_m_000190_569
[2021-05-16 20:15:09,037] {docker.py:276} INFO - 21/05/16 23:15:09 INFO Executor: Finished task 190.0 in stage 4.0 (TID 569). 4544 bytes result sent to driver
[2021-05-16 20:15:09,039] {docker.py:276} INFO - 21/05/16 23:15:09 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 573) (764b1db4ecfd, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:09,040] {docker.py:276} INFO - 21/05/16 23:15:09 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 569) in 2820 ms on 764b1db4ecfd (executor driver) (191/200)
[2021-05-16 20:15:09,041] {docker.py:276} INFO - 21/05/16 23:15:09 INFO Executor: Running task 194.0 in stage 4.0 (TID 573)
[2021-05-16 20:15:09,051] {docker.py:276} INFO - 21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:09,052] {docker.py:276} INFO - 21/05/16 23:15:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:09,053] {docker.py:276} INFO - 21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312245608384557457927207_0004_m_000194_573, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245608384557457927207_0004_m_000194_573}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312245608384557457927207_0004}; taskId=attempt_202105162312245608384557457927207_0004_m_000194_573, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@64a780ab}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:09,053] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312245608384557457927207_0004_m_000194_573: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245608384557457927207_0004_m_000194_573
[2021-05-16 20:15:09,056] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_202105162312245608384557457927207_0004_m_000194_573: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312245608384557457927207_0004_m_000194_573 : duration 0:00.003s
[2021-05-16 20:15:09,204] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312246751984745232191868_0004_m_000193_572: needsTaskCommit() Task attempt_202105162312246751984745232191868_0004_m_000193_572
[2021-05-16 20:15:09,205] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_202105162312246751984745232191868_0004_m_000193_572: needsTaskCommit() Task attempt_202105162312246751984745232191868_0004_m_000193_572: duration 0:00.001s
21/05/16 23:15:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312246751984745232191868_0004_m_000193_572
[2021-05-16 20:15:09,206] {docker.py:276} INFO - 21/05/16 23:15:09 INFO Executor: Finished task 193.0 in stage 4.0 (TID 572). 4544 bytes result sent to driver
[2021-05-16 20:15:09,207] {docker.py:276} INFO - 21/05/16 23:15:09 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 574) (764b1db4ecfd, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:09,208] {docker.py:276} INFO - 21/05/16 23:15:09 INFO Executor: Running task 195.0 in stage 4.0 (TID 574)
21/05/16 23:15:09 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 572) in 2346 ms on 764b1db4ecfd (executor driver) (192/200)
[2021-05-16 20:15:09,216] {docker.py:276} INFO - 21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Getting 6 (27.1 KiB) non-empty blocks including 6 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:09,218] {docker.py:276} INFO - 21/05/16 23:15:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:09,219] {docker.py:276} INFO - 21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312247314076116226012759_0004_m_000195_574, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247314076116226012759_0004_m_000195_574}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312247314076116226012759_0004}; taskId=attempt_202105162312247314076116226012759_0004_m_000195_574, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ed114f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312247314076116226012759_0004_m_000195_574: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247314076116226012759_0004_m_000195_574
[2021-05-16 20:15:09,222] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_202105162312247314076116226012759_0004_m_000195_574: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312247314076116226012759_0004_m_000195_574 : duration 0:00.003s
[2021-05-16 20:15:09,520] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_202105162312245153307625225529220_0004_m_000191_570: needsTaskCommit() Task attempt_202105162312245153307625225529220_0004_m_000191_570
[2021-05-16 20:15:09,521] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_202105162312245153307625225529220_0004_m_000191_570: needsTaskCommit() Task attempt_202105162312245153307625225529220_0004_m_000191_570: duration 0:00.001s
[2021-05-16 20:15:09,521] {docker.py:276} INFO - 21/05/16 23:15:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245153307625225529220_0004_m_000191_570
[2021-05-16 20:15:09,523] {docker.py:276} INFO - 21/05/16 23:15:09 INFO Executor: Finished task 191.0 in stage 4.0 (TID 570). 4544 bytes result sent to driver
[2021-05-16 20:15:09,523] {docker.py:276} INFO - 21/05/16 23:15:09 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 575) (764b1db4ecfd, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:09,524] {docker.py:276} INFO - 21/05/16 23:15:09 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 570) in 2973 ms on 764b1db4ecfd (executor driver) (193/200)
21/05/16 23:15:09 INFO Executor: Running task 196.0 in stage 4.0 (TID 575)
[2021-05-16 20:15:09,535] {docker.py:276} INFO - 21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Getting 6 (25.4 KiB) non-empty blocks including 6 (25.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:09,536] {docker.py:276} INFO - 21/05/16 23:15:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210516231224973922259930935854_0004_m_000196_575, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224973922259930935854_0004_m_000196_575}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210516231224973922259930935854_0004}; taskId=attempt_20210516231224973922259930935854_0004_m_000196_575, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1a04c01}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:09,537] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Starting: Task committer attempt_20210516231224973922259930935854_0004_m_000196_575: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224973922259930935854_0004_m_000196_575
[2021-05-16 20:15:09,540] {docker.py:276} INFO - 21/05/16 23:15:09 INFO StagingCommitter: Task committer attempt_20210516231224973922259930935854_0004_m_000196_575: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_20210516231224973922259930935854_0004_m_000196_575 : duration 0:00.004s
[2021-05-16 20:15:10,765] {docker.py:276} INFO - 21/05/16 23:15:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312241426937454899843392_0004_m_000192_571: needsTaskCommit() Task attempt_202105162312241426937454899843392_0004_m_000192_571
[2021-05-16 20:15:10,766] {docker.py:276} INFO - 21/05/16 23:15:10 INFO StagingCommitter: Task committer attempt_202105162312241426937454899843392_0004_m_000192_571: needsTaskCommit() Task attempt_202105162312241426937454899843392_0004_m_000192_571: duration 0:00.001s
21/05/16 23:15:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312241426937454899843392_0004_m_000192_571
[2021-05-16 20:15:10,767] {docker.py:276} INFO - 21/05/16 23:15:10 INFO Executor: Finished task 192.0 in stage 4.0 (TID 571). 4544 bytes result sent to driver
[2021-05-16 20:15:10,768] {docker.py:276} INFO - 21/05/16 23:15:10 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 576) (764b1db4ecfd, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:10,769] {docker.py:276} INFO - 21/05/16 23:15:10 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 571) in 4095 ms on 764b1db4ecfd (executor driver) (194/200)
[2021-05-16 20:15:10,771] {docker.py:276} INFO - 21/05/16 23:15:10 INFO Executor: Running task 197.0 in stage 4.0 (TID 576)
[2021-05-16 20:15:10,782] {docker.py:276} INFO - 21/05/16 23:15:10 INFO ShuffleBlockFetcherIterator: Getting 6 (26.6 KiB) non-empty blocks including 6 (26.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/16 23:15:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:10,784] {docker.py:276} INFO - 21/05/16 23:15:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248467690514076100540_0004_m_000197_576, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248467690514076100540_0004_m_000197_576}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248467690514076100540_0004}; taskId=attempt_202105162312248467690514076100540_0004_m_000197_576, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7e2c3992}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/16 23:15:10 INFO StagingCommitter: Starting: Task committer attempt_202105162312248467690514076100540_0004_m_000197_576: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248467690514076100540_0004_m_000197_576
[2021-05-16 20:15:10,787] {docker.py:276} INFO - 21/05/16 23:15:10 INFO StagingCommitter: Task committer attempt_202105162312248467690514076100540_0004_m_000197_576: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248467690514076100540_0004_m_000197_576 : duration 0:00.003s
[2021-05-16 20:15:12,281] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Starting: Task committer attempt_20210516231224973922259930935854_0004_m_000196_575: needsTaskCommit() Task attempt_20210516231224973922259930935854_0004_m_000196_575
[2021-05-16 20:15:12,283] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Task committer attempt_20210516231224973922259930935854_0004_m_000196_575: needsTaskCommit() Task attempt_20210516231224973922259930935854_0004_m_000196_575: duration 0:00.001s
21/05/16 23:15:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210516231224973922259930935854_0004_m_000196_575
[2021-05-16 20:15:12,285] {docker.py:276} INFO - 21/05/16 23:15:12 INFO Executor: Finished task 196.0 in stage 4.0 (TID 575). 4544 bytes result sent to driver
[2021-05-16 20:15:12,286] {docker.py:276} INFO - 21/05/16 23:15:12 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 577) (764b1db4ecfd, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:12,287] {docker.py:276} INFO - 21/05/16 23:15:12 INFO Executor: Running task 198.0 in stage 4.0 (TID 577)
[2021-05-16 20:15:12,288] {docker.py:276} INFO - 21/05/16 23:15:12 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 575) in 2769 ms on 764b1db4ecfd (executor driver) (195/200)
[2021-05-16 20:15:12,297] {docker.py:276} INFO - 21/05/16 23:15:12 INFO ShuffleBlockFetcherIterator: Getting 6 (26.7 KiB) non-empty blocks including 6 (26.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:15:12,298] {docker.py:276} INFO - 21/05/16 23:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:12,299] {docker.py:276} INFO - 21/05/16 23:15:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-16 20:15:12,300] {docker.py:276} INFO - 21/05/16 23:15:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:12,301] {docker.py:276} INFO - 21/05/16 23:15:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312248879768810275113367_0004_m_000198_577, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248879768810275113367_0004_m_000198_577}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312248879768810275113367_0004}; taskId=attempt_202105162312248879768810275113367_0004_m_000198_577, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d116222}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:12,301] {docker.py:276} INFO - 21/05/16 23:15:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:12,302] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312248879768810275113367_0004_m_000198_577: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248879768810275113367_0004_m_000198_577
[2021-05-16 20:15:12,304] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Task committer attempt_202105162312248879768810275113367_0004_m_000198_577: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312248879768810275113367_0004_m_000198_577 : duration 0:00.003s
[2021-05-16 20:15:12,398] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312245608384557457927207_0004_m_000194_573: needsTaskCommit() Task attempt_202105162312245608384557457927207_0004_m_000194_573
[2021-05-16 20:15:12,399] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Task committer attempt_202105162312245608384557457927207_0004_m_000194_573: needsTaskCommit() Task attempt_202105162312245608384557457927207_0004_m_000194_573: duration 0:00.001s
[2021-05-16 20:15:12,400] {docker.py:276} INFO - 21/05/16 23:15:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312245608384557457927207_0004_m_000194_573
[2021-05-16 20:15:12,401] {docker.py:276} INFO - 21/05/16 23:15:12 INFO Executor: Finished task 194.0 in stage 4.0 (TID 573). 4544 bytes result sent to driver
[2021-05-16 20:15:12,402] {docker.py:276} INFO - 21/05/16 23:15:12 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 578) (764b1db4ecfd, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-16 20:15:12,404] {docker.py:276} INFO - 21/05/16 23:15:12 INFO Executor: Running task 199.0 in stage 4.0 (TID 578)
[2021-05-16 20:15:12,404] {docker.py:276} INFO - 21/05/16 23:15:12 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 573) in 3370 ms on 764b1db4ecfd (executor driver) (196/200)
[2021-05-16 20:15:12,413] {docker.py:276} INFO - 21/05/16 23:15:12 INFO ShuffleBlockFetcherIterator: Getting 6 (25.8 KiB) non-empty blocks including 6 (25.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-16 20:15:12,414] {docker.py:276} INFO - 21/05/16 23:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-16 20:15:12,416] {docker.py:276} INFO - 21/05/16 23:15:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/16 23:15:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/16 23:15:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/16 23:15:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105162312243485999415170795606_0004_m_000199_578, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243485999415170795606_0004_m_000199_578}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105162312243485999415170795606_0004}; taskId=attempt_202105162312243485999415170795606_0004_m_000199_578, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4cfa01e7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/90c8a688-758a-450f-b7ac-13fac3343bfb/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/16 23:15:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-16 20:15:12,416] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312243485999415170795606_0004_m_000199_578: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243485999415170795606_0004_m_000199_578
[2021-05-16 20:15:12,419] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Task committer attempt_202105162312243485999415170795606_0004_m_000199_578: setup task attempt path file:/tmp/hadoop-jovyan/s3a/90c8a688-758a-450f-b7ac-13fac3343bfb/_temporary/0/_temporary/attempt_202105162312243485999415170795606_0004_m_000199_578 : duration 0:00.004s
[2021-05-16 20:15:12,535] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Starting: Task committer attempt_202105162312247314076116226012759_0004_m_000195_574: needsTaskCommit() Task attempt_202105162312247314076116226012759_0004_m_000195_574
[2021-05-16 20:15:12,536] {docker.py:276} INFO - 21/05/16 23:15:12 INFO StagingCommitter: Task committer attempt_202105162312247314076116226012759_0004_m_000195_574: needsTaskCommit() Task attempt_202105162312247314076116226012759_0004_m_000195_574: duration 0:00.001s
[2021-05-16 20:15:12,537] {docker.py:276} INFO - 21/05/16 23:15:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312247314076116226012759_0004_m_000195_574
[2021-05-16 20:15:12,538] {docker.py:276} INFO - 21/05/16 23:15:12 INFO Executor: Finished task 195.0 in stage 4.0 (TID 574). 4544 bytes result sent to driver
[2021-05-16 20:15:12,540] {docker.py:276} INFO - 21/05/16 23:15:12 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 574) in 3337 ms on 764b1db4ecfd (executor driver) (197/200)
[2021-05-16 20:15:13,570] {docker.py:276} INFO - 21/05/16 23:15:13 INFO StagingCommitter: Starting: Task committer attempt_202105162312248467690514076100540_0004_m_000197_576: needsTaskCommit() Task attempt_202105162312248467690514076100540_0004_m_000197_576
21/05/16 23:15:13 INFO StagingCommitter: Task committer attempt_202105162312248467690514076100540_0004_m_000197_576: needsTaskCommit() Task attempt_202105162312248467690514076100540_0004_m_000197_576: duration 0:00.000s
21/05/16 23:15:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248467690514076100540_0004_m_000197_576
[2021-05-16 20:15:13,571] {docker.py:276} INFO - 21/05/16 23:15:13 INFO Executor: Finished task 197.0 in stage 4.0 (TID 576). 4544 bytes result sent to driver
[2021-05-16 20:15:13,573] {docker.py:276} INFO - 21/05/16 23:15:13 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 576) in 2809 ms on 764b1db4ecfd (executor driver) (198/200)
[2021-05-16 20:15:14,769] {docker.py:276} INFO - 21/05/16 23:15:14 INFO StagingCommitter: Starting: Task committer attempt_202105162312248879768810275113367_0004_m_000198_577: needsTaskCommit() Task attempt_202105162312248879768810275113367_0004_m_000198_577
[2021-05-16 20:15:14,770] {docker.py:276} INFO - 21/05/16 23:15:14 INFO StagingCommitter: Task committer attempt_202105162312248879768810275113367_0004_m_000198_577: needsTaskCommit() Task attempt_202105162312248879768810275113367_0004_m_000198_577: duration 0:00.001s
21/05/16 23:15:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312248879768810275113367_0004_m_000198_577
[2021-05-16 20:15:14,772] {docker.py:276} INFO - 21/05/16 23:15:14 INFO Executor: Finished task 198.0 in stage 4.0 (TID 577). 4544 bytes result sent to driver
[2021-05-16 20:15:14,773] {docker.py:276} INFO - 21/05/16 23:15:14 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 577) in 2490 ms on 764b1db4ecfd (executor driver) (199/200)
[2021-05-16 20:15:15,379] {docker.py:276} INFO - 21/05/16 23:15:15 INFO StagingCommitter: Starting: Task committer attempt_202105162312243485999415170795606_0004_m_000199_578: needsTaskCommit() Task attempt_202105162312243485999415170795606_0004_m_000199_578
[2021-05-16 20:15:15,380] {docker.py:276} INFO - 21/05/16 23:15:15 INFO StagingCommitter: Task committer attempt_202105162312243485999415170795606_0004_m_000199_578: needsTaskCommit() Task attempt_202105162312243485999415170795606_0004_m_000199_578: duration 0:00.000s
[2021-05-16 20:15:15,380] {docker.py:276} INFO - 21/05/16 23:15:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105162312243485999415170795606_0004_m_000199_578
[2021-05-16 20:15:15,382] {docker.py:276} INFO - 21/05/16 23:15:15 INFO Executor: Finished task 199.0 in stage 4.0 (TID 578). 4544 bytes result sent to driver
[2021-05-16 20:15:15,383] {docker.py:276} INFO - 21/05/16 23:15:15 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 578) in 2985 ms on 764b1db4ecfd (executor driver) (200/200)
[2021-05-16 20:15:15,384] {docker.py:276} INFO - 21/05/16 23:15:15 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 144.754 s
21/05/16 23:15:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-16 20:15:15,385] {docker.py:276} INFO - 21/05/16 23:15:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-16 20:15:15,385] {docker.py:276} INFO - 21/05/16 23:15:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-16 20:15:15,385] {docker.py:276} INFO - 21/05/16 23:15:15 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 170.985962 s
[2021-05-16 20:15:15,387] {docker.py:276} INFO - 21/05/16 23:15:15 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105162312244394340570080125175_0000_m_000000_0: commitJob((no job ID))
[2021-05-16 20:15:15,410] {docker.py:276} INFO - 21/05/16 23:15:15 WARN AbstractS3ACommitter: Task committer attempt_202105162312244394340570080125175_0000_m_000000_0: No pending uploads to commit
[2021-05-16 20:15:15,934] {docker.py:276} INFO - 21/05/16 23:15:15 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/16 23:15:15 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-16 20:15:16,113] {docker.py:276} INFO - 21/05/16 23:15:16 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.180s
[2021-05-16 20:15:16,114] {docker.py:276} INFO - 21/05/16 23:15:16 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.180s
[2021-05-16 20:15:16,115] {docker.py:276} INFO - 21/05/16 23:15:16 INFO AbstractS3ACommitter: Task committer attempt_202105162312244394340570080125175_0000_m_000000_0: commitJob((no job ID)): duration 0:00.729s
[2021-05-16 20:15:16,622] {docker.py:276} INFO - 21/05/16 23:15:16 INFO FileFormatWriter: Write Job 90c8a688-758a-450f-b7ac-13fac3343bfb committed.
[2021-05-16 20:15:16,630] {docker.py:276} INFO - 21/05/16 23:15:16 INFO FileFormatWriter: Finished processing stats for write job 90c8a688-758a-450f-b7ac-13fac3343bfb.
[2021-05-16 20:15:16,767] {docker.py:276} INFO - 21/05/16 23:15:16 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-16 20:15:16,788] {docker.py:276} INFO - 21/05/16 23:15:16 INFO SparkUI: Stopped Spark web UI at http://764b1db4ecfd:4040
[2021-05-16 20:15:16,815] {docker.py:276} INFO - 21/05/16 23:15:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-16 20:15:16,837] {docker.py:276} INFO - 21/05/16 23:15:16 INFO MemoryStore: MemoryStore cleared
[2021-05-16 20:15:16,837] {docker.py:276} INFO - 21/05/16 23:15:16 INFO BlockManager: BlockManager stopped
[2021-05-16 20:15:16,842] {docker.py:276} INFO - 21/05/16 23:15:16 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-16 20:15:16,848] {docker.py:276} INFO - 21/05/16 23:15:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-16 20:15:16,857] {docker.py:276} INFO - 21/05/16 23:15:16 INFO SparkContext: Successfully stopped SparkContext
[2021-05-16 20:15:16,858] {docker.py:276} INFO - 21/05/16 23:15:16 INFO ShutdownHookManager: Shutdown hook called
[2021-05-16 20:15:16,859] {docker.py:276} INFO - 21/05/16 23:15:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-30ed20f0-b853-4dd2-a6bd-1cfedd108f51
[2021-05-16 20:15:16,861] {docker.py:276} INFO - 21/05/16 23:15:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-30ed20f0-b853-4dd2-a6bd-1cfedd108f51/pyspark-50be1eca-afe8-4e66-87c7-dd0bdf01c9f3
[2021-05-16 20:15:16,863] {docker.py:276} INFO - 21/05/16 23:15:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-eae3ba43-d41c-481e-8a08-f1be956757a6
[2021-05-16 20:15:16,870] {docker.py:276} INFO - 21/05/16 23:15:16 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-16 20:15:16,871] {docker.py:276} INFO - 21/05/16 23:15:16 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-16 20:15:16,872] {docker.py:276} INFO - 21/05/16 23:15:16 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-16 20:15:17,157] {taskinstance.py:1192} INFO - Marking task as SUCCESS. dag_id=etl, task_id=run_spark_job, execution_date=20210515T220000, start_date=20210516T231140, end_date=20210516T231517
[2021-05-16 20:15:17,216] {taskinstance.py:1246} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2021-05-16 20:15:17,246] {local_task_job.py:146} INFO - Task exited with return code 0
