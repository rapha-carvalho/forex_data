[2021-05-14 11:16:00,512] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T14:15:05.494803+00:00 [queued]>
[2021-05-14 11:16:00,518] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: etl.run_spark_job 2021-05-14T14:15:05.494803+00:00 [queued]>
[2021-05-14 11:16:00,518] {taskinstance.py:1068} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 11:16:00,518] {taskinstance.py:1069} INFO - Starting attempt 1 of 1
[2021-05-14 11:16:00,518] {taskinstance.py:1070} INFO - 
--------------------------------------------------------------------------------
[2021-05-14 11:16:00,523] {taskinstance.py:1089} INFO - Executing <Task(DockerOperator): run_spark_job> on 2021-05-14T14:15:05.494803+00:00
[2021-05-14 11:16:00,526] {standard_task_runner.py:52} INFO - Started process 19810 to run task
[2021-05-14 11:16:00,532] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'etl', 'run_spark_job', '2021-05-14T14:15:05.494803+00:00', '--job-id', '399', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmplxypzsn0', '--error-file', '/var/folders/_f/vg3x3jmj23bcz6s88l7v6fb40000gn/T/tmpripv10cx']
[2021-05-14 11:16:00,534] {standard_task_runner.py:77} INFO - Job 399: Subtask run_spark_job
[2021-05-14 11:16:00,561] {logging_mixin.py:104} INFO - Running <TaskInstance: etl.run_spark_job 2021-05-14T14:15:05.494803+00:00 [running]> on host 27.2.168.192.in-addr.arpa
[2021-05-14 11:16:00,584] {taskinstance.py:1283} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=etl
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2021-05-14T14:15:05.494803+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-14T14:15:05.494803+00:00
[2021-05-14 11:16:00,587] {docker.py:303} INFO - Pulling docker image raphacarvalho/udac_spark
[2021-05-14 11:16:03,576] {docker.py:317} INFO - latest: Pulling from raphacarvalho/udac_spark
[2021-05-14 11:16:03,580] {docker.py:312} INFO - Digest: sha256:59c324d0ea6269fbd4f520bfa9d09121c4dbc9d525570712d6805ba6501a358c
[2021-05-14 11:16:03,581] {docker.py:312} INFO - Status: Image is up to date for raphacarvalho/udac_spark
[2021-05-14 11:16:03,583] {docker.py:232} INFO - Starting docker container from image raphacarvalho/udac_spark
[2021-05-14 11:16:05,617] {docker.py:276} INFO - WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[2021-05-14 11:16:06,189] {docker.py:276} INFO - 21/05/14 14:16:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-05-14 11:16:08,475] {docker.py:276} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-05-14 11:16:08,489] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SparkContext: Running Spark version 3.1.1
[2021-05-14 11:16:08,564] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceUtils: ==============================================================
[2021-05-14 11:16:08,565] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-05-14 11:16:08,566] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceUtils: ==============================================================
[2021-05-14 11:16:08,567] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SparkContext: Submitted application: spark.py
[2021-05-14 11:16:08,601] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 884, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 500, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-05-14 11:16:08,616] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceProfile: Limiting resource is cpu
[2021-05-14 11:16:08,618] {docker.py:276} INFO - 21/05/14 14:16:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-05-14 11:16:08,684] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SecurityManager: Changing view acls to: jovyan
[2021-05-14 11:16:08,685] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SecurityManager: Changing modify acls to: jovyan
[2021-05-14 11:16:08,685] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SecurityManager: Changing view acls groups to:
[2021-05-14 11:16:08,686] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SecurityManager: Changing modify acls groups to:
[2021-05-14 11:16:08,686] {docker.py:276} INFO - 21/05/14 14:16:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()
[2021-05-14 11:16:09,024] {docker.py:276} INFO - 21/05/14 14:16:09 INFO Utils: Successfully started service 'sparkDriver' on port 39561.
[2021-05-14 11:16:09,062] {docker.py:276} INFO - 21/05/14 14:16:09 INFO SparkEnv: Registering MapOutputTracker
[2021-05-14 11:16:09,119] {docker.py:276} INFO - 21/05/14 14:16:09 INFO SparkEnv: Registering BlockManagerMaster
[2021-05-14 11:16:09,156] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-05-14 11:16:09,157] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-05-14 11:16:09,166] {docker.py:276} INFO - 21/05/14 14:16:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-05-14 11:16:09,185] {docker.py:276} INFO - 21/05/14 14:16:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-79768a6a-0928-4f7c-bf64-de133fbd37a7
[2021-05-14 11:16:09,213] {docker.py:276} INFO - 21/05/14 14:16:09 INFO MemoryStore: MemoryStore started with capacity 934.4 MiB
[2021-05-14 11:16:09,239] {docker.py:276} INFO - 21/05/14 14:16:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-05-14 11:16:09,522] {docker.py:276} INFO - 21/05/14 14:16:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-05-14 11:16:09,612] {docker.py:276} INFO - 21/05/14 14:16:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://0d1d3e01326c:4040
[2021-05-14 11:16:09,849] {docker.py:276} INFO - 21/05/14 14:16:09 INFO Executor: Starting executor ID driver on host 0d1d3e01326c
[2021-05-14 11:16:09,896] {docker.py:276} INFO - 21/05/14 14:16:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42543.
[2021-05-14 11:16:09,896] {docker.py:276} INFO - 21/05/14 14:16:09 INFO NettyBlockTransferService: Server created on 0d1d3e01326c:42543
[2021-05-14 11:16:09,899] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-05-14 11:16:09,911] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0d1d3e01326c, 42543, None)
[2021-05-14 11:16:09,919] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManagerMasterEndpoint: Registering block manager 0d1d3e01326c:42543 with 934.4 MiB RAM, BlockManagerId(driver, 0d1d3e01326c, 42543, None)
[2021-05-14 11:16:09,922] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0d1d3e01326c, 42543, None)
[2021-05-14 11:16:09,924] {docker.py:276} INFO - 21/05/14 14:16:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0d1d3e01326c, 42543, None)
[2021-05-14 11:16:10,480] {docker.py:276} INFO - 21/05/14 14:16:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').
[2021-05-14 11:16:10,480] {docker.py:276} INFO - 21/05/14 14:16:10 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2021-05-14 11:16:11,550] {docker.py:276} INFO - 21/05/14 14:16:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2021-05-14 11:16:11,633] {docker.py:276} INFO - 21/05/14 14:16:11 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/05/14 14:16:11 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2021-05-14 11:16:21,088] {docker.py:276} INFO - 21/05/14 14:16:21 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14/from_1620915321_to_1620917121.csv, s3a://udac-forex-project/1/2021-05-14/from_1620917121_to_1620918921.csv, s3a://udac-forex-project/1/2021-05-14/from_1620918921_to_1620920721.csv, s3a://udac-forex-project/1/2021-05-14/from_1620920721_to_1620922521.csv, s3a://udac-forex-project/1/2021-05-14/from_1620922521_to_1620924321.csv, s3a://udac-forex-project/1/2021-05-14/from_1620924321_to_1620926121.csv, s3a://udac-forex-project/1/2021-05-14/from_1620926121_to_1620927921.csv, s3a://udac-forex-project/1/2021-05-14/from_1620927921_to_1620929721.csv, s3a://udac-forex-project/1/2021-05-14/from_1620929721_to_1620931521.csv, s3a://udac-forex-project/1/2021-05-14/from_1620931521_to_1620933321.csv.
[2021-05-14 11:16:21,608] {docker.py:276} INFO - 21/05/14 14:16:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:21,634] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
[2021-05-14 11:16:21,635] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 11:16:21,637] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:16:21,639] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:16:21,653] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:16:21,785] {docker.py:276} INFO - 21/05/14 14:16:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.8 KiB, free 934.3 MiB)
[2021-05-14 11:16:21,842] {docker.py:276} INFO - 21/05/14 14:16:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 934.3 MiB)
[2021-05-14 11:16:21,846] {docker.py:276} INFO - 21/05/14 14:16:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0d1d3e01326c:42543 (size: 30.2 KiB, free: 934.4 MiB)
[2021-05-14 11:16:21,851] {docker.py:276} INFO - 21/05/14 14:16:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:16:21,881] {docker.py:276} INFO - 21/05/14 14:16:21 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2021-05-14 11:16:21,882] {docker.py:276} INFO - 21/05/14 14:16:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 141 tasks resource profile 0
[2021-05-14 11:16:21,981] {docker.py:276} INFO - 21/05/14 14:16:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0d1d3e01326c, executor driver, partition 0, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:21,986] {docker.py:276} INFO - 21/05/14 14:16:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (0d1d3e01326c, executor driver, partition 1, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:21,987] {docker.py:276} INFO - 21/05/14 14:16:21 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (0d1d3e01326c, executor driver, partition 2, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:21,988] {docker.py:276} INFO - 21/05/14 14:16:22 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (0d1d3e01326c, executor driver, partition 3, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:22,013] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2021-05-14 11:16:22,014] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2021-05-14 11:16:22,015] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
[2021-05-14 11:16:22,020] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
[2021-05-14 11:16:22,554] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1868 bytes result sent to driver
[2021-05-14 11:16:22,560] {docker.py:276} INFO - 21/05/14 14:16:22 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (0d1d3e01326c, executor driver, partition 4, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:22,563] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
[2021-05-14 11:16:22,568] {docker.py:276} INFO - 21/05/14 14:16:22 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 579 ms on 0d1d3e01326c (executor driver) (1/141)
[2021-05-14 11:16:22,893] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1868 bytes result sent to driver
[2021-05-14 11:16:22,895] {docker.py:276} INFO - 21/05/14 14:16:22 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (0d1d3e01326c, executor driver, partition 5, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:22,898] {docker.py:276} INFO - 21/05/14 14:16:22 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
[2021-05-14 11:16:22,901] {docker.py:276} INFO - 21/05/14 14:16:22 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 342 ms on 0d1d3e01326c (executor driver) (2/141)
[2021-05-14 11:16:23,088] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1868 bytes result sent to driver
[2021-05-14 11:16:23,092] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (0d1d3e01326c, executor driver, partition 6, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,093] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1868 bytes result sent to driver
[2021-05-14 11:16:23,094] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
[2021-05-14 11:16:23,097] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (0d1d3e01326c, executor driver, partition 7, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,098] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
[2021-05-14 11:16:23,099] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1868 bytes result sent to driver
[2021-05-14 11:16:23,102] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1825 bytes result sent to driver
[2021-05-14 11:16:23,103] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (0d1d3e01326c, executor driver, partition 8, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,104] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1119 ms on 0d1d3e01326c (executor driver) (3/141)
[2021-05-14 11:16:23,105] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1159 ms on 0d1d3e01326c (executor driver) (4/141)
[2021-05-14 11:16:23,106] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
[2021-05-14 11:16:23,108] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 213 ms on 0d1d3e01326c (executor driver) (5/141)
[2021-05-14 11:16:23,115] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (0d1d3e01326c, executor driver, partition 9, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,116] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1132 ms on 0d1d3e01326c (executor driver) (6/141)
[2021-05-14 11:16:23,117] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
[2021-05-14 11:16:23,416] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1825 bytes result sent to driver
21/05/14 14:16:23 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1825 bytes result sent to driver
21/05/14 14:16:23 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1825 bytes result sent to driver
21/05/14 14:16:23 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (0d1d3e01326c, executor driver, partition 10, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,420] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 321 ms on 0d1d3e01326c (executor driver) (7/141)
[2021-05-14 11:16:23,420] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1825 bytes result sent to driver
[2021-05-14 11:16:23,421] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (0d1d3e01326c, executor driver, partition 11, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,422] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
[2021-05-14 11:16:23,423] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
[2021-05-14 11:16:23,424] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (0d1d3e01326c, executor driver, partition 12, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,425] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 317 ms on 0d1d3e01326c (executor driver) (8/141)
[2021-05-14 11:16:23,426] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (0d1d3e01326c, executor driver, partition 13, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,427] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 337 ms on 0d1d3e01326c (executor driver) (9/141)
[2021-05-14 11:16:23,428] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 327 ms on 0d1d3e01326c (executor driver) (10/141)
[2021-05-14 11:16:23,432] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
[2021-05-14 11:16:23,433] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
[2021-05-14 11:16:23,605] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1825 bytes result sent to driver
[2021-05-14 11:16:23,608] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (0d1d3e01326c, executor driver, partition 14, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,613] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 199 ms on 0d1d3e01326c (executor driver) (11/141)
[2021-05-14 11:16:23,614] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1825 bytes result sent to driver
[2021-05-14 11:16:23,615] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
[2021-05-14 11:16:23,616] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1825 bytes result sent to driver
[2021-05-14 11:16:23,617] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (0d1d3e01326c, executor driver, partition 15, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,619] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
[2021-05-14 11:16:23,621] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (0d1d3e01326c, executor driver, partition 16, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,622] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
[2021-05-14 11:16:23,623] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 203 ms on 0d1d3e01326c (executor driver) (12/141)
[2021-05-14 11:16:23,624] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 201 ms on 0d1d3e01326c (executor driver) (13/141)
[2021-05-14 11:16:23,624] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1825 bytes result sent to driver
[2021-05-14 11:16:23,626] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (0d1d3e01326c, executor driver, partition 17, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,627] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 201 ms on 0d1d3e01326c (executor driver) (14/141)
21/05/14 14:16:23 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
[2021-05-14 11:16:23,807] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1868 bytes result sent to driver
[2021-05-14 11:16:23,808] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (0d1d3e01326c, executor driver, partition 18, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,809] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
21/05/14 14:16:23 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 190 ms on 0d1d3e01326c (executor driver) (15/141)
[2021-05-14 11:16:23,965] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1868 bytes result sent to driver
[2021-05-14 11:16:23,966] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1868 bytes result sent to driver
[2021-05-14 11:16:23,967] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1868 bytes result sent to driver
[2021-05-14 11:16:23,968] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (0d1d3e01326c, executor driver, partition 19, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,969] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
[2021-05-14 11:16:23,971] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (0d1d3e01326c, executor driver, partition 20, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,972] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
[2021-05-14 11:16:23,973] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (0d1d3e01326c, executor driver, partition 21, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,974] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 369 ms on 0d1d3e01326c (executor driver) (16/141)
[2021-05-14 11:16:23,975] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 349 ms on 0d1d3e01326c (executor driver) (17/141)
[2021-05-14 11:16:23,975] {docker.py:276} INFO - 21/05/14 14:16:23 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
[2021-05-14 11:16:23,977] {docker.py:276} INFO - 21/05/14 14:16:23 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 362 ms on 0d1d3e01326c (executor driver) (18/141)
[2021-05-14 11:16:23,989] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1825 bytes result sent to driver
[2021-05-14 11:16:23,991] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (0d1d3e01326c, executor driver, partition 22, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:23,992] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
[2021-05-14 11:16:23,993] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 185 ms on 0d1d3e01326c (executor driver) (19/141)
[2021-05-14 11:16:24,149] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1825 bytes result sent to driver
[2021-05-14 11:16:24,154] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (0d1d3e01326c, executor driver, partition 23, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,156] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
[2021-05-14 11:16:24,156] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 190 ms on 0d1d3e01326c (executor driver) (20/141)
[2021-05-14 11:16:24,159] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 1825 bytes result sent to driver
[2021-05-14 11:16:24,160] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (0d1d3e01326c, executor driver, partition 24, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:24 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 1825 bytes result sent to driver
[2021-05-14 11:16:24,161] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 189 ms on 0d1d3e01326c (executor driver) (21/141)
[2021-05-14 11:16:24,162] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
[2021-05-14 11:16:24,163] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (0d1d3e01326c, executor driver, partition 25, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,164] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
[2021-05-14 11:16:24,165] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 196 ms on 0d1d3e01326c (executor driver) (22/141)
[2021-05-14 11:16:24,299] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 1825 bytes result sent to driver
[2021-05-14 11:16:24,301] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (0d1d3e01326c, executor driver, partition 26, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,303] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 313 ms on 0d1d3e01326c (executor driver) (23/141)
21/05/14 14:16:24 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
[2021-05-14 11:16:24,479] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 1825 bytes result sent to driver
21/05/14 14:16:24 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 1825 bytes result sent to driver
[2021-05-14 11:16:24,481] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 1825 bytes result sent to driver
[2021-05-14 11:16:24,483] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (0d1d3e01326c, executor driver, partition 27, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,488] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 1825 bytes result sent to driver
[2021-05-14 11:16:24,489] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 327 ms on 0d1d3e01326c (executor driver) (24/141)
[2021-05-14 11:16:24,489] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (0d1d3e01326c, executor driver, partition 28, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,490] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
[2021-05-14 11:16:24,490] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
[2021-05-14 11:16:24,491] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (0d1d3e01326c, executor driver, partition 29, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,491] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 336 ms on 0d1d3e01326c (executor driver) (25/141)
[2021-05-14 11:16:24,492] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
[2021-05-14 11:16:24,492] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 329 ms on 0d1d3e01326c (executor driver) (26/141)
[2021-05-14 11:16:24,498] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (0d1d3e01326c, executor driver, partition 30, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,501] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 201 ms on 0d1d3e01326c (executor driver) (27/141)
21/05/14 14:16:24 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
[2021-05-14 11:16:24,669] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 1825 bytes result sent to driver
[2021-05-14 11:16:24,671] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (0d1d3e01326c, executor driver, partition 31, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,672] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 1825 bytes result sent to driver
[2021-05-14 11:16:24,674] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
[2021-05-14 11:16:24,676] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 1825 bytes result sent to driver
21/05/14 14:16:24 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (0d1d3e01326c, executor driver, partition 32, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,677] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 191 ms on 0d1d3e01326c (executor driver) (28/141)
[2021-05-14 11:16:24,678] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
[2021-05-14 11:16:24,689] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 201 ms on 0d1d3e01326c (executor driver) (29/141)
[2021-05-14 11:16:24,690] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 1868 bytes result sent to driver
[2021-05-14 11:16:24,691] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 209 ms on 0d1d3e01326c (executor driver) (30/141)
[2021-05-14 11:16:24,692] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (0d1d3e01326c, executor driver, partition 33, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,693] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
[2021-05-14 11:16:24,695] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (0d1d3e01326c, executor driver, partition 34, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,696] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 200 ms on 0d1d3e01326c (executor driver) (31/141)
[2021-05-14 11:16:24,703] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
[2021-05-14 11:16:24,874] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 1868 bytes result sent to driver
[2021-05-14 11:16:24,875] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 1868 bytes result sent to driver
[2021-05-14 11:16:24,877] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (0d1d3e01326c, executor driver, partition 35, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,879] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
[2021-05-14 11:16:24,879] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (0d1d3e01326c, executor driver, partition 36, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:24,880] {docker.py:276} INFO - 21/05/14 14:16:24 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
[2021-05-14 11:16:24,881] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 210 ms on 0d1d3e01326c (executor driver) (32/141)
[2021-05-14 11:16:24,882] {docker.py:276} INFO - 21/05/14 14:16:24 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 208 ms on 0d1d3e01326c (executor driver) (33/141)
[2021-05-14 11:16:25,020] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 1825 bytes result sent to driver
21/05/14 14:16:25 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 1825 bytes result sent to driver
[2021-05-14 11:16:25,021] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (0d1d3e01326c, executor driver, partition 37, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,022] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
[2021-05-14 11:16:25,023] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (0d1d3e01326c, executor driver, partition 38, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,024] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 333 ms on 0d1d3e01326c (executor driver) (34/141)
[2021-05-14 11:16:25,025] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
[2021-05-14 11:16:25,026] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 332 ms on 0d1d3e01326c (executor driver) (35/141)
[2021-05-14 11:16:25,205] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 1825 bytes result sent to driver
[2021-05-14 11:16:25,206] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 1825 bytes result sent to driver
[2021-05-14 11:16:25,209] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 1825 bytes result sent to driver
[2021-05-14 11:16:25,210] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 1825 bytes result sent to driver
[2021-05-14 11:16:25,211] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (0d1d3e01326c, executor driver, partition 39, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,212] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 337 ms on 0d1d3e01326c (executor driver) (36/141)
[2021-05-14 11:16:25,214] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
[2021-05-14 11:16:25,216] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (0d1d3e01326c, executor driver, partition 40, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,217] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 196 ms on 0d1d3e01326c (executor driver) (37/141)
[2021-05-14 11:16:25,218] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
[2021-05-14 11:16:25,227] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (0d1d3e01326c, executor driver, partition 41, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,228] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
[2021-05-14 11:16:25,231] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (0d1d3e01326c, executor driver, partition 42, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,232] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 209 ms on 0d1d3e01326c (executor driver) (38/141)
[2021-05-14 11:16:25,233] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
[2021-05-14 11:16:25,234] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 355 ms on 0d1d3e01326c (executor driver) (39/141)
[2021-05-14 11:16:25,542] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 1825 bytes result sent to driver
[2021-05-14 11:16:25,546] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 1825 bytes result sent to driver
[2021-05-14 11:16:25,547] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (0d1d3e01326c, executor driver, partition 43, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,549] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (0d1d3e01326c, executor driver, partition 44, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,549] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
[2021-05-14 11:16:25,550] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 335 ms on 0d1d3e01326c (executor driver) (40/141)
[2021-05-14 11:16:25,551] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
[2021-05-14 11:16:25,551] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 345 ms on 0d1d3e01326c (executor driver) (41/141)
21/05/14 14:16:25 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 1825 bytes result sent to driver
[2021-05-14 11:16:25,552] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 324 ms on 0d1d3e01326c (executor driver) (42/141)
[2021-05-14 11:16:25,553] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (0d1d3e01326c, executor driver, partition 45, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,554] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
[2021-05-14 11:16:25,557] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 1825 bytes result sent to driver
[2021-05-14 11:16:25,558] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (0d1d3e01326c, executor driver, partition 46, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,559] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 341 ms on 0d1d3e01326c (executor driver) (43/141)
[2021-05-14 11:16:25,560] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
[2021-05-14 11:16:25,751] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 1868 bytes result sent to driver
[2021-05-14 11:16:25,753] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 1868 bytes result sent to driver
[2021-05-14 11:16:25,754] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 1868 bytes result sent to driver
[2021-05-14 11:16:25,754] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (0d1d3e01326c, executor driver, partition 47, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,755] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 203 ms on 0d1d3e01326c (executor driver) (44/141)
[2021-05-14 11:16:25,756] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
[2021-05-14 11:16:25,758] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (0d1d3e01326c, executor driver, partition 48, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,759] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 212 ms on 0d1d3e01326c (executor driver) (45/141)
[2021-05-14 11:16:25,760] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 212 ms on 0d1d3e01326c (executor driver) (46/141)
[2021-05-14 11:16:25,762] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (0d1d3e01326c, executor driver, partition 49, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,763] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
[2021-05-14 11:16:25,764] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
[2021-05-14 11:16:25,770] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 1868 bytes result sent to driver
[2021-05-14 11:16:25,772] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (0d1d3e01326c, executor driver, partition 50, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,773] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 214 ms on 0d1d3e01326c (executor driver) (47/141)
[2021-05-14 11:16:25,773] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
[2021-05-14 11:16:25,946] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 1825 bytes result sent to driver
[2021-05-14 11:16:25,948] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 1825 bytes result sent to driver
[2021-05-14 11:16:25,949] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (0d1d3e01326c, executor driver, partition 51, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,951] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 189 ms on 0d1d3e01326c (executor driver) (48/141)
[2021-05-14 11:16:25,952] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 1825 bytes result sent to driver
[2021-05-14 11:16:25,954] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (0d1d3e01326c, executor driver, partition 52, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,955] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 1825 bytes result sent to driver
[2021-05-14 11:16:25,956] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 203 ms on 0d1d3e01326c (executor driver) (49/141)
[2021-05-14 11:16:25,957] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
[2021-05-14 11:16:25,959] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (0d1d3e01326c, executor driver, partition 53, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,960] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 203 ms on 0d1d3e01326c (executor driver) (50/141)
[2021-05-14 11:16:25,961] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
[2021-05-14 11:16:25,962] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
[2021-05-14 11:16:25,969] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (0d1d3e01326c, executor driver, partition 54, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:25,970] {docker.py:276} INFO - 21/05/14 14:16:25 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 198 ms on 0d1d3e01326c (executor driver) (51/141)
[2021-05-14 11:16:25,971] {docker.py:276} INFO - 21/05/14 14:16:25 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
[2021-05-14 11:16:26,150] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 1825 bytes result sent to driver
[2021-05-14 11:16:26,151] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 1825 bytes result sent to driver
[2021-05-14 11:16:26,152] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 1825 bytes result sent to driver
[2021-05-14 11:16:26,153] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (0d1d3e01326c, executor driver, partition 55, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,156] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (0d1d3e01326c, executor driver, partition 56, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,156] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 196 ms on 0d1d3e01326c (executor driver) (52/141)
[2021-05-14 11:16:26,157] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
[2021-05-14 11:16:26,158] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
[2021-05-14 11:16:26,158] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 187 ms on 0d1d3e01326c (executor driver) (53/141)
[2021-05-14 11:16:26,159] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 1825 bytes result sent to driver
[2021-05-14 11:16:26,160] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (0d1d3e01326c, executor driver, partition 57, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,161] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
[2021-05-14 11:16:26,161] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (0d1d3e01326c, executor driver, partition 58, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,163] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 216 ms on 0d1d3e01326c (executor driver) (54/141)
[2021-05-14 11:16:26,163] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 211 ms on 0d1d3e01326c (executor driver) (55/141)
[2021-05-14 11:16:26,166] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
[2021-05-14 11:16:26,346] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 1825 bytes result sent to driver
[2021-05-14 11:16:26,349] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (0d1d3e01326c, executor driver, partition 59, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,350] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 189 ms on 0d1d3e01326c (executor driver) (56/141)
[2021-05-14 11:16:26,351] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
[2021-05-14 11:16:26,389] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 1868 bytes result sent to driver
[2021-05-14 11:16:26,391] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (0d1d3e01326c, executor driver, partition 60, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,392] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 1868 bytes result sent to driver
[2021-05-14 11:16:26,393] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
[2021-05-14 11:16:26,393] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 241 ms on 0d1d3e01326c (executor driver) (57/141)
[2021-05-14 11:16:26,395] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 1868 bytes result sent to driver
[2021-05-14 11:16:26,395] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (0d1d3e01326c, executor driver, partition 61, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,396] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
[2021-05-14 11:16:26,398] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (0d1d3e01326c, executor driver, partition 62, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,399] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 239 ms on 0d1d3e01326c (executor driver) (58/141)
[2021-05-14 11:16:26,400] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 245 ms on 0d1d3e01326c (executor driver) (59/141)
[2021-05-14 11:16:26,403] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
[2021-05-14 11:16:26,576] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 1825 bytes result sent to driver
[2021-05-14 11:16:26,577] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 1825 bytes result sent to driver
[2021-05-14 11:16:26,577] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 1825 bytes result sent to driver
[2021-05-14 11:16:26,578] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (0d1d3e01326c, executor driver, partition 63, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,580] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 183 ms on 0d1d3e01326c (executor driver) (60/141)
[2021-05-14 11:16:26,581] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
[2021-05-14 11:16:26,582] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (0d1d3e01326c, executor driver, partition 64, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,585] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
[2021-05-14 11:16:26,586] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 191 ms on 0d1d3e01326c (executor driver) (61/141)
[2021-05-14 11:16:26,587] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 1868 bytes result sent to driver
[2021-05-14 11:16:26,591] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 198 ms on 0d1d3e01326c (executor driver) (62/141)
[2021-05-14 11:16:26,591] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (0d1d3e01326c, executor driver, partition 65, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,592] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
[2021-05-14 11:16:26,598] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (0d1d3e01326c, executor driver, partition 66, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,599] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
[2021-05-14 11:16:26,600] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 252 ms on 0d1d3e01326c (executor driver) (63/141)
[2021-05-14 11:16:26,759] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 1825 bytes result sent to driver
[2021-05-14 11:16:26,761] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (0d1d3e01326c, executor driver, partition 67, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,762] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
[2021-05-14 11:16:26,763] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 187 ms on 0d1d3e01326c (executor driver) (64/141)
[2021-05-14 11:16:26,766] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 1825 bytes result sent to driver
[2021-05-14 11:16:26,767] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 1825 bytes result sent to driver
[2021-05-14 11:16:26,768] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (0d1d3e01326c, executor driver, partition 68, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,770] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 1825 bytes result sent to driver
[2021-05-14 11:16:26,771] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (0d1d3e01326c, executor driver, partition 69, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,772] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
21/05/14 14:16:26 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 182 ms on 0d1d3e01326c (executor driver) (65/141)
[2021-05-14 11:16:26,773] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 192 ms on 0d1d3e01326c (executor driver) (66/141)
[2021-05-14 11:16:26,774] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 176 ms on 0d1d3e01326c (executor driver) (67/141)
[2021-05-14 11:16:26,775] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (0d1d3e01326c, executor driver, partition 70, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,776] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
[2021-05-14 11:16:26,776] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
[2021-05-14 11:16:26,949] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 1825 bytes result sent to driver
[2021-05-14 11:16:26,951] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (0d1d3e01326c, executor driver, partition 71, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,952] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 177 ms on 0d1d3e01326c (executor driver) (68/141)
[2021-05-14 11:16:26,954] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
21/05/14 14:16:26 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 1825 bytes result sent to driver
[2021-05-14 11:16:26,955] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 1825 bytes result sent to driver
[2021-05-14 11:16:26,957] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (0d1d3e01326c, executor driver, partition 72, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,958] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 1825 bytes result sent to driver
[2021-05-14 11:16:26,959] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
[2021-05-14 11:16:26,960] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (0d1d3e01326c, executor driver, partition 73, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,960] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 193 ms on 0d1d3e01326c (executor driver) (69/141)
[2021-05-14 11:16:26,961] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 191 ms on 0d1d3e01326c (executor driver) (70/141)
[2021-05-14 11:16:26,963] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
[2021-05-14 11:16:26,964] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (0d1d3e01326c, executor driver, partition 74, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:26,965] {docker.py:276} INFO - 21/05/14 14:16:26 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 205 ms on 0d1d3e01326c (executor driver) (71/141)
[2021-05-14 11:16:26,968] {docker.py:276} INFO - 21/05/14 14:16:26 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
[2021-05-14 11:16:27,141] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 1825 bytes result sent to driver
[2021-05-14 11:16:27,141] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (0d1d3e01326c, executor driver, partition 75, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,142] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 1825 bytes result sent to driver
[2021-05-14 11:16:27,143] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 1825 bytes result sent to driver
[2021-05-14 11:16:27,144] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
[2021-05-14 11:16:27,154] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (0d1d3e01326c, executor driver, partition 76, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,154] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 1868 bytes result sent to driver
[2021-05-14 11:16:27,156] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 196 ms on 0d1d3e01326c (executor driver) (72/141)
[2021-05-14 11:16:27,157] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (0d1d3e01326c, executor driver, partition 77, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,158] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
[2021-05-14 11:16:27,164] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (0d1d3e01326c, executor driver, partition 78, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,164] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
[2021-05-14 11:16:27,165] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 215 ms on 0d1d3e01326c (executor driver) (73/141)
21/05/14 14:16:27 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
[2021-05-14 11:16:27,165] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 209 ms on 0d1d3e01326c (executor driver) (74/141)
[2021-05-14 11:16:27,166] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 202 ms on 0d1d3e01326c (executor driver) (75/141)
[2021-05-14 11:16:27,333] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 1868 bytes result sent to driver
[2021-05-14 11:16:27,334] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (0d1d3e01326c, executor driver, partition 79, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,336] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 196 ms on 0d1d3e01326c (executor driver) (76/141)
[2021-05-14 11:16:27,336] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
[2021-05-14 11:16:27,338] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 1825 bytes result sent to driver
[2021-05-14 11:16:27,339] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (0d1d3e01326c, executor driver, partition 80, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,340] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
[2021-05-14 11:16:27,346] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 1825 bytes result sent to driver
[2021-05-14 11:16:27,346] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 188 ms on 0d1d3e01326c (executor driver) (77/141)
[2021-05-14 11:16:27,347] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (0d1d3e01326c, executor driver, partition 81, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,348] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 192 ms on 0d1d3e01326c (executor driver) (78/141)
[2021-05-14 11:16:27,351] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
[2021-05-14 11:16:27,352] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 1825 bytes result sent to driver
[2021-05-14 11:16:27,353] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (0d1d3e01326c, executor driver, partition 82, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,355] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 196 ms on 0d1d3e01326c (executor driver) (79/141)
[2021-05-14 11:16:27,355] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
[2021-05-14 11:16:27,525] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 1825 bytes result sent to driver
[2021-05-14 11:16:27,527] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 1825 bytes result sent to driver
[2021-05-14 11:16:27,528] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (0d1d3e01326c, executor driver, partition 83, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,529] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 1825 bytes result sent to driver
[2021-05-14 11:16:27,530] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
[2021-05-14 11:16:27,531] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (0d1d3e01326c, executor driver, partition 84, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,532] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 199 ms on 0d1d3e01326c (executor driver) (80/141)
[2021-05-14 11:16:27,533] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 186 ms on 0d1d3e01326c (executor driver) (81/141)
[2021-05-14 11:16:27,534] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 1825 bytes result sent to driver
[2021-05-14 11:16:27,535] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 195 ms on 0d1d3e01326c (executor driver) (82/141)
[2021-05-14 11:16:27,536] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
[2021-05-14 11:16:27,537] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (0d1d3e01326c, executor driver, partition 85, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,539] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
[2021-05-14 11:16:27,540] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (0d1d3e01326c, executor driver, partition 86, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,541] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 188 ms on 0d1d3e01326c (executor driver) (83/141)
[2021-05-14 11:16:27,542] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
[2021-05-14 11:16:27,716] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 1825 bytes result sent to driver
[2021-05-14 11:16:27,717] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 1825 bytes result sent to driver
[2021-05-14 11:16:27,719] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (0d1d3e01326c, executor driver, partition 87, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,720] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 1825 bytes result sent to driver
[2021-05-14 11:16:27,724] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
21/05/14 14:16:27 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 192 ms on 0d1d3e01326c (executor driver) (84/141)
[2021-05-14 11:16:27,725] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (0d1d3e01326c, executor driver, partition 88, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,725] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 1825 bytes result sent to driver
[2021-05-14 11:16:27,726] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 186 ms on 0d1d3e01326c (executor driver) (85/141)
[2021-05-14 11:16:27,726] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
[2021-05-14 11:16:27,727] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (0d1d3e01326c, executor driver, partition 89, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,727] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
[2021-05-14 11:16:27,728] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (0d1d3e01326c, executor driver, partition 90, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,729] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 190 ms on 0d1d3e01326c (executor driver) (86/141)
[2021-05-14 11:16:27,729] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
[2021-05-14 11:16:27,730] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 204 ms on 0d1d3e01326c (executor driver) (87/141)
[2021-05-14 11:16:27,912] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 1868 bytes result sent to driver
[2021-05-14 11:16:27,915] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 1868 bytes result sent to driver
21/05/14 14:16:27 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 1868 bytes result sent to driver
[2021-05-14 11:16:27,917] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (0d1d3e01326c, executor driver, partition 91, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:27 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 1868 bytes result sent to driver
[2021-05-14 11:16:27,918] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 201 ms on 0d1d3e01326c (executor driver) (88/141)
[2021-05-14 11:16:27,919] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
[2021-05-14 11:16:27,920] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 195 ms on 0d1d3e01326c (executor driver) (89/141)
[2021-05-14 11:16:27,921] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (0d1d3e01326c, executor driver, partition 92, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,922] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (0d1d3e01326c, executor driver, partition 93, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,923] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 196 ms on 0d1d3e01326c (executor driver) (90/141)
[2021-05-14 11:16:27,924] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
[2021-05-14 11:16:27,925] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
[2021-05-14 11:16:27,930] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (0d1d3e01326c, executor driver, partition 94, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:27,931] {docker.py:276} INFO - 21/05/14 14:16:27 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 209 ms on 0d1d3e01326c (executor driver) (91/141)
[2021-05-14 11:16:27,932] {docker.py:276} INFO - 21/05/14 14:16:27 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
[2021-05-14 11:16:28,098] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 1825 bytes result sent to driver
[2021-05-14 11:16:28,100] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (0d1d3e01326c, executor driver, partition 95, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,102] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
21/05/14 14:16:28 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 185 ms on 0d1d3e01326c (executor driver) (92/141)
[2021-05-14 11:16:28,106] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 1825 bytes result sent to driver
[2021-05-14 11:16:28,107] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (0d1d3e01326c, executor driver, partition 96, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,108] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 178 ms on 0d1d3e01326c (executor driver) (93/141)
[2021-05-14 11:16:28,108] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
[2021-05-14 11:16:28,113] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 1825 bytes result sent to driver
[2021-05-14 11:16:28,114] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (0d1d3e01326c, executor driver, partition 97, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,116] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 194 ms on 0d1d3e01326c (executor driver) (94/141)
21/05/14 14:16:28 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 1825 bytes result sent to driver
[2021-05-14 11:16:28,117] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
[2021-05-14 11:16:28,117] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (0d1d3e01326c, executor driver, partition 98, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,119] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 199 ms on 0d1d3e01326c (executor driver) (95/141)
21/05/14 14:16:28 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
[2021-05-14 11:16:28,287] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 1825 bytes result sent to driver
[2021-05-14 11:16:28,288] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 1825 bytes result sent to driver
[2021-05-14 11:16:28,289] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (0d1d3e01326c, executor driver, partition 99, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,292] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100) (0d1d3e01326c, executor driver, partition 100, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,293] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
[2021-05-14 11:16:28,294] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 194 ms on 0d1d3e01326c (executor driver) (96/141)
[2021-05-14 11:16:28,295] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 189 ms on 0d1d3e01326c (executor driver) (97/141)
[2021-05-14 11:16:28,295] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
[2021-05-14 11:16:28,298] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 1825 bytes result sent to driver
[2021-05-14 11:16:28,299] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101) (0d1d3e01326c, executor driver, partition 101, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,300] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 187 ms on 0d1d3e01326c (executor driver) (98/141)
[2021-05-14 11:16:28,301] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
[2021-05-14 11:16:28,474] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 1825 bytes result sent to driver
[2021-05-14 11:16:28,476] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102) (0d1d3e01326c, executor driver, partition 102, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,478] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
[2021-05-14 11:16:28,479] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 189 ms on 0d1d3e01326c (executor driver) (99/141)
[2021-05-14 11:16:28,479] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 1825 bytes result sent to driver
[2021-05-14 11:16:28,481] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103) (0d1d3e01326c, executor driver, partition 103, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,481] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
21/05/14 14:16:28 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 182 ms on 0d1d3e01326c (executor driver) (100/141)
[2021-05-14 11:16:28,650] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 1825 bytes result sent to driver
[2021-05-14 11:16:28,652] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104) (0d1d3e01326c, executor driver, partition 104, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,652] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 172 ms on 0d1d3e01326c (executor driver) (101/141)
[2021-05-14 11:16:28,653] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
[2021-05-14 11:16:28,655] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 1825 bytes result sent to driver
[2021-05-14 11:16:28,660] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105) (0d1d3e01326c, executor driver, partition 105, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,661] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 182 ms on 0d1d3e01326c (executor driver) (102/141)
[2021-05-14 11:16:28,661] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
[2021-05-14 11:16:28,767] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 1868 bytes result sent to driver
[2021-05-14 11:16:28,769] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106) (0d1d3e01326c, executor driver, partition 106, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,771] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 654 ms on 0d1d3e01326c (executor driver) (103/141)
[2021-05-14 11:16:28,771] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
[2021-05-14 11:16:28,773] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 1868 bytes result sent to driver
[2021-05-14 11:16:28,775] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107) (0d1d3e01326c, executor driver, partition 107, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,776] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 485 ms on 0d1d3e01326c (executor driver) (104/141)
[2021-05-14 11:16:28,777] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
[2021-05-14 11:16:28,824] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 1868 bytes result sent to driver
[2021-05-14 11:16:28,826] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108) (0d1d3e01326c, executor driver, partition 108, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,827] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 176 ms on 0d1d3e01326c (executor driver) (105/141)
[2021-05-14 11:16:28,828] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
[2021-05-14 11:16:28,836] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 1868 bytes result sent to driver
[2021-05-14 11:16:28,837] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109) (0d1d3e01326c, executor driver, partition 109, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,838] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 182 ms on 0d1d3e01326c (executor driver) (106/141)
[2021-05-14 11:16:28,839] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
[2021-05-14 11:16:28,953] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 1825 bytes result sent to driver
[2021-05-14 11:16:28,954] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 1825 bytes result sent to driver
[2021-05-14 11:16:28,956] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110) (0d1d3e01326c, executor driver, partition 110, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,957] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 188 ms on 0d1d3e01326c (executor driver) (107/141)
[2021-05-14 11:16:28,958] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
[2021-05-14 11:16:28,959] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111) (0d1d3e01326c, executor driver, partition 111, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:28,960] {docker.py:276} INFO - 21/05/14 14:16:28 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 186 ms on 0d1d3e01326c (executor driver) (108/141)
[2021-05-14 11:16:28,961] {docker.py:276} INFO - 21/05/14 14:16:28 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
[2021-05-14 11:16:29,004] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 1825 bytes result sent to driver
[2021-05-14 11:16:29,005] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112) (0d1d3e01326c, executor driver, partition 112, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,007] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 181 ms on 0d1d3e01326c (executor driver) (109/141)
[2021-05-14 11:16:29,007] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
[2021-05-14 11:16:29,010] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 1825 bytes result sent to driver
[2021-05-14 11:16:29,011] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 174 ms on 0d1d3e01326c (executor driver) (110/141)
[2021-05-14 11:16:29,014] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113) (0d1d3e01326c, executor driver, partition 113, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,015] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
[2021-05-14 11:16:29,136] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 1825 bytes result sent to driver
[2021-05-14 11:16:29,137] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 1825 bytes result sent to driver
[2021-05-14 11:16:29,138] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114) (0d1d3e01326c, executor driver, partition 114, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,139] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 185 ms on 0d1d3e01326c (executor driver) (111/141)
[2021-05-14 11:16:29,141] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
[2021-05-14 11:16:29,141] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115) (0d1d3e01326c, executor driver, partition 115, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,142] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 183 ms on 0d1d3e01326c (executor driver) (112/141)
[2021-05-14 11:16:29,145] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
[2021-05-14 11:16:29,185] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 1825 bytes result sent to driver
21/05/14 14:16:29 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 1825 bytes result sent to driver
[2021-05-14 11:16:29,185] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116) (0d1d3e01326c, executor driver, partition 116, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,186] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
[2021-05-14 11:16:29,187] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117) (0d1d3e01326c, executor driver, partition 117, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,187] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
[2021-05-14 11:16:29,188] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 183 ms on 0d1d3e01326c (executor driver) (113/141)
[2021-05-14 11:16:29,189] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 177 ms on 0d1d3e01326c (executor driver) (114/141)
[2021-05-14 11:16:29,322] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 1825 bytes result sent to driver
[2021-05-14 11:16:29,329] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118) (0d1d3e01326c, executor driver, partition 118, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:29 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
[2021-05-14 11:16:29,329] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 188 ms on 0d1d3e01326c (executor driver) (115/141)
[2021-05-14 11:16:29,330] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 1825 bytes result sent to driver
[2021-05-14 11:16:29,331] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119) (0d1d3e01326c, executor driver, partition 119, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,331] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 188 ms on 0d1d3e01326c (executor driver) (116/141)
[2021-05-14 11:16:29,335] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
[2021-05-14 11:16:29,359] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 1825 bytes result sent to driver
[2021-05-14 11:16:29,360] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 1825 bytes result sent to driver
[2021-05-14 11:16:29,361] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120) (0d1d3e01326c, executor driver, partition 120, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,362] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
[2021-05-14 11:16:29,362] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 177 ms on 0d1d3e01326c (executor driver) (117/141)
[2021-05-14 11:16:29,363] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 177 ms on 0d1d3e01326c (executor driver) (118/141)
[2021-05-14 11:16:29,364] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121) (0d1d3e01326c, executor driver, partition 121, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,365] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
[2021-05-14 11:16:29,513] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 1868 bytes result sent to driver
[2021-05-14 11:16:29,514] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 1868 bytes result sent to driver
[2021-05-14 11:16:29,515] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122) (0d1d3e01326c, executor driver, partition 122, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,517] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
21/05/14 14:16:29 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123) (0d1d3e01326c, executor driver, partition 123, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,518] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 197 ms on 0d1d3e01326c (executor driver) (119/141)
[2021-05-14 11:16:29,518] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
[2021-05-14 11:16:29,519] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 192 ms on 0d1d3e01326c (executor driver) (120/141)
[2021-05-14 11:16:29,535] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 1868 bytes result sent to driver
[2021-05-14 11:16:29,536] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124) (0d1d3e01326c, executor driver, partition 124, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,537] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 177 ms on 0d1d3e01326c (executor driver) (121/141)
[2021-05-14 11:16:29,538] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
[2021-05-14 11:16:29,542] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 1868 bytes result sent to driver
[2021-05-14 11:16:29,543] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125) (0d1d3e01326c, executor driver, partition 125, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,544] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 180 ms on 0d1d3e01326c (executor driver) (122/141)
[2021-05-14 11:16:29,545] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
[2021-05-14 11:16:29,699] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 1825 bytes result sent to driver
[2021-05-14 11:16:29,700] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 1825 bytes result sent to driver
[2021-05-14 11:16:29,701] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126) (0d1d3e01326c, executor driver, partition 126, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,702] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 186 ms on 0d1d3e01326c (executor driver) (123/141)
[2021-05-14 11:16:29,703] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
[2021-05-14 11:16:29,705] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127) (0d1d3e01326c, executor driver, partition 127, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,705] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 192 ms on 0d1d3e01326c (executor driver) (124/141)
[2021-05-14 11:16:29,707] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
[2021-05-14 11:16:29,710] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 1825 bytes result sent to driver
[2021-05-14 11:16:29,711] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 175 ms on 0d1d3e01326c (executor driver) (125/141)
[2021-05-14 11:16:29,713] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128) (0d1d3e01326c, executor driver, partition 128, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:29 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 1825 bytes result sent to driver
[2021-05-14 11:16:29,714] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
[2021-05-14 11:16:29,715] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129) (0d1d3e01326c, executor driver, partition 129, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,715] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 173 ms on 0d1d3e01326c (executor driver) (126/141)
[2021-05-14 11:16:29,716] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
[2021-05-14 11:16:29,880] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 1825 bytes result sent to driver
[2021-05-14 11:16:29,883] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130) (0d1d3e01326c, executor driver, partition 130, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,885] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
[2021-05-14 11:16:29,886] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 185 ms on 0d1d3e01326c (executor driver) (127/141)
[2021-05-14 11:16:29,889] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 1825 bytes result sent to driver
[2021-05-14 11:16:29,891] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131) (0d1d3e01326c, executor driver, partition 131, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,892] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
[2021-05-14 11:16:29,892] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 179 ms on 0d1d3e01326c (executor driver) (128/141)
[2021-05-14 11:16:29,918] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 1825 bytes result sent to driver
[2021-05-14 11:16:29,920] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132) (0d1d3e01326c, executor driver, partition 132, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,921] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 218 ms on 0d1d3e01326c (executor driver) (129/141)
[2021-05-14 11:16:29,922] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
[2021-05-14 11:16:29,926] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 1825 bytes result sent to driver
[2021-05-14 11:16:29,927] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133) (0d1d3e01326c, executor driver, partition 133, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:29,927] {docker.py:276} INFO - 21/05/14 14:16:29 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 214 ms on 0d1d3e01326c (executor driver) (130/141)
[2021-05-14 11:16:29,928] {docker.py:276} INFO - 21/05/14 14:16:29 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
[2021-05-14 11:16:30,063] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 1825 bytes result sent to driver
[2021-05-14 11:16:30,065] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 1825 bytes result sent to driver
[2021-05-14 11:16:30,066] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134) (0d1d3e01326c, executor driver, partition 134, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,068] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 186 ms on 0d1d3e01326c (executor driver) (131/141)
[2021-05-14 11:16:30,069] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
[2021-05-14 11:16:30,069] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 179 ms on 0d1d3e01326c (executor driver) (132/141)
[2021-05-14 11:16:30,071] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135) (0d1d3e01326c, executor driver, partition 135, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,071] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
[2021-05-14 11:16:30,112] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 1825 bytes result sent to driver
[2021-05-14 11:16:30,113] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 1825 bytes result sent to driver
[2021-05-14 11:16:30,115] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136) (0d1d3e01326c, executor driver, partition 136, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,117] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 197 ms on 0d1d3e01326c (executor driver) (133/141)
21/05/14 14:16:30 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
[2021-05-14 11:16:30,119] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137) (0d1d3e01326c, executor driver, partition 137, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,120] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 193 ms on 0d1d3e01326c (executor driver) (134/141)
[2021-05-14 11:16:30,121] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
[2021-05-14 11:16:30,243] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 1868 bytes result sent to driver
[2021-05-14 11:16:30,244] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 1868 bytes result sent to driver
[2021-05-14 11:16:30,245] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138) (0d1d3e01326c, executor driver, partition 138, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,247] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 175 ms on 0d1d3e01326c (executor driver) (135/141)
[2021-05-14 11:16:30,248] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
[2021-05-14 11:16:30,249] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139) (0d1d3e01326c, executor driver, partition 139, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,249] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 184 ms on 0d1d3e01326c (executor driver) (136/141)
[2021-05-14 11:16:30,250] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
[2021-05-14 11:16:30,305] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 1868 bytes result sent to driver
[2021-05-14 11:16:30,306] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140) (0d1d3e01326c, executor driver, partition 140, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,308] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
21/05/14 14:16:30 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 1868 bytes result sent to driver
21/05/14 14:16:30 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 193 ms on 0d1d3e01326c (executor driver) (137/141)
[2021-05-14 11:16:30,316] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 198 ms on 0d1d3e01326c (executor driver) (138/141)
[2021-05-14 11:16:30,420] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 1825 bytes result sent to driver
[2021-05-14 11:16:30,422] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 178 ms on 0d1d3e01326c (executor driver) (139/141)
[2021-05-14 11:16:30,425] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 1825 bytes result sent to driver
[2021-05-14 11:16:30,426] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 178 ms on 0d1d3e01326c (executor driver) (140/141)
[2021-05-14 11:16:30,488] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 1825 bytes result sent to driver
[2021-05-14 11:16:30,489] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 183 ms on 0d1d3e01326c (executor driver) (141/141)
[2021-05-14 11:16:30,493] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-05-14 11:16:30,495] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 8.805 s
[2021-05-14 11:16:30,502] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 11:16:30,503] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2021-05-14 11:16:30,508] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 8.908898 s
[2021-05-14 11:16:30,555] {docker.py:276} INFO - 21/05/14 14:16:30 INFO InMemoryFileIndex: It took 9490 ms to list leaf files for 141 paths.
[2021-05-14 11:16:30,689] {docker.py:276} INFO - 21/05/14 14:16:30 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 141 paths. The first several paths are: s3a://udac-forex-project/1/2021-05-14/from_1620915321_to_1620917121.csv, s3a://udac-forex-project/1/2021-05-14/from_1620917121_to_1620918921.csv, s3a://udac-forex-project/1/2021-05-14/from_1620918921_to_1620920721.csv, s3a://udac-forex-project/1/2021-05-14/from_1620920721_to_1620922521.csv, s3a://udac-forex-project/1/2021-05-14/from_1620922521_to_1620924321.csv, s3a://udac-forex-project/1/2021-05-14/from_1620924321_to_1620926121.csv, s3a://udac-forex-project/1/2021-05-14/from_1620926121_to_1620927921.csv, s3a://udac-forex-project/1/2021-05-14/from_1620927921_to_1620929721.csv, s3a://udac-forex-project/1/2021-05-14/from_1620929721_to_1620931521.csv, s3a://udac-forex-project/1/2021-05-14/from_1620931521_to_1620933321.csv.
[2021-05-14 11:16:30,731] {docker.py:276} INFO - 21/05/14 14:16:30 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:30,734] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 141 output partitions
21/05/14 14:16:30 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
21/05/14 14:16:30 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:16:30,735] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:16:30,736] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:16:30,751] {docker.py:276} INFO - 21/05/14 14:16:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 84.9 KiB, free 934.2 MiB)
[2021-05-14 11:16:30,761] {docker.py:276} INFO - 21/05/14 14:16:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 934.2 MiB)
[2021-05-14 11:16:30,762] {docker.py:276} INFO - 21/05/14 14:16:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0d1d3e01326c:42543 (size: 30.3 KiB, free: 934.3 MiB)
[2021-05-14 11:16:30,763] {docker.py:276} INFO - 21/05/14 14:16:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:16:30,765] {docker.py:276} INFO - 21/05/14 14:16:30 INFO DAGScheduler: Submitting 141 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 14:16:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 141 tasks resource profile 0
[2021-05-14 11:16:30,767] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 141) (0d1d3e01326c, executor driver, partition 0, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,768] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 142) (0d1d3e01326c, executor driver, partition 1, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,769] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 143) (0d1d3e01326c, executor driver, partition 2, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,769] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 144) (0d1d3e01326c, executor driver, partition 3, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,770] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 2.0 in stage 1.0 (TID 143)
[2021-05-14 11:16:30,771] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 3.0 in stage 1.0 (TID 144)
[2021-05-14 11:16:30,772] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 1.0 in stage 1.0 (TID 142)
[2021-05-14 11:16:30,772] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 141)
[2021-05-14 11:16:30,853] {docker.py:276} INFO - 21/05/14 14:16:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0d1d3e01326c:42543 in memory (size: 30.2 KiB, free: 934.4 MiB)
[2021-05-14 11:16:30,944] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 1.0 in stage 1.0 (TID 142). 1825 bytes result sent to driver
[2021-05-14 11:16:30,945] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 3.0 in stage 1.0 (TID 144). 1825 bytes result sent to driver
[2021-05-14 11:16:30,945] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 145) (0d1d3e01326c, executor driver, partition 4, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,946] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 142) in 179 ms on 0d1d3e01326c (executor driver) (1/141)
[2021-05-14 11:16:30,947] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 4.0 in stage 1.0 (TID 145)
[2021-05-14 11:16:30,948] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 146) (0d1d3e01326c, executor driver, partition 5, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,949] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 141). 1825 bytes result sent to driver
21/05/14 14:16:30 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 144) in 180 ms on 0d1d3e01326c (executor driver) (2/141)
[2021-05-14 11:16:30,950] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 147) (0d1d3e01326c, executor driver, partition 6, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,951] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 5.0 in stage 1.0 (TID 146)
[2021-05-14 11:16:30,952] {docker.py:276} INFO - 21/05/14 14:16:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 141) in 185 ms on 0d1d3e01326c (executor driver) (3/141)
[2021-05-14 11:16:30,953] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Running task 6.0 in stage 1.0 (TID 147)
[2021-05-14 11:16:30,976] {docker.py:276} INFO - 21/05/14 14:16:30 INFO Executor: Finished task 2.0 in stage 1.0 (TID 143). 1825 bytes result sent to driver
[2021-05-14 11:16:30,977] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 148) (0d1d3e01326c, executor driver, partition 7, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:30,978] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 143) in 210 ms on 0d1d3e01326c (executor driver) (4/141)
21/05/14 14:16:31 INFO Executor: Running task 7.0 in stage 1.0 (TID 148)
[2021-05-14 11:16:31,124] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 4.0 in stage 1.0 (TID 145). 1825 bytes result sent to driver
[2021-05-14 11:16:31,126] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 149) (0d1d3e01326c, executor driver, partition 8, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,127] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 145) in 182 ms on 0d1d3e01326c (executor driver) (5/141)
[2021-05-14 11:16:31,127] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 8.0 in stage 1.0 (TID 149)
[2021-05-14 11:16:31,129] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 6.0 in stage 1.0 (TID 147). 1825 bytes result sent to driver
[2021-05-14 11:16:31,131] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 150) (0d1d3e01326c, executor driver, partition 9, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,132] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 9.0 in stage 1.0 (TID 150)
21/05/14 14:16:31 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 147) in 182 ms on 0d1d3e01326c (executor driver) (6/141)
[2021-05-14 11:16:31,136] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 5.0 in stage 1.0 (TID 146). 1825 bytes result sent to driver
[2021-05-14 11:16:31,137] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 151) (0d1d3e01326c, executor driver, partition 10, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,138] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 146) in 190 ms on 0d1d3e01326c (executor driver) (7/141)
[2021-05-14 11:16:31,139] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 10.0 in stage 1.0 (TID 151)
[2021-05-14 11:16:31,148] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 7.0 in stage 1.0 (TID 148). 1825 bytes result sent to driver
[2021-05-14 11:16:31,149] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 152) (0d1d3e01326c, executor driver, partition 11, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,151] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 148) in 174 ms on 0d1d3e01326c (executor driver) (8/141)
[2021-05-14 11:16:31,152] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 11.0 in stage 1.0 (TID 152)
[2021-05-14 11:16:31,309] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 8.0 in stage 1.0 (TID 149). 1825 bytes result sent to driver
[2021-05-14 11:16:31,311] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 10.0 in stage 1.0 (TID 151). 1825 bytes result sent to driver
[2021-05-14 11:16:31,312] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 153) (0d1d3e01326c, executor driver, partition 12, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,313] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 12.0 in stage 1.0 (TID 153)
[2021-05-14 11:16:31,314] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 154) (0d1d3e01326c, executor driver, partition 13, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,315] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 149) in 190 ms on 0d1d3e01326c (executor driver) (9/141)
[2021-05-14 11:16:31,316] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 13.0 in stage 1.0 (TID 154)
[2021-05-14 11:16:31,317] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 151) in 180 ms on 0d1d3e01326c (executor driver) (10/141)
[2021-05-14 11:16:31,318] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 9.0 in stage 1.0 (TID 150). 1825 bytes result sent to driver
[2021-05-14 11:16:31,319] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 155) (0d1d3e01326c, executor driver, partition 14, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,321] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 14.0 in stage 1.0 (TID 155)
[2021-05-14 11:16:31,322] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 150) in 191 ms on 0d1d3e01326c (executor driver) (11/141)
[2021-05-14 11:16:31,334] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 11.0 in stage 1.0 (TID 152). 1868 bytes result sent to driver
[2021-05-14 11:16:31,335] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 156) (0d1d3e01326c, executor driver, partition 15, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,336] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 152) in 187 ms on 0d1d3e01326c (executor driver) (12/141)
[2021-05-14 11:16:31,337] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 15.0 in stage 1.0 (TID 156)
[2021-05-14 11:16:31,498] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 12.0 in stage 1.0 (TID 153). 1868 bytes result sent to driver
[2021-05-14 11:16:31,500] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 14.0 in stage 1.0 (TID 155). 1868 bytes result sent to driver
21/05/14 14:16:31 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 157) (0d1d3e01326c, executor driver, partition 16, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,501] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 16.0 in stage 1.0 (TID 157)
[2021-05-14 11:16:31,501] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 13.0 in stage 1.0 (TID 154). 1868 bytes result sent to driver
[2021-05-14 11:16:31,502] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 158) (0d1d3e01326c, executor driver, partition 17, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,504] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 153) in 194 ms on 0d1d3e01326c (executor driver) (13/141)
[2021-05-14 11:16:31,504] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 17.0 in stage 1.0 (TID 158)
[2021-05-14 11:16:31,505] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 155) in 186 ms on 0d1d3e01326c (executor driver) (14/141)
[2021-05-14 11:16:31,506] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 154) in 193 ms on 0d1d3e01326c (executor driver) (15/141)
[2021-05-14 11:16:31,508] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 159) (0d1d3e01326c, executor driver, partition 18, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,511] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 18.0 in stage 1.0 (TID 159)
[2021-05-14 11:16:31,512] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 15.0 in stage 1.0 (TID 156). 1825 bytes result sent to driver
[2021-05-14 11:16:31,513] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 160) (0d1d3e01326c, executor driver, partition 19, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,515] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 156) in 180 ms on 0d1d3e01326c (executor driver) (16/141)
21/05/14 14:16:31 INFO Executor: Running task 19.0 in stage 1.0 (TID 160)
[2021-05-14 11:16:31,675] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 16.0 in stage 1.0 (TID 157). 1825 bytes result sent to driver
[2021-05-14 11:16:31,677] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 161) (0d1d3e01326c, executor driver, partition 20, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,679] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 157) in 180 ms on 0d1d3e01326c (executor driver) (17/141)
[2021-05-14 11:16:31,680] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 20.0 in stage 1.0 (TID 161)
[2021-05-14 11:16:31,685] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 17.0 in stage 1.0 (TID 158). 1825 bytes result sent to driver
[2021-05-14 11:16:31,686] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 162) (0d1d3e01326c, executor driver, partition 21, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,688] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 158) in 186 ms on 0d1d3e01326c (executor driver) (18/141)
[2021-05-14 11:16:31,690] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 21.0 in stage 1.0 (TID 162)
[2021-05-14 11:16:31,693] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 19.0 in stage 1.0 (TID 160). 1825 bytes result sent to driver
[2021-05-14 11:16:31,694] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 163) (0d1d3e01326c, executor driver, partition 22, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,695] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 160) in 182 ms on 0d1d3e01326c (executor driver) (19/141)
[2021-05-14 11:16:31,696] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 22.0 in stage 1.0 (TID 163)
[2021-05-14 11:16:31,697] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 18.0 in stage 1.0 (TID 159). 1825 bytes result sent to driver
[2021-05-14 11:16:31,698] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 164) (0d1d3e01326c, executor driver, partition 23, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,699] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 159) in 192 ms on 0d1d3e01326c (executor driver) (20/141)
[2021-05-14 11:16:31,700] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 23.0 in stage 1.0 (TID 164)
[2021-05-14 11:16:31,862] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 21.0 in stage 1.0 (TID 162). 1825 bytes result sent to driver
[2021-05-14 11:16:31,864] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 165) (0d1d3e01326c, executor driver, partition 24, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,866] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 162) in 181 ms on 0d1d3e01326c (executor driver) (21/141)
[2021-05-14 11:16:31,867] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 24.0 in stage 1.0 (TID 165)
[2021-05-14 11:16:31,868] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 23.0 in stage 1.0 (TID 164). 1825 bytes result sent to driver
[2021-05-14 11:16:31,869] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 166) (0d1d3e01326c, executor driver, partition 25, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,870] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Finished task 20.0 in stage 1.0 (TID 161). 1825 bytes result sent to driver
[2021-05-14 11:16:31,872] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 164) in 172 ms on 0d1d3e01326c (executor driver) (22/141)
[2021-05-14 11:16:31,873] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 25.0 in stage 1.0 (TID 166)
21/05/14 14:16:31 INFO Executor: Finished task 22.0 in stage 1.0 (TID 163). 1825 bytes result sent to driver
[2021-05-14 11:16:31,874] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 167) (0d1d3e01326c, executor driver, partition 26, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,876] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 161) in 200 ms on 0d1d3e01326c (executor driver) (23/141)
[2021-05-14 11:16:31,876] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 26.0 in stage 1.0 (TID 167)
[2021-05-14 11:16:31,877] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 163) in 183 ms on 0d1d3e01326c (executor driver) (24/141)
[2021-05-14 11:16:31,878] {docker.py:276} INFO - 21/05/14 14:16:31 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 168) (0d1d3e01326c, executor driver, partition 27, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:31,880] {docker.py:276} INFO - 21/05/14 14:16:31 INFO Executor: Running task 27.0 in stage 1.0 (TID 168)
[2021-05-14 11:16:32,051] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 27.0 in stage 1.0 (TID 168). 1825 bytes result sent to driver
[2021-05-14 11:16:32,057] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 169) (0d1d3e01326c, executor driver, partition 28, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,057] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 168) in 175 ms on 0d1d3e01326c (executor driver) (25/141)
[2021-05-14 11:16:32,059] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 26.0 in stage 1.0 (TID 167). 1825 bytes result sent to driver
[2021-05-14 11:16:32,060] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 28.0 in stage 1.0 (TID 169)
[2021-05-14 11:16:32,060] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 170) (0d1d3e01326c, executor driver, partition 29, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,061] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 167) in 182 ms on 0d1d3e01326c (executor driver) (26/141)
[2021-05-14 11:16:32,066] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 29.0 in stage 1.0 (TID 170)
[2021-05-14 11:16:32,066] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 24.0 in stage 1.0 (TID 165). 1825 bytes result sent to driver
[2021-05-14 11:16:32,066] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 171) (0d1d3e01326c, executor driver, partition 30, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,067] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 165) in 197 ms on 0d1d3e01326c (executor driver) (27/141)
[2021-05-14 11:16:32,077] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 30.0 in stage 1.0 (TID 171)
[2021-05-14 11:16:32,143] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 25.0 in stage 1.0 (TID 166). 1868 bytes result sent to driver
[2021-05-14 11:16:32,145] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 172) (0d1d3e01326c, executor driver, partition 31, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,146] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 31.0 in stage 1.0 (TID 172)
21/05/14 14:16:32 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 166) in 278 ms on 0d1d3e01326c (executor driver) (28/141)
[2021-05-14 11:16:32,268] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 28.0 in stage 1.0 (TID 169). 1868 bytes result sent to driver
[2021-05-14 11:16:32,269] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 173) (0d1d3e01326c, executor driver, partition 32, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,270] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 30.0 in stage 1.0 (TID 171). 1868 bytes result sent to driver
[2021-05-14 11:16:32,271] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 169) in 220 ms on 0d1d3e01326c (executor driver) (29/141)
[2021-05-14 11:16:32,272] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 32.0 in stage 1.0 (TID 173)
[2021-05-14 11:16:32,274] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 174) (0d1d3e01326c, executor driver, partition 33, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,275] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 171) in 216 ms on 0d1d3e01326c (executor driver) (30/141)
21/05/14 14:16:32 INFO Executor: Running task 33.0 in stage 1.0 (TID 174)
[2021-05-14 11:16:32,279] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 29.0 in stage 1.0 (TID 170). 1868 bytes result sent to driver
[2021-05-14 11:16:32,280] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 175) (0d1d3e01326c, executor driver, partition 34, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,280] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 170) in 225 ms on 0d1d3e01326c (executor driver) (31/141)
[2021-05-14 11:16:32,280] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 34.0 in stage 1.0 (TID 175)
[2021-05-14 11:16:32,329] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 31.0 in stage 1.0 (TID 172). 1825 bytes result sent to driver
[2021-05-14 11:16:32,331] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 176) (0d1d3e01326c, executor driver, partition 35, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,333] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 172) in 188 ms on 0d1d3e01326c (executor driver) (32/141)
[2021-05-14 11:16:32,334] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 35.0 in stage 1.0 (TID 176)
[2021-05-14 11:16:32,459] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 32.0 in stage 1.0 (TID 173). 1825 bytes result sent to driver
[2021-05-14 11:16:32,461] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 33.0 in stage 1.0 (TID 174). 1825 bytes result sent to driver
[2021-05-14 11:16:32,462] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 34.0 in stage 1.0 (TID 175). 1825 bytes result sent to driver
[2021-05-14 11:16:32,462] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 177) (0d1d3e01326c, executor driver, partition 36, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,463] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 36.0 in stage 1.0 (TID 177)
21/05/14 14:16:32 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 178) (0d1d3e01326c, executor driver, partition 37, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,465] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 37.0 in stage 1.0 (TID 178)
21/05/14 14:16:32 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 174) in 191 ms on 0d1d3e01326c (executor driver) (33/141)
[2021-05-14 11:16:32,466] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 173) in 198 ms on 0d1d3e01326c (executor driver) (34/141)
[2021-05-14 11:16:32,467] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 175) in 188 ms on 0d1d3e01326c (executor driver) (35/141)
[2021-05-14 11:16:32,469] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 179) (0d1d3e01326c, executor driver, partition 38, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,470] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 38.0 in stage 1.0 (TID 179)
[2021-05-14 11:16:32,520] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 35.0 in stage 1.0 (TID 176). 1825 bytes result sent to driver
[2021-05-14 11:16:32,522] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 180) (0d1d3e01326c, executor driver, partition 39, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,523] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 39.0 in stage 1.0 (TID 180)
[2021-05-14 11:16:32,523] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 176) in 192 ms on 0d1d3e01326c (executor driver) (36/141)
[2021-05-14 11:16:32,640] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 36.0 in stage 1.0 (TID 177). 1825 bytes result sent to driver
[2021-05-14 11:16:32,642] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 38.0 in stage 1.0 (TID 179). 1825 bytes result sent to driver
[2021-05-14 11:16:32,642] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 37.0 in stage 1.0 (TID 178). 1825 bytes result sent to driver
21/05/14 14:16:32 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 181) (0d1d3e01326c, executor driver, partition 40, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,646] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 177) in 184 ms on 0d1d3e01326c (executor driver) (37/141)
[2021-05-14 11:16:32,647] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 40.0 in stage 1.0 (TID 181)
[2021-05-14 11:16:32,648] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 182) (0d1d3e01326c, executor driver, partition 41, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,648] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 179) in 177 ms on 0d1d3e01326c (executor driver) (38/141)
[2021-05-14 11:16:32,648] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 41.0 in stage 1.0 (TID 182)
21/05/14 14:16:32 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 183) (0d1d3e01326c, executor driver, partition 42, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,649] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 42.0 in stage 1.0 (TID 183)
21/05/14 14:16:32 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 178) in 185 ms on 0d1d3e01326c (executor driver) (39/141)
[2021-05-14 11:16:32,698] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 39.0 in stage 1.0 (TID 180). 1825 bytes result sent to driver
[2021-05-14 11:16:32,699] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 184) (0d1d3e01326c, executor driver, partition 43, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,700] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 43.0 in stage 1.0 (TID 184)
[2021-05-14 11:16:32,701] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 180) in 180 ms on 0d1d3e01326c (executor driver) (40/141)
[2021-05-14 11:16:32,823] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 40.0 in stage 1.0 (TID 181). 1868 bytes result sent to driver
[2021-05-14 11:16:32,824] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 41.0 in stage 1.0 (TID 182). 1868 bytes result sent to driver
[2021-05-14 11:16:32,825] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 185) (0d1d3e01326c, executor driver, partition 44, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,826] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 44.0 in stage 1.0 (TID 185)
[2021-05-14 11:16:32,827] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 186) (0d1d3e01326c, executor driver, partition 45, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,828] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 181) in 187 ms on 0d1d3e01326c (executor driver) (41/141)
[2021-05-14 11:16:32,828] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 182) in 183 ms on 0d1d3e01326c (executor driver) (42/141)
[2021-05-14 11:16:32,830] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 42.0 in stage 1.0 (TID 183). 1868 bytes result sent to driver
[2021-05-14 11:16:32,831] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 187) (0d1d3e01326c, executor driver, partition 46, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:32 INFO Executor: Running task 45.0 in stage 1.0 (TID 186)
[2021-05-14 11:16:32,832] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 183) in 186 ms on 0d1d3e01326c (executor driver) (43/141)
21/05/14 14:16:32 INFO Executor: Running task 46.0 in stage 1.0 (TID 187)
[2021-05-14 11:16:32,885] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Finished task 43.0 in stage 1.0 (TID 184). 1868 bytes result sent to driver
[2021-05-14 11:16:32,886] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 188) (0d1d3e01326c, executor driver, partition 47, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:32,888] {docker.py:276} INFO - 21/05/14 14:16:32 INFO Executor: Running task 47.0 in stage 1.0 (TID 188)
[2021-05-14 11:16:32,889] {docker.py:276} INFO - 21/05/14 14:16:32 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 184) in 189 ms on 0d1d3e01326c (executor driver) (44/141)
[2021-05-14 11:16:33,003] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 46.0 in stage 1.0 (TID 187). 1825 bytes result sent to driver
[2021-05-14 11:16:33,004] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 45.0 in stage 1.0 (TID 186). 1825 bytes result sent to driver
21/05/14 14:16:33 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 189) (0d1d3e01326c, executor driver, partition 48, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,007] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 48.0 in stage 1.0 (TID 189)
21/05/14 14:16:33 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 190) (0d1d3e01326c, executor driver, partition 49, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,008] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 187) in 176 ms on 0d1d3e01326c (executor driver) (45/141)
[2021-05-14 11:16:33,008] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 49.0 in stage 1.0 (TID 190)
[2021-05-14 11:16:33,009] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 44.0 in stage 1.0 (TID 185). 1825 bytes result sent to driver
21/05/14 14:16:33 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 186) in 183 ms on 0d1d3e01326c (executor driver) (46/141)
[2021-05-14 11:16:33,010] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 191) (0d1d3e01326c, executor driver, partition 50, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,012] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 185) in 187 ms on 0d1d3e01326c (executor driver) (47/141)
[2021-05-14 11:16:33,013] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 50.0 in stage 1.0 (TID 191)
[2021-05-14 11:16:33,068] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 47.0 in stage 1.0 (TID 188). 1825 bytes result sent to driver
21/05/14 14:16:33 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 192) (0d1d3e01326c, executor driver, partition 51, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:33 INFO Executor: Running task 51.0 in stage 1.0 (TID 192)
21/05/14 14:16:33 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 188) in 177 ms on 0d1d3e01326c (executor driver) (48/141)
[2021-05-14 11:16:33,189] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 50.0 in stage 1.0 (TID 191). 1825 bytes result sent to driver
21/05/14 14:16:33 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 193) (0d1d3e01326c, executor driver, partition 52, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:33 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 191) in 176 ms on 0d1d3e01326c (executor driver) (49/141)
[2021-05-14 11:16:33,190] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 52.0 in stage 1.0 (TID 193)
[2021-05-14 11:16:33,190] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 48.0 in stage 1.0 (TID 189). 1825 bytes result sent to driver
[2021-05-14 11:16:33,191] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 194) (0d1d3e01326c, executor driver, partition 53, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,192] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 189) in 186 ms on 0d1d3e01326c (executor driver) (50/141)
21/05/14 14:16:33 INFO Executor: Running task 53.0 in stage 1.0 (TID 194)
[2021-05-14 11:16:33,209] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 49.0 in stage 1.0 (TID 190). 1825 bytes result sent to driver
[2021-05-14 11:16:33,210] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 195) (0d1d3e01326c, executor driver, partition 54, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,211] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 190) in 206 ms on 0d1d3e01326c (executor driver) (51/141)
[2021-05-14 11:16:33,212] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 54.0 in stage 1.0 (TID 195)
[2021-05-14 11:16:33,240] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 51.0 in stage 1.0 (TID 192). 1825 bytes result sent to driver
[2021-05-14 11:16:33,242] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 196) (0d1d3e01326c, executor driver, partition 55, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,242] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 192) in 182 ms on 0d1d3e01326c (executor driver) (52/141)
21/05/14 14:16:33 INFO Executor: Running task 55.0 in stage 1.0 (TID 196)
[2021-05-14 11:16:33,362] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 52.0 in stage 1.0 (TID 193). 1825 bytes result sent to driver
[2021-05-14 11:16:33,363] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 197) (0d1d3e01326c, executor driver, partition 56, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,364] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 193) in 180 ms on 0d1d3e01326c (executor driver) (53/141)
21/05/14 14:16:33 INFO Executor: Running task 56.0 in stage 1.0 (TID 197)
[2021-05-14 11:16:33,365] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 53.0 in stage 1.0 (TID 194). 1825 bytes result sent to driver
[2021-05-14 11:16:33,366] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 198) (0d1d3e01326c, executor driver, partition 57, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,367] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 194) in 178 ms on 0d1d3e01326c (executor driver) (54/141)
[2021-05-14 11:16:33,367] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 57.0 in stage 1.0 (TID 198)
[2021-05-14 11:16:33,387] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 54.0 in stage 1.0 (TID 195). 1825 bytes result sent to driver
[2021-05-14 11:16:33,389] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 199) (0d1d3e01326c, executor driver, partition 58, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,391] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 195) in 180 ms on 0d1d3e01326c (executor driver) (55/141)
21/05/14 14:16:33 INFO Executor: Running task 58.0 in stage 1.0 (TID 199)
[2021-05-14 11:16:33,423] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 55.0 in stage 1.0 (TID 196). 1868 bytes result sent to driver
[2021-05-14 11:16:33,424] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 200) (0d1d3e01326c, executor driver, partition 59, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,425] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 59.0 in stage 1.0 (TID 200)
[2021-05-14 11:16:33,426] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 196) in 184 ms on 0d1d3e01326c (executor driver) (56/141)
[2021-05-14 11:16:33,533] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 56.0 in stage 1.0 (TID 197). 1868 bytes result sent to driver
[2021-05-14 11:16:33,534] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 57.0 in stage 1.0 (TID 198). 1868 bytes result sent to driver
[2021-05-14 11:16:33,535] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 201) (0d1d3e01326c, executor driver, partition 60, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,539] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 60.0 in stage 1.0 (TID 201)
[2021-05-14 11:16:33,539] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 202) (0d1d3e01326c, executor driver, partition 61, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:33 INFO Executor: Running task 61.0 in stage 1.0 (TID 202)
21/05/14 14:16:33 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 198) in 172 ms on 0d1d3e01326c (executor driver) (57/141)
[2021-05-14 11:16:33,540] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 197) in 175 ms on 0d1d3e01326c (executor driver) (58/141)
[2021-05-14 11:16:33,576] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 58.0 in stage 1.0 (TID 199). 1868 bytes result sent to driver
[2021-05-14 11:16:33,577] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 203) (0d1d3e01326c, executor driver, partition 62, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,578] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 199) in 189 ms on 0d1d3e01326c (executor driver) (59/141)
[2021-05-14 11:16:33,579] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 62.0 in stage 1.0 (TID 203)
[2021-05-14 11:16:33,604] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 59.0 in stage 1.0 (TID 200). 1825 bytes result sent to driver
[2021-05-14 11:16:33,606] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 204) (0d1d3e01326c, executor driver, partition 63, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,607] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 200) in 182 ms on 0d1d3e01326c (executor driver) (60/141)
[2021-05-14 11:16:33,608] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 63.0 in stage 1.0 (TID 204)
[2021-05-14 11:16:33,709] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 60.0 in stage 1.0 (TID 201). 1825 bytes result sent to driver
[2021-05-14 11:16:33,710] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 205) (0d1d3e01326c, executor driver, partition 64, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,712] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 201) in 177 ms on 0d1d3e01326c (executor driver) (61/141)
21/05/14 14:16:33 INFO Executor: Running task 64.0 in stage 1.0 (TID 205)
[2021-05-14 11:16:33,714] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 61.0 in stage 1.0 (TID 202). 1825 bytes result sent to driver
[2021-05-14 11:16:33,715] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 206) (0d1d3e01326c, executor driver, partition 65, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,716] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 202) in 179 ms on 0d1d3e01326c (executor driver) (62/141)
[2021-05-14 11:16:33,716] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 65.0 in stage 1.0 (TID 206)
[2021-05-14 11:16:33,757] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 62.0 in stage 1.0 (TID 203). 1825 bytes result sent to driver
[2021-05-14 11:16:33,758] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 207) (0d1d3e01326c, executor driver, partition 66, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,759] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 66.0 in stage 1.0 (TID 207)
21/05/14 14:16:33 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 203) in 182 ms on 0d1d3e01326c (executor driver) (63/141)
[2021-05-14 11:16:33,822] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 63.0 in stage 1.0 (TID 204). 1825 bytes result sent to driver
[2021-05-14 11:16:33,824] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 208) (0d1d3e01326c, executor driver, partition 67, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,825] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 67.0 in stage 1.0 (TID 208)
[2021-05-14 11:16:33,826] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 204) in 220 ms on 0d1d3e01326c (executor driver) (64/141)
[2021-05-14 11:16:33,884] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 65.0 in stage 1.0 (TID 206). 1825 bytes result sent to driver
[2021-05-14 11:16:33,885] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 209) (0d1d3e01326c, executor driver, partition 68, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,886] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 64.0 in stage 1.0 (TID 205). 1825 bytes result sent to driver
[2021-05-14 11:16:33,886] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 206) in 172 ms on 0d1d3e01326c (executor driver) (65/141)
[2021-05-14 11:16:33,887] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 68.0 in stage 1.0 (TID 209)
[2021-05-14 11:16:33,888] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 210) (0d1d3e01326c, executor driver, partition 69, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,889] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 205) in 180 ms on 0d1d3e01326c (executor driver) (66/141)
[2021-05-14 11:16:33,890] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 69.0 in stage 1.0 (TID 210)
[2021-05-14 11:16:33,940] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Finished task 66.0 in stage 1.0 (TID 207). 1825 bytes result sent to driver
[2021-05-14 11:16:33,941] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 211) (0d1d3e01326c, executor driver, partition 70, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:33,942] {docker.py:276} INFO - 21/05/14 14:16:33 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 207) in 185 ms on 0d1d3e01326c (executor driver) (67/141)
[2021-05-14 11:16:33,943] {docker.py:276} INFO - 21/05/14 14:16:33 INFO Executor: Running task 70.0 in stage 1.0 (TID 211)
[2021-05-14 11:16:34,005] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 67.0 in stage 1.0 (TID 208). 1825 bytes result sent to driver
[2021-05-14 11:16:34,008] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 212) (0d1d3e01326c, executor driver, partition 71, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,009] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 208) in 186 ms on 0d1d3e01326c (executor driver) (68/141)
21/05/14 14:16:34 INFO Executor: Running task 71.0 in stage 1.0 (TID 212)
[2021-05-14 11:16:34,062] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 69.0 in stage 1.0 (TID 210). 1825 bytes result sent to driver
[2021-05-14 11:16:34,064] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 213) (0d1d3e01326c, executor driver, partition 72, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,065] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 210) in 178 ms on 0d1d3e01326c (executor driver) (69/141)
[2021-05-14 11:16:34,066] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 72.0 in stage 1.0 (TID 213)
[2021-05-14 11:16:34,077] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 68.0 in stage 1.0 (TID 209). 1825 bytes result sent to driver
[2021-05-14 11:16:34,084] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 214) (0d1d3e01326c, executor driver, partition 73, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,084] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 209) in 201 ms on 0d1d3e01326c (executor driver) (70/141)
21/05/14 14:16:34 INFO Executor: Running task 73.0 in stage 1.0 (TID 214)
[2021-05-14 11:16:34,124] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 70.0 in stage 1.0 (TID 211). 1868 bytes result sent to driver
[2021-05-14 11:16:34,125] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 215) (0d1d3e01326c, executor driver, partition 74, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,126] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 74.0 in stage 1.0 (TID 215)
21/05/14 14:16:34 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 211) in 186 ms on 0d1d3e01326c (executor driver) (71/141)
[2021-05-14 11:16:34,190] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 71.0 in stage 1.0 (TID 212). 1868 bytes result sent to driver
[2021-05-14 11:16:34,191] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 216) (0d1d3e01326c, executor driver, partition 75, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,192] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 212) in 185 ms on 0d1d3e01326c (executor driver) (72/141)
21/05/14 14:16:34 INFO Executor: Running task 75.0 in stage 1.0 (TID 216)
[2021-05-14 11:16:34,237] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 72.0 in stage 1.0 (TID 213). 1868 bytes result sent to driver
[2021-05-14 11:16:34,239] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 217) (0d1d3e01326c, executor driver, partition 76, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,241] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 76.0 in stage 1.0 (TID 217)
21/05/14 14:16:34 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 213) in 177 ms on 0d1d3e01326c (executor driver) (73/141)
[2021-05-14 11:16:34,252] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 73.0 in stage 1.0 (TID 214). 1825 bytes result sent to driver
[2021-05-14 11:16:34,254] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 218) (0d1d3e01326c, executor driver, partition 77, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,254] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 214) in 171 ms on 0d1d3e01326c (executor driver) (74/141)
[2021-05-14 11:16:34,255] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 77.0 in stage 1.0 (TID 218)
[2021-05-14 11:16:34,309] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 74.0 in stage 1.0 (TID 215). 1825 bytes result sent to driver
21/05/14 14:16:34 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 219) (0d1d3e01326c, executor driver, partition 78, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
21/05/14 14:16:34 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 215) in 183 ms on 0d1d3e01326c (executor driver) (75/141)
21/05/14 14:16:34 INFO Executor: Running task 78.0 in stage 1.0 (TID 219)
[2021-05-14 11:16:34,370] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 75.0 in stage 1.0 (TID 216). 1825 bytes result sent to driver
[2021-05-14 11:16:34,372] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 220) (0d1d3e01326c, executor driver, partition 79, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,374] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 79.0 in stage 1.0 (TID 220)
21/05/14 14:16:34 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 216) in 182 ms on 0d1d3e01326c (executor driver) (76/141)
[2021-05-14 11:16:34,422] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 76.0 in stage 1.0 (TID 217). 1825 bytes result sent to driver
[2021-05-14 11:16:34,422] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 77.0 in stage 1.0 (TID 218). 1825 bytes result sent to driver
[2021-05-14 11:16:34,423] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 221) (0d1d3e01326c, executor driver, partition 80, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,424] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 217) in 186 ms on 0d1d3e01326c (executor driver) (77/141)
[2021-05-14 11:16:34,425] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 218) in 171 ms on 0d1d3e01326c (executor driver) (78/141)
[2021-05-14 11:16:34,425] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 80.0 in stage 1.0 (TID 221)
[2021-05-14 11:16:34,426] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 222) (0d1d3e01326c, executor driver, partition 81, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,429] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 81.0 in stage 1.0 (TID 222)
[2021-05-14 11:16:34,483] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 78.0 in stage 1.0 (TID 219). 1825 bytes result sent to driver
[2021-05-14 11:16:34,484] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 223) (0d1d3e01326c, executor driver, partition 82, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,485] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 219) in 179 ms on 0d1d3e01326c (executor driver) (79/141)
[2021-05-14 11:16:34,486] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 82.0 in stage 1.0 (TID 223)
[2021-05-14 11:16:34,553] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 79.0 in stage 1.0 (TID 220). 1825 bytes result sent to driver
[2021-05-14 11:16:34,555] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 224) (0d1d3e01326c, executor driver, partition 83, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,556] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 220) in 184 ms on 0d1d3e01326c (executor driver) (80/141)
[2021-05-14 11:16:34,557] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 83.0 in stage 1.0 (TID 224)
[2021-05-14 11:16:34,600] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 80.0 in stage 1.0 (TID 221). 1825 bytes result sent to driver
[2021-05-14 11:16:34,600] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 81.0 in stage 1.0 (TID 222). 1825 bytes result sent to driver
[2021-05-14 11:16:34,602] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 225) (0d1d3e01326c, executor driver, partition 84, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,603] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 221) in 179 ms on 0d1d3e01326c (executor driver) (81/141)
[2021-05-14 11:16:34,605] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 84.0 in stage 1.0 (TID 225)
[2021-05-14 11:16:34,606] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 226) (0d1d3e01326c, executor driver, partition 85, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,607] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 222) in 181 ms on 0d1d3e01326c (executor driver) (82/141)
[2021-05-14 11:16:34,608] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 85.0 in stage 1.0 (TID 226)
[2021-05-14 11:16:34,662] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 82.0 in stage 1.0 (TID 223). 1825 bytes result sent to driver
[2021-05-14 11:16:34,663] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 227) (0d1d3e01326c, executor driver, partition 86, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,664] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 223) in 181 ms on 0d1d3e01326c (executor driver) (83/141)
[2021-05-14 11:16:34,664] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 86.0 in stage 1.0 (TID 227)
[2021-05-14 11:16:34,733] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 83.0 in stage 1.0 (TID 224). 1825 bytes result sent to driver
[2021-05-14 11:16:34,734] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 228) (0d1d3e01326c, executor driver, partition 87, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,734] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 87.0 in stage 1.0 (TID 228)
[2021-05-14 11:16:34,735] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 224) in 181 ms on 0d1d3e01326c (executor driver) (84/141)
[2021-05-14 11:16:34,779] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 85.0 in stage 1.0 (TID 226). 1868 bytes result sent to driver
[2021-05-14 11:16:34,780] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 229) (0d1d3e01326c, executor driver, partition 88, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,782] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 88.0 in stage 1.0 (TID 229)
[2021-05-14 11:16:34,782] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 226) in 178 ms on 0d1d3e01326c (executor driver) (85/141)
[2021-05-14 11:16:34,783] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 84.0 in stage 1.0 (TID 225). 1868 bytes result sent to driver
[2021-05-14 11:16:34,786] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 230) (0d1d3e01326c, executor driver, partition 89, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,787] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 89.0 in stage 1.0 (TID 230)
[2021-05-14 11:16:34,788] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 225) in 187 ms on 0d1d3e01326c (executor driver) (86/141)
[2021-05-14 11:16:34,839] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 86.0 in stage 1.0 (TID 227). 1868 bytes result sent to driver
[2021-05-14 11:16:34,840] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 231) (0d1d3e01326c, executor driver, partition 90, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,841] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 227) in 179 ms on 0d1d3e01326c (executor driver) (87/141)
21/05/14 14:16:34 INFO Executor: Running task 90.0 in stage 1.0 (TID 231)
[2021-05-14 11:16:34,916] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 87.0 in stage 1.0 (TID 228). 1868 bytes result sent to driver
[2021-05-14 11:16:34,918] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 232) (0d1d3e01326c, executor driver, partition 91, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,919] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 228) in 184 ms on 0d1d3e01326c (executor driver) (88/141)
[2021-05-14 11:16:34,920] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 91.0 in stage 1.0 (TID 232)
[2021-05-14 11:16:34,963] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Finished task 89.0 in stage 1.0 (TID 230). 1825 bytes result sent to driver
[2021-05-14 11:16:34,965] {docker.py:276} INFO - 21/05/14 14:16:34 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 233) (0d1d3e01326c, executor driver, partition 92, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:34,967] {docker.py:276} INFO - 21/05/14 14:16:34 INFO Executor: Running task 92.0 in stage 1.0 (TID 233)
21/05/14 14:16:34 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 230) in 181 ms on 0d1d3e01326c (executor driver) (89/141)
[2021-05-14 11:16:35,013] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 90.0 in stage 1.0 (TID 231). 1825 bytes result sent to driver
[2021-05-14 11:16:35,014] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 234) (0d1d3e01326c, executor driver, partition 93, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,015] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 231) in 175 ms on 0d1d3e01326c (executor driver) (90/141)
[2021-05-14 11:16:35,016] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 93.0 in stage 1.0 (TID 234)
[2021-05-14 11:16:35,097] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 91.0 in stage 1.0 (TID 232). 1825 bytes result sent to driver
[2021-05-14 11:16:35,098] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 235) (0d1d3e01326c, executor driver, partition 94, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,099] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 94.0 in stage 1.0 (TID 235)
[2021-05-14 11:16:35,100] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 232) in 183 ms on 0d1d3e01326c (executor driver) (91/141)
[2021-05-14 11:16:35,135] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 92.0 in stage 1.0 (TID 233). 1825 bytes result sent to driver
[2021-05-14 11:16:35,137] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 236) (0d1d3e01326c, executor driver, partition 95, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,138] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 95.0 in stage 1.0 (TID 236)
[2021-05-14 11:16:35,139] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 233) in 174 ms on 0d1d3e01326c (executor driver) (92/141)
[2021-05-14 11:16:35,189] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 93.0 in stage 1.0 (TID 234). 1825 bytes result sent to driver
[2021-05-14 11:16:35,191] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 237) (0d1d3e01326c, executor driver, partition 96, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,193] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 234) in 177 ms on 0d1d3e01326c (executor driver) (93/141)
[2021-05-14 11:16:35,193] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 96.0 in stage 1.0 (TID 237)
[2021-05-14 11:16:35,278] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 94.0 in stage 1.0 (TID 235). 1825 bytes result sent to driver
[2021-05-14 11:16:35,280] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 238) (0d1d3e01326c, executor driver, partition 97, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,281] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 235) in 184 ms on 0d1d3e01326c (executor driver) (94/141)
[2021-05-14 11:16:35,282] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 97.0 in stage 1.0 (TID 238)
[2021-05-14 11:16:35,306] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 95.0 in stage 1.0 (TID 236). 1825 bytes result sent to driver
[2021-05-14 11:16:35,307] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 239) (0d1d3e01326c, executor driver, partition 98, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,309] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 236) in 172 ms on 0d1d3e01326c (executor driver) (95/141)
21/05/14 14:16:35 INFO Executor: Running task 98.0 in stage 1.0 (TID 239)
[2021-05-14 11:16:35,371] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 96.0 in stage 1.0 (TID 237). 1825 bytes result sent to driver
[2021-05-14 11:16:35,373] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 240) (0d1d3e01326c, executor driver, partition 99, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,375] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 99.0 in stage 1.0 (TID 240)
21/05/14 14:16:35 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 237) in 184 ms on 0d1d3e01326c (executor driver) (96/141)
[2021-05-14 11:16:35,457] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 97.0 in stage 1.0 (TID 238). 1825 bytes result sent to driver
[2021-05-14 11:16:35,460] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 241) (0d1d3e01326c, executor driver, partition 100, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,462] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 238) in 183 ms on 0d1d3e01326c (executor driver) (97/141)
[2021-05-14 11:16:35,463] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 100.0 in stage 1.0 (TID 241)
[2021-05-14 11:16:35,473] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 88.0 in stage 1.0 (TID 229). 1825 bytes result sent to driver
[2021-05-14 11:16:35,473] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 242) (0d1d3e01326c, executor driver, partition 101, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,474] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 101.0 in stage 1.0 (TID 242)
21/05/14 14:16:35 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 229) in 695 ms on 0d1d3e01326c (executor driver) (98/141)
[2021-05-14 11:16:35,484] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 98.0 in stage 1.0 (TID 239). 1868 bytes result sent to driver
[2021-05-14 11:16:35,485] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 243) (0d1d3e01326c, executor driver, partition 102, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,485] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 239) in 178 ms on 0d1d3e01326c (executor driver) (99/141)
[2021-05-14 11:16:35,486] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 102.0 in stage 1.0 (TID 243)
[2021-05-14 11:16:35,551] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 99.0 in stage 1.0 (TID 240). 1868 bytes result sent to driver
[2021-05-14 11:16:35,552] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 244) (0d1d3e01326c, executor driver, partition 103, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,553] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 240) in 181 ms on 0d1d3e01326c (executor driver) (100/141)
21/05/14 14:16:35 INFO Executor: Running task 103.0 in stage 1.0 (TID 244)
[2021-05-14 11:16:35,633] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 100.0 in stage 1.0 (TID 241). 1868 bytes result sent to driver
[2021-05-14 11:16:35,634] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 245) (0d1d3e01326c, executor driver, partition 104, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,635] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 241) in 176 ms on 0d1d3e01326c (executor driver) (101/141)
21/05/14 14:16:35 INFO Executor: Running task 104.0 in stage 1.0 (TID 245)
[2021-05-14 11:16:35,649] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 101.0 in stage 1.0 (TID 242). 1868 bytes result sent to driver
[2021-05-14 11:16:35,650] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 246) (0d1d3e01326c, executor driver, partition 105, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,651] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 242) in 178 ms on 0d1d3e01326c (executor driver) (102/141)
[2021-05-14 11:16:35,651] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 105.0 in stage 1.0 (TID 246)
[2021-05-14 11:16:35,658] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 102.0 in stage 1.0 (TID 243). 1825 bytes result sent to driver
[2021-05-14 11:16:35,659] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 247) (0d1d3e01326c, executor driver, partition 106, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,659] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 243) in 175 ms on 0d1d3e01326c (executor driver) (103/141)
[2021-05-14 11:16:35,660] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 106.0 in stage 1.0 (TID 247)
[2021-05-14 11:16:35,726] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 103.0 in stage 1.0 (TID 244). 1825 bytes result sent to driver
[2021-05-14 11:16:35,728] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 248) (0d1d3e01326c, executor driver, partition 107, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,729] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 107.0 in stage 1.0 (TID 248)
21/05/14 14:16:35 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 244) in 178 ms on 0d1d3e01326c (executor driver) (104/141)
[2021-05-14 11:16:35,805] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 104.0 in stage 1.0 (TID 245). 1825 bytes result sent to driver
[2021-05-14 11:16:35,805] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 249) (0d1d3e01326c, executor driver, partition 108, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,806] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 245) in 173 ms on 0d1d3e01326c (executor driver) (105/141)
21/05/14 14:16:35 INFO Executor: Running task 108.0 in stage 1.0 (TID 249)
[2021-05-14 11:16:35,818] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 105.0 in stage 1.0 (TID 246). 1825 bytes result sent to driver
[2021-05-14 11:16:35,819] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 250) (0d1d3e01326c, executor driver, partition 109, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,819] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 109.0 in stage 1.0 (TID 250)
[2021-05-14 11:16:35,820] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 246) in 170 ms on 0d1d3e01326c (executor driver) (106/141)
[2021-05-14 11:16:35,845] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 106.0 in stage 1.0 (TID 247). 1825 bytes result sent to driver
[2021-05-14 11:16:35,847] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 251) (0d1d3e01326c, executor driver, partition 110, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,849] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 110.0 in stage 1.0 (TID 251)
21/05/14 14:16:35 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 247) in 190 ms on 0d1d3e01326c (executor driver) (107/141)
[2021-05-14 11:16:35,906] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Finished task 107.0 in stage 1.0 (TID 248). 1825 bytes result sent to driver
[2021-05-14 11:16:35,908] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 252) (0d1d3e01326c, executor driver, partition 111, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,909] {docker.py:276} INFO - 21/05/14 14:16:35 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 248) in 181 ms on 0d1d3e01326c (executor driver) (108/141)
[2021-05-14 11:16:35,910] {docker.py:276} INFO - 21/05/14 14:16:35 INFO Executor: Running task 111.0 in stage 1.0 (TID 252)
[2021-05-14 11:16:35,982] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 108.0 in stage 1.0 (TID 249). 1825 bytes result sent to driver
[2021-05-14 11:16:35,984] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 253) (0d1d3e01326c, executor driver, partition 112, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,986] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 249) in 180 ms on 0d1d3e01326c (executor driver) (109/141)
[2021-05-14 11:16:35,986] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 112.0 in stage 1.0 (TID 253)
[2021-05-14 11:16:35,992] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 109.0 in stage 1.0 (TID 250). 1825 bytes result sent to driver
[2021-05-14 11:16:35,993] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 254) (0d1d3e01326c, executor driver, partition 113, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:35,994] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 113.0 in stage 1.0 (TID 254)
21/05/14 14:16:36 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 250) in 176 ms on 0d1d3e01326c (executor driver) (110/141)
[2021-05-14 11:16:36,022] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 110.0 in stage 1.0 (TID 251). 1825 bytes result sent to driver
[2021-05-14 11:16:36,023] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 255) (0d1d3e01326c, executor driver, partition 114, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,024] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 251) in 177 ms on 0d1d3e01326c (executor driver) (111/141)
[2021-05-14 11:16:36,025] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 114.0 in stage 1.0 (TID 255)
[2021-05-14 11:16:36,084] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 111.0 in stage 1.0 (TID 252). 1825 bytes result sent to driver
[2021-05-14 11:16:36,085] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 256) (0d1d3e01326c, executor driver, partition 115, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,087] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 252) in 180 ms on 0d1d3e01326c (executor driver) (112/141)
21/05/14 14:16:36 INFO Executor: Running task 115.0 in stage 1.0 (TID 256)
[2021-05-14 11:16:36,157] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 112.0 in stage 1.0 (TID 253). 1825 bytes result sent to driver
[2021-05-14 11:16:36,158] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 253) in 175 ms on 0d1d3e01326c (executor driver) (113/141)
[2021-05-14 11:16:36,160] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 257) (0d1d3e01326c, executor driver, partition 116, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,161] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 116.0 in stage 1.0 (TID 257)
[2021-05-14 11:16:36,178] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 113.0 in stage 1.0 (TID 254). 1868 bytes result sent to driver
[2021-05-14 11:16:36,178] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 258) (0d1d3e01326c, executor driver, partition 117, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,179] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 117.0 in stage 1.0 (TID 258)
[2021-05-14 11:16:36,180] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 254) in 187 ms on 0d1d3e01326c (executor driver) (114/141)
[2021-05-14 11:16:36,196] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 114.0 in stage 1.0 (TID 255). 1868 bytes result sent to driver
[2021-05-14 11:16:36,199] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 259) (0d1d3e01326c, executor driver, partition 118, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,200] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 118.0 in stage 1.0 (TID 259)
[2021-05-14 11:16:36,200] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 255) in 175 ms on 0d1d3e01326c (executor driver) (115/141)
[2021-05-14 11:16:36,261] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 115.0 in stage 1.0 (TID 256). 1868 bytes result sent to driver
[2021-05-14 11:16:36,262] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 256) in 178 ms on 0d1d3e01326c (executor driver) (116/141)
[2021-05-14 11:16:36,263] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 260) (0d1d3e01326c, executor driver, partition 119, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,264] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 119.0 in stage 1.0 (TID 260)
[2021-05-14 11:16:36,352] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 117.0 in stage 1.0 (TID 258). 1825 bytes result sent to driver
[2021-05-14 11:16:36,353] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 261) (0d1d3e01326c, executor driver, partition 120, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,354] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 258) in 176 ms on 0d1d3e01326c (executor driver) (117/141)
[2021-05-14 11:16:36,356] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 120.0 in stage 1.0 (TID 261)
[2021-05-14 11:16:36,357] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 116.0 in stage 1.0 (TID 257). 1868 bytes result sent to driver
[2021-05-14 11:16:36,357] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 262) (0d1d3e01326c, executor driver, partition 121, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,358] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 257) in 199 ms on 0d1d3e01326c (executor driver) (118/141)
[2021-05-14 11:16:36,359] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 121.0 in stage 1.0 (TID 262)
[2021-05-14 11:16:36,378] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 118.0 in stage 1.0 (TID 259). 1825 bytes result sent to driver
[2021-05-14 11:16:36,379] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 263) (0d1d3e01326c, executor driver, partition 122, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,379] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 259) in 183 ms on 0d1d3e01326c (executor driver) (119/141)
[2021-05-14 11:16:36,381] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 122.0 in stage 1.0 (TID 263)
[2021-05-14 11:16:36,437] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 119.0 in stage 1.0 (TID 260). 1825 bytes result sent to driver
[2021-05-14 11:16:36,438] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 264) (0d1d3e01326c, executor driver, partition 123, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,439] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 260) in 176 ms on 0d1d3e01326c (executor driver) (120/141)
[2021-05-14 11:16:36,440] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 123.0 in stage 1.0 (TID 264)
[2021-05-14 11:16:36,539] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 121.0 in stage 1.0 (TID 262). 1825 bytes result sent to driver
[2021-05-14 11:16:36,540] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 265) (0d1d3e01326c, executor driver, partition 124, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,541] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 124.0 in stage 1.0 (TID 265)
21/05/14 14:16:36 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 262) in 185 ms on 0d1d3e01326c (executor driver) (121/141)
[2021-05-14 11:16:36,552] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 120.0 in stage 1.0 (TID 261). 1825 bytes result sent to driver
[2021-05-14 11:16:36,553] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 266) (0d1d3e01326c, executor driver, partition 125, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,554] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 261) in 202 ms on 0d1d3e01326c (executor driver) (122/141)
21/05/14 14:16:36 INFO Executor: Running task 125.0 in stage 1.0 (TID 266)
[2021-05-14 11:16:36,555] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 122.0 in stage 1.0 (TID 263). 1825 bytes result sent to driver
[2021-05-14 11:16:36,556] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 267) (0d1d3e01326c, executor driver, partition 126, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,557] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 126.0 in stage 1.0 (TID 267)
[2021-05-14 11:16:36,557] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 263) in 179 ms on 0d1d3e01326c (executor driver) (123/141)
[2021-05-14 11:16:36,617] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 123.0 in stage 1.0 (TID 264). 1825 bytes result sent to driver
[2021-05-14 11:16:36,619] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 268) (0d1d3e01326c, executor driver, partition 127, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,620] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 127.0 in stage 1.0 (TID 268)
[2021-05-14 11:16:36,621] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 264) in 182 ms on 0d1d3e01326c (executor driver) (124/141)
[2021-05-14 11:16:36,716] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 124.0 in stage 1.0 (TID 265). 1825 bytes result sent to driver
[2021-05-14 11:16:36,717] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 269) (0d1d3e01326c, executor driver, partition 128, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,718] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 265) in 178 ms on 0d1d3e01326c (executor driver) (125/141)
21/05/14 14:16:36 INFO Executor: Running task 128.0 in stage 1.0 (TID 269)
[2021-05-14 11:16:36,730] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 125.0 in stage 1.0 (TID 266). 1825 bytes result sent to driver
[2021-05-14 11:16:36,730] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 270) (0d1d3e01326c, executor driver, partition 129, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,733] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 266) in 178 ms on 0d1d3e01326c (executor driver) (126/141)
[2021-05-14 11:16:36,733] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 129.0 in stage 1.0 (TID 270)
[2021-05-14 11:16:36,760] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 126.0 in stage 1.0 (TID 267). 1825 bytes result sent to driver
[2021-05-14 11:16:36,761] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 271) (0d1d3e01326c, executor driver, partition 130, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,762] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 267) in 206 ms on 0d1d3e01326c (executor driver) (127/141)
[2021-05-14 11:16:36,763] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 130.0 in stage 1.0 (TID 271)
[2021-05-14 11:16:36,798] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 127.0 in stage 1.0 (TID 268). 1825 bytes result sent to driver
[2021-05-14 11:16:36,800] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 272) (0d1d3e01326c, executor driver, partition 131, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,801] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 131.0 in stage 1.0 (TID 272)
[2021-05-14 11:16:36,802] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 268) in 183 ms on 0d1d3e01326c (executor driver) (128/141)
[2021-05-14 11:16:36,898] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 128.0 in stage 1.0 (TID 269). 1868 bytes result sent to driver
[2021-05-14 11:16:36,900] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 273) (0d1d3e01326c, executor driver, partition 132, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,900] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 269) in 183 ms on 0d1d3e01326c (executor driver) (129/141)
[2021-05-14 11:16:36,901] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 132.0 in stage 1.0 (TID 273)
[2021-05-14 11:16:36,940] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 130.0 in stage 1.0 (TID 271). 1868 bytes result sent to driver
[2021-05-14 11:16:36,941] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 274) (0d1d3e01326c, executor driver, partition 133, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,942] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 271) in 182 ms on 0d1d3e01326c (executor driver) (130/141)
21/05/14 14:16:36 INFO Executor: Running task 133.0 in stage 1.0 (TID 274)
[2021-05-14 11:16:36,953] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Finished task 129.0 in stage 1.0 (TID 270). 1868 bytes result sent to driver
[2021-05-14 11:16:36,953] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 270) in 224 ms on 0d1d3e01326c (executor driver) (131/141)
[2021-05-14 11:16:36,954] {docker.py:276} INFO - 21/05/14 14:16:36 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 275) (0d1d3e01326c, executor driver, partition 134, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,955] {docker.py:276} INFO - 21/05/14 14:16:36 INFO Executor: Running task 134.0 in stage 1.0 (TID 275)
[2021-05-14 11:16:36,983] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 131.0 in stage 1.0 (TID 272). 1868 bytes result sent to driver
[2021-05-14 11:16:36,984] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 276) (0d1d3e01326c, executor driver, partition 135, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:36,985] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 272) in 187 ms on 0d1d3e01326c (executor driver) (132/141)
[2021-05-14 11:16:36,986] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 135.0 in stage 1.0 (TID 276)
[2021-05-14 11:16:37,073] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 132.0 in stage 1.0 (TID 273). 1825 bytes result sent to driver
[2021-05-14 11:16:37,075] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 277) (0d1d3e01326c, executor driver, partition 136, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:37,076] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 273) in 178 ms on 0d1d3e01326c (executor driver) (133/141)
[2021-05-14 11:16:37,077] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 136.0 in stage 1.0 (TID 277)
[2021-05-14 11:16:37,116] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 133.0 in stage 1.0 (TID 274). 1825 bytes result sent to driver
[2021-05-14 11:16:37,118] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 278) (0d1d3e01326c, executor driver, partition 137, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:37,119] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 274) in 178 ms on 0d1d3e01326c (executor driver) (134/141)
[2021-05-14 11:16:37,119] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 137.0 in stage 1.0 (TID 278)
[2021-05-14 11:16:37,125] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 134.0 in stage 1.0 (TID 275). 1825 bytes result sent to driver
[2021-05-14 11:16:37,127] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 279) (0d1d3e01326c, executor driver, partition 138, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:37,128] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 138.0 in stage 1.0 (TID 279)
[2021-05-14 11:16:37,128] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 275) in 174 ms on 0d1d3e01326c (executor driver) (135/141)
[2021-05-14 11:16:37,160] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 135.0 in stage 1.0 (TID 276). 1825 bytes result sent to driver
[2021-05-14 11:16:37,162] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 280) (0d1d3e01326c, executor driver, partition 139, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:37,163] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 276) in 180 ms on 0d1d3e01326c (executor driver) (136/141)
[2021-05-14 11:16:37,164] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 139.0 in stage 1.0 (TID 280)
[2021-05-14 11:16:37,252] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 136.0 in stage 1.0 (TID 277). 1825 bytes result sent to driver
[2021-05-14 11:16:37,253] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 281) (0d1d3e01326c, executor driver, partition 140, PROCESS_LOCAL, 4551 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:37,254] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 277) in 179 ms on 0d1d3e01326c (executor driver) (137/141)
[2021-05-14 11:16:37,255] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Running task 140.0 in stage 1.0 (TID 281)
[2021-05-14 11:16:37,294] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 137.0 in stage 1.0 (TID 278). 1825 bytes result sent to driver
[2021-05-14 11:16:37,295] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 278) in 178 ms on 0d1d3e01326c (executor driver) (138/141)
[2021-05-14 11:16:37,300] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 138.0 in stage 1.0 (TID 279). 1825 bytes result sent to driver
[2021-05-14 11:16:37,301] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 279) in 175 ms on 0d1d3e01326c (executor driver) (139/141)
[2021-05-14 11:16:37,339] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 139.0 in stage 1.0 (TID 280). 1825 bytes result sent to driver
[2021-05-14 11:16:37,341] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 280) in 179 ms on 0d1d3e01326c (executor driver) (140/141)
[2021-05-14 11:16:37,423] {docker.py:276} INFO - 21/05/14 14:16:37 INFO Executor: Finished task 140.0 in stage 1.0 (TID 281). 1825 bytes result sent to driver
[2021-05-14 11:16:37,424] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 281) in 172 ms on 0d1d3e01326c (executor driver) (141/141)
21/05/14 14:16:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2021-05-14 11:16:37,426] {docker.py:276} INFO - 21/05/14 14:16:37 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.691 s
[2021-05-14 11:16:37,428] {docker.py:276} INFO - 21/05/14 14:16:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2021-05-14 11:16:37,429] {docker.py:276} INFO - 21/05/14 14:16:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2021-05-14 11:16:37,430] {docker.py:276} INFO - 21/05/14 14:16:37 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.704492 s
[2021-05-14 11:16:37,450] {docker.py:276} INFO - 21/05/14 14:16:37 INFO InMemoryFileIndex: It took 6770 ms to list leaf files for 141 paths.
[2021-05-14 11:16:40,129] {docker.py:276} INFO - 21/05/14 14:16:40 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 11:16:40,135] {docker.py:276} INFO - 21/05/14 14:16:40 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-05-14 11:16:40,139] {docker.py:276} INFO - 21/05/14 14:16:40 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 11:16:40,326] {docker.py:276} INFO - 21/05/14 14:16:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0d1d3e01326c:42543 in memory (size: 30.3 KiB, free: 934.4 MiB)
[2021-05-14 11:16:40,667] {docker.py:276} INFO - 21/05/14 14:16:40 INFO CodeGenerator: Code generated in 267.3606 ms
[2021-05-14 11:16:40,680] {docker.py:276} INFO - 21/05/14 14:16:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 177.4 KiB, free 934.2 MiB)
[2021-05-14 11:16:40,698] {docker.py:276} INFO - 21/05/14 14:16:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.2 MiB)
[2021-05-14 11:16:40,699] {docker.py:276} INFO - 21/05/14 14:16:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0d1d3e01326c:42543 (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:16:40,700] {docker.py:276} INFO - 21/05/14 14:16:40 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:40,718] {docker.py:276} INFO - 21/05/14 14:16:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:16:40,832] {docker.py:276} INFO - 21/05/14 14:16:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:40,834] {docker.py:276} INFO - 21/05/14 14:16:40 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/05/14 14:16:40 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 11:16:40,834] {docker.py:276} INFO - 21/05/14 14:16:40 INFO DAGScheduler: Parents of final stage: List()
[2021-05-14 11:16:40,835] {docker.py:276} INFO - 21/05/14 14:16:40 INFO DAGScheduler: Missing parents: List()
[2021-05-14 11:16:40,835] {docker.py:276} INFO - 21/05/14 14:16:40 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:16:40,871] {docker.py:276} INFO - 21/05/14 14:16:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 934.2 MiB)
[2021-05-14 11:16:40,884] {docker.py:276} INFO - 21/05/14 14:16:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 934.2 MiB)
[2021-05-14 11:16:40,884] {docker.py:276} INFO - 21/05/14 14:16:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0d1d3e01326c:42543 (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 11:16:40,887] {docker.py:276} INFO - 21/05/14 14:16:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:16:40,887] {docker.py:276} INFO - 21/05/14 14:16:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/05/14 14:16:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2021-05-14 11:16:40,890] {docker.py:276} INFO - 21/05/14 14:16:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 282) (0d1d3e01326c, executor driver, partition 0, PROCESS_LOCAL, 8027 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:40,891] {docker.py:276} INFO - 21/05/14 14:16:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 282)
[2021-05-14 11:16:40,987] {docker.py:276} INFO - 21/05/14 14:16:40 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620938721_to_1620940521.csv, range: 0-111643, partition values: [empty row]
[2021-05-14 11:16:41,025] {docker.py:276} INFO - 21/05/14 14:16:41 INFO CodeGenerator: Code generated in 28.9409 ms
[2021-05-14 11:16:41,571] {docker.py:276} INFO - 21/05/14 14:16:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 282). 1564 bytes result sent to driver
[2021-05-14 11:16:41,573] {docker.py:276} INFO - 21/05/14 14:16:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 282) in 687 ms on 0d1d3e01326c (executor driver) (1/1)
[2021-05-14 11:16:41,574] {docker.py:276} INFO - 21/05/14 14:16:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2021-05-14 11:16:41,575] {docker.py:276} INFO - 21/05/14 14:16:41 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.735 s
[2021-05-14 11:16:41,575] {docker.py:276} INFO - 21/05/14 14:16:41 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 14:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2021-05-14 11:16:41,576] {docker.py:276} INFO - 21/05/14 14:16:41 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.743141 s
[2021-05-14 11:16:41,610] {docker.py:276} INFO - 21/05/14 14:16:41 INFO CodeGenerator: Code generated in 17.1719 ms
[2021-05-14 11:16:41,680] {docker.py:276} INFO - 21/05/14 14:16:41 INFO FileSourceStrategy: Pushed Filters: 
21/05/14 14:16:41 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 11:16:41,680] {docker.py:276} INFO - 21/05/14 14:16:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-05-14 11:16:41,688] {docker.py:276} INFO - 21/05/14 14:16:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 177.4 KiB, free 934.0 MiB)
[2021-05-14 11:16:41,710] {docker.py:276} INFO - 21/05/14 14:16:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 0d1d3e01326c:42543 in memory (size: 5.4 KiB, free: 934.4 MiB)
[2021-05-14 11:16:41,715] {docker.py:276} INFO - 21/05/14 14:16:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 934.0 MiB)
[2021-05-14 11:16:41,716] {docker.py:276} INFO - 21/05/14 14:16:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0d1d3e01326c:42543 (size: 28.2 KiB, free: 934.3 MiB)
[2021-05-14 11:16:41,718] {docker.py:276} INFO - 21/05/14 14:16:41 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:41,723] {docker.py:276} INFO - 21/05/14 14:16:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:16:42,411] {docker.py:276} INFO - 21/05/14 14:16:42 INFO FileSourceStrategy: Pushed Filters:
[2021-05-14 11:16:42,412] {docker.py:276} INFO - 21/05/14 14:16:42 INFO FileSourceStrategy: Post-Scan Filters:
[2021-05-14 11:16:42,412] {docker.py:276} INFO - 21/05/14 14:16:42 INFO FileSourceStrategy: Output Data Schema: struct<ts: string, n: string, bid: string, ask: string, value: string ... 7 more fields>
[2021-05-14 11:16:43,064] {docker.py:276} INFO - 21/05/14 14:16:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:16:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:16:43,068] {docker.py:276} INFO - 21/05/14 14:16:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:16:43,068] {docker.py:276} INFO - 21/05/14 14:16:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434402549272011988574_0000_m_000000_0, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434402549272011988574_0000_m_000000_0}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434402549272011988574_0000}; taskId=attempt_202105141416434402549272011988574_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37318fa4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:16:43,069] {docker.py:276} INFO - 21/05/14 14:16:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:16:43,101] {docker.py:276} INFO - 21/05/14 14:16:43 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 11:16:43,234] {docker.py:276} INFO - 21/05/14 14:16:43 INFO CodeGenerator: Code generated in 99.202 ms
[2021-05-14 11:16:43,239] {docker.py:276} INFO - 21/05/14 14:16:43 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2021-05-14 11:16:43,290] {docker.py:276} INFO - 21/05/14 14:16:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 0d1d3e01326c:42543 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:16:43,345] {docker.py:276} INFO - 21/05/14 14:16:43 INFO CodeGenerator: Code generated in 87.869 ms
[2021-05-14 11:16:43,348] {docker.py:276} INFO - 21/05/14 14:16:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.3 KiB, free 934.0 MiB)
[2021-05-14 11:16:43,364] {docker.py:276} INFO - 21/05/14 14:16:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 934.0 MiB)
[2021-05-14 11:16:43,365] {docker.py:276} INFO - 21/05/14 14:16:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0d1d3e01326c:42543 (size: 28.1 KiB, free: 934.3 MiB)
[2021-05-14 11:16:43,366] {docker.py:276} INFO - 21/05/14 14:16:43 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:43,374] {docker.py:276} INFO - 21/05/14 14:16:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
[2021-05-14 11:16:43,444] {docker.py:276} INFO - 21/05/14 14:16:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 0d1d3e01326c:42543 in memory (size: 28.2 KiB, free: 934.4 MiB)
[2021-05-14 11:16:43,555] {docker.py:276} INFO - 21/05/14 14:16:43 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-05-14 11:16:43,559] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2021-05-14 11:16:43,561] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2021-05-14 11:16:43,562] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2021-05-14 11:16:43,562] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2021-05-14 11:16:43,564] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2021-05-14 11:16:43,567] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:16:43,591] {docker.py:276} INFO - 21/05/14 14:16:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 27.9 KiB, free 934.2 MiB)
[2021-05-14 11:16:43,608] {docker.py:276} INFO - 21/05/14 14:16:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 934.2 MiB)
[2021-05-14 11:16:43,610] {docker.py:276} INFO - 21/05/14 14:16:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0d1d3e01326c:42543 (size: 12.0 KiB, free: 934.4 MiB)
[2021-05-14 11:16:43,611] {docker.py:276} INFO - 21/05/14 14:16:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:16:43,613] {docker.py:276} INFO - 21/05/14 14:16:43 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21/05/14 14:16:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
[2021-05-14 11:16:43,616] {docker.py:276} INFO - 21/05/14 14:16:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 283) (0d1d3e01326c, executor driver, partition 0, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:43,616] {docker.py:276} INFO - 21/05/14 14:16:43 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 284) (0d1d3e01326c, executor driver, partition 1, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:43,617] {docker.py:276} INFO - 21/05/14 14:16:43 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 285) (0d1d3e01326c, executor driver, partition 2, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:43,617] {docker.py:276} INFO - 21/05/14 14:16:43 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 286) (0d1d3e01326c, executor driver, partition 3, PROCESS_LOCAL, 8016 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:43,618] {docker.py:276} INFO - 21/05/14 14:16:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 283)
[2021-05-14 11:16:43,619] {docker.py:276} INFO - 21/05/14 14:16:43 INFO Executor: Running task 2.0 in stage 3.0 (TID 285)
[2021-05-14 11:16:43,625] {docker.py:276} INFO - 21/05/14 14:16:43 INFO Executor: Running task 1.0 in stage 3.0 (TID 284)
[2021-05-14 11:16:43,627] {docker.py:276} INFO - 21/05/14 14:16:43 INFO Executor: Running task 3.0 in stage 3.0 (TID 286)
[2021-05-14 11:16:43,774] {docker.py:276} INFO - 21/05/14 14:16:43 INFO CodeGenerator: Code generated in 58.9111 ms
[2021-05-14 11:16:43,815] {docker.py:276} INFO - 21/05/14 14:16:43 INFO CodeGenerator: Code generated in 12.6079 ms
[2021-05-14 11:16:43,848] {docker.py:276} INFO - 21/05/14 14:16:43 INFO CodeGenerator: Code generated in 24.3606 ms
[2021-05-14 11:16:43,873] {docker.py:276} INFO - 21/05/14 14:16:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620981921_to_1620983721.csv, range: 0-104025, partition values: [empty row]
21/05/14 14:16:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620938721_to_1620940521.csv, range: 0-111643, partition values: [empty row]
[2021-05-14 11:16:43,874] {docker.py:276} INFO - 21/05/14 14:16:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620944121_to_1620945921.csv, range: 0-104187, partition values: [empty row]
21/05/14 14:16:43 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620972921_to_1620974721.csv, range: 0-104252, partition values: [empty row]
[2021-05-14 11:16:44,763] {docker.py:276} INFO - 21/05/14 14:16:44 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620920721_to_1620922521.csv, range: 0-104016, partition values: [empty row]
[2021-05-14 11:16:45,137] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620936921_to_1620938721.csv, range: 0-104006, partition values: [empty row]
[2021-05-14 11:16:45,214] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620940521_to_1620942321.csv, range: 0-111624, partition values: [empty row]
[2021-05-14 11:16:45,283] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620926121_to_1620927921.csv, range: 0-104182, partition values: [empty row]
[2021-05-14 11:16:45,285] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620987321_to_1620989121.csv, range: 0-104251, partition values: [empty row]
[2021-05-14 11:16:45,643] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620917121_to_1620918921.csv, range: 0-104003, partition values: [empty row]
[2021-05-14 11:16:45,746] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620944121_to_1620945921.csv, range: 0-111618, partition values: [empty row]
[2021-05-14 11:16:45,811] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620989121_to_1620990921.csv, range: 0-104182, partition values: [empty row]
[2021-05-14 11:16:45,812] {docker.py:276} INFO - 21/05/14 14:16:45 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620998121_to_1620999921.csv, range: 0-104248, partition values: [empty row]
[2021-05-14 11:16:45,996] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620947721_to_1620949521.csv, range: 0-103991, partition values: [empty row]
[2021-05-14 11:16:46,083] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620942321_to_1620944121.csv, range: 0-111605, partition values: [empty row]
[2021-05-14 11:16:46,177] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620936921_to_1620938721.csv, range: 0-104245, partition values: [empty row]
[2021-05-14 11:16:46,256] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620976521_to_1620978321.csv, range: 0-104181, partition values: [empty row]
[2021-05-14 11:16:46,372] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620926121_to_1620927921.csv, range: 0-103990, partition values: [empty row]
[2021-05-14 11:16:46,429] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620936921_to_1620938721.csv, range: 0-111584, partition values: [empty row]
[2021-05-14 11:16:46,551] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620971121_to_1620972921.csv, range: 0-104244, partition values: [empty row]
[2021-05-14 11:16:46,607] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620987321_to_1620989121.csv, range: 0-104180, partition values: [empty row]
[2021-05-14 11:16:46,742] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620924321_to_1620926121.csv, range: 0-103979, partition values: [empty row]
[2021-05-14 11:16:46,787] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620935121_to_1620936921.csv, range: 0-107999, partition values: [empty row]
[2021-05-14 11:16:46,969] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620994521_to_1620996321.csv, range: 0-104243, partition values: [empty row]
[2021-05-14 11:16:46,985] {docker.py:276} INFO - 21/05/14 14:16:46 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620980121_to_1620981921.csv, range: 0-104176, partition values: [empty row]
[2021-05-14 11:16:47,101] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620929721_to_1620931521.csv, range: 0-103969, partition values: [empty row]
[2021-05-14 11:16:47,133] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620945921_to_1620947721.csv, range: 0-107923, partition values: [empty row]
[2021-05-14 11:16:47,328] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620994521_to_1620996321.csv, range: 0-104241, partition values: [empty row]
[2021-05-14 11:16:47,381] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620990921_to_1620992721.csv, range: 0-104176, partition values: [empty row]
[2021-05-14 11:16:47,461] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620938721_to_1620940521.csv, range: 0-103964, partition values: [empty row]
[2021-05-14 11:16:47,500] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620940521_to_1620942321.csv, range: 0-104385, partition values: [empty row]
[2021-05-14 11:16:47,693] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620920721_to_1620922521.csv, range: 0-104238, partition values: [empty row]
[2021-05-14 11:16:47,729] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620998121_to_1620999921.csv, range: 0-104174, partition values: [empty row]
[2021-05-14 11:16:47,810] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620933321_to_1620935121.csv, range: 0-103956, partition values: [empty row]
[2021-05-14 11:16:47,856] {docker.py:276} INFO - 21/05/14 14:16:47 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620933321_to_1620935121.csv, range: 0-104365, partition values: [empty row]
[2021-05-14 11:16:48,050] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620918921_to_1620920721.csv, range: 0-104233, partition values: [empty row]
[2021-05-14 11:16:48,076] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620917121_to_1620918921.csv, range: 0-104169, partition values: [empty row]
[2021-05-14 11:16:48,169] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620931521_to_1620933321.csv, range: 0-103954, partition values: [empty row]
[2021-05-14 11:16:48,195] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620958521_to_1620960321.csv, range: 0-104320, partition values: [empty row]
[2021-05-14 11:16:48,410] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620924321_to_1620926121.csv, range: 0-104233, partition values: [empty row]
[2021-05-14 11:16:48,422] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620918921_to_1620920721.csv, range: 0-104163, partition values: [empty row]
[2021-05-14 11:16:48,539] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620962121_to_1620963921.csv, range: 0-104315, partition values: [empty row]
[2021-05-14 11:16:48,539] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620963921_to_1620965721.csv, range: 0-103949, partition values: [empty row]
[2021-05-14 11:16:48,778] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620990921_to_1620992721.csv, range: 0-104151, partition values: [empty row]
[2021-05-14 11:16:48,785] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620927921_to_1620929721.csv, range: 0-104232, partition values: [empty row]
[2021-05-14 11:16:48,881] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620965721_to_1620967521.csv, range: 0-103945, partition values: [empty row]
[2021-05-14 11:16:48,896] {docker.py:276} INFO - 21/05/14 14:16:48 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620962121_to_1620963921.csv, range: 0-104310, partition values: [empty row]
[2021-05-14 11:16:49,126] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620976521_to_1620978321.csv, range: 0-104150, partition values: [empty row]
[2021-05-14 11:16:49,150] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620931521_to_1620933321.csv, range: 0-104232, partition values: [empty row]
[2021-05-14 11:16:49,228] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620987321_to_1620989121.csv, range: 0-103920, partition values: [empty row]
[2021-05-14 11:16:49,254] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620992721_to_1620994521.csv, range: 0-104309, partition values: [empty row]
[2021-05-14 11:16:49,502] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620965721_to_1620967521.csv, range: 0-104140, partition values: [empty row]
[2021-05-14 11:16:49,527] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620922521_to_1620924321.csv, range: 0-104229, partition values: [empty row]
[2021-05-14 11:16:49,585] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620954921_to_1620956721.csv, range: 0-103910, partition values: [empty row]
[2021-05-14 11:16:49,590] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620985521_to_1620987321.csv, range: 0-104308, partition values: [empty row]
[2021-05-14 11:16:49,874] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620929721_to_1620931521.csv, range: 0-104125, partition values: [empty row]
[2021-05-14 11:16:49,952] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620958521_to_1620960321.csv, range: 0-103901, partition values: [empty row]
[2021-05-14 11:16:49,953] {docker.py:276} INFO - 21/05/14 14:16:49 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620953121_to_1620954921.csv, range: 0-104306, partition values: [empty row]
[2021-05-14 11:16:50,247] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620963921_to_1620965721.csv, range: 0-104116, partition values: [empty row]
[2021-05-14 11:16:50,316] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620962121_to_1620963921.csv, range: 0-103901, partition values: [empty row]
[2021-05-14 11:16:50,316] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620954921_to_1620956721.csv, range: 0-104301, partition values: [empty row]
[2021-05-14 11:16:50,600] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620996321_to_1620998121.csv, range: 0-104114, partition values: [empty row]
[2021-05-14 11:16:50,656] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620926121_to_1620927921.csv, range: 0-104299, partition values: [empty row]
[2021-05-14 11:16:50,663] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620956721_to_1620958521.csv, range: 0-103889, partition values: [empty row]
[2021-05-14 11:16:50,797] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620960321_to_1620962121.csv, range: 0-104226, partition values: [empty row]
[2021-05-14 11:16:50,971] {docker.py:276} INFO - 21/05/14 14:16:50 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620915321_to_1620917121.csv, range: 0-104111, partition values: [empty row]
[2021-05-14 11:16:51,012] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620972921_to_1620974721.csv, range: 0-104298, partition values: [empty row]
[2021-05-14 11:16:51,032] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620920721_to_1620922521.csv, range: 0-103878, partition values: [empty row]
[2021-05-14 11:16:51,286] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620942321_to_1620944121.csv, range: 0-104224, partition values: [empty row]
[2021-05-14 11:16:51,319] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620938721_to_1620940521.csv, range: 0-104104, partition values: [empty row]
[2021-05-14 11:16:51,373] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620967521_to_1620969321.csv, range: 0-103877, partition values: [empty row]
[2021-05-14 11:16:51,373] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620981921_to_1620983721.csv, range: 0-104298, partition values: [empty row]
[2021-05-14 11:16:51,629] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620958521_to_1620960321.csv, range: 0-104223, partition values: [empty row]
[2021-05-14 11:16:51,663] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620949521_to_1620951321.csv, range: 0-104096, partition values: [empty row]
[2021-05-14 11:16:51,721] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620951321_to_1620953121.csv, range: 0-103872, partition values: [empty row]
[2021-05-14 11:16:51,734] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620915321_to_1620917121.csv, range: 0-104284, partition values: [empty row]
[2021-05-14 11:16:51,967] {docker.py:276} INFO - 21/05/14 14:16:51 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620927921_to_1620929721.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 11:16:52,016] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620924321_to_1620926121.csv, range: 0-104094, partition values: [empty row]
[2021-05-14 11:16:52,071] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620960321_to_1620962121.csv, range: 0-104278, partition values: [empty row]
[2021-05-14 11:16:52,094] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620922521_to_1620924321.csv, range: 0-103864, partition values: [empty row]
[2021-05-14 11:16:52,315] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620963921_to_1620965721.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 11:16:52,367] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620980121_to_1620981921.csv, range: 0-104084, partition values: [empty row]
[2021-05-14 11:16:52,416] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620956721_to_1620958521.csv, range: 0-104274, partition values: [empty row]
[2021-05-14 11:16:52,460] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620960321_to_1620962121.csv, range: 0-103862, partition values: [empty row]
[2021-05-14 11:16:52,652] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620971121_to_1620972921.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 11:16:52,765] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620983721_to_1620985521.csv, range: 0-104272, partition values: [empty row]
[2021-05-14 11:16:52,767] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620951321_to_1620953121.csv, range: 0-104075, partition values: [empty row]
[2021-05-14 11:16:52,844] {docker.py:276} INFO - 21/05/14 14:16:52 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620989121_to_1620990921.csv, range: 0-103855, partition values: [empty row]
[2021-05-14 11:16:53,012] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620996321_to_1620998121.csv, range: 0-104219, partition values: [empty row]
[2021-05-14 11:16:53,123] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620967521_to_1620969321.csv, range: 0-104072, partition values: [empty row]
[2021-05-14 11:16:53,124] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620969321_to_1620971121.csv, range: 0-104268, partition values: [empty row]
[2021-05-14 11:16:53,205] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620954921_to_1620956721.csv, range: 0-103853, partition values: [empty row]
[2021-05-14 11:16:53,380] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620945921_to_1620947721.csv, range: 0-104209, partition values: [empty row]
[2021-05-14 11:16:53,454] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620940521_to_1620942321.csv, range: 0-104064, partition values: [empty row]
[2021-05-14 11:16:53,468] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620983721_to_1620985521.csv, range: 0-104266, partition values: [empty row]
[2021-05-14 11:16:53,558] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620953121_to_1620954921.csv, range: 0-103850, partition values: [empty row]
[2021-05-14 11:16:53,797] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620931521_to_1620933321.csv, range: 0-104052, partition values: [empty row]
[2021-05-14 11:16:53,813] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620953121_to_1620954921.csv, range: 0-104207, partition values: [empty row]
[2021-05-14 11:16:53,838] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620965721_to_1620967521.csv, range: 0-104265, partition values: [empty row]
[2021-05-14 11:16:53,886] {docker.py:276} INFO - 21/05/14 14:16:53 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620990921_to_1620992721.csv, range: 0-103837, partition values: [empty row]
[2021-05-14 11:16:54,136] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620971121_to_1620972921.csv, range: 0-104050, partition values: [empty row]
[2021-05-14 11:16:54,166] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620922521_to_1620924321.csv, range: 0-104206, partition values: [empty row]
[2021-05-14 11:16:54,185] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620967521_to_1620969321.csv, range: 0-104263, partition values: [empty row]
[2021-05-14 11:16:54,257] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620942321_to_1620944121.csv, range: 0-103837, partition values: [empty row]
[2021-05-14 11:16:54,475] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620935121_to_1620936921.csv, range: 0-104048, partition values: [empty row]
[2021-05-14 11:16:54,509] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620974721_to_1620976521.csv, range: 0-104204, partition values: [empty row]
[2021-05-14 11:16:54,531] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620978321_to_1620980121.csv, range: 0-104262, partition values: [empty row]
[2021-05-14 11:16:54,593] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620951321_to_1620953121.csv, range: 0-103827, partition values: [empty row]
[2021-05-14 11:16:54,821] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620927921_to_1620929721.csv, range: 0-104041, partition values: [empty row]
[2021-05-14 11:16:54,867] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620978321_to_1620980121.csv, range: 0-104203, partition values: [empty row]
[2021-05-14 11:16:54,911] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620969321_to_1620971121.csv, range: 0-104262, partition values: [empty row]
[2021-05-14 11:16:54,959] {docker.py:276} INFO - 21/05/14 14:16:54 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620998121_to_1620999921.csv, range: 0-103821, partition values: [empty row]
[2021-05-14 11:16:55,157] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620949521_to_1620951321.csv, range: 0-104040, partition values: [empty row]
[2021-05-14 11:16:55,227] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620917121_to_1620918921.csv, range: 0-104201, partition values: [empty row]
[2021-05-14 11:16:55,280] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620947721_to_1620949521.csv, range: 0-104257, partition values: [empty row]
[2021-05-14 11:16:55,314] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620985521_to_1620987321.csv, range: 0-103818, partition values: [empty row]
[2021-05-14 11:16:55,490] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620994521_to_1620996321.csv, range: 0-104034, partition values: [empty row]
[2021-05-14 11:16:55,584] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620980121_to_1620981921.csv, range: 0-104199, partition values: [empty row]
[2021-05-14 11:16:55,641] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620929721_to_1620931521.csv, range: 0-104257, partition values: [empty row]
[2021-05-14 11:16:55,677] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620915321_to_1620917121.csv, range: 0-103816, partition values: [empty row]
[2021-05-14 11:16:55,854] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620933321_to_1620935121.csv, range: 0-104028, partition values: [empty row]
[2021-05-14 11:16:55,952] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620981921_to_1620983721.csv, range: 0-104198, partition values: [empty row]
[2021-05-14 11:16:55,982] {docker.py:276} INFO - 21/05/14 14:16:55 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620992721_to_1620994521.csv, range: 0-104255, partition values: [empty row]
[2021-05-14 11:16:56,202] {docker.py:276} INFO - 21/05/14 14:16:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620978321_to_1620980121.csv, range: 0-104025, partition values: [empty row]
[2021-05-14 11:16:56,297] {docker.py:276} INFO - 21/05/14 14:16:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620974721_to_1620976521.csv, range: 0-104197, partition values: [empty row]
[2021-05-14 11:16:56,413] {docker.py:276} INFO - 21/05/14 14:16:56 INFO Executor: Finished task 3.0 in stage 3.0 (TID 286). 2722 bytes result sent to driver
[2021-05-14 11:16:56,414] {docker.py:276} INFO - 21/05/14 14:16:56 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 287) (0d1d3e01326c, executor driver, partition 4, PROCESS_LOCAL, 6097 bytes) taskResourceAssignments Map()
[2021-05-14 11:16:56,415] {docker.py:276} INFO - 21/05/14 14:16:56 INFO Executor: Running task 4.0 in stage 3.0 (TID 287)
[2021-05-14 11:16:56,417] {docker.py:276} INFO - 21/05/14 14:16:56 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 286) in 12814 ms on 0d1d3e01326c (executor driver) (1/5)
[2021-05-14 11:16:56,430] {docker.py:276} INFO - 21/05/14 14:16:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620969321_to_1620971121.csv, range: 0-103807, partition values: [empty row]
[2021-05-14 11:16:56,539] {docker.py:276} INFO - 21/05/14 14:16:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 283). 2679 bytes result sent to driver
[2021-05-14 11:16:56,541] {docker.py:276} INFO - 21/05/14 14:16:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 283) in 12940 ms on 0d1d3e01326c (executor driver) (2/5)
[2021-05-14 11:16:56,638] {docker.py:276} INFO - 21/05/14 14:16:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620985521_to_1620987321.csv, range: 0-104196, partition values: [empty row]
[2021-05-14 11:16:56,728] {docker.py:276} INFO - 21/05/14 14:16:56 INFO Executor: Finished task 2.0 in stage 3.0 (TID 285). 2679 bytes result sent to driver
[2021-05-14 11:16:56,729] {docker.py:276} INFO - 21/05/14 14:16:56 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 285) in 13125 ms on 0d1d3e01326c (executor driver) (3/5)
[2021-05-14 11:16:56,787] {docker.py:276} INFO - 21/05/14 14:16:56 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620974721_to_1620976521.csv, range: 0-103805, partition values: [empty row]
[2021-05-14 11:16:56,998] {docker.py:276} INFO - 21/05/14 14:16:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620989121_to_1620990921.csv, range: 0-104193, partition values: [empty row]
[2021-05-14 11:16:57,176] {docker.py:276} INFO - 21/05/14 14:16:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620935121_to_1620936921.csv, range: 0-103794, partition values: [empty row]
[2021-05-14 11:16:57,368] {docker.py:276} INFO - 21/05/14 14:16:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620956721_to_1620958521.csv, range: 0-104191, partition values: [empty row]
[2021-05-14 11:16:57,540] {docker.py:276} INFO - 21/05/14 14:16:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620976521_to_1620978321.csv, range: 0-103791, partition values: [empty row]
[2021-05-14 11:16:57,873] {docker.py:276} INFO - 21/05/14 14:16:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 284). 2679 bytes result sent to driver
[2021-05-14 11:16:57,874] {docker.py:276} INFO - 21/05/14 14:16:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 284) in 14274 ms on 0d1d3e01326c (executor driver) (4/5)
[2021-05-14 11:16:57,896] {docker.py:276} INFO - 21/05/14 14:16:57 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620992721_to_1620994521.csv, range: 0-103789, partition values: [empty row]
[2021-05-14 11:16:58,258] {docker.py:276} INFO - 21/05/14 14:16:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620983721_to_1620985521.csv, range: 0-103759, partition values: [empty row]
[2021-05-14 11:16:58,619] {docker.py:276} INFO - 21/05/14 14:16:58 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620972921_to_1620974721.csv, range: 0-103758, partition values: [empty row]
[2021-05-14 11:16:59,012] {docker.py:276} INFO - 21/05/14 14:16:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/3/2021-05-14/from_1620947721_to_1620949521.csv, range: 0-103750, partition values: [empty row]
[2021-05-14 11:16:59,382] {docker.py:276} INFO - 21/05/14 14:16:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620944121_to_1620945921.csv, range: 0-103736, partition values: [empty row]
[2021-05-14 11:16:59,733] {docker.py:276} INFO - 21/05/14 14:16:59 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620918921_to_1620920721.csv, range: 0-103723, partition values: [empty row]
[2021-05-14 11:17:00,138] {docker.py:276} INFO - 21/05/14 14:17:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620949521_to_1620951321.csv, range: 0-103703, partition values: [empty row]
[2021-05-14 11:17:00,486] {docker.py:276} INFO - 21/05/14 14:17:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/1/2021-05-14/from_1620996321_to_1620998121.csv, range: 0-103620, partition values: [empty row]
[2021-05-14 11:17:00,871] {docker.py:276} INFO - 21/05/14 14:17:00 INFO FileScanRDD: Reading File path: s3a://udac-forex-project/2/2021-05-14/from_1620945921_to_1620947721.csv, range: 0-103189, partition values: [empty row]
[2021-05-14 11:17:01,343] {docker.py:276} INFO - 21/05/14 14:17:01 INFO Executor: Finished task 4.0 in stage 3.0 (TID 287). 2679 bytes result sent to driver
[2021-05-14 11:17:01,343] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 287) in 4935 ms on 0d1d3e01326c (executor driver) (5/5)
[2021-05-14 11:17:01,344] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2021-05-14 11:17:01,345] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 17.786 s
[2021-05-14 11:17:01,346] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: looking for newly runnable stages
[2021-05-14 11:17:01,347] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: running: Set()
[2021-05-14 11:17:01,349] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: waiting: Set(ResultStage 4)
[2021-05-14 11:17:01,350] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: failed: Set()
[2021-05-14 11:17:01,355] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-05-14 11:17:01,412] {docker.py:276} INFO - 21/05/14 14:17:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 203.6 KiB, free 934.0 MiB)
[2021-05-14 11:17:01,414] {docker.py:276} INFO - 21/05/14 14:17:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 75.9 KiB, free 933.9 MiB)
[2021-05-14 11:17:01,416] {docker.py:276} INFO - 21/05/14 14:17:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0d1d3e01326c:42543 (size: 75.9 KiB, free: 934.3 MiB)
[2021-05-14 11:17:01,417] {docker.py:276} INFO - 21/05/14 14:17:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
[2021-05-14 11:17:01,418] {docker.py:276} INFO - 21/05/14 14:17:01 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/05/14 14:17:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks resource profile 0
[2021-05-14 11:17:01,426] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 288) (0d1d3e01326c, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:01,427] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 289) (0d1d3e01326c, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:01,428] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 290) (0d1d3e01326c, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:01,428] {docker.py:276} INFO - 21/05/14 14:17:01 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 291) (0d1d3e01326c, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:01,429] {docker.py:276} INFO - 21/05/14 14:17:01 INFO Executor: Running task 1.0 in stage 4.0 (TID 289)
21/05/14 14:17:01 INFO Executor: Running task 0.0 in stage 4.0 (TID 288)
[2021-05-14 11:17:01,436] {docker.py:276} INFO - 21/05/14 14:17:01 INFO Executor: Running task 2.0 in stage 4.0 (TID 290)
[2021-05-14 11:17:01,437] {docker.py:276} INFO - 21/05/14 14:17:01 INFO Executor: Running task 3.0 in stage 4.0 (TID 291)
[2021-05-14 11:17:01,547] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:01,548] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Getting 5 (39.5 KiB) non-empty blocks including 5 (39.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:01,549] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:01,550] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 11:17:01,551] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2021-05-14 11:17:01,551] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-14 11:17:01,551] {docker.py:276} INFO - 21/05/14 14:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2021-05-14 11:17:01,578] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 13.6974 ms
[2021-05-14 11:17:01,607] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 20.7129 ms
[2021-05-14 11:17:01,642] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 20.4356 ms
[2021-05-14 11:17:01,746] {docker.py:276} INFO - 21/05/14 14:17:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:01,747] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,747] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434335183819144812763_0004_m_000001_289, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434335183819144812763_0004_m_000001_289}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434335183819144812763_0004}; taskId=attempt_202105141416434335183819144812763_0004_m_000001_289, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3bcf8346}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,747] {docker.py:276} INFO - 21/05/14 14:17:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:01,748] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416434335183819144812763_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434335183819144812763_0004_m_000001_289
[2021-05-14 11:17:01,754] {docker.py:276} INFO - 21/05/14 14:17:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:01,754] {docker.py:276} INFO - 21/05/14 14:17:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:17:01,756] {docker.py:276} INFO - 21/05/14 14:17:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:01,756] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,757] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436916035937638108224_0004_m_000002_290, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436916035937638108224_0004_m_000002_290}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436916035937638108224_0004}; taskId=attempt_202105141416436916035937638108224_0004_m_000002_290, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d8d2a12}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,757] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,758] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433713452858630909096_0004_m_000000_288, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433713452858630909096_0004_m_000000_288}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433713452858630909096_0004}; taskId=attempt_202105141416433713452858630909096_0004_m_000000_288, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13b25dcb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,759] {docker.py:276} INFO - 21/05/14 14:17:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:01,765] {docker.py:276} INFO - 21/05/14 14:17:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416433713452858630909096_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433713452858630909096_0004_m_000000_288 
21/05/14 14:17:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416436916035937638108224_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436916035937638108224_0004_m_000002_290
[2021-05-14 11:17:01,766] {docker.py:276} INFO - 21/05/14 14:17:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:01,766] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,768] {docker.py:276} INFO - 21/05/14 14:17:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433654252212594839323_0004_m_000003_291, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433654252212594839323_0004_m_000003_291}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433654252212594839323_0004}; taskId=attempt_202105141416433654252212594839323_0004_m_000003_291, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1bee3a68}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:01,768] {docker.py:276} INFO - 21/05/14 14:17:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:01,769] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416433654252212594839323_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433654252212594839323_0004_m_000003_291
[2021-05-14 11:17:01,780] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Task committer attempt_202105141416433654252212594839323_0004_m_000003_291: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433654252212594839323_0004_m_000003_291 : duration 0:00.012s
[2021-05-14 11:17:01,784] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Task committer attempt_202105141416433713452858630909096_0004_m_000000_288: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433713452858630909096_0004_m_000000_288 : duration 0:00.022s
[2021-05-14 11:17:01,800] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 12.4517 ms
[2021-05-14 11:17:01,811] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Task committer attempt_202105141416434335183819144812763_0004_m_000001_289: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434335183819144812763_0004_m_000001_289 : duration 0:00.063s
[2021-05-14 11:17:01,816] {docker.py:276} INFO - 21/05/14 14:17:01 INFO StagingCommitter: Task committer attempt_202105141416436916035937638108224_0004_m_000002_290: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436916035937638108224_0004_m_000002_290 : duration 0:00.051s
[2021-05-14 11:17:01,828] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 12.7787 ms
[2021-05-14 11:17:01,872] {docker.py:276} INFO - 21/05/14 14:17:01 INFO CodeGenerator: Code generated in 18.7716 ms
[2021-05-14 11:17:08,502] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416433713452858630909096_0004_m_000000_288: needsTaskCommit() Task attempt_202105141416433713452858630909096_0004_m_000000_288
[2021-05-14 11:17:08,503] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416433713452858630909096_0004_m_000000_288: needsTaskCommit() Task attempt_202105141416433713452858630909096_0004_m_000000_288: duration 0:00.001s
[2021-05-14 11:17:08,504] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416436916035937638108224_0004_m_000002_290: needsTaskCommit() Task attempt_202105141416436916035937638108224_0004_m_000002_290
[2021-05-14 11:17:08,505] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416436916035937638108224_0004_m_000002_290: needsTaskCommit() Task attempt_202105141416436916035937638108224_0004_m_000002_290: duration 0:00.001s
[2021-05-14 11:17:08,505] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433713452858630909096_0004_m_000000_288
[2021-05-14 11:17:08,506] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436916035937638108224_0004_m_000002_290
[2021-05-14 11:17:08,514] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Finished task 2.0 in stage 4.0 (TID 290). 5192 bytes result sent to driver
[2021-05-14 11:17:08,515] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 288). 5192 bytes result sent to driver
[2021-05-14 11:17:08,516] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 292) (0d1d3e01326c, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:08,517] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 293) (0d1d3e01326c, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:08,517] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Running task 5.0 in stage 4.0 (TID 293)
[2021-05-14 11:17:08,519] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 288) in 7102 ms on 0d1d3e01326c (executor driver) (1/200)
[2021-05-14 11:17:08,519] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 290) in 7100 ms on 0d1d3e01326c (executor driver) (2/200)
[2021-05-14 11:17:08,520] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Running task 4.0 in stage 4.0 (TID 292)
[2021-05-14 11:17:08,534] {docker.py:276} INFO - 21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:08,534] {docker.py:276} INFO - 21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:08,535] {docker.py:276} INFO - 21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:08,554] {docker.py:276} INFO - 21/05/14 14:17:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:17:08,555] {docker.py:276} INFO - 21/05/14 14:17:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:08,555] {docker.py:276} INFO - 21/05/14 14:17:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:08,555] {docker.py:276} INFO - 21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:08,556] {docker.py:276} INFO - 21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435717253494701734139_0004_m_000004_292, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435717253494701734139_0004_m_000004_292}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435717253494701734139_0004}; taskId=attempt_202105141416435717253494701734139_0004_m_000004_292, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48c56963}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:08,556] {docker.py:276} INFO - 21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:08,556] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:08,557] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416435717253494701734139_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435717253494701734139_0004_m_000004_292
[2021-05-14 11:17:08,557] {docker.py:276} INFO - 21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431634599267137183477_0004_m_000005_293, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431634599267137183477_0004_m_000005_293}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431634599267137183477_0004}; taskId=attempt_202105141416431634599267137183477_0004_m_000005_293, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16bd7992}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:08,558] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:08,558] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416431634599267137183477_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431634599267137183477_0004_m_000005_293
[2021-05-14 11:17:08,563] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416431634599267137183477_0004_m_000005_293: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431634599267137183477_0004_m_000005_293 : duration 0:00.006s
[2021-05-14 11:17:08,572] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416435717253494701734139_0004_m_000004_292: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435717253494701734139_0004_m_000004_292 : duration 0:00.015s
[2021-05-14 11:17:08,585] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416433654252212594839323_0004_m_000003_291: needsTaskCommit() Task attempt_202105141416433654252212594839323_0004_m_000003_291
[2021-05-14 11:17:08,585] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416433654252212594839323_0004_m_000003_291: needsTaskCommit() Task attempt_202105141416433654252212594839323_0004_m_000003_291: duration 0:00.000s
[2021-05-14 11:17:08,586] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433654252212594839323_0004_m_000003_291
[2021-05-14 11:17:08,588] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Finished task 3.0 in stage 4.0 (TID 291). 5149 bytes result sent to driver
[2021-05-14 11:17:08,589] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 294) (0d1d3e01326c, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:08,591] {docker.py:276} INFO - 21/05/14 14:17:08 INFO Executor: Running task 6.0 in stage 4.0 (TID 294)
[2021-05-14 11:17:08,591] {docker.py:276} INFO - 21/05/14 14:17:08 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 291) in 7171 ms on 0d1d3e01326c (executor driver) (3/200)
[2021-05-14 11:17:08,600] {docker.py:276} INFO - 21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:08,616] {docker.py:276} INFO - 21/05/14 14:17:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:08,617] {docker.py:276} INFO - 21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431813628859660389443_0004_m_000006_294, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431813628859660389443_0004_m_000006_294}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431813628859660389443_0004}; taskId=attempt_202105141416431813628859660389443_0004_m_000006_294, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@67c95f95}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:08,617] {docker.py:276} INFO - 21/05/14 14:17:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416431813628859660389443_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431813628859660389443_0004_m_000006_294
[2021-05-14 11:17:08,620] {docker.py:276} INFO - 21/05/14 14:17:08 INFO StagingCommitter: Task committer attempt_202105141416431813628859660389443_0004_m_000006_294: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431813628859660389443_0004_m_000006_294 : duration 0:00.004s
[2021-05-14 11:17:09,690] {docker.py:276} INFO - 21/05/14 14:17:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416434335183819144812763_0004_m_000001_289: needsTaskCommit() Task attempt_202105141416434335183819144812763_0004_m_000001_289
[2021-05-14 11:17:09,693] {docker.py:276} INFO - 21/05/14 14:17:09 INFO StagingCommitter: Task committer attempt_202105141416434335183819144812763_0004_m_000001_289: needsTaskCommit() Task attempt_202105141416434335183819144812763_0004_m_000001_289: duration 0:00.005s
[2021-05-14 11:17:09,693] {docker.py:276} INFO - 21/05/14 14:17:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434335183819144812763_0004_m_000001_289
[2021-05-14 11:17:09,695] {docker.py:276} INFO - 21/05/14 14:17:09 INFO Executor: Finished task 1.0 in stage 4.0 (TID 289). 5149 bytes result sent to driver
[2021-05-14 11:17:09,696] {docker.py:276} INFO - 21/05/14 14:17:09 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 295) (0d1d3e01326c, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:09,697] {docker.py:276} INFO - 21/05/14 14:17:09 INFO Executor: Running task 7.0 in stage 4.0 (TID 295)
[2021-05-14 11:17:09,698] {docker.py:276} INFO - 21/05/14 14:17:09 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 289) in 8280 ms on 0d1d3e01326c (executor driver) (4/200)
[2021-05-14 11:17:09,709] {docker.py:276} INFO - 21/05/14 14:17:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:09,726] {docker.py:276} INFO - 21/05/14 14:17:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:09,727] {docker.py:276} INFO - 21/05/14 14:17:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:09,727] {docker.py:276} INFO - 21/05/14 14:17:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432847349719203853559_0004_m_000007_295, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432847349719203853559_0004_m_000007_295}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432847349719203853559_0004}; taskId=attempt_202105141416432847349719203853559_0004_m_000007_295, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5da952c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:09,728] {docker.py:276} INFO - 21/05/14 14:17:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416432847349719203853559_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432847349719203853559_0004_m_000007_295
[2021-05-14 11:17:09,732] {docker.py:276} INFO - 21/05/14 14:17:09 INFO StagingCommitter: Task committer attempt_202105141416432847349719203853559_0004_m_000007_295: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432847349719203853559_0004_m_000007_295 : duration 0:00.005s
[2021-05-14 11:17:15,156] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416431813628859660389443_0004_m_000006_294: needsTaskCommit() Task attempt_202105141416431813628859660389443_0004_m_000006_294
[2021-05-14 11:17:15,157] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416431813628859660389443_0004_m_000006_294: needsTaskCommit() Task attempt_202105141416431813628859660389443_0004_m_000006_294: duration 0:00.004s
[2021-05-14 11:17:15,158] {docker.py:276} INFO - 21/05/14 14:17:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431813628859660389443_0004_m_000006_294
[2021-05-14 11:17:15,161] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Finished task 6.0 in stage 4.0 (TID 294). 5149 bytes result sent to driver
[2021-05-14 11:17:15,162] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 296) (0d1d3e01326c, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:15,163] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Running task 8.0 in stage 4.0 (TID 296)
[2021-05-14 11:17:15,164] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 294) in 6547 ms on 0d1d3e01326c (executor driver) (5/200)
[2021-05-14 11:17:15,176] {docker.py:276} INFO - 21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:15,192] {docker.py:276} INFO - 21/05/14 14:17:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:15,192] {docker.py:276} INFO - 21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435665878110333232374_0004_m_000008_296, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435665878110333232374_0004_m_000008_296}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435665878110333232374_0004}; taskId=attempt_202105141416435665878110333232374_0004_m_000008_296, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bcaba13}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:15,193] {docker.py:276} INFO - 21/05/14 14:17:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416435665878110333232374_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435665878110333232374_0004_m_000008_296
[2021-05-14 11:17:15,199] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416435665878110333232374_0004_m_000008_296: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435665878110333232374_0004_m_000008_296 : duration 0:00.007s
[2021-05-14 11:17:15,279] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416431634599267137183477_0004_m_000005_293: needsTaskCommit() Task attempt_202105141416431634599267137183477_0004_m_000005_293
[2021-05-14 11:17:15,280] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416431634599267137183477_0004_m_000005_293: needsTaskCommit() Task attempt_202105141416431634599267137183477_0004_m_000005_293: duration 0:00.001s
21/05/14 14:17:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431634599267137183477_0004_m_000005_293
[2021-05-14 11:17:15,281] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Finished task 5.0 in stage 4.0 (TID 293). 5149 bytes result sent to driver
[2021-05-14 11:17:15,282] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 297) (0d1d3e01326c, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:15,283] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Running task 9.0 in stage 4.0 (TID 297)
[2021-05-14 11:17:15,284] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 293) in 6741 ms on 0d1d3e01326c (executor driver) (6/200)
[2021-05-14 11:17:15,293] {docker.py:276} INFO - 21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:15,308] {docker.py:276} INFO - 21/05/14 14:17:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:15,309] {docker.py:276} INFO - 21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434355651844414767541_0004_m_000009_297, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434355651844414767541_0004_m_000009_297}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434355651844414767541_0004}; taskId=attempt_202105141416434355651844414767541_0004_m_000009_297, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@97992ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:15,309] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416434355651844414767541_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434355651844414767541_0004_m_000009_297
[2021-05-14 11:17:15,313] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416434355651844414767541_0004_m_000009_297: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434355651844414767541_0004_m_000009_297 : duration 0:00.004s
[2021-05-14 11:17:15,378] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416435717253494701734139_0004_m_000004_292: needsTaskCommit() Task attempt_202105141416435717253494701734139_0004_m_000004_292
[2021-05-14 11:17:15,378] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416435717253494701734139_0004_m_000004_292: needsTaskCommit() Task attempt_202105141416435717253494701734139_0004_m_000004_292: duration 0:00.001s
[2021-05-14 11:17:15,379] {docker.py:276} INFO - 21/05/14 14:17:15 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435717253494701734139_0004_m_000004_292
[2021-05-14 11:17:15,380] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Finished task 4.0 in stage 4.0 (TID 292). 5149 bytes result sent to driver
[2021-05-14 11:17:15,381] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 298) (0d1d3e01326c, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:15,381] {docker.py:276} INFO - 21/05/14 14:17:15 INFO Executor: Running task 10.0 in stage 4.0 (TID 298)
[2021-05-14 11:17:15,382] {docker.py:276} INFO - 21/05/14 14:17:15 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 292) in 6840 ms on 0d1d3e01326c (executor driver) (7/200)
[2021-05-14 11:17:15,391] {docker.py:276} INFO - 21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:15,407] {docker.py:276} INFO - 21/05/14 14:17:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:15,407] {docker.py:276} INFO - 21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:15 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439079748067993518616_0004_m_000010_298, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439079748067993518616_0004_m_000010_298}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439079748067993518616_0004}; taskId=attempt_202105141416439079748067993518616_0004_m_000010_298, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21937210}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:15 INFO StagingCommitter: Starting: Task committer attempt_202105141416439079748067993518616_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439079748067993518616_0004_m_000010_298
[2021-05-14 11:17:15,413] {docker.py:276} INFO - 21/05/14 14:17:15 INFO StagingCommitter: Task committer attempt_202105141416439079748067993518616_0004_m_000010_298: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439079748067993518616_0004_m_000010_298 : duration 0:00.006s
[2021-05-14 11:17:16,635] {docker.py:276} INFO - 21/05/14 14:17:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416432847349719203853559_0004_m_000007_295: needsTaskCommit() Task attempt_202105141416432847349719203853559_0004_m_000007_295
[2021-05-14 11:17:16,637] {docker.py:276} INFO - 21/05/14 14:17:16 INFO StagingCommitter: Task committer attempt_202105141416432847349719203853559_0004_m_000007_295: needsTaskCommit() Task attempt_202105141416432847349719203853559_0004_m_000007_295: duration 0:00.003s
21/05/14 14:17:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432847349719203853559_0004_m_000007_295
[2021-05-14 11:17:16,641] {docker.py:276} INFO - 21/05/14 14:17:16 INFO Executor: Finished task 7.0 in stage 4.0 (TID 295). 5106 bytes result sent to driver
[2021-05-14 11:17:16,642] {docker.py:276} INFO - 21/05/14 14:17:16 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 299) (0d1d3e01326c, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:16,644] {docker.py:276} INFO - 21/05/14 14:17:16 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 295) in 6922 ms on 0d1d3e01326c (executor driver) (8/200)
21/05/14 14:17:16 INFO Executor: Running task 11.0 in stage 4.0 (TID 299)
[2021-05-14 11:17:16,653] {docker.py:276} INFO - 21/05/14 14:17:16 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:16,678] {docker.py:276} INFO - 21/05/14 14:17:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:16,679] {docker.py:276} INFO - 21/05/14 14:17:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432057002236372982352_0004_m_000011_299, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432057002236372982352_0004_m_000011_299}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432057002236372982352_0004}; taskId=attempt_202105141416432057002236372982352_0004_m_000011_299, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@122cdec9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:16,679] {docker.py:276} INFO - 21/05/14 14:17:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416432057002236372982352_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432057002236372982352_0004_m_000011_299
[2021-05-14 11:17:16,684] {docker.py:276} INFO - 21/05/14 14:17:16 INFO StagingCommitter: Task committer attempt_202105141416432057002236372982352_0004_m_000011_299: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432057002236372982352_0004_m_000011_299 : duration 0:00.005s
[2021-05-14 11:17:21,501] {docker.py:276} INFO - 21/05/14 14:17:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416439079748067993518616_0004_m_000010_298: needsTaskCommit() Task attempt_202105141416439079748067993518616_0004_m_000010_298
[2021-05-14 11:17:21,502] {docker.py:276} INFO - 21/05/14 14:17:21 INFO StagingCommitter: Task committer attempt_202105141416439079748067993518616_0004_m_000010_298: needsTaskCommit() Task attempt_202105141416439079748067993518616_0004_m_000010_298: duration 0:00.003s
[2021-05-14 11:17:21,502] {docker.py:276} INFO - 21/05/14 14:17:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439079748067993518616_0004_m_000010_298
[2021-05-14 11:17:21,504] {docker.py:276} INFO - 21/05/14 14:17:21 INFO Executor: Finished task 10.0 in stage 4.0 (TID 298). 5149 bytes result sent to driver
[2021-05-14 11:17:21,506] {docker.py:276} INFO - 21/05/14 14:17:21 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 300) (0d1d3e01326c, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:21,507] {docker.py:276} INFO - 21/05/14 14:17:21 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 298) in 6132 ms on 0d1d3e01326c (executor driver) (9/200)
[2021-05-14 11:17:21,508] {docker.py:276} INFO - 21/05/14 14:17:21 INFO Executor: Running task 12.0 in stage 4.0 (TID 300)
[2021-05-14 11:17:21,518] {docker.py:276} INFO - 21/05/14 14:17:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:21,533] {docker.py:276} INFO - 21/05/14 14:17:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:21,533] {docker.py:276} INFO - 21/05/14 14:17:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431709898771933861305_0004_m_000012_300, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431709898771933861305_0004_m_000012_300}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431709898771933861305_0004}; taskId=attempt_202105141416431709898771933861305_0004_m_000012_300, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3af0c088}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:21,533] {docker.py:276} INFO - 21/05/14 14:17:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416431709898771933861305_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431709898771933861305_0004_m_000012_300
[2021-05-14 11:17:21,537] {docker.py:276} INFO - 21/05/14 14:17:21 INFO StagingCommitter: Task committer attempt_202105141416431709898771933861305_0004_m_000012_300: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431709898771933861305_0004_m_000012_300 : duration 0:00.004s
[2021-05-14 11:17:22,290] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Starting: Task committer attempt_202105141416434355651844414767541_0004_m_000009_297: needsTaskCommit() Task attempt_202105141416434355651844414767541_0004_m_000009_297
[2021-05-14 11:17:22,291] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Task committer attempt_202105141416434355651844414767541_0004_m_000009_297: needsTaskCommit() Task attempt_202105141416434355651844414767541_0004_m_000009_297: duration 0:00.002s
21/05/14 14:17:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434355651844414767541_0004_m_000009_297
[2021-05-14 11:17:22,292] {docker.py:276} INFO - 21/05/14 14:17:22 INFO Executor: Finished task 9.0 in stage 4.0 (TID 297). 5149 bytes result sent to driver
[2021-05-14 11:17:22,293] {docker.py:276} INFO - 21/05/14 14:17:22 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 301) (0d1d3e01326c, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:22,294] {docker.py:276} INFO - 21/05/14 14:17:22 INFO Executor: Running task 13.0 in stage 4.0 (TID 301)
[2021-05-14 11:17:22,296] {docker.py:276} INFO - 21/05/14 14:17:22 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 297) in 7021 ms on 0d1d3e01326c (executor driver) (10/200)
[2021-05-14 11:17:22,306] {docker.py:276} INFO - 21/05/14 14:17:22 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:22,321] {docker.py:276} INFO - 21/05/14 14:17:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:22,322] {docker.py:276} INFO - 21/05/14 14:17:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:22,322] {docker.py:276} INFO - 21/05/14 14:17:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438147765594107790499_0004_m_000013_301, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438147765594107790499_0004_m_000013_301}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438147765594107790499_0004}; taskId=attempt_202105141416438147765594107790499_0004_m_000013_301, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2781074e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:22,322] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Starting: Task committer attempt_202105141416438147765594107790499_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438147765594107790499_0004_m_000013_301
[2021-05-14 11:17:22,327] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Task committer attempt_202105141416438147765594107790499_0004_m_000013_301: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438147765594107790499_0004_m_000013_301 : duration 0:00.005s
[2021-05-14 11:17:22,683] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Starting: Task committer attempt_202105141416435665878110333232374_0004_m_000008_296: needsTaskCommit() Task attempt_202105141416435665878110333232374_0004_m_000008_296
21/05/14 14:17:22 INFO StagingCommitter: Task committer attempt_202105141416435665878110333232374_0004_m_000008_296: needsTaskCommit() Task attempt_202105141416435665878110333232374_0004_m_000008_296: duration 0:00.002s
[2021-05-14 11:17:22,683] {docker.py:276} INFO - 21/05/14 14:17:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435665878110333232374_0004_m_000008_296
[2021-05-14 11:17:22,685] {docker.py:276} INFO - 21/05/14 14:17:22 INFO Executor: Finished task 8.0 in stage 4.0 (TID 296). 5149 bytes result sent to driver
[2021-05-14 11:17:22,686] {docker.py:276} INFO - 21/05/14 14:17:22 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 302) (0d1d3e01326c, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:22,689] {docker.py:276} INFO - 21/05/14 14:17:22 INFO Executor: Running task 14.0 in stage 4.0 (TID 302)
[2021-05-14 11:17:22,689] {docker.py:276} INFO - 21/05/14 14:17:22 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 296) in 7535 ms on 0d1d3e01326c (executor driver) (11/200)
[2021-05-14 11:17:22,700] {docker.py:276} INFO - 21/05/14 14:17:22 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:22,717] {docker.py:276} INFO - 21/05/14 14:17:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:22,717] {docker.py:276} INFO - 21/05/14 14:17:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435569230652771704571_0004_m_000014_302, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435569230652771704571_0004_m_000014_302}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435569230652771704571_0004}; taskId=attempt_202105141416435569230652771704571_0004_m_000014_302, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53cf913d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:22,717] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Starting: Task committer attempt_202105141416435569230652771704571_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435569230652771704571_0004_m_000014_302
[2021-05-14 11:17:22,721] {docker.py:276} INFO - 21/05/14 14:17:22 INFO StagingCommitter: Task committer attempt_202105141416435569230652771704571_0004_m_000014_302: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435569230652771704571_0004_m_000014_302 : duration 0:00.005s
[2021-05-14 11:17:23,769] {docker.py:276} INFO - 21/05/14 14:17:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416432057002236372982352_0004_m_000011_299: needsTaskCommit() Task attempt_202105141416432057002236372982352_0004_m_000011_299
21/05/14 14:17:23 INFO StagingCommitter: Task committer attempt_202105141416432057002236372982352_0004_m_000011_299: needsTaskCommit() Task attempt_202105141416432057002236372982352_0004_m_000011_299: duration 0:00.002s
21/05/14 14:17:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432057002236372982352_0004_m_000011_299
[2021-05-14 11:17:23,770] {docker.py:276} INFO - 21/05/14 14:17:23 INFO Executor: Finished task 11.0 in stage 4.0 (TID 299). 5149 bytes result sent to driver
[2021-05-14 11:17:23,771] {docker.py:276} INFO - 21/05/14 14:17:23 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 303) (0d1d3e01326c, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:23,773] {docker.py:276} INFO - 21/05/14 14:17:23 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 299) in 7138 ms on 0d1d3e01326c (executor driver) (12/200)
[2021-05-14 11:17:23,773] {docker.py:276} INFO - 21/05/14 14:17:23 INFO Executor: Running task 15.0 in stage 4.0 (TID 303)
[2021-05-14 11:17:23,785] {docker.py:276} INFO - 21/05/14 14:17:23 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:23,801] {docker.py:276} INFO - 21/05/14 14:17:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:23,801] {docker.py:276} INFO - 21/05/14 14:17:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436699867450534821531_0004_m_000015_303, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699867450534821531_0004_m_000015_303}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436699867450534821531_0004}; taskId=attempt_202105141416436699867450534821531_0004_m_000015_303, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2e856795}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:23,802] {docker.py:276} INFO - 21/05/14 14:17:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416436699867450534821531_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699867450534821531_0004_m_000015_303
[2021-05-14 11:17:23,805] {docker.py:276} INFO - 21/05/14 14:17:23 INFO StagingCommitter: Task committer attempt_202105141416436699867450534821531_0004_m_000015_303: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699867450534821531_0004_m_000015_303 : duration 0:00.003s
[2021-05-14 11:17:28,851] {docker.py:276} INFO - 21/05/14 14:17:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416431709898771933861305_0004_m_000012_300: needsTaskCommit() Task attempt_202105141416431709898771933861305_0004_m_000012_300
[2021-05-14 11:17:28,852] {docker.py:276} INFO - 21/05/14 14:17:28 INFO StagingCommitter: Task committer attempt_202105141416431709898771933861305_0004_m_000012_300: needsTaskCommit() Task attempt_202105141416431709898771933861305_0004_m_000012_300: duration 0:00.003s
[2021-05-14 11:17:28,852] {docker.py:276} INFO - 21/05/14 14:17:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431709898771933861305_0004_m_000012_300
[2021-05-14 11:17:28,855] {docker.py:276} INFO - 21/05/14 14:17:28 INFO Executor: Finished task 12.0 in stage 4.0 (TID 300). 5149 bytes result sent to driver
[2021-05-14 11:17:28,856] {docker.py:276} INFO - 21/05/14 14:17:28 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 304) (0d1d3e01326c, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:28,858] {docker.py:276} INFO - 21/05/14 14:17:28 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 300) in 7361 ms on 0d1d3e01326c (executor driver) (13/200)
[2021-05-14 11:17:28,859] {docker.py:276} INFO - 21/05/14 14:17:28 INFO Executor: Running task 16.0 in stage 4.0 (TID 304)
[2021-05-14 11:17:28,869] {docker.py:276} INFO - 21/05/14 14:17:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:28,885] {docker.py:276} INFO - 21/05/14 14:17:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:28,885] {docker.py:276} INFO - 21/05/14 14:17:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643605171316848726307_0004_m_000016_304, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643605171316848726307_0004_m_000016_304}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643605171316848726307_0004}; taskId=attempt_20210514141643605171316848726307_0004_m_000016_304, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26845536}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:28,886] {docker.py:276} INFO - 21/05/14 14:17:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:28,886] {docker.py:276} INFO - 21/05/14 14:17:28 INFO StagingCommitter: Starting: Task committer attempt_20210514141643605171316848726307_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643605171316848726307_0004_m_000016_304
[2021-05-14 11:17:28,891] {docker.py:276} INFO - 21/05/14 14:17:28 INFO StagingCommitter: Task committer attempt_20210514141643605171316848726307_0004_m_000016_304: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643605171316848726307_0004_m_000016_304 : duration 0:00.005s
[2021-05-14 11:17:29,074] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416438147765594107790499_0004_m_000013_301: needsTaskCommit() Task attempt_202105141416438147765594107790499_0004_m_000013_301
[2021-05-14 11:17:29,075] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Task committer attempt_202105141416438147765594107790499_0004_m_000013_301: needsTaskCommit() Task attempt_202105141416438147765594107790499_0004_m_000013_301: duration 0:00.003s
[2021-05-14 11:17:29,075] {docker.py:276} INFO - 21/05/14 14:17:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438147765594107790499_0004_m_000013_301
[2021-05-14 11:17:29,076] {docker.py:276} INFO - 21/05/14 14:17:29 INFO Executor: Finished task 13.0 in stage 4.0 (TID 301). 5149 bytes result sent to driver
[2021-05-14 11:17:29,079] {docker.py:276} INFO - 21/05/14 14:17:29 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 305) (0d1d3e01326c, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:29,080] {docker.py:276} INFO - 21/05/14 14:17:29 INFO Executor: Running task 17.0 in stage 4.0 (TID 305)
21/05/14 14:17:29 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 301) in 6795 ms on 0d1d3e01326c (executor driver) (14/200)
[2021-05-14 11:17:29,088] {docker.py:276} INFO - 21/05/14 14:17:29 INFO ShuffleBlockFetcherIterator: Getting 5 (46.8 KiB) non-empty blocks including 5 (46.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:29,104] {docker.py:276} INFO - 21/05/14 14:17:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:29,104] {docker.py:276} INFO - 21/05/14 14:17:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435543676163946912479_0004_m_000017_305, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435543676163946912479_0004_m_000017_305}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435543676163946912479_0004}; taskId=attempt_202105141416435543676163946912479_0004_m_000017_305, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16b15952}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:29,105] {docker.py:276} INFO - 21/05/14 14:17:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416435543676163946912479_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435543676163946912479_0004_m_000017_305
[2021-05-14 11:17:29,108] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Task committer attempt_202105141416435543676163946912479_0004_m_000017_305: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435543676163946912479_0004_m_000017_305 : duration 0:00.004s
[2021-05-14 11:17:29,803] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416435569230652771704571_0004_m_000014_302: needsTaskCommit() Task attempt_202105141416435569230652771704571_0004_m_000014_302
[2021-05-14 11:17:29,804] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Task committer attempt_202105141416435569230652771704571_0004_m_000014_302: needsTaskCommit() Task attempt_202105141416435569230652771704571_0004_m_000014_302: duration 0:00.003s
[2021-05-14 11:17:29,805] {docker.py:276} INFO - 21/05/14 14:17:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435569230652771704571_0004_m_000014_302
[2021-05-14 11:17:29,807] {docker.py:276} INFO - 21/05/14 14:17:29 INFO Executor: Finished task 14.0 in stage 4.0 (TID 302). 5149 bytes result sent to driver
[2021-05-14 11:17:29,808] {docker.py:276} INFO - 21/05/14 14:17:29 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 306) (0d1d3e01326c, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:29,809] {docker.py:276} INFO - 21/05/14 14:17:29 INFO Executor: Running task 18.0 in stage 4.0 (TID 306)
[2021-05-14 11:17:29,810] {docker.py:276} INFO - 21/05/14 14:17:29 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 302) in 7132 ms on 0d1d3e01326c (executor driver) (15/200)
[2021-05-14 11:17:29,820] {docker.py:276} INFO - 21/05/14 14:17:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:29,820] {docker.py:276} INFO - 21/05/14 14:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:29,835] {docker.py:276} INFO - 21/05/14 14:17:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:29,836] {docker.py:276} INFO - 21/05/14 14:17:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:29,836] {docker.py:276} INFO - 21/05/14 14:17:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433015144750573938339_0004_m_000018_306, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433015144750573938339_0004_m_000018_306}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433015144750573938339_0004}; taskId=attempt_202105141416433015144750573938339_0004_m_000018_306, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48ef7b11}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:29,837] {docker.py:276} INFO - 21/05/14 14:17:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:29,837] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416433015144750573938339_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433015144750573938339_0004_m_000018_306
[2021-05-14 11:17:29,842] {docker.py:276} INFO - 21/05/14 14:17:29 INFO StagingCommitter: Task committer attempt_202105141416433015144750573938339_0004_m_000018_306: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433015144750573938339_0004_m_000018_306 : duration 0:00.007s
[2021-05-14 11:17:30,965] {docker.py:276} INFO - 21/05/14 14:17:30 INFO StagingCommitter: Starting: Task committer attempt_202105141416436699867450534821531_0004_m_000015_303: needsTaskCommit() Task attempt_202105141416436699867450534821531_0004_m_000015_303
[2021-05-14 11:17:30,966] {docker.py:276} INFO - 21/05/14 14:17:30 INFO StagingCommitter: Task committer attempt_202105141416436699867450534821531_0004_m_000015_303: needsTaskCommit() Task attempt_202105141416436699867450534821531_0004_m_000015_303: duration 0:00.001s
21/05/14 14:17:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436699867450534821531_0004_m_000015_303
[2021-05-14 11:17:30,967] {docker.py:276} INFO - 21/05/14 14:17:30 INFO Executor: Finished task 15.0 in stage 4.0 (TID 303). 5149 bytes result sent to driver
[2021-05-14 11:17:30,968] {docker.py:276} INFO - 21/05/14 14:17:30 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 307) (0d1d3e01326c, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:30,969] {docker.py:276} INFO - 21/05/14 14:17:30 INFO Executor: Running task 19.0 in stage 4.0 (TID 307)
[2021-05-14 11:17:30,972] {docker.py:276} INFO - 21/05/14 14:17:30 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 303) in 7210 ms on 0d1d3e01326c (executor driver) (16/200)
[2021-05-14 11:17:30,980] {docker.py:276} INFO - 21/05/14 14:17:31 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:30,981] {docker.py:276} INFO - 21/05/14 14:17:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:30,997] {docker.py:276} INFO - 21/05/14 14:17:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:30,998] {docker.py:276} INFO - 21/05/14 14:17:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437626028585980099630_0004_m_000019_307, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437626028585980099630_0004_m_000019_307}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437626028585980099630_0004}; taskId=attempt_202105141416437626028585980099630_0004_m_000019_307, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17627fa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:30,998] {docker.py:276} INFO - 21/05/14 14:17:31 INFO StagingCommitter: Starting: Task committer attempt_202105141416437626028585980099630_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437626028585980099630_0004_m_000019_307
[2021-05-14 11:17:31,002] {docker.py:276} INFO - 21/05/14 14:17:31 INFO StagingCommitter: Task committer attempt_202105141416437626028585980099630_0004_m_000019_307: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437626028585980099630_0004_m_000019_307 : duration 0:00.004s
[2021-05-14 11:17:35,304] {docker.py:276} INFO - 21/05/14 14:17:35 INFO StagingCommitter: Starting: Task committer attempt_20210514141643605171316848726307_0004_m_000016_304: needsTaskCommit() Task attempt_20210514141643605171316848726307_0004_m_000016_304
[2021-05-14 11:17:35,305] {docker.py:276} INFO - 21/05/14 14:17:35 INFO StagingCommitter: Task committer attempt_20210514141643605171316848726307_0004_m_000016_304: needsTaskCommit() Task attempt_20210514141643605171316848726307_0004_m_000016_304: duration 0:00.002s
21/05/14 14:17:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643605171316848726307_0004_m_000016_304
[2021-05-14 11:17:35,307] {docker.py:276} INFO - 21/05/14 14:17:35 INFO Executor: Finished task 16.0 in stage 4.0 (TID 304). 5149 bytes result sent to driver
[2021-05-14 11:17:35,308] {docker.py:276} INFO - 21/05/14 14:17:35 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 308) (0d1d3e01326c, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:35,309] {docker.py:276} INFO - 21/05/14 14:17:35 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 304) in 6461 ms on 0d1d3e01326c (executor driver) (17/200)
[2021-05-14 11:17:35,310] {docker.py:276} INFO - 21/05/14 14:17:35 INFO Executor: Running task 20.0 in stage 4.0 (TID 308)
[2021-05-14 11:17:35,319] {docker.py:276} INFO - 21/05/14 14:17:35 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:35,334] {docker.py:276} INFO - 21/05/14 14:17:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:35,335] {docker.py:276} INFO - 21/05/14 14:17:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431236522346883561660_0004_m_000020_308, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431236522346883561660_0004_m_000020_308}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431236522346883561660_0004}; taskId=attempt_202105141416431236522346883561660_0004_m_000020_308, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bda4e9f}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416431236522346883561660_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431236522346883561660_0004_m_000020_308
[2021-05-14 11:17:35,342] {docker.py:276} INFO - 21/05/14 14:17:35 INFO StagingCommitter: Task committer attempt_202105141416431236522346883561660_0004_m_000020_308: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431236522346883561660_0004_m_000020_308 : duration 0:00.007s
[2021-05-14 11:17:36,006] {docker.py:276} INFO - 21/05/14 14:17:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416435543676163946912479_0004_m_000017_305: needsTaskCommit() Task attempt_202105141416435543676163946912479_0004_m_000017_305
[2021-05-14 11:17:36,006] {docker.py:276} INFO - 21/05/14 14:17:36 INFO StagingCommitter: Task committer attempt_202105141416435543676163946912479_0004_m_000017_305: needsTaskCommit() Task attempt_202105141416435543676163946912479_0004_m_000017_305: duration 0:00.002s
21/05/14 14:17:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435543676163946912479_0004_m_000017_305
[2021-05-14 11:17:36,008] {docker.py:276} INFO - 21/05/14 14:17:36 INFO Executor: Finished task 17.0 in stage 4.0 (TID 305). 5149 bytes result sent to driver
[2021-05-14 11:17:36,009] {docker.py:276} INFO - 21/05/14 14:17:36 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 309) (0d1d3e01326c, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:36,010] {docker.py:276} INFO - 21/05/14 14:17:36 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 305) in 6941 ms on 0d1d3e01326c (executor driver) (18/200)
21/05/14 14:17:36 INFO Executor: Running task 21.0 in stage 4.0 (TID 309)
[2021-05-14 11:17:36,020] {docker.py:276} INFO - 21/05/14 14:17:36 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:36,032] {docker.py:276} INFO - 21/05/14 14:17:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:36,033] {docker.py:276} INFO - 21/05/14 14:17:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435468262991591850136_0004_m_000021_309, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435468262991591850136_0004_m_000021_309}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435468262991591850136_0004}; taskId=attempt_202105141416435468262991591850136_0004_m_000021_309, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b2af5c3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416435468262991591850136_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435468262991591850136_0004_m_000021_309
[2021-05-14 11:17:36,037] {docker.py:276} INFO - 21/05/14 14:17:36 INFO StagingCommitter: Task committer attempt_202105141416435468262991591850136_0004_m_000021_309: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435468262991591850136_0004_m_000021_309 : duration 0:00.005s
[2021-05-14 11:17:36,860] {docker.py:276} INFO - 21/05/14 14:17:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416433015144750573938339_0004_m_000018_306: needsTaskCommit() Task attempt_202105141416433015144750573938339_0004_m_000018_306
21/05/14 14:17:36 INFO StagingCommitter: Task committer attempt_202105141416433015144750573938339_0004_m_000018_306: needsTaskCommit() Task attempt_202105141416433015144750573938339_0004_m_000018_306: duration 0:00.003s
21/05/14 14:17:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433015144750573938339_0004_m_000018_306
[2021-05-14 11:17:36,864] {docker.py:276} INFO - 21/05/14 14:17:36 INFO Executor: Finished task 18.0 in stage 4.0 (TID 306). 5149 bytes result sent to driver
[2021-05-14 11:17:36,865] {docker.py:276} INFO - 21/05/14 14:17:36 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 310) (0d1d3e01326c, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:36,866] {docker.py:276} INFO - 21/05/14 14:17:36 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 306) in 7066 ms on 0d1d3e01326c (executor driver) (19/200)
21/05/14 14:17:36 INFO Executor: Running task 22.0 in stage 4.0 (TID 310)
[2021-05-14 11:17:36,877] {docker.py:276} INFO - 21/05/14 14:17:36 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:36,890] {docker.py:276} INFO - 21/05/14 14:17:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:36,890] {docker.py:276} INFO - 21/05/14 14:17:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438319743044763850439_0004_m_000022_310, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438319743044763850439_0004_m_000022_310}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438319743044763850439_0004}; taskId=attempt_202105141416438319743044763850439_0004_m_000022_310, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f541ce1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416438319743044763850439_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438319743044763850439_0004_m_000022_310
[2021-05-14 11:17:36,894] {docker.py:276} INFO - 21/05/14 14:17:36 INFO StagingCommitter: Task committer attempt_202105141416438319743044763850439_0004_m_000022_310: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438319743044763850439_0004_m_000022_310 : duration 0:00.005s
[2021-05-14 11:17:38,444] {docker.py:276} INFO - 21/05/14 14:17:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416437626028585980099630_0004_m_000019_307: needsTaskCommit() Task attempt_202105141416437626028585980099630_0004_m_000019_307
[2021-05-14 11:17:38,445] {docker.py:276} INFO - 21/05/14 14:17:38 INFO StagingCommitter: Task committer attempt_202105141416437626028585980099630_0004_m_000019_307: needsTaskCommit() Task attempt_202105141416437626028585980099630_0004_m_000019_307: duration 0:00.002s
21/05/14 14:17:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437626028585980099630_0004_m_000019_307
[2021-05-14 11:17:38,447] {docker.py:276} INFO - 21/05/14 14:17:38 INFO Executor: Finished task 19.0 in stage 4.0 (TID 307). 5149 bytes result sent to driver
[2021-05-14 11:17:38,449] {docker.py:276} INFO - 21/05/14 14:17:38 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 311) (0d1d3e01326c, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:38,450] {docker.py:276} INFO - 21/05/14 14:17:38 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 307) in 7490 ms on 0d1d3e01326c (executor driver) (20/200)
[2021-05-14 11:17:38,451] {docker.py:276} INFO - 21/05/14 14:17:38 INFO Executor: Running task 23.0 in stage 4.0 (TID 311)
[2021-05-14 11:17:38,460] {docker.py:276} INFO - 21/05/14 14:17:38 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:38,461] {docker.py:276} INFO - 21/05/14 14:17:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:38,472] {docker.py:276} INFO - 21/05/14 14:17:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:38,473] {docker.py:276} INFO - 21/05/14 14:17:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439110529351340343672_0004_m_000023_311, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439110529351340343672_0004_m_000023_311}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439110529351340343672_0004}; taskId=attempt_202105141416439110529351340343672_0004_m_000023_311, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@44b42fc8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416439110529351340343672_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439110529351340343672_0004_m_000023_311
[2021-05-14 11:17:38,480] {docker.py:276} INFO - 21/05/14 14:17:38 INFO StagingCommitter: Task committer attempt_202105141416439110529351340343672_0004_m_000023_311: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439110529351340343672_0004_m_000023_311 : duration 0:00.006s
[2021-05-14 11:17:41,218] {docker.py:276} INFO - 21/05/14 14:17:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416431236522346883561660_0004_m_000020_308: needsTaskCommit() Task attempt_202105141416431236522346883561660_0004_m_000020_308
[2021-05-14 11:17:41,219] {docker.py:276} INFO - 21/05/14 14:17:41 INFO StagingCommitter: Task committer attempt_202105141416431236522346883561660_0004_m_000020_308: needsTaskCommit() Task attempt_202105141416431236522346883561660_0004_m_000020_308: duration 0:00.002s
21/05/14 14:17:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431236522346883561660_0004_m_000020_308
[2021-05-14 11:17:41,221] {docker.py:276} INFO - 21/05/14 14:17:41 INFO Executor: Finished task 20.0 in stage 4.0 (TID 308). 5106 bytes result sent to driver
[2021-05-14 11:17:41,222] {docker.py:276} INFO - 21/05/14 14:17:41 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 312) (0d1d3e01326c, executor driver, partition 24, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:41,223] {docker.py:276} INFO - 21/05/14 14:17:41 INFO Executor: Running task 24.0 in stage 4.0 (TID 312)
[2021-05-14 11:17:41,224] {docker.py:276} INFO - 21/05/14 14:17:41 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 308) in 5887 ms on 0d1d3e01326c (executor driver) (21/200)
[2021-05-14 11:17:41,234] {docker.py:276} INFO - 21/05/14 14:17:41 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:41,247] {docker.py:276} INFO - 21/05/14 14:17:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:41,247] {docker.py:276} INFO - 21/05/14 14:17:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:41,248] {docker.py:276} INFO - 21/05/14 14:17:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435785085676773651354_0004_m_000024_312, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435785085676773651354_0004_m_000024_312}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435785085676773651354_0004}; taskId=attempt_202105141416435785085676773651354_0004_m_000024_312, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@69a79f3c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416435785085676773651354_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435785085676773651354_0004_m_000024_312
[2021-05-14 11:17:41,252] {docker.py:276} INFO - 21/05/14 14:17:41 INFO StagingCommitter: Task committer attempt_202105141416435785085676773651354_0004_m_000024_312: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435785085676773651354_0004_m_000024_312 : duration 0:00.004s
[2021-05-14 11:17:42,853] {docker.py:276} INFO - 21/05/14 14:17:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416435468262991591850136_0004_m_000021_309: needsTaskCommit() Task attempt_202105141416435468262991591850136_0004_m_000021_309
[2021-05-14 11:17:42,853] {docker.py:276} INFO - 21/05/14 14:17:42 INFO StagingCommitter: Task committer attempt_202105141416435468262991591850136_0004_m_000021_309: needsTaskCommit() Task attempt_202105141416435468262991591850136_0004_m_000021_309: duration 0:00.001s
21/05/14 14:17:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435468262991591850136_0004_m_000021_309
[2021-05-14 11:17:42,855] {docker.py:276} INFO - 21/05/14 14:17:42 INFO Executor: Finished task 21.0 in stage 4.0 (TID 309). 5106 bytes result sent to driver
[2021-05-14 11:17:42,855] {docker.py:276} INFO - 21/05/14 14:17:42 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 313) (0d1d3e01326c, executor driver, partition 25, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:42,856] {docker.py:276} INFO - 21/05/14 14:17:42 INFO Executor: Running task 25.0 in stage 4.0 (TID 313)
[2021-05-14 11:17:42,857] {docker.py:276} INFO - 21/05/14 14:17:42 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 309) in 6821 ms on 0d1d3e01326c (executor driver) (22/200)
[2021-05-14 11:17:42,876] {docker.py:276} INFO - 21/05/14 14:17:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:42,891] {docker.py:276} INFO - 21/05/14 14:17:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:42,892] {docker.py:276} INFO - 21/05/14 14:17:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432819824803214115426_0004_m_000025_313, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432819824803214115426_0004_m_000025_313}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432819824803214115426_0004}; taskId=attempt_202105141416432819824803214115426_0004_m_000025_313, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b8960ee}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416432819824803214115426_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432819824803214115426_0004_m_000025_313
[2021-05-14 11:17:42,896] {docker.py:276} INFO - 21/05/14 14:17:42 INFO StagingCommitter: Task committer attempt_202105141416432819824803214115426_0004_m_000025_313: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432819824803214115426_0004_m_000025_313 : duration 0:00.005s
[2021-05-14 11:17:43,129] {docker.py:276} INFO - 21/05/14 14:17:43 INFO StagingCommitter: Starting: Task committer attempt_202105141416438319743044763850439_0004_m_000022_310: needsTaskCommit() Task attempt_202105141416438319743044763850439_0004_m_000022_310
[2021-05-14 11:17:43,131] {docker.py:276} INFO - 21/05/14 14:17:43 INFO StagingCommitter: Task committer attempt_202105141416438319743044763850439_0004_m_000022_310: needsTaskCommit() Task attempt_202105141416438319743044763850439_0004_m_000022_310: duration 0:00.002s
[2021-05-14 11:17:43,131] {docker.py:276} INFO - 21/05/14 14:17:43 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438319743044763850439_0004_m_000022_310
[2021-05-14 11:17:43,132] {docker.py:276} INFO - 21/05/14 14:17:43 INFO Executor: Finished task 22.0 in stage 4.0 (TID 310). 5149 bytes result sent to driver
[2021-05-14 11:17:43,134] {docker.py:276} INFO - 21/05/14 14:17:43 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 314) (0d1d3e01326c, executor driver, partition 26, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:43,135] {docker.py:276} INFO - 21/05/14 14:17:43 INFO Executor: Running task 26.0 in stage 4.0 (TID 314)
[2021-05-14 11:17:43,135] {docker.py:276} INFO - 21/05/14 14:17:43 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 310) in 6243 ms on 0d1d3e01326c (executor driver) (23/200)
[2021-05-14 11:17:43,145] {docker.py:276} INFO - 21/05/14 14:17:43 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:43,146] {docker.py:276} INFO - 21/05/14 14:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:43,161] {docker.py:276} INFO - 21/05/14 14:17:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:43 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:43,162] {docker.py:276} INFO - 21/05/14 14:17:43 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436482836154742572800_0004_m_000026_314, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436482836154742572800_0004_m_000026_314}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436482836154742572800_0004}; taskId=attempt_202105141416436482836154742572800_0004_m_000026_314, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6e731b20}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:43,162] {docker.py:276} INFO - 21/05/14 14:17:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:43,162] {docker.py:276} INFO - 21/05/14 14:17:43 INFO StagingCommitter: Starting: Task committer attempt_202105141416436482836154742572800_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436482836154742572800_0004_m_000026_314
[2021-05-14 11:17:43,166] {docker.py:276} INFO - 21/05/14 14:17:43 INFO StagingCommitter: Task committer attempt_202105141416436482836154742572800_0004_m_000026_314: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436482836154742572800_0004_m_000026_314 : duration 0:00.004s
[2021-05-14 11:17:45,385] {docker.py:276} INFO - 21/05/14 14:17:45 INFO StagingCommitter: Starting: Task committer attempt_202105141416439110529351340343672_0004_m_000023_311: needsTaskCommit() Task attempt_202105141416439110529351340343672_0004_m_000023_311
[2021-05-14 11:17:45,386] {docker.py:276} INFO - 21/05/14 14:17:45 INFO StagingCommitter: Task committer attempt_202105141416439110529351340343672_0004_m_000023_311: needsTaskCommit() Task attempt_202105141416439110529351340343672_0004_m_000023_311: duration 0:00.003s
21/05/14 14:17:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439110529351340343672_0004_m_000023_311
[2021-05-14 11:17:45,389] {docker.py:276} INFO - 21/05/14 14:17:45 INFO Executor: Finished task 23.0 in stage 4.0 (TID 311). 5149 bytes result sent to driver
[2021-05-14 11:17:45,391] {docker.py:276} INFO - 21/05/14 14:17:45 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 315) (0d1d3e01326c, executor driver, partition 27, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:45,392] {docker.py:276} INFO - 21/05/14 14:17:45 INFO Executor: Running task 27.0 in stage 4.0 (TID 315)
21/05/14 14:17:45 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 311) in 6917 ms on 0d1d3e01326c (executor driver) (24/200)
[2021-05-14 11:17:45,404] {docker.py:276} INFO - 21/05/14 14:17:45 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:45,417] {docker.py:276} INFO - 21/05/14 14:17:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436766356474567405659_0004_m_000027_315, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436766356474567405659_0004_m_000027_315}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436766356474567405659_0004}; taskId=attempt_202105141416436766356474567405659_0004_m_000027_315, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@264009ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:45,417] {docker.py:276} INFO - 21/05/14 14:17:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:45 INFO StagingCommitter: Starting: Task committer attempt_202105141416436766356474567405659_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436766356474567405659_0004_m_000027_315
[2021-05-14 11:17:45,420] {docker.py:276} INFO - 21/05/14 14:17:45 INFO StagingCommitter: Task committer attempt_202105141416436766356474567405659_0004_m_000027_315: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436766356474567405659_0004_m_000027_315 : duration 0:00.003s
[2021-05-14 11:17:48,209] {docker.py:276} INFO - 21/05/14 14:17:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416435785085676773651354_0004_m_000024_312: needsTaskCommit() Task attempt_202105141416435785085676773651354_0004_m_000024_312
[2021-05-14 11:17:48,209] {docker.py:276} INFO - 21/05/14 14:17:48 INFO StagingCommitter: Task committer attempt_202105141416435785085676773651354_0004_m_000024_312: needsTaskCommit() Task attempt_202105141416435785085676773651354_0004_m_000024_312: duration 0:00.006s
[2021-05-14 11:17:48,210] {docker.py:276} INFO - 21/05/14 14:17:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435785085676773651354_0004_m_000024_312
[2021-05-14 11:17:48,212] {docker.py:276} INFO - 21/05/14 14:17:48 INFO Executor: Finished task 24.0 in stage 4.0 (TID 312). 5149 bytes result sent to driver
[2021-05-14 11:17:48,213] {docker.py:276} INFO - 21/05/14 14:17:48 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 316) (0d1d3e01326c, executor driver, partition 28, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:48,215] {docker.py:276} INFO - 21/05/14 14:17:48 INFO Executor: Running task 28.0 in stage 4.0 (TID 316)
[2021-05-14 11:17:48,215] {docker.py:276} INFO - 21/05/14 14:17:48 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 312) in 7002 ms on 0d1d3e01326c (executor driver) (25/200)
[2021-05-14 11:17:48,223] {docker.py:276} INFO - 21/05/14 14:17:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:48,236] {docker.py:276} INFO - 21/05/14 14:17:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:17:48,237] {docker.py:276} INFO - 21/05/14 14:17:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:48,237] {docker.py:276} INFO - 21/05/14 14:17:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:48,237] {docker.py:276} INFO - 21/05/14 14:17:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432423118654678470965_0004_m_000028_316, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432423118654678470965_0004_m_000028_316}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432423118654678470965_0004}; taskId=attempt_202105141416432423118654678470965_0004_m_000028_316, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@de75381}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:48,237] {docker.py:276} INFO - 21/05/14 14:17:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416432423118654678470965_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432423118654678470965_0004_m_000028_316
[2021-05-14 11:17:48,241] {docker.py:276} INFO - 21/05/14 14:17:48 INFO StagingCommitter: Task committer attempt_202105141416432423118654678470965_0004_m_000028_316: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432423118654678470965_0004_m_000028_316 : duration 0:00.004s
[2021-05-14 11:17:50,352] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416436482836154742572800_0004_m_000026_314: needsTaskCommit() Task attempt_202105141416436482836154742572800_0004_m_000026_314
[2021-05-14 11:17:50,353] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Task committer attempt_202105141416436482836154742572800_0004_m_000026_314: needsTaskCommit() Task attempt_202105141416436482836154742572800_0004_m_000026_314: duration 0:00.002s
[2021-05-14 11:17:50,354] {docker.py:276} INFO - 21/05/14 14:17:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436482836154742572800_0004_m_000026_314
[2021-05-14 11:17:50,356] {docker.py:276} INFO - 21/05/14 14:17:50 INFO Executor: Finished task 26.0 in stage 4.0 (TID 314). 5106 bytes result sent to driver
[2021-05-14 11:17:50,357] {docker.py:276} INFO - 21/05/14 14:17:50 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 317) (0d1d3e01326c, executor driver, partition 29, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:50,358] {docker.py:276} INFO - 21/05/14 14:17:50 INFO Executor: Running task 29.0 in stage 4.0 (TID 317)
[2021-05-14 11:17:50,359] {docker.py:276} INFO - 21/05/14 14:17:50 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 314) in 7233 ms on 0d1d3e01326c (executor driver) (26/200)
[2021-05-14 11:17:50,368] {docker.py:276} INFO - 21/05/14 14:17:50 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:50,380] {docker.py:276} INFO - 21/05/14 14:17:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:50,380] {docker.py:276} INFO - 21/05/14 14:17:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432604013805943076330_0004_m_000029_317, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432604013805943076330_0004_m_000029_317}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432604013805943076330_0004}; taskId=attempt_202105141416432604013805943076330_0004_m_000029_317, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@649b0a6a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:50,381] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416432604013805943076330_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432604013805943076330_0004_m_000029_317
[2021-05-14 11:17:50,385] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Task committer attempt_202105141416432604013805943076330_0004_m_000029_317: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432604013805943076330_0004_m_000029_317 : duration 0:00.005s
[2021-05-14 11:17:50,505] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416432819824803214115426_0004_m_000025_313: needsTaskCommit() Task attempt_202105141416432819824803214115426_0004_m_000025_313
[2021-05-14 11:17:50,505] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Task committer attempt_202105141416432819824803214115426_0004_m_000025_313: needsTaskCommit() Task attempt_202105141416432819824803214115426_0004_m_000025_313: duration 0:00.002s
[2021-05-14 11:17:50,505] {docker.py:276} INFO - 21/05/14 14:17:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432819824803214115426_0004_m_000025_313
[2021-05-14 11:17:50,506] {docker.py:276} INFO - 21/05/14 14:17:50 INFO Executor: Finished task 25.0 in stage 4.0 (TID 313). 5149 bytes result sent to driver
[2021-05-14 11:17:50,508] {docker.py:276} INFO - 21/05/14 14:17:50 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 318) (0d1d3e01326c, executor driver, partition 30, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:50,508] {docker.py:276} INFO - 21/05/14 14:17:50 INFO Executor: Running task 30.0 in stage 4.0 (TID 318)
[2021-05-14 11:17:50,508] {docker.py:276} INFO - 21/05/14 14:17:50 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 313) in 7662 ms on 0d1d3e01326c (executor driver) (27/200)
[2021-05-14 11:17:50,517] {docker.py:276} INFO - 21/05/14 14:17:50 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:50,528] {docker.py:276} INFO - 21/05/14 14:17:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438880255811964904931_0004_m_000030_318, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438880255811964904931_0004_m_000030_318}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438880255811964904931_0004}; taskId=attempt_202105141416438880255811964904931_0004_m_000030_318, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a8adaf4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416438880255811964904931_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438880255811964904931_0004_m_000030_318
[2021-05-14 11:17:50,531] {docker.py:276} INFO - 21/05/14 14:17:50 INFO StagingCommitter: Task committer attempt_202105141416438880255811964904931_0004_m_000030_318: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438880255811964904931_0004_m_000030_318 : duration 0:00.003s
[2021-05-14 11:17:52,457] {docker.py:276} INFO - 21/05/14 14:17:52 INFO StagingCommitter: Starting: Task committer attempt_202105141416436766356474567405659_0004_m_000027_315: needsTaskCommit() Task attempt_202105141416436766356474567405659_0004_m_000027_315
[2021-05-14 11:17:52,458] {docker.py:276} INFO - 21/05/14 14:17:52 INFO StagingCommitter: Task committer attempt_202105141416436766356474567405659_0004_m_000027_315: needsTaskCommit() Task attempt_202105141416436766356474567405659_0004_m_000027_315: duration 0:00.001s
[2021-05-14 11:17:52,458] {docker.py:276} INFO - 21/05/14 14:17:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436766356474567405659_0004_m_000027_315
[2021-05-14 11:17:52,460] {docker.py:276} INFO - 21/05/14 14:17:52 INFO Executor: Finished task 27.0 in stage 4.0 (TID 315). 5149 bytes result sent to driver
[2021-05-14 11:17:52,462] {docker.py:276} INFO - 21/05/14 14:17:52 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 319) (0d1d3e01326c, executor driver, partition 31, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:52,463] {docker.py:276} INFO - 21/05/14 14:17:52 INFO Executor: Running task 31.0 in stage 4.0 (TID 319)
21/05/14 14:17:52 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 315) in 7081 ms on 0d1d3e01326c (executor driver) (28/200)
[2021-05-14 11:17:52,472] {docker.py:276} INFO - 21/05/14 14:17:52 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:17:52,472] {docker.py:276} INFO - 21/05/14 14:17:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:52,485] {docker.py:276} INFO - 21/05/14 14:17:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:52 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:52,485] {docker.py:276} INFO - 21/05/14 14:17:52 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431082241002712264060_0004_m_000031_319, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431082241002712264060_0004_m_000031_319}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431082241002712264060_0004}; taskId=attempt_202105141416431082241002712264060_0004_m_000031_319, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@170e6500}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:52,486] {docker.py:276} INFO - 21/05/14 14:17:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:52 INFO StagingCommitter: Starting: Task committer attempt_202105141416431082241002712264060_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431082241002712264060_0004_m_000031_319
[2021-05-14 11:17:52,492] {docker.py:276} INFO - 21/05/14 14:17:52 INFO StagingCommitter: Task committer attempt_202105141416431082241002712264060_0004_m_000031_319: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431082241002712264060_0004_m_000031_319 : duration 0:00.006s
[2021-05-14 11:17:55,113] {docker.py:276} INFO - 21/05/14 14:17:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416432423118654678470965_0004_m_000028_316: needsTaskCommit() Task attempt_202105141416432423118654678470965_0004_m_000028_316
[2021-05-14 11:17:55,114] {docker.py:276} INFO - 21/05/14 14:17:55 INFO StagingCommitter: Task committer attempt_202105141416432423118654678470965_0004_m_000028_316: needsTaskCommit() Task attempt_202105141416432423118654678470965_0004_m_000028_316: duration 0:00.004s
21/05/14 14:17:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432423118654678470965_0004_m_000028_316
[2021-05-14 11:17:55,115] {docker.py:276} INFO - 21/05/14 14:17:55 INFO Executor: Finished task 28.0 in stage 4.0 (TID 316). 5149 bytes result sent to driver
[2021-05-14 11:17:55,116] {docker.py:276} INFO - 21/05/14 14:17:55 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 320) (0d1d3e01326c, executor driver, partition 32, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:55,118] {docker.py:276} INFO - 21/05/14 14:17:55 INFO Executor: Running task 32.0 in stage 4.0 (TID 320)
[2021-05-14 11:17:55,118] {docker.py:276} INFO - 21/05/14 14:17:55 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 316) in 6913 ms on 0d1d3e01326c (executor driver) (29/200)
[2021-05-14 11:17:55,128] {docker.py:276} INFO - 21/05/14 14:17:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:55,140] {docker.py:276} INFO - 21/05/14 14:17:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643307202763232782408_0004_m_000032_320, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307202763232782408_0004_m_000032_320}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643307202763232782408_0004}; taskId=attempt_20210514141643307202763232782408_0004_m_000032_320, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@37f29867}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:17:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:55 INFO StagingCommitter: Starting: Task committer attempt_20210514141643307202763232782408_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307202763232782408_0004_m_000032_320
[2021-05-14 11:17:55,144] {docker.py:276} INFO - 21/05/14 14:17:55 INFO StagingCommitter: Task committer attempt_20210514141643307202763232782408_0004_m_000032_320: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307202763232782408_0004_m_000032_320 : duration 0:00.004s
[2021-05-14 11:17:57,564] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416432604013805943076330_0004_m_000029_317: needsTaskCommit() Task attempt_202105141416432604013805943076330_0004_m_000029_317
21/05/14 14:17:57 INFO StagingCommitter: Task committer attempt_202105141416432604013805943076330_0004_m_000029_317: needsTaskCommit() Task attempt_202105141416432604013805943076330_0004_m_000029_317: duration 0:00.002s
21/05/14 14:17:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432604013805943076330_0004_m_000029_317
[2021-05-14 11:17:57,566] {docker.py:276} INFO - 21/05/14 14:17:57 INFO Executor: Finished task 29.0 in stage 4.0 (TID 317). 5149 bytes result sent to driver
[2021-05-14 11:17:57,568] {docker.py:276} INFO - 21/05/14 14:17:57 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 321) (0d1d3e01326c, executor driver, partition 33, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:57,569] {docker.py:276} INFO - 21/05/14 14:17:57 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 317) in 7220 ms on 0d1d3e01326c (executor driver) (30/200)
21/05/14 14:17:57 INFO Executor: Running task 33.0 in stage 4.0 (TID 321)
[2021-05-14 11:17:57,580] {docker.py:276} INFO - 21/05/14 14:17:57 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:57,591] {docker.py:276} INFO - 21/05/14 14:17:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:57,591] {docker.py:276} INFO - 21/05/14 14:17:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437386715757550651924_0004_m_000033_321, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437386715757550651924_0004_m_000033_321}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437386715757550651924_0004}; taskId=attempt_202105141416437386715757550651924_0004_m_000033_321, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bbfe9fb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:57,592] {docker.py:276} INFO - 21/05/14 14:17:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416437386715757550651924_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437386715757550651924_0004_m_000033_321
[2021-05-14 11:17:57,596] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Task committer attempt_202105141416437386715757550651924_0004_m_000033_321: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437386715757550651924_0004_m_000033_321 : duration 0:00.004s
[2021-05-14 11:17:57,634] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416438880255811964904931_0004_m_000030_318: needsTaskCommit() Task attempt_202105141416438880255811964904931_0004_m_000030_318
[2021-05-14 11:17:57,634] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Task committer attempt_202105141416438880255811964904931_0004_m_000030_318: needsTaskCommit() Task attempt_202105141416438880255811964904931_0004_m_000030_318: duration 0:00.001s
[2021-05-14 11:17:57,635] {docker.py:276} INFO - 21/05/14 14:17:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438880255811964904931_0004_m_000030_318
[2021-05-14 11:17:57,636] {docker.py:276} INFO - 21/05/14 14:17:57 INFO Executor: Finished task 30.0 in stage 4.0 (TID 318). 5149 bytes result sent to driver
[2021-05-14 11:17:57,637] {docker.py:276} INFO - 21/05/14 14:17:57 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 322) (0d1d3e01326c, executor driver, partition 34, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:57,638] {docker.py:276} INFO - 21/05/14 14:17:57 INFO Executor: Running task 34.0 in stage 4.0 (TID 322)
[2021-05-14 11:17:57,639] {docker.py:276} INFO - 21/05/14 14:17:57 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 318) in 7140 ms on 0d1d3e01326c (executor driver) (31/200)
[2021-05-14 11:17:57,647] {docker.py:276} INFO - 21/05/14 14:17:57 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:57,658] {docker.py:276} INFO - 21/05/14 14:17:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:17:57,659] {docker.py:276} INFO - 21/05/14 14:17:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:57,659] {docker.py:276} INFO - 21/05/14 14:17:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433465010184722011226_0004_m_000034_322, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433465010184722011226_0004_m_000034_322}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433465010184722011226_0004}; taskId=attempt_202105141416433465010184722011226_0004_m_000034_322, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25b3f93c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:57,659] {docker.py:276} INFO - 21/05/14 14:17:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:17:57,659] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416433465010184722011226_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433465010184722011226_0004_m_000034_322
[2021-05-14 11:17:57,663] {docker.py:276} INFO - 21/05/14 14:17:57 INFO StagingCommitter: Task committer attempt_202105141416433465010184722011226_0004_m_000034_322: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433465010184722011226_0004_m_000034_322 : duration 0:00.004s
[2021-05-14 11:17:59,353] {docker.py:276} INFO - 21/05/14 14:17:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416431082241002712264060_0004_m_000031_319: needsTaskCommit() Task attempt_202105141416431082241002712264060_0004_m_000031_319
21/05/14 14:17:59 INFO StagingCommitter: Task committer attempt_202105141416431082241002712264060_0004_m_000031_319: needsTaskCommit() Task attempt_202105141416431082241002712264060_0004_m_000031_319: duration 0:00.002s
[2021-05-14 11:17:59,354] {docker.py:276} INFO - 21/05/14 14:17:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431082241002712264060_0004_m_000031_319
[2021-05-14 11:17:59,356] {docker.py:276} INFO - 21/05/14 14:17:59 INFO Executor: Finished task 31.0 in stage 4.0 (TID 319). 5106 bytes result sent to driver
[2021-05-14 11:17:59,358] {docker.py:276} INFO - 21/05/14 14:17:59 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 323) (0d1d3e01326c, executor driver, partition 35, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:17:59,359] {docker.py:276} INFO - 21/05/14 14:17:59 INFO Executor: Running task 35.0 in stage 4.0 (TID 323)
[2021-05-14 11:17:59,360] {docker.py:276} INFO - 21/05/14 14:17:59 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 319) in 6907 ms on 0d1d3e01326c (executor driver) (32/200)
[2021-05-14 11:17:59,370] {docker.py:276} INFO - 21/05/14 14:17:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:17:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:17:59,395] {docker.py:276} INFO - 21/05/14 14:17:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:17:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:17:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:17:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437079585868183716125_0004_m_000035_323, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437079585868183716125_0004_m_000035_323}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437079585868183716125_0004}; taskId=attempt_202105141416437079585868183716125_0004_m_000035_323, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33c312f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:17:59,396] {docker.py:276} INFO - 21/05/14 14:17:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:17:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416437079585868183716125_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437079585868183716125_0004_m_000035_323
[2021-05-14 11:17:59,401] {docker.py:276} INFO - 21/05/14 14:17:59 INFO StagingCommitter: Task committer attempt_202105141416437079585868183716125_0004_m_000035_323: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437079585868183716125_0004_m_000035_323 : duration 0:00.005s
[2021-05-14 11:18:02,068] {docker.py:276} INFO - 21/05/14 14:18:02 INFO StagingCommitter: Starting: Task committer attempt_20210514141643307202763232782408_0004_m_000032_320: needsTaskCommit() Task attempt_20210514141643307202763232782408_0004_m_000032_320
[2021-05-14 11:18:02,069] {docker.py:276} INFO - 21/05/14 14:18:02 INFO StagingCommitter: Task committer attempt_20210514141643307202763232782408_0004_m_000032_320: needsTaskCommit() Task attempt_20210514141643307202763232782408_0004_m_000032_320: duration 0:00.003s
21/05/14 14:18:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643307202763232782408_0004_m_000032_320
[2021-05-14 11:18:02,071] {docker.py:276} INFO - 21/05/14 14:18:02 INFO Executor: Finished task 32.0 in stage 4.0 (TID 320). 5149 bytes result sent to driver
[2021-05-14 11:18:02,073] {docker.py:276} INFO - 21/05/14 14:18:02 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 324) (0d1d3e01326c, executor driver, partition 36, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:02,074] {docker.py:276} INFO - 21/05/14 14:18:02 INFO Executor: Running task 36.0 in stage 4.0 (TID 324)
[2021-05-14 11:18:02,075] {docker.py:276} INFO - 21/05/14 14:18:02 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 320) in 6966 ms on 0d1d3e01326c (executor driver) (33/200)
[2021-05-14 11:18:02,084] {docker.py:276} INFO - 21/05/14 14:18:02 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:02,096] {docker.py:276} INFO - 21/05/14 14:18:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:02,096] {docker.py:276} INFO - 21/05/14 14:18:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438470295823239321610_0004_m_000036_324, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438470295823239321610_0004_m_000036_324}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438470295823239321610_0004}; taskId=attempt_202105141416438470295823239321610_0004_m_000036_324, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e4ac48c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:02 INFO StagingCommitter: Starting: Task committer attempt_202105141416438470295823239321610_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438470295823239321610_0004_m_000036_324
[2021-05-14 11:18:02,100] {docker.py:276} INFO - 21/05/14 14:18:02 INFO StagingCommitter: Task committer attempt_202105141416438470295823239321610_0004_m_000036_324: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438470295823239321610_0004_m_000036_324 : duration 0:00.004s
[2021-05-14 11:18:04,343] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Starting: Task committer attempt_202105141416433465010184722011226_0004_m_000034_322: needsTaskCommit() Task attempt_202105141416433465010184722011226_0004_m_000034_322
[2021-05-14 11:18:04,343] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Task committer attempt_202105141416433465010184722011226_0004_m_000034_322: needsTaskCommit() Task attempt_202105141416433465010184722011226_0004_m_000034_322: duration 0:00.002s
[2021-05-14 11:18:04,344] {docker.py:276} INFO - 21/05/14 14:18:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433465010184722011226_0004_m_000034_322
[2021-05-14 11:18:04,345] {docker.py:276} INFO - 21/05/14 14:18:04 INFO Executor: Finished task 34.0 in stage 4.0 (TID 322). 5149 bytes result sent to driver
[2021-05-14 11:18:04,346] {docker.py:276} INFO - 21/05/14 14:18:04 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 325) (0d1d3e01326c, executor driver, partition 37, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:04,348] {docker.py:276} INFO - 21/05/14 14:18:04 INFO Executor: Running task 37.0 in stage 4.0 (TID 325)
21/05/14 14:18:04 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 322) in 6718 ms on 0d1d3e01326c (executor driver) (34/200)
[2021-05-14 11:18:04,357] {docker.py:276} INFO - 21/05/14 14:18:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:04,366] {docker.py:276} INFO - 21/05/14 14:18:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:04,366] {docker.py:276} INFO - 21/05/14 14:18:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431409882965404940325_0004_m_000037_325, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431409882965404940325_0004_m_000037_325}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431409882965404940325_0004}; taskId=attempt_202105141416431409882965404940325_0004_m_000037_325, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1721bf4b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:04 INFO StagingCommitter: Starting: Task committer attempt_202105141416431409882965404940325_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431409882965404940325_0004_m_000037_325
[2021-05-14 11:18:04,371] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Task committer attempt_202105141416431409882965404940325_0004_m_000037_325: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431409882965404940325_0004_m_000037_325 : duration 0:00.005s
[2021-05-14 11:18:04,527] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Starting: Task committer attempt_202105141416437386715757550651924_0004_m_000033_321: needsTaskCommit() Task attempt_202105141416437386715757550651924_0004_m_000033_321
[2021-05-14 11:18:04,528] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Task committer attempt_202105141416437386715757550651924_0004_m_000033_321: needsTaskCommit() Task attempt_202105141416437386715757550651924_0004_m_000033_321: duration 0:00.002s
21/05/14 14:18:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437386715757550651924_0004_m_000033_321
[2021-05-14 11:18:04,530] {docker.py:276} INFO - 21/05/14 14:18:04 INFO Executor: Finished task 33.0 in stage 4.0 (TID 321). 5149 bytes result sent to driver
[2021-05-14 11:18:04,532] {docker.py:276} INFO - 21/05/14 14:18:04 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 326) (0d1d3e01326c, executor driver, partition 38, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:18:04 INFO Executor: Running task 38.0 in stage 4.0 (TID 326)
21/05/14 14:18:04 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 321) in 6973 ms on 0d1d3e01326c (executor driver) (35/200)
[2021-05-14 11:18:04,542] {docker.py:276} INFO - 21/05/14 14:18:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:04,542] {docker.py:276} INFO - 21/05/14 14:18:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:04,554] {docker.py:276} INFO - 21/05/14 14:18:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:18:04,554] {docker.py:276} INFO - 21/05/14 14:18:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:04,555] {docker.py:276} INFO - 21/05/14 14:18:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:04,555] {docker.py:276} INFO - 21/05/14 14:18:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431517869141160465567_0004_m_000038_326, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431517869141160465567_0004_m_000038_326}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431517869141160465567_0004}; taskId=attempt_202105141416431517869141160465567_0004_m_000038_326, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13bbdf5c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:04 INFO StagingCommitter: Starting: Task committer attempt_202105141416431517869141160465567_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431517869141160465567_0004_m_000038_326
[2021-05-14 11:18:04,560] {docker.py:276} INFO - 21/05/14 14:18:04 INFO StagingCommitter: Task committer attempt_202105141416431517869141160465567_0004_m_000038_326: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431517869141160465567_0004_m_000038_326 : duration 0:00.005s
[2021-05-14 11:18:06,429] {docker.py:276} INFO - 21/05/14 14:18:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416437079585868183716125_0004_m_000035_323: needsTaskCommit() Task attempt_202105141416437079585868183716125_0004_m_000035_323
[2021-05-14 11:18:06,430] {docker.py:276} INFO - 21/05/14 14:18:06 INFO StagingCommitter: Task committer attempt_202105141416437079585868183716125_0004_m_000035_323: needsTaskCommit() Task attempt_202105141416437079585868183716125_0004_m_000035_323: duration 0:00.002s
21/05/14 14:18:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437079585868183716125_0004_m_000035_323
[2021-05-14 11:18:06,432] {docker.py:276} INFO - 21/05/14 14:18:06 INFO Executor: Finished task 35.0 in stage 4.0 (TID 323). 5149 bytes result sent to driver
[2021-05-14 11:18:06,433] {docker.py:276} INFO - 21/05/14 14:18:06 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 327) (0d1d3e01326c, executor driver, partition 39, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:06,435] {docker.py:276} INFO - 21/05/14 14:18:06 INFO Executor: Running task 39.0 in stage 4.0 (TID 327)
[2021-05-14 11:18:06,435] {docker.py:276} INFO - 21/05/14 14:18:06 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 323) in 7086 ms on 0d1d3e01326c (executor driver) (36/200)
[2021-05-14 11:18:06,445] {docker.py:276} INFO - 21/05/14 14:18:06 INFO ShuffleBlockFetcherIterator: Getting 5 (40.2 KiB) non-empty blocks including 5 (40.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:06,445] {docker.py:276} INFO - 21/05/14 14:18:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:06,456] {docker.py:276} INFO - 21/05/14 14:18:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:18:06,457] {docker.py:276} INFO - 21/05/14 14:18:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:06,457] {docker.py:276} INFO - 21/05/14 14:18:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:06,458] {docker.py:276} INFO - 21/05/14 14:18:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432063250976903122008_0004_m_000039_327, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432063250976903122008_0004_m_000039_327}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432063250976903122008_0004}; taskId=attempt_202105141416432063250976903122008_0004_m_000039_327, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@736d9ed4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:06,458] {docker.py:276} INFO - 21/05/14 14:18:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:06,458] {docker.py:276} INFO - 21/05/14 14:18:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416432063250976903122008_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432063250976903122008_0004_m_000039_327
[2021-05-14 11:18:06,462] {docker.py:276} INFO - 21/05/14 14:18:06 INFO StagingCommitter: Task committer attempt_202105141416432063250976903122008_0004_m_000039_327: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432063250976903122008_0004_m_000039_327 : duration 0:00.005s
[2021-05-14 11:18:09,174] {docker.py:276} INFO - 21/05/14 14:18:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416438470295823239321610_0004_m_000036_324: needsTaskCommit() Task attempt_202105141416438470295823239321610_0004_m_000036_324
[2021-05-14 11:18:09,175] {docker.py:276} INFO - 21/05/14 14:18:09 INFO StagingCommitter: Task committer attempt_202105141416438470295823239321610_0004_m_000036_324: needsTaskCommit() Task attempt_202105141416438470295823239321610_0004_m_000036_324: duration 0:00.001s
21/05/14 14:18:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438470295823239321610_0004_m_000036_324
[2021-05-14 11:18:09,176] {docker.py:276} INFO - 21/05/14 14:18:09 INFO Executor: Finished task 36.0 in stage 4.0 (TID 324). 5106 bytes result sent to driver
[2021-05-14 11:18:09,177] {docker.py:276} INFO - 21/05/14 14:18:09 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 328) (0d1d3e01326c, executor driver, partition 40, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:09,178] {docker.py:276} INFO - 21/05/14 14:18:09 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 324) in 7115 ms on 0d1d3e01326c (executor driver) (37/200)
21/05/14 14:18:09 INFO Executor: Running task 40.0 in stage 4.0 (TID 328)
[2021-05-14 11:18:09,197] {docker.py:276} INFO - 21/05/14 14:18:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:09,198] {docker.py:276} INFO - 21/05/14 14:18:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:18:09,214] {docker.py:276} INFO - 21/05/14 14:18:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:09,214] {docker.py:276} INFO - 21/05/14 14:18:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432566080844061596590_0004_m_000040_328, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432566080844061596590_0004_m_000040_328}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432566080844061596590_0004}; taskId=attempt_202105141416432566080844061596590_0004_m_000040_328, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66639e02}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416432566080844061596590_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432566080844061596590_0004_m_000040_328
[2021-05-14 11:18:09,218] {docker.py:276} INFO - 21/05/14 14:18:09 INFO StagingCommitter: Task committer attempt_202105141416432566080844061596590_0004_m_000040_328: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432566080844061596590_0004_m_000040_328 : duration 0:00.003s
[2021-05-14 11:18:11,146] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Starting: Task committer attempt_202105141416431409882965404940325_0004_m_000037_325: needsTaskCommit() Task attempt_202105141416431409882965404940325_0004_m_000037_325
[2021-05-14 11:18:11,147] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Task committer attempt_202105141416431409882965404940325_0004_m_000037_325: needsTaskCommit() Task attempt_202105141416431409882965404940325_0004_m_000037_325: duration 0:00.002s
21/05/14 14:18:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431409882965404940325_0004_m_000037_325
[2021-05-14 11:18:11,148] {docker.py:276} INFO - 21/05/14 14:18:11 INFO Executor: Finished task 37.0 in stage 4.0 (TID 325). 5149 bytes result sent to driver
[2021-05-14 11:18:11,149] {docker.py:276} INFO - 21/05/14 14:18:11 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 329) (0d1d3e01326c, executor driver, partition 41, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:11,150] {docker.py:276} INFO - 21/05/14 14:18:11 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 325) in 6777 ms on 0d1d3e01326c (executor driver) (38/200)
21/05/14 14:18:11 INFO Executor: Running task 41.0 in stage 4.0 (TID 329)
[2021-05-14 11:18:11,160] {docker.py:276} INFO - 21/05/14 14:18:11 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:11,169] {docker.py:276} INFO - 21/05/14 14:18:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643640141099082552671_0004_m_000041_329, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643640141099082552671_0004_m_000041_329}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643640141099082552671_0004}; taskId=attempt_20210514141643640141099082552671_0004_m_000041_329, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4be64e4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:11,170] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Starting: Task committer attempt_20210514141643640141099082552671_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643640141099082552671_0004_m_000041_329
[2021-05-14 11:18:11,174] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Task committer attempt_20210514141643640141099082552671_0004_m_000041_329: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643640141099082552671_0004_m_000041_329 : duration 0:00.004s
[2021-05-14 11:18:11,290] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Starting: Task committer attempt_202105141416431517869141160465567_0004_m_000038_326: needsTaskCommit() Task attempt_202105141416431517869141160465567_0004_m_000038_326
[2021-05-14 11:18:11,290] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Task committer attempt_202105141416431517869141160465567_0004_m_000038_326: needsTaskCommit() Task attempt_202105141416431517869141160465567_0004_m_000038_326: duration 0:00.001s
21/05/14 14:18:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431517869141160465567_0004_m_000038_326
[2021-05-14 11:18:11,291] {docker.py:276} INFO - 21/05/14 14:18:11 INFO Executor: Finished task 38.0 in stage 4.0 (TID 326). 5149 bytes result sent to driver
[2021-05-14 11:18:11,292] {docker.py:276} INFO - 21/05/14 14:18:11 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 330) (0d1d3e01326c, executor driver, partition 42, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:11,293] {docker.py:276} INFO - 21/05/14 14:18:11 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 326) in 6736 ms on 0d1d3e01326c (executor driver) (39/200)
[2021-05-14 11:18:11,294] {docker.py:276} INFO - 21/05/14 14:18:11 INFO Executor: Running task 42.0 in stage 4.0 (TID 330)
[2021-05-14 11:18:11,302] {docker.py:276} INFO - 21/05/14 14:18:11 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:11,311] {docker.py:276} INFO - 21/05/14 14:18:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:11,312] {docker.py:276} INFO - 21/05/14 14:18:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433790284584882198324_0004_m_000042_330, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433790284584882198324_0004_m_000042_330}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433790284584882198324_0004}; taskId=attempt_202105141416433790284584882198324_0004_m_000042_330, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ad74332}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:11,312] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Starting: Task committer attempt_202105141416433790284584882198324_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433790284584882198324_0004_m_000042_330
[2021-05-14 11:18:11,316] {docker.py:276} INFO - 21/05/14 14:18:11 INFO StagingCommitter: Task committer attempt_202105141416433790284584882198324_0004_m_000042_330: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433790284584882198324_0004_m_000042_330 : duration 0:00.004s
[2021-05-14 11:18:13,481] {docker.py:276} INFO - 21/05/14 14:18:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416432063250976903122008_0004_m_000039_327: needsTaskCommit() Task attempt_202105141416432063250976903122008_0004_m_000039_327
[2021-05-14 11:18:13,482] {docker.py:276} INFO - 21/05/14 14:18:13 INFO StagingCommitter: Task committer attempt_202105141416432063250976903122008_0004_m_000039_327: needsTaskCommit() Task attempt_202105141416432063250976903122008_0004_m_000039_327: duration 0:00.002s
[2021-05-14 11:18:13,482] {docker.py:276} INFO - 21/05/14 14:18:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432063250976903122008_0004_m_000039_327
[2021-05-14 11:18:13,483] {docker.py:276} INFO - 21/05/14 14:18:13 INFO Executor: Finished task 39.0 in stage 4.0 (TID 327). 5149 bytes result sent to driver
[2021-05-14 11:18:13,485] {docker.py:276} INFO - 21/05/14 14:18:13 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 331) (0d1d3e01326c, executor driver, partition 43, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:13,487] {docker.py:276} INFO - 21/05/14 14:18:13 INFO Executor: Running task 43.0 in stage 4.0 (TID 331)
21/05/14 14:18:13 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 327) in 7027 ms on 0d1d3e01326c (executor driver) (40/200)
[2021-05-14 11:18:13,497] {docker.py:276} INFO - 21/05/14 14:18:13 INFO ShuffleBlockFetcherIterator: Getting 5 (44.5 KiB) non-empty blocks including 5 (44.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:13,497] {docker.py:276} INFO - 21/05/14 14:18:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:13,507] {docker.py:276} INFO - 21/05/14 14:18:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:13,507] {docker.py:276} INFO - 21/05/14 14:18:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643892113597937973879_0004_m_000043_331, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643892113597937973879_0004_m_000043_331}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643892113597937973879_0004}; taskId=attempt_20210514141643892113597937973879_0004_m_000043_331, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76f72d36}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:13 INFO StagingCommitter: Starting: Task committer attempt_20210514141643892113597937973879_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643892113597937973879_0004_m_000043_331
[2021-05-14 11:18:13,511] {docker.py:276} INFO - 21/05/14 14:18:13 INFO StagingCommitter: Task committer attempt_20210514141643892113597937973879_0004_m_000043_331: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643892113597937973879_0004_m_000043_331 : duration 0:00.004s
[2021-05-14 11:18:16,120] {docker.py:276} INFO - 21/05/14 14:18:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416432566080844061596590_0004_m_000040_328: needsTaskCommit() Task attempt_202105141416432566080844061596590_0004_m_000040_328
[2021-05-14 11:18:16,121] {docker.py:276} INFO - 21/05/14 14:18:16 INFO StagingCommitter: Task committer attempt_202105141416432566080844061596590_0004_m_000040_328: needsTaskCommit() Task attempt_202105141416432566080844061596590_0004_m_000040_328: duration 0:00.003s
21/05/14 14:18:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432566080844061596590_0004_m_000040_328
[2021-05-14 11:18:16,124] {docker.py:276} INFO - 21/05/14 14:18:16 INFO Executor: Finished task 40.0 in stage 4.0 (TID 328). 5149 bytes result sent to driver
21/05/14 14:18:16 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 332) (0d1d3e01326c, executor driver, partition 44, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:16,125] {docker.py:276} INFO - 21/05/14 14:18:16 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 328) in 6922 ms on 0d1d3e01326c (executor driver) (41/200)
[2021-05-14 11:18:16,126] {docker.py:276} INFO - 21/05/14 14:18:16 INFO Executor: Running task 44.0 in stage 4.0 (TID 332)
[2021-05-14 11:18:16,138] {docker.py:276} INFO - 21/05/14 14:18:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:16,148] {docker.py:276} INFO - 21/05/14 14:18:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:16,148] {docker.py:276} INFO - 21/05/14 14:18:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434150581836546133475_0004_m_000044_332, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434150581836546133475_0004_m_000044_332}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434150581836546133475_0004}; taskId=attempt_202105141416434150581836546133475_0004_m_000044_332, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@54325d62}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416434150581836546133475_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434150581836546133475_0004_m_000044_332
[2021-05-14 11:18:16,152] {docker.py:276} INFO - 21/05/14 14:18:16 INFO StagingCommitter: Task committer attempt_202105141416434150581836546133475_0004_m_000044_332: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434150581836546133475_0004_m_000044_332 : duration 0:00.004s
[2021-05-14 11:18:18,015] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416433790284584882198324_0004_m_000042_330: needsTaskCommit() Task attempt_202105141416433790284584882198324_0004_m_000042_330
[2021-05-14 11:18:18,016] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Task committer attempt_202105141416433790284584882198324_0004_m_000042_330: needsTaskCommit() Task attempt_202105141416433790284584882198324_0004_m_000042_330: duration 0:00.003s
21/05/14 14:18:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433790284584882198324_0004_m_000042_330
[2021-05-14 11:18:18,018] {docker.py:276} INFO - 21/05/14 14:18:18 INFO Executor: Finished task 42.0 in stage 4.0 (TID 330). 5106 bytes result sent to driver
[2021-05-14 11:18:18,030] {docker.py:276} INFO - 21/05/14 14:18:18 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 333) (0d1d3e01326c, executor driver, partition 45, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:18,031] {docker.py:276} INFO - 21/05/14 14:18:18 INFO Executor: Running task 45.0 in stage 4.0 (TID 333)
[2021-05-14 11:18:18,032] {docker.py:276} INFO - 21/05/14 14:18:18 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 330) in 6747 ms on 0d1d3e01326c (executor driver) (42/200)
[2021-05-14 11:18:18,042] {docker.py:276} INFO - 21/05/14 14:18:18 INFO ShuffleBlockFetcherIterator: Getting 5 (44.7 KiB) non-empty blocks including 5 (44.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:18,051] {docker.py:276} INFO - 21/05/14 14:18:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:18,052] {docker.py:276} INFO - 21/05/14 14:18:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436709428392759094252_0004_m_000045_333, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436709428392759094252_0004_m_000045_333}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436709428392759094252_0004}; taskId=attempt_202105141416436709428392759094252_0004_m_000045_333, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b86791e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416436709428392759094252_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436709428392759094252_0004_m_000045_333
[2021-05-14 11:18:18,057] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Task committer attempt_202105141416436709428392759094252_0004_m_000045_333: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436709428392759094252_0004_m_000045_333 : duration 0:00.005s
[2021-05-14 11:18:18,112] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Starting: Task committer attempt_20210514141643640141099082552671_0004_m_000041_329: needsTaskCommit() Task attempt_20210514141643640141099082552671_0004_m_000041_329
[2021-05-14 11:18:18,113] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Task committer attempt_20210514141643640141099082552671_0004_m_000041_329: needsTaskCommit() Task attempt_20210514141643640141099082552671_0004_m_000041_329: duration 0:00.002s
21/05/14 14:18:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643640141099082552671_0004_m_000041_329
[2021-05-14 11:18:18,115] {docker.py:276} INFO - 21/05/14 14:18:18 INFO Executor: Finished task 41.0 in stage 4.0 (TID 329). 5149 bytes result sent to driver
[2021-05-14 11:18:18,116] {docker.py:276} INFO - 21/05/14 14:18:18 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 334) (0d1d3e01326c, executor driver, partition 46, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:18,117] {docker.py:276} INFO - 21/05/14 14:18:18 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 329) in 6976 ms on 0d1d3e01326c (executor driver) (43/200)
[2021-05-14 11:18:18,118] {docker.py:276} INFO - 21/05/14 14:18:18 INFO Executor: Running task 46.0 in stage 4.0 (TID 334)
[2021-05-14 11:18:18,127] {docker.py:276} INFO - 21/05/14 14:18:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:18,137] {docker.py:276} INFO - 21/05/14 14:18:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431363701526253555367_0004_m_000046_334, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431363701526253555367_0004_m_000046_334}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431363701526253555367_0004}; taskId=attempt_202105141416431363701526253555367_0004_m_000046_334, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@33b04cf3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416431363701526253555367_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431363701526253555367_0004_m_000046_334
[2021-05-14 11:18:18,142] {docker.py:276} INFO - 21/05/14 14:18:18 INFO StagingCommitter: Task committer attempt_202105141416431363701526253555367_0004_m_000046_334: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431363701526253555367_0004_m_000046_334 : duration 0:00.005s
[2021-05-14 11:18:20,694] {docker.py:276} INFO - 21/05/14 14:18:20 INFO StagingCommitter: Starting: Task committer attempt_20210514141643892113597937973879_0004_m_000043_331: needsTaskCommit() Task attempt_20210514141643892113597937973879_0004_m_000043_331
[2021-05-14 11:18:20,695] {docker.py:276} INFO - 21/05/14 14:18:20 INFO StagingCommitter: Task committer attempt_20210514141643892113597937973879_0004_m_000043_331: needsTaskCommit() Task attempt_20210514141643892113597937973879_0004_m_000043_331: duration 0:00.003s
21/05/14 14:18:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643892113597937973879_0004_m_000043_331
[2021-05-14 11:18:20,698] {docker.py:276} INFO - 21/05/14 14:18:20 INFO Executor: Finished task 43.0 in stage 4.0 (TID 331). 5149 bytes result sent to driver
[2021-05-14 11:18:20,699] {docker.py:276} INFO - 21/05/14 14:18:20 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 335) (0d1d3e01326c, executor driver, partition 47, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:20,699] {docker.py:276} INFO - 21/05/14 14:18:20 INFO Executor: Running task 47.0 in stage 4.0 (TID 335)
[2021-05-14 11:18:20,700] {docker.py:276} INFO - 21/05/14 14:18:20 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 331) in 7223 ms on 0d1d3e01326c (executor driver) (44/200)
[2021-05-14 11:18:20,709] {docker.py:276} INFO - 21/05/14 14:18:20 INFO ShuffleBlockFetcherIterator: Getting 5 (40.5 KiB) non-empty blocks including 5 (40.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:20,710] {docker.py:276} INFO - 21/05/14 14:18:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:20,720] {docker.py:276} INFO - 21/05/14 14:18:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431456140309581794951_0004_m_000047_335, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431456140309581794951_0004_m_000047_335}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431456140309581794951_0004}; taskId=attempt_202105141416431456140309581794951_0004_m_000047_335, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@487f1b66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:20,720] {docker.py:276} INFO - 21/05/14 14:18:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416431456140309581794951_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431456140309581794951_0004_m_000047_335
[2021-05-14 11:18:20,724] {docker.py:276} INFO - 21/05/14 14:18:20 INFO StagingCommitter: Task committer attempt_202105141416431456140309581794951_0004_m_000047_335: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431456140309581794951_0004_m_000047_335 : duration 0:00.004s
[2021-05-14 11:18:23,163] {docker.py:276} INFO - 21/05/14 14:18:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416434150581836546133475_0004_m_000044_332: needsTaskCommit() Task attempt_202105141416434150581836546133475_0004_m_000044_332
[2021-05-14 11:18:23,164] {docker.py:276} INFO - 21/05/14 14:18:23 INFO StagingCommitter: Task committer attempt_202105141416434150581836546133475_0004_m_000044_332: needsTaskCommit() Task attempt_202105141416434150581836546133475_0004_m_000044_332: duration 0:00.001s
21/05/14 14:18:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434150581836546133475_0004_m_000044_332
[2021-05-14 11:18:23,164] {docker.py:276} INFO - 21/05/14 14:18:23 INFO Executor: Finished task 44.0 in stage 4.0 (TID 332). 5149 bytes result sent to driver
[2021-05-14 11:18:23,165] {docker.py:276} INFO - 21/05/14 14:18:23 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 336) (0d1d3e01326c, executor driver, partition 48, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:23,166] {docker.py:276} INFO - 21/05/14 14:18:23 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 332) in 7050 ms on 0d1d3e01326c (executor driver) (45/200)
[2021-05-14 11:18:23,167] {docker.py:276} INFO - 21/05/14 14:18:23 INFO Executor: Running task 48.0 in stage 4.0 (TID 336)
[2021-05-14 11:18:23,176] {docker.py:276} INFO - 21/05/14 14:18:23 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:23,186] {docker.py:276} INFO - 21/05/14 14:18:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:23,186] {docker.py:276} INFO - 21/05/14 14:18:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437522082321108132780_0004_m_000048_336, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437522082321108132780_0004_m_000048_336}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437522082321108132780_0004}; taskId=attempt_202105141416437522082321108132780_0004_m_000048_336, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5fd02b59}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416437522082321108132780_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437522082321108132780_0004_m_000048_336
[2021-05-14 11:18:23,191] {docker.py:276} INFO - 21/05/14 14:18:23 INFO StagingCommitter: Task committer attempt_202105141416437522082321108132780_0004_m_000048_336: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437522082321108132780_0004_m_000048_336 : duration 0:00.004s
[2021-05-14 11:18:25,101] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416431363701526253555367_0004_m_000046_334: needsTaskCommit() Task attempt_202105141416431363701526253555367_0004_m_000046_334
[2021-05-14 11:18:25,102] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Task committer attempt_202105141416431363701526253555367_0004_m_000046_334: needsTaskCommit() Task attempt_202105141416431363701526253555367_0004_m_000046_334: duration 0:00.003s
21/05/14 14:18:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431363701526253555367_0004_m_000046_334
[2021-05-14 11:18:25,106] {docker.py:276} INFO - 21/05/14 14:18:25 INFO Executor: Finished task 46.0 in stage 4.0 (TID 334). 5106 bytes result sent to driver
[2021-05-14 11:18:25,107] {docker.py:276} INFO - 21/05/14 14:18:25 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 337) (0d1d3e01326c, executor driver, partition 49, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:25,107] {docker.py:276} INFO - 21/05/14 14:18:25 INFO Executor: Running task 49.0 in stage 4.0 (TID 337)
[2021-05-14 11:18:25,108] {docker.py:276} INFO - 21/05/14 14:18:25 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 334) in 6999 ms on 0d1d3e01326c (executor driver) (46/200)
[2021-05-14 11:18:25,117] {docker.py:276} INFO - 21/05/14 14:18:25 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:25,126] {docker.py:276} INFO - 21/05/14 14:18:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432953271249062255641_0004_m_000049_337, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432953271249062255641_0004_m_000049_337}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432953271249062255641_0004}; taskId=attempt_202105141416432953271249062255641_0004_m_000049_337, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@ed4fcd0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:25,127] {docker.py:276} INFO - 21/05/14 14:18:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416432953271249062255641_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432953271249062255641_0004_m_000049_337
[2021-05-14 11:18:25,131] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416436709428392759094252_0004_m_000045_333: needsTaskCommit() Task attempt_202105141416436709428392759094252_0004_m_000045_333
21/05/14 14:18:25 INFO StagingCommitter: Task committer attempt_202105141416432953271249062255641_0004_m_000049_337: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432953271249062255641_0004_m_000049_337 : duration 0:00.003s
[2021-05-14 11:18:25,131] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Task committer attempt_202105141416436709428392759094252_0004_m_000045_333: needsTaskCommit() Task attempt_202105141416436709428392759094252_0004_m_000045_333: duration 0:00.001s
[2021-05-14 11:18:25,131] {docker.py:276} INFO - 21/05/14 14:18:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436709428392759094252_0004_m_000045_333
[2021-05-14 11:18:25,133] {docker.py:276} INFO - 21/05/14 14:18:25 INFO Executor: Finished task 45.0 in stage 4.0 (TID 333). 5106 bytes result sent to driver
[2021-05-14 11:18:25,134] {docker.py:276} INFO - 21/05/14 14:18:25 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 338) (0d1d3e01326c, executor driver, partition 50, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:25,134] {docker.py:276} INFO - 21/05/14 14:18:25 INFO Executor: Running task 50.0 in stage 4.0 (TID 338)
21/05/14 14:18:25 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 333) in 7113 ms on 0d1d3e01326c (executor driver) (47/200)
[2021-05-14 11:18:25,142] {docker.py:276} INFO - 21/05/14 14:18:25 INFO ShuffleBlockFetcherIterator: Getting 5 (41.2 KiB) non-empty blocks including 5 (41.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:25,151] {docker.py:276} INFO - 21/05/14 14:18:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435561461146750792086_0004_m_000050_338, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435561461146750792086_0004_m_000050_338}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435561461146750792086_0004}; taskId=attempt_202105141416435561461146750792086_0004_m_000050_338, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@215fef65}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:25,151] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416435561461146750792086_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435561461146750792086_0004_m_000050_338
[2021-05-14 11:18:25,156] {docker.py:276} INFO - 21/05/14 14:18:25 INFO StagingCommitter: Task committer attempt_202105141416435561461146750792086_0004_m_000050_338: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435561461146750792086_0004_m_000050_338 : duration 0:00.006s
[2021-05-14 11:18:27,712] {docker.py:276} INFO - 21/05/14 14:18:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416431456140309581794951_0004_m_000047_335: needsTaskCommit() Task attempt_202105141416431456140309581794951_0004_m_000047_335
[2021-05-14 11:18:27,713] {docker.py:276} INFO - 21/05/14 14:18:27 INFO StagingCommitter: Task committer attempt_202105141416431456140309581794951_0004_m_000047_335: needsTaskCommit() Task attempt_202105141416431456140309581794951_0004_m_000047_335: duration 0:00.004s
21/05/14 14:18:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431456140309581794951_0004_m_000047_335
[2021-05-14 11:18:27,715] {docker.py:276} INFO - 21/05/14 14:18:27 INFO Executor: Finished task 47.0 in stage 4.0 (TID 335). 5149 bytes result sent to driver
[2021-05-14 11:18:27,718] {docker.py:276} INFO - 21/05/14 14:18:27 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 339) (0d1d3e01326c, executor driver, partition 51, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:27,720] {docker.py:276} INFO - 21/05/14 14:18:27 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 335) in 7030 ms on 0d1d3e01326c (executor driver) (48/200)
21/05/14 14:18:27 INFO Executor: Running task 51.0 in stage 4.0 (TID 339)
[2021-05-14 11:18:27,734] {docker.py:276} INFO - 21/05/14 14:18:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:27,734] {docker.py:276} INFO - 21/05/14 14:18:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:18:27,750] {docker.py:276} INFO - 21/05/14 14:18:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:27,750] {docker.py:276} INFO - 21/05/14 14:18:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434993234236992156849_0004_m_000051_339, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434993234236992156849_0004_m_000051_339}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434993234236992156849_0004}; taskId=attempt_202105141416434993234236992156849_0004_m_000051_339, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19e8e7aa}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416434993234236992156849_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434993234236992156849_0004_m_000051_339
[2021-05-14 11:18:27,754] {docker.py:276} INFO - 21/05/14 14:18:27 INFO StagingCommitter: Task committer attempt_202105141416434993234236992156849_0004_m_000051_339: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434993234236992156849_0004_m_000051_339 : duration 0:00.004s
[2021-05-14 11:18:29,546] {docker.py:276} INFO - 21/05/14 14:18:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416437522082321108132780_0004_m_000048_336: needsTaskCommit() Task attempt_202105141416437522082321108132780_0004_m_000048_336
[2021-05-14 11:18:29,546] {docker.py:276} INFO - 21/05/14 14:18:29 INFO StagingCommitter: Task committer attempt_202105141416437522082321108132780_0004_m_000048_336: needsTaskCommit() Task attempt_202105141416437522082321108132780_0004_m_000048_336: duration 0:00.001s
21/05/14 14:18:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437522082321108132780_0004_m_000048_336
[2021-05-14 11:18:29,547] {docker.py:276} INFO - 21/05/14 14:18:29 INFO Executor: Finished task 48.0 in stage 4.0 (TID 336). 5149 bytes result sent to driver
[2021-05-14 11:18:29,548] {docker.py:276} INFO - 21/05/14 14:18:29 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 340) (0d1d3e01326c, executor driver, partition 52, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:29,549] {docker.py:276} INFO - 21/05/14 14:18:29 INFO Executor: Running task 52.0 in stage 4.0 (TID 340)
[2021-05-14 11:18:29,549] {docker.py:276} INFO - 21/05/14 14:18:29 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 336) in 6391 ms on 0d1d3e01326c (executor driver) (49/200)
[2021-05-14 11:18:29,558] {docker.py:276} INFO - 21/05/14 14:18:29 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:29,558] {docker.py:276} INFO - 21/05/14 14:18:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:29,568] {docker.py:276} INFO - 21/05/14 14:18:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:29,569] {docker.py:276} INFO - 21/05/14 14:18:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432384325849423828185_0004_m_000052_340, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432384325849423828185_0004_m_000052_340}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432384325849423828185_0004}; taskId=attempt_202105141416432384325849423828185_0004_m_000052_340, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5c393f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:29,569] {docker.py:276} INFO - 21/05/14 14:18:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416432384325849423828185_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432384325849423828185_0004_m_000052_340
[2021-05-14 11:18:29,572] {docker.py:276} INFO - 21/05/14 14:18:29 INFO StagingCommitter: Task committer attempt_202105141416432384325849423828185_0004_m_000052_340: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432384325849423828185_0004_m_000052_340 : duration 0:00.004s
[2021-05-14 11:18:32,000] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416435561461146750792086_0004_m_000050_338: needsTaskCommit() Task attempt_202105141416435561461146750792086_0004_m_000050_338
[2021-05-14 11:18:32,002] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Task committer attempt_202105141416435561461146750792086_0004_m_000050_338: needsTaskCommit() Task attempt_202105141416435561461146750792086_0004_m_000050_338: duration 0:00.004s
21/05/14 14:18:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435561461146750792086_0004_m_000050_338
[2021-05-14 11:18:32,003] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416432953271249062255641_0004_m_000049_337: needsTaskCommit() Task attempt_202105141416432953271249062255641_0004_m_000049_337
[2021-05-14 11:18:32,004] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Task committer attempt_202105141416432953271249062255641_0004_m_000049_337: needsTaskCommit() Task attempt_202105141416432953271249062255641_0004_m_000049_337: duration 0:00.001s
[2021-05-14 11:18:32,005] {docker.py:276} INFO - 21/05/14 14:18:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432953271249062255641_0004_m_000049_337
[2021-05-14 11:18:32,005] {docker.py:276} INFO - 21/05/14 14:18:32 INFO Executor: Finished task 50.0 in stage 4.0 (TID 338). 5192 bytes result sent to driver
[2021-05-14 11:18:32,008] {docker.py:276} INFO - 21/05/14 14:18:32 INFO Executor: Finished task 49.0 in stage 4.0 (TID 337). 5149 bytes result sent to driver
21/05/14 14:18:32 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 341) (0d1d3e01326c, executor driver, partition 53, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:32,009] {docker.py:276} INFO - 21/05/14 14:18:32 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 342) (0d1d3e01326c, executor driver, partition 54, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:32,010] {docker.py:276} INFO - 21/05/14 14:18:32 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 338) in 6884 ms on 0d1d3e01326c (executor driver) (50/200)
[2021-05-14 11:18:32,010] {docker.py:276} INFO - 21/05/14 14:18:32 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 337) in 6913 ms on 0d1d3e01326c (executor driver) (51/200)
[2021-05-14 11:18:32,012] {docker.py:276} INFO - 21/05/14 14:18:32 INFO Executor: Running task 53.0 in stage 4.0 (TID 341)
[2021-05-14 11:18:32,013] {docker.py:276} INFO - 21/05/14 14:18:32 INFO Executor: Running task 54.0 in stage 4.0 (TID 342)
[2021-05-14 11:18:32,022] {docker.py:276} INFO - 21/05/14 14:18:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:32,022] {docker.py:276} INFO - 21/05/14 14:18:32 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:32,023] {docker.py:276} INFO - 21/05/14 14:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:32,031] {docker.py:276} INFO - 21/05/14 14:18:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:18:32,031] {docker.py:276} INFO - 21/05/14 14:18:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:32,031] {docker.py:276} INFO - 21/05/14 14:18:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:32,032] {docker.py:276} INFO - 21/05/14 14:18:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431166369499787262430_0004_m_000053_341, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431166369499787262430_0004_m_000053_341}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431166369499787262430_0004}; taskId=attempt_202105141416431166369499787262430_0004_m_000053_341, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45a90b8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416431166369499787262430_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431166369499787262430_0004_m_000053_341
[2021-05-14 11:18:32,033] {docker.py:276} INFO - 21/05/14 14:18:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:32,034] {docker.py:276} INFO - 21/05/14 14:18:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432710697522112087096_0004_m_000054_342, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432710697522112087096_0004_m_000054_342}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432710697522112087096_0004}; taskId=attempt_202105141416432710697522112087096_0004_m_000054_342, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@21f3b293}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:32,034] {docker.py:276} INFO - 21/05/14 14:18:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:32,037] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Task committer attempt_202105141416431166369499787262430_0004_m_000053_341: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431166369499787262430_0004_m_000053_341 : duration 0:00.005s
[2021-05-14 11:18:32,037] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416432710697522112087096_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432710697522112087096_0004_m_000054_342
[2021-05-14 11:18:32,045] {docker.py:276} INFO - 21/05/14 14:18:32 INFO StagingCommitter: Task committer attempt_202105141416432710697522112087096_0004_m_000054_342: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432710697522112087096_0004_m_000054_342 : duration 0:00.010s
[2021-05-14 11:18:34,781] {docker.py:276} INFO - 21/05/14 14:18:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416434993234236992156849_0004_m_000051_339: needsTaskCommit() Task attempt_202105141416434993234236992156849_0004_m_000051_339
[2021-05-14 11:18:34,782] {docker.py:276} INFO - 21/05/14 14:18:34 INFO StagingCommitter: Task committer attempt_202105141416434993234236992156849_0004_m_000051_339: needsTaskCommit() Task attempt_202105141416434993234236992156849_0004_m_000051_339: duration 0:00.001s
21/05/14 14:18:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434993234236992156849_0004_m_000051_339
[2021-05-14 11:18:34,784] {docker.py:276} INFO - 21/05/14 14:18:34 INFO Executor: Finished task 51.0 in stage 4.0 (TID 339). 5106 bytes result sent to driver
[2021-05-14 11:18:34,786] {docker.py:276} INFO - 21/05/14 14:18:34 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 343) (0d1d3e01326c, executor driver, partition 55, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:34,787] {docker.py:276} INFO - 21/05/14 14:18:34 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 339) in 7079 ms on 0d1d3e01326c (executor driver) (52/200)
[2021-05-14 11:18:34,788] {docker.py:276} INFO - 21/05/14 14:18:34 INFO Executor: Running task 55.0 in stage 4.0 (TID 343)
[2021-05-14 11:18:34,797] {docker.py:276} INFO - 21/05/14 14:18:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:34,807] {docker.py:276} INFO - 21/05/14 14:18:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431281320052618897935_0004_m_000055_343, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431281320052618897935_0004_m_000055_343}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431281320052618897935_0004}; taskId=attempt_202105141416431281320052618897935_0004_m_000055_343, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@353153af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416431281320052618897935_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431281320052618897935_0004_m_000055_343
[2021-05-14 11:18:34,810] {docker.py:276} INFO - 21/05/14 14:18:34 INFO StagingCommitter: Task committer attempt_202105141416431281320052618897935_0004_m_000055_343: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431281320052618897935_0004_m_000055_343 : duration 0:00.004s
[2021-05-14 11:18:36,013] {docker.py:276} INFO - 21/05/14 14:18:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416432384325849423828185_0004_m_000052_340: needsTaskCommit() Task attempt_202105141416432384325849423828185_0004_m_000052_340
21/05/14 14:18:36 INFO StagingCommitter: Task committer attempt_202105141416432384325849423828185_0004_m_000052_340: needsTaskCommit() Task attempt_202105141416432384325849423828185_0004_m_000052_340: duration 0:00.003s
21/05/14 14:18:36 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432384325849423828185_0004_m_000052_340
[2021-05-14 11:18:36,016] {docker.py:276} INFO - 21/05/14 14:18:36 INFO Executor: Finished task 52.0 in stage 4.0 (TID 340). 5149 bytes result sent to driver
[2021-05-14 11:18:36,017] {docker.py:276} INFO - 21/05/14 14:18:36 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 344) (0d1d3e01326c, executor driver, partition 56, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:36,018] {docker.py:276} INFO - 21/05/14 14:18:36 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 340) in 6477 ms on 0d1d3e01326c (executor driver) (53/200)
[2021-05-14 11:18:36,019] {docker.py:276} INFO - 21/05/14 14:18:36 INFO Executor: Running task 56.0 in stage 4.0 (TID 344)
[2021-05-14 11:18:36,028] {docker.py:276} INFO - 21/05/14 14:18:36 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:36,037] {docker.py:276} INFO - 21/05/14 14:18:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:36,038] {docker.py:276} INFO - 21/05/14 14:18:36 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:36,038] {docker.py:276} INFO - 21/05/14 14:18:36 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437631047691629848704_0004_m_000056_344, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437631047691629848704_0004_m_000056_344}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437631047691629848704_0004}; taskId=attempt_202105141416437631047691629848704_0004_m_000056_344, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3350edd8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:36 INFO StagingCommitter: Starting: Task committer attempt_202105141416437631047691629848704_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437631047691629848704_0004_m_000056_344
[2021-05-14 11:18:36,042] {docker.py:276} INFO - 21/05/14 14:18:36 INFO StagingCommitter: Task committer attempt_202105141416437631047691629848704_0004_m_000056_344: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437631047691629848704_0004_m_000056_344 : duration 0:00.003s
[2021-05-14 11:18:38,838] {docker.py:276} INFO - 21/05/14 14:18:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416431166369499787262430_0004_m_000053_341: needsTaskCommit() Task attempt_202105141416431166369499787262430_0004_m_000053_341
[2021-05-14 11:18:38,839] {docker.py:276} INFO - 21/05/14 14:18:38 INFO StagingCommitter: Task committer attempt_202105141416431166369499787262430_0004_m_000053_341: needsTaskCommit() Task attempt_202105141416431166369499787262430_0004_m_000053_341: duration 0:00.003s
21/05/14 14:18:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431166369499787262430_0004_m_000053_341
[2021-05-14 11:18:38,842] {docker.py:276} INFO - 21/05/14 14:18:38 INFO Executor: Finished task 53.0 in stage 4.0 (TID 341). 5149 bytes result sent to driver
[2021-05-14 11:18:38,843] {docker.py:276} INFO - 21/05/14 14:18:38 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 345) (0d1d3e01326c, executor driver, partition 57, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:38,844] {docker.py:276} INFO - 21/05/14 14:18:38 INFO Executor: Running task 57.0 in stage 4.0 (TID 345)
[2021-05-14 11:18:38,845] {docker.py:276} INFO - 21/05/14 14:18:38 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 341) in 6845 ms on 0d1d3e01326c (executor driver) (54/200)
[2021-05-14 11:18:38,854] {docker.py:276} INFO - 21/05/14 14:18:38 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:38,863] {docker.py:276} INFO - 21/05/14 14:18:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431824756346434657082_0004_m_000057_345, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431824756346434657082_0004_m_000057_345}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431824756346434657082_0004}; taskId=attempt_202105141416431824756346434657082_0004_m_000057_345, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@58e0cc9b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416431824756346434657082_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431824756346434657082_0004_m_000057_345
[2021-05-14 11:18:38,867] {docker.py:276} INFO - 21/05/14 14:18:38 INFO StagingCommitter: Task committer attempt_202105141416431824756346434657082_0004_m_000057_345: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431824756346434657082_0004_m_000057_345 : duration 0:00.004s
[2021-05-14 11:18:39,096] {docker.py:276} INFO - 21/05/14 14:18:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416432710697522112087096_0004_m_000054_342: needsTaskCommit() Task attempt_202105141416432710697522112087096_0004_m_000054_342
[2021-05-14 11:18:39,097] {docker.py:276} INFO - 21/05/14 14:18:39 INFO StagingCommitter: Task committer attempt_202105141416432710697522112087096_0004_m_000054_342: needsTaskCommit() Task attempt_202105141416432710697522112087096_0004_m_000054_342: duration 0:00.003s
21/05/14 14:18:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432710697522112087096_0004_m_000054_342
[2021-05-14 11:18:39,100] {docker.py:276} INFO - 21/05/14 14:18:39 INFO Executor: Finished task 54.0 in stage 4.0 (TID 342). 5149 bytes result sent to driver
[2021-05-14 11:18:39,102] {docker.py:276} INFO - 21/05/14 14:18:39 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 346) (0d1d3e01326c, executor driver, partition 58, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:39,103] {docker.py:276} INFO - 21/05/14 14:18:39 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 342) in 7102 ms on 0d1d3e01326c (executor driver) (55/200)
[2021-05-14 11:18:39,104] {docker.py:276} INFO - 21/05/14 14:18:39 INFO Executor: Running task 58.0 in stage 4.0 (TID 346)
[2021-05-14 11:18:39,114] {docker.py:276} INFO - 21/05/14 14:18:39 INFO ShuffleBlockFetcherIterator: Getting 5 (45.3 KiB) non-empty blocks including 5 (45.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:39,123] {docker.py:276} INFO - 21/05/14 14:18:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:39,124] {docker.py:276} INFO - 21/05/14 14:18:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437518618066341111755_0004_m_000058_346, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437518618066341111755_0004_m_000058_346}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437518618066341111755_0004}; taskId=attempt_202105141416437518618066341111755_0004_m_000058_346, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1794e47e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:39,124] {docker.py:276} INFO - 21/05/14 14:18:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:39,124] {docker.py:276} INFO - 21/05/14 14:18:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416437518618066341111755_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437518618066341111755_0004_m_000058_346
[2021-05-14 11:18:39,128] {docker.py:276} INFO - 21/05/14 14:18:39 INFO StagingCommitter: Task committer attempt_202105141416437518618066341111755_0004_m_000058_346: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437518618066341111755_0004_m_000058_346 : duration 0:00.005s
[2021-05-14 11:18:41,402] {docker.py:276} INFO - 21/05/14 14:18:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416431281320052618897935_0004_m_000055_343: needsTaskCommit() Task attempt_202105141416431281320052618897935_0004_m_000055_343
[2021-05-14 11:18:41,403] {docker.py:276} INFO - 21/05/14 14:18:41 INFO StagingCommitter: Task committer attempt_202105141416431281320052618897935_0004_m_000055_343: needsTaskCommit() Task attempt_202105141416431281320052618897935_0004_m_000055_343: duration 0:00.005s
21/05/14 14:18:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431281320052618897935_0004_m_000055_343
[2021-05-14 11:18:41,408] {docker.py:276} INFO - 21/05/14 14:18:41 INFO Executor: Finished task 55.0 in stage 4.0 (TID 343). 5149 bytes result sent to driver
21/05/14 14:18:41 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 347) (0d1d3e01326c, executor driver, partition 59, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:41,409] {docker.py:276} INFO - 21/05/14 14:18:41 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 343) in 6596 ms on 0d1d3e01326c (executor driver) (56/200)
[2021-05-14 11:18:41,411] {docker.py:276} INFO - 21/05/14 14:18:41 INFO Executor: Running task 59.0 in stage 4.0 (TID 347)
[2021-05-14 11:18:41,421] {docker.py:276} INFO - 21/05/14 14:18:41 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:41,421] {docker.py:276} INFO - 21/05/14 14:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:41,430] {docker.py:276} INFO - 21/05/14 14:18:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:41,431] {docker.py:276} INFO - 21/05/14 14:18:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434831399707893935713_0004_m_000059_347, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434831399707893935713_0004_m_000059_347}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434831399707893935713_0004}; taskId=attempt_202105141416434831399707893935713_0004_m_000059_347, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5dd70d34}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:41,431] {docker.py:276} INFO - 21/05/14 14:18:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:41,431] {docker.py:276} INFO - 21/05/14 14:18:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416434831399707893935713_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434831399707893935713_0004_m_000059_347
[2021-05-14 11:18:41,435] {docker.py:276} INFO - 21/05/14 14:18:41 INFO StagingCommitter: Task committer attempt_202105141416434831399707893935713_0004_m_000059_347: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434831399707893935713_0004_m_000059_347 : duration 0:00.005s
[2021-05-14 11:18:42,794] {docker.py:276} INFO - 21/05/14 14:18:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416437631047691629848704_0004_m_000056_344: needsTaskCommit() Task attempt_202105141416437631047691629848704_0004_m_000056_344
21/05/14 14:18:42 INFO StagingCommitter: Task committer attempt_202105141416437631047691629848704_0004_m_000056_344: needsTaskCommit() Task attempt_202105141416437631047691629848704_0004_m_000056_344: duration 0:00.003s
21/05/14 14:18:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437631047691629848704_0004_m_000056_344
[2021-05-14 11:18:42,796] {docker.py:276} INFO - 21/05/14 14:18:42 INFO Executor: Finished task 56.0 in stage 4.0 (TID 344). 5106 bytes result sent to driver
[2021-05-14 11:18:42,797] {docker.py:276} INFO - 21/05/14 14:18:42 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 348) (0d1d3e01326c, executor driver, partition 60, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:42,798] {docker.py:276} INFO - 21/05/14 14:18:42 INFO Executor: Running task 60.0 in stage 4.0 (TID 348)
[2021-05-14 11:18:42,799] {docker.py:276} INFO - 21/05/14 14:18:42 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 344) in 6756 ms on 0d1d3e01326c (executor driver) (57/200)
[2021-05-14 11:18:42,809] {docker.py:276} INFO - 21/05/14 14:18:42 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:42,818] {docker.py:276} INFO - 21/05/14 14:18:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438220224022254940163_0004_m_000060_348, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438220224022254940163_0004_m_000060_348}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438220224022254940163_0004}; taskId=attempt_202105141416438220224022254940163_0004_m_000060_348, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b74a71a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416438220224022254940163_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438220224022254940163_0004_m_000060_348
[2021-05-14 11:18:42,821] {docker.py:276} INFO - 21/05/14 14:18:42 INFO StagingCommitter: Task committer attempt_202105141416438220224022254940163_0004_m_000060_348: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438220224022254940163_0004_m_000060_348 : duration 0:00.003s
[2021-05-14 11:18:46,267] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416431824756346434657082_0004_m_000057_345: needsTaskCommit() Task attempt_202105141416431824756346434657082_0004_m_000057_345
[2021-05-14 11:18:46,268] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Task committer attempt_202105141416431824756346434657082_0004_m_000057_345: needsTaskCommit() Task attempt_202105141416431824756346434657082_0004_m_000057_345: duration 0:00.003s
[2021-05-14 11:18:46,268] {docker.py:276} INFO - 21/05/14 14:18:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431824756346434657082_0004_m_000057_345
[2021-05-14 11:18:46,270] {docker.py:276} INFO - 21/05/14 14:18:46 INFO Executor: Finished task 57.0 in stage 4.0 (TID 345). 5149 bytes result sent to driver
[2021-05-14 11:18:46,272] {docker.py:276} INFO - 21/05/14 14:18:46 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 349) (0d1d3e01326c, executor driver, partition 61, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:46,273] {docker.py:276} INFO - 21/05/14 14:18:46 INFO Executor: Running task 61.0 in stage 4.0 (TID 349)
[2021-05-14 11:18:46,273] {docker.py:276} INFO - 21/05/14 14:18:46 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 345) in 7405 ms on 0d1d3e01326c (executor driver) (58/200)
[2021-05-14 11:18:46,282] {docker.py:276} INFO - 21/05/14 14:18:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.8 KiB) non-empty blocks including 5 (42.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:46,291] {docker.py:276} INFO - 21/05/14 14:18:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434557387485573301326_0004_m_000061_349, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434557387485573301326_0004_m_000061_349}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434557387485573301326_0004}; taskId=attempt_202105141416434557387485573301326_0004_m_000061_349, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5f48354b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:46,292] {docker.py:276} INFO - 21/05/14 14:18:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416434557387485573301326_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434557387485573301326_0004_m_000061_349
[2021-05-14 11:18:46,296] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Task committer attempt_202105141416434557387485573301326_0004_m_000061_349: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434557387485573301326_0004_m_000061_349 : duration 0:00.004s
[2021-05-14 11:18:46,347] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416437518618066341111755_0004_m_000058_346: needsTaskCommit() Task attempt_202105141416437518618066341111755_0004_m_000058_346
[2021-05-14 11:18:46,347] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Task committer attempt_202105141416437518618066341111755_0004_m_000058_346: needsTaskCommit() Task attempt_202105141416437518618066341111755_0004_m_000058_346: duration 0:00.001s
21/05/14 14:18:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437518618066341111755_0004_m_000058_346
[2021-05-14 11:18:46,349] {docker.py:276} INFO - 21/05/14 14:18:46 INFO Executor: Finished task 58.0 in stage 4.0 (TID 346). 5149 bytes result sent to driver
[2021-05-14 11:18:46,350] {docker.py:276} INFO - 21/05/14 14:18:46 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 350) (0d1d3e01326c, executor driver, partition 62, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:46,350] {docker.py:276} INFO - 21/05/14 14:18:46 INFO Executor: Running task 62.0 in stage 4.0 (TID 350)
[2021-05-14 11:18:46,351] {docker.py:276} INFO - 21/05/14 14:18:46 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 346) in 7224 ms on 0d1d3e01326c (executor driver) (59/200)
[2021-05-14 11:18:46,358] {docker.py:276} INFO - 21/05/14 14:18:46 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:46,367] {docker.py:276} INFO - 21/05/14 14:18:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:46,368] {docker.py:276} INFO - 21/05/14 14:18:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:46,368] {docker.py:276} INFO - 21/05/14 14:18:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438851468171935314341_0004_m_000062_350, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438851468171935314341_0004_m_000062_350}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438851468171935314341_0004}; taskId=attempt_202105141416438851468171935314341_0004_m_000062_350, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@60930ed7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:46,368] {docker.py:276} INFO - 21/05/14 14:18:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416438851468171935314341_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438851468171935314341_0004_m_000062_350
[2021-05-14 11:18:46,372] {docker.py:276} INFO - 21/05/14 14:18:46 INFO StagingCommitter: Task committer attempt_202105141416438851468171935314341_0004_m_000062_350: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438851468171935314341_0004_m_000062_350 : duration 0:00.003s
[2021-05-14 11:18:48,232] {docker.py:276} INFO - 21/05/14 14:18:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416434831399707893935713_0004_m_000059_347: needsTaskCommit() Task attempt_202105141416434831399707893935713_0004_m_000059_347
[2021-05-14 11:18:48,233] {docker.py:276} INFO - 21/05/14 14:18:48 INFO StagingCommitter: Task committer attempt_202105141416434831399707893935713_0004_m_000059_347: needsTaskCommit() Task attempt_202105141416434831399707893935713_0004_m_000059_347: duration 0:00.003s
21/05/14 14:18:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434831399707893935713_0004_m_000059_347
[2021-05-14 11:18:48,235] {docker.py:276} INFO - 21/05/14 14:18:48 INFO Executor: Finished task 59.0 in stage 4.0 (TID 347). 5149 bytes result sent to driver
[2021-05-14 11:18:48,237] {docker.py:276} INFO - 21/05/14 14:18:48 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 351) (0d1d3e01326c, executor driver, partition 63, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:48,238] {docker.py:276} INFO - 21/05/14 14:18:48 INFO Executor: Running task 63.0 in stage 4.0 (TID 351)
21/05/14 14:18:48 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 347) in 6837 ms on 0d1d3e01326c (executor driver) (60/200)
[2021-05-14 11:18:48,248] {docker.py:276} INFO - 21/05/14 14:18:48 INFO ShuffleBlockFetcherIterator: Getting 5 (46.2 KiB) non-empty blocks including 5 (46.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:48,257] {docker.py:276} INFO - 21/05/14 14:18:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:48,258] {docker.py:276} INFO - 21/05/14 14:18:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432890477780732407049_0004_m_000063_351, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432890477780732407049_0004_m_000063_351}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432890477780732407049_0004}; taskId=attempt_202105141416432890477780732407049_0004_m_000063_351, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c0f7fa7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416432890477780732407049_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432890477780732407049_0004_m_000063_351
[2021-05-14 11:18:48,262] {docker.py:276} INFO - 21/05/14 14:18:48 INFO StagingCommitter: Task committer attempt_202105141416432890477780732407049_0004_m_000063_351: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432890477780732407049_0004_m_000063_351 : duration 0:00.005s
[2021-05-14 11:18:49,671] {docker.py:276} INFO - 21/05/14 14:18:49 INFO StagingCommitter: Starting: Task committer attempt_202105141416438220224022254940163_0004_m_000060_348: needsTaskCommit() Task attempt_202105141416438220224022254940163_0004_m_000060_348
[2021-05-14 11:18:49,672] {docker.py:276} INFO - 21/05/14 14:18:49 INFO StagingCommitter: Task committer attempt_202105141416438220224022254940163_0004_m_000060_348: needsTaskCommit() Task attempt_202105141416438220224022254940163_0004_m_000060_348: duration 0:00.003s
21/05/14 14:18:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438220224022254940163_0004_m_000060_348
[2021-05-14 11:18:49,674] {docker.py:276} INFO - 21/05/14 14:18:49 INFO Executor: Finished task 60.0 in stage 4.0 (TID 348). 5149 bytes result sent to driver
[2021-05-14 11:18:49,676] {docker.py:276} INFO - 21/05/14 14:18:49 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 352) (0d1d3e01326c, executor driver, partition 64, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:49,677] {docker.py:276} INFO - 21/05/14 14:18:49 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 348) in 6888 ms on 0d1d3e01326c (executor driver) (61/200)
21/05/14 14:18:49 INFO Executor: Running task 64.0 in stage 4.0 (TID 352)
[2021-05-14 11:18:49,688] {docker.py:276} INFO - 21/05/14 14:18:49 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:49,697] {docker.py:276} INFO - 21/05/14 14:18:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:49,698] {docker.py:276} INFO - 21/05/14 14:18:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:49,698] {docker.py:276} INFO - 21/05/14 14:18:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432030809981542769095_0004_m_000064_352, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432030809981542769095_0004_m_000064_352}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432030809981542769095_0004}; taskId=attempt_202105141416432030809981542769095_0004_m_000064_352, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fd98e4a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:49 INFO StagingCommitter: Starting: Task committer attempt_202105141416432030809981542769095_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432030809981542769095_0004_m_000064_352
[2021-05-14 11:18:49,701] {docker.py:276} INFO - 21/05/14 14:18:49 INFO StagingCommitter: Task committer attempt_202105141416432030809981542769095_0004_m_000064_352: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432030809981542769095_0004_m_000064_352 : duration 0:00.004s
[2021-05-14 11:18:53,383] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416434557387485573301326_0004_m_000061_349: needsTaskCommit() Task attempt_202105141416434557387485573301326_0004_m_000061_349
[2021-05-14 11:18:53,385] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Task committer attempt_202105141416434557387485573301326_0004_m_000061_349: needsTaskCommit() Task attempt_202105141416434557387485573301326_0004_m_000061_349: duration 0:00.003s
21/05/14 14:18:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434557387485573301326_0004_m_000061_349
[2021-05-14 11:18:53,386] {docker.py:276} INFO - 21/05/14 14:18:53 INFO Executor: Finished task 61.0 in stage 4.0 (TID 349). 5106 bytes result sent to driver
[2021-05-14 11:18:53,387] {docker.py:276} INFO - 21/05/14 14:18:53 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 353) (0d1d3e01326c, executor driver, partition 65, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:53,388] {docker.py:276} INFO - 21/05/14 14:18:53 INFO Executor: Running task 65.0 in stage 4.0 (TID 353)
[2021-05-14 11:18:53,390] {docker.py:276} INFO - 21/05/14 14:18:53 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 349) in 7126 ms on 0d1d3e01326c (executor driver) (62/200)
[2021-05-14 11:18:53,398] {docker.py:276} INFO - 21/05/14 14:18:53 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:53,407] {docker.py:276} INFO - 21/05/14 14:18:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439100210184918165921_0004_m_000065_353, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439100210184918165921_0004_m_000065_353}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439100210184918165921_0004}; taskId=attempt_202105141416439100210184918165921_0004_m_000065_353, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1fc245d6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:53,408] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416439100210184918165921_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439100210184918165921_0004_m_000065_353
[2021-05-14 11:18:53,409] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416438851468171935314341_0004_m_000062_350: needsTaskCommit() Task attempt_202105141416438851468171935314341_0004_m_000062_350
[2021-05-14 11:18:53,409] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Task committer attempt_202105141416438851468171935314341_0004_m_000062_350: needsTaskCommit() Task attempt_202105141416438851468171935314341_0004_m_000062_350: duration 0:00.001s
21/05/14 14:18:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438851468171935314341_0004_m_000062_350
[2021-05-14 11:18:53,410] {docker.py:276} INFO - 21/05/14 14:18:53 INFO Executor: Finished task 62.0 in stage 4.0 (TID 350). 5106 bytes result sent to driver
[2021-05-14 11:18:53,413] {docker.py:276} INFO - 21/05/14 14:18:53 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 354) (0d1d3e01326c, executor driver, partition 66, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:53,413] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Task committer attempt_202105141416439100210184918165921_0004_m_000065_353: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439100210184918165921_0004_m_000065_353 : duration 0:00.005s
[2021-05-14 11:18:53,414] {docker.py:276} INFO - 21/05/14 14:18:53 INFO Executor: Running task 66.0 in stage 4.0 (TID 354)
[2021-05-14 11:18:53,415] {docker.py:276} INFO - 21/05/14 14:18:53 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 350) in 7073 ms on 0d1d3e01326c (executor driver) (63/200)
[2021-05-14 11:18:53,436] {docker.py:276} INFO - 21/05/14 14:18:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:53,444] {docker.py:276} INFO - 21/05/14 14:18:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:53,444] {docker.py:276} INFO - 21/05/14 14:18:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437625759262381825445_0004_m_000066_354, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437625759262381825445_0004_m_000066_354}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437625759262381825445_0004}; taskId=attempt_202105141416437625759262381825445_0004_m_000066_354, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@365fbd75}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416437625759262381825445_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437625759262381825445_0004_m_000066_354
[2021-05-14 11:18:53,448] {docker.py:276} INFO - 21/05/14 14:18:53 INFO StagingCommitter: Task committer attempt_202105141416437625759262381825445_0004_m_000066_354: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437625759262381825445_0004_m_000066_354 : duration 0:00.004s
[2021-05-14 11:18:55,078] {docker.py:276} INFO - 21/05/14 14:18:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416432890477780732407049_0004_m_000063_351: needsTaskCommit() Task attempt_202105141416432890477780732407049_0004_m_000063_351
[2021-05-14 11:18:55,079] {docker.py:276} INFO - 21/05/14 14:18:55 INFO StagingCommitter: Task committer attempt_202105141416432890477780732407049_0004_m_000063_351: needsTaskCommit() Task attempt_202105141416432890477780732407049_0004_m_000063_351: duration 0:00.002s
21/05/14 14:18:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432890477780732407049_0004_m_000063_351
[2021-05-14 11:18:55,082] {docker.py:276} INFO - 21/05/14 14:18:55 INFO Executor: Finished task 63.0 in stage 4.0 (TID 351). 5149 bytes result sent to driver
[2021-05-14 11:18:55,083] {docker.py:276} INFO - 21/05/14 14:18:55 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 355) (0d1d3e01326c, executor driver, partition 67, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:55,084] {docker.py:276} INFO - 21/05/14 14:18:55 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 351) in 6856 ms on 0d1d3e01326c (executor driver) (64/200)
[2021-05-14 11:18:55,085] {docker.py:276} INFO - 21/05/14 14:18:55 INFO Executor: Running task 67.0 in stage 4.0 (TID 355)
[2021-05-14 11:18:55,094] {docker.py:276} INFO - 21/05/14 14:18:55 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:18:55,095] {docker.py:276} INFO - 21/05/14 14:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:55,104] {docker.py:276} INFO - 21/05/14 14:18:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:18:55,104] {docker.py:276} INFO - 21/05/14 14:18:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:55,105] {docker.py:276} INFO - 21/05/14 14:18:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:55,105] {docker.py:276} INFO - 21/05/14 14:18:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438155313802486798096_0004_m_000067_355, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438155313802486798096_0004_m_000067_355}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438155313802486798096_0004}; taskId=attempt_202105141416438155313802486798096_0004_m_000067_355, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b7a54f9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:55,105] {docker.py:276} INFO - 21/05/14 14:18:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416438155313802486798096_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438155313802486798096_0004_m_000067_355
[2021-05-14 11:18:55,109] {docker.py:276} INFO - 21/05/14 14:18:55 INFO StagingCommitter: Task committer attempt_202105141416438155313802486798096_0004_m_000067_355: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438155313802486798096_0004_m_000067_355 : duration 0:00.004s
[2021-05-14 11:18:56,473] {docker.py:276} INFO - 21/05/14 14:18:56 INFO StagingCommitter: Starting: Task committer attempt_202105141416432030809981542769095_0004_m_000064_352: needsTaskCommit() Task attempt_202105141416432030809981542769095_0004_m_000064_352
[2021-05-14 11:18:56,474] {docker.py:276} INFO - 21/05/14 14:18:56 INFO StagingCommitter: Task committer attempt_202105141416432030809981542769095_0004_m_000064_352: needsTaskCommit() Task attempt_202105141416432030809981542769095_0004_m_000064_352: duration 0:00.002s
21/05/14 14:18:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432030809981542769095_0004_m_000064_352
[2021-05-14 11:18:56,476] {docker.py:276} INFO - 21/05/14 14:18:56 INFO Executor: Finished task 64.0 in stage 4.0 (TID 352). 5149 bytes result sent to driver
[2021-05-14 11:18:56,476] {docker.py:276} INFO - 21/05/14 14:18:56 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 356) (0d1d3e01326c, executor driver, partition 68, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:56,477] {docker.py:276} INFO - 21/05/14 14:18:56 INFO Executor: Running task 68.0 in stage 4.0 (TID 356)
[2021-05-14 11:18:56,478] {docker.py:276} INFO - 21/05/14 14:18:56 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 352) in 6811 ms on 0d1d3e01326c (executor driver) (65/200)
[2021-05-14 11:18:56,488] {docker.py:276} INFO - 21/05/14 14:18:56 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:56,497] {docker.py:276} INFO - 21/05/14 14:18:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:56,497] {docker.py:276} INFO - 21/05/14 14:18:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:56,498] {docker.py:276} INFO - 21/05/14 14:18:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433824343414327593909_0004_m_000068_356, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433824343414327593909_0004_m_000068_356}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433824343414327593909_0004}; taskId=attempt_202105141416433824343414327593909_0004_m_000068_356, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1f253e84}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:56,498] {docker.py:276} INFO - 21/05/14 14:18:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:18:56,498] {docker.py:276} INFO - 21/05/14 14:18:56 INFO StagingCommitter: Starting: Task committer attempt_202105141416433824343414327593909_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433824343414327593909_0004_m_000068_356
[2021-05-14 11:18:56,506] {docker.py:276} INFO - 21/05/14 14:18:56 INFO StagingCommitter: Task committer attempt_202105141416433824343414327593909_0004_m_000068_356: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433824343414327593909_0004_m_000068_356 : duration 0:00.008s
[2021-05-14 11:18:59,494] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416439100210184918165921_0004_m_000065_353: needsTaskCommit() Task attempt_202105141416439100210184918165921_0004_m_000065_353
[2021-05-14 11:18:59,494] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Task committer attempt_202105141416439100210184918165921_0004_m_000065_353: needsTaskCommit() Task attempt_202105141416439100210184918165921_0004_m_000065_353: duration 0:00.002s
21/05/14 14:18:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439100210184918165921_0004_m_000065_353
[2021-05-14 11:18:59,496] {docker.py:276} INFO - 21/05/14 14:18:59 INFO Executor: Finished task 65.0 in stage 4.0 (TID 353). 5149 bytes result sent to driver
[2021-05-14 11:18:59,498] {docker.py:276} INFO - 21/05/14 14:18:59 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 357) (0d1d3e01326c, executor driver, partition 69, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:59,498] {docker.py:276} INFO - 21/05/14 14:18:59 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 353) in 6119 ms on 0d1d3e01326c (executor driver) (66/200)
21/05/14 14:18:59 INFO Executor: Running task 69.0 in stage 4.0 (TID 357)
[2021-05-14 11:18:59,508] {docker.py:276} INFO - 21/05/14 14:18:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.7 KiB) non-empty blocks including 5 (42.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:59,516] {docker.py:276} INFO - 21/05/14 14:18:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:18:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:18:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438275569113198771625_0004_m_000069_357, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438275569113198771625_0004_m_000069_357}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438275569113198771625_0004}; taskId=attempt_202105141416438275569113198771625_0004_m_000069_357, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@687dd8f8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:59,517] {docker.py:276} INFO - 21/05/14 14:18:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416438275569113198771625_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438275569113198771625_0004_m_000069_357
[2021-05-14 11:18:59,522] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Task committer attempt_202105141416438275569113198771625_0004_m_000069_357: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438275569113198771625_0004_m_000069_357 : duration 0:00.005s
[2021-05-14 11:18:59,764] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416437625759262381825445_0004_m_000066_354: needsTaskCommit() Task attempt_202105141416437625759262381825445_0004_m_000066_354
[2021-05-14 11:18:59,765] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Task committer attempt_202105141416437625759262381825445_0004_m_000066_354: needsTaskCommit() Task attempt_202105141416437625759262381825445_0004_m_000066_354: duration 0:00.002s
21/05/14 14:18:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437625759262381825445_0004_m_000066_354
[2021-05-14 11:18:59,767] {docker.py:276} INFO - 21/05/14 14:18:59 INFO Executor: Finished task 66.0 in stage 4.0 (TID 354). 5149 bytes result sent to driver
[2021-05-14 11:18:59,768] {docker.py:276} INFO - 21/05/14 14:18:59 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 358) (0d1d3e01326c, executor driver, partition 70, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:18:59,769] {docker.py:276} INFO - 21/05/14 14:18:59 INFO Executor: Running task 70.0 in stage 4.0 (TID 358)
[2021-05-14 11:18:59,770] {docker.py:276} INFO - 21/05/14 14:18:59 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 354) in 6366 ms on 0d1d3e01326c (executor driver) (67/200)
[2021-05-14 11:18:59,779] {docker.py:276} INFO - 21/05/14 14:18:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:18:59,787] {docker.py:276} INFO - 21/05/14 14:18:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:18:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:18:59,788] {docker.py:276} INFO - 21/05/14 14:18:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:18:59,788] {docker.py:276} INFO - 21/05/14 14:18:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433224570158727187018_0004_m_000070_358, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433224570158727187018_0004_m_000070_358}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433224570158727187018_0004}; taskId=attempt_202105141416433224570158727187018_0004_m_000070_358, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@272f8004}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:18:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:18:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416433224570158727187018_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433224570158727187018_0004_m_000070_358
[2021-05-14 11:18:59,792] {docker.py:276} INFO - 21/05/14 14:18:59 INFO StagingCommitter: Task committer attempt_202105141416433224570158727187018_0004_m_000070_358: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433224570158727187018_0004_m_000070_358 : duration 0:00.005s
[2021-05-14 11:19:02,800] {docker.py:276} INFO - 21/05/14 14:19:02 INFO StagingCommitter: Starting: Task committer attempt_202105141416438155313802486798096_0004_m_000067_355: needsTaskCommit() Task attempt_202105141416438155313802486798096_0004_m_000067_355
21/05/14 14:19:02 INFO StagingCommitter: Task committer attempt_202105141416438155313802486798096_0004_m_000067_355: needsTaskCommit() Task attempt_202105141416438155313802486798096_0004_m_000067_355: duration 0:00.003s
21/05/14 14:19:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438155313802486798096_0004_m_000067_355
[2021-05-14 11:19:02,801] {docker.py:276} INFO - 21/05/14 14:19:02 INFO Executor: Finished task 67.0 in stage 4.0 (TID 355). 5106 bytes result sent to driver
[2021-05-14 11:19:02,802] {docker.py:276} INFO - 21/05/14 14:19:02 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 359) (0d1d3e01326c, executor driver, partition 71, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:02,803] {docker.py:276} INFO - 21/05/14 14:19:02 INFO Executor: Running task 71.0 in stage 4.0 (TID 359)
21/05/14 14:19:02 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 355) in 7729 ms on 0d1d3e01326c (executor driver) (68/200)
[2021-05-14 11:19:02,822] {docker.py:276} INFO - 21/05/14 14:19:02 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:02,830] {docker.py:276} INFO - 21/05/14 14:19:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:02,831] {docker.py:276} INFO - 21/05/14 14:19:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643307638316705925910_0004_m_000071_359, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307638316705925910_0004_m_000071_359}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643307638316705925910_0004}; taskId=attempt_20210514141643307638316705925910_0004_m_000071_359, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@161ad876}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:02,831] {docker.py:276} INFO - 21/05/14 14:19:02 INFO StagingCommitter: Starting: Task committer attempt_20210514141643307638316705925910_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307638316705925910_0004_m_000071_359
[2021-05-14 11:19:02,835] {docker.py:276} INFO - 21/05/14 14:19:02 INFO StagingCommitter: Task committer attempt_20210514141643307638316705925910_0004_m_000071_359: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643307638316705925910_0004_m_000071_359 : duration 0:00.003s
[2021-05-14 11:19:03,253] {docker.py:276} INFO - 21/05/14 14:19:03 INFO StagingCommitter: Starting: Task committer attempt_202105141416433824343414327593909_0004_m_000068_356: needsTaskCommit() Task attempt_202105141416433824343414327593909_0004_m_000068_356
[2021-05-14 11:19:03,254] {docker.py:276} INFO - 21/05/14 14:19:03 INFO StagingCommitter: Task committer attempt_202105141416433824343414327593909_0004_m_000068_356: needsTaskCommit() Task attempt_202105141416433824343414327593909_0004_m_000068_356: duration 0:00.003s
21/05/14 14:19:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433824343414327593909_0004_m_000068_356
[2021-05-14 11:19:03,256] {docker.py:276} INFO - 21/05/14 14:19:03 INFO Executor: Finished task 68.0 in stage 4.0 (TID 356). 5149 bytes result sent to driver
[2021-05-14 11:19:03,258] {docker.py:276} INFO - 21/05/14 14:19:03 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 360) (0d1d3e01326c, executor driver, partition 72, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:03,259] {docker.py:276} INFO - 21/05/14 14:19:03 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 356) in 6790 ms on 0d1d3e01326c (executor driver) (69/200)
[2021-05-14 11:19:03,260] {docker.py:276} INFO - 21/05/14 14:19:03 INFO Executor: Running task 72.0 in stage 4.0 (TID 360)
[2021-05-14 11:19:03,270] {docker.py:276} INFO - 21/05/14 14:19:03 INFO ShuffleBlockFetcherIterator: Getting 5 (43.2 KiB) non-empty blocks including 5 (43.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:03,279] {docker.py:276} INFO - 21/05/14 14:19:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431666871275136144570_0004_m_000072_360, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431666871275136144570_0004_m_000072_360}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431666871275136144570_0004}; taskId=attempt_202105141416431666871275136144570_0004_m_000072_360, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@74d91ed1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:03,279] {docker.py:276} INFO - 21/05/14 14:19:03 INFO StagingCommitter: Starting: Task committer attempt_202105141416431666871275136144570_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431666871275136144570_0004_m_000072_360
[2021-05-14 11:19:03,283] {docker.py:276} INFO - 21/05/14 14:19:03 INFO StagingCommitter: Task committer attempt_202105141416431666871275136144570_0004_m_000072_360: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431666871275136144570_0004_m_000072_360 : duration 0:00.004s
[2021-05-14 11:19:06,508] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416438275569113198771625_0004_m_000069_357: needsTaskCommit() Task attempt_202105141416438275569113198771625_0004_m_000069_357
[2021-05-14 11:19:06,509] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Task committer attempt_202105141416438275569113198771625_0004_m_000069_357: needsTaskCommit() Task attempt_202105141416438275569113198771625_0004_m_000069_357: duration 0:00.002s
[2021-05-14 11:19:06,509] {docker.py:276} INFO - 21/05/14 14:19:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438275569113198771625_0004_m_000069_357
[2021-05-14 11:19:06,511] {docker.py:276} INFO - 21/05/14 14:19:06 INFO Executor: Finished task 69.0 in stage 4.0 (TID 357). 5149 bytes result sent to driver
[2021-05-14 11:19:06,513] {docker.py:276} INFO - 21/05/14 14:19:06 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 361) (0d1d3e01326c, executor driver, partition 73, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:06,514] {docker.py:276} INFO - 21/05/14 14:19:06 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 357) in 7024 ms on 0d1d3e01326c (executor driver) (70/200)
[2021-05-14 11:19:06,515] {docker.py:276} INFO - 21/05/14 14:19:06 INFO Executor: Running task 73.0 in stage 4.0 (TID 361)
[2021-05-14 11:19:06,523] {docker.py:276} INFO - 21/05/14 14:19:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:06,532] {docker.py:276} INFO - 21/05/14 14:19:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:06,532] {docker.py:276} INFO - 21/05/14 14:19:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437553178206739147796_0004_m_000073_361, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437553178206739147796_0004_m_000073_361}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437553178206739147796_0004}; taskId=attempt_202105141416437553178206739147796_0004_m_000073_361, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76691151}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:06,532] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416437553178206739147796_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437553178206739147796_0004_m_000073_361
[2021-05-14 11:19:06,536] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Task committer attempt_202105141416437553178206739147796_0004_m_000073_361: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437553178206739147796_0004_m_000073_361 : duration 0:00.004s
[2021-05-14 11:19:06,692] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416433224570158727187018_0004_m_000070_358: needsTaskCommit() Task attempt_202105141416433224570158727187018_0004_m_000070_358
[2021-05-14 11:19:06,693] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Task committer attempt_202105141416433224570158727187018_0004_m_000070_358: needsTaskCommit() Task attempt_202105141416433224570158727187018_0004_m_000070_358: duration 0:00.003s
21/05/14 14:19:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433224570158727187018_0004_m_000070_358
[2021-05-14 11:19:06,695] {docker.py:276} INFO - 21/05/14 14:19:06 INFO Executor: Finished task 70.0 in stage 4.0 (TID 358). 5149 bytes result sent to driver
[2021-05-14 11:19:06,696] {docker.py:276} INFO - 21/05/14 14:19:06 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 362) (0d1d3e01326c, executor driver, partition 74, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:06,697] {docker.py:276} INFO - 21/05/14 14:19:06 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 358) in 6938 ms on 0d1d3e01326c (executor driver) (71/200)
[2021-05-14 11:19:06,698] {docker.py:276} INFO - 21/05/14 14:19:06 INFO Executor: Running task 74.0 in stage 4.0 (TID 362)
[2021-05-14 11:19:06,706] {docker.py:276} INFO - 21/05/14 14:19:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:06,715] {docker.py:276} INFO - 21/05/14 14:19:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437849170604353143281_0004_m_000074_362, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437849170604353143281_0004_m_000074_362}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437849170604353143281_0004}; taskId=attempt_202105141416437849170604353143281_0004_m_000074_362, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4eb06af4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416437849170604353143281_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437849170604353143281_0004_m_000074_362
[2021-05-14 11:19:06,718] {docker.py:276} INFO - 21/05/14 14:19:06 INFO StagingCommitter: Task committer attempt_202105141416437849170604353143281_0004_m_000074_362: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437849170604353143281_0004_m_000074_362 : duration 0:00.003s
[2021-05-14 11:19:09,758] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416431666871275136144570_0004_m_000072_360: needsTaskCommit() Task attempt_202105141416431666871275136144570_0004_m_000072_360
[2021-05-14 11:19:09,759] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Task committer attempt_202105141416431666871275136144570_0004_m_000072_360: needsTaskCommit() Task attempt_202105141416431666871275136144570_0004_m_000072_360: duration 0:00.002s
21/05/14 14:19:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431666871275136144570_0004_m_000072_360
[2021-05-14 11:19:09,762] {docker.py:276} INFO - 21/05/14 14:19:09 INFO Executor: Finished task 72.0 in stage 4.0 (TID 360). 5106 bytes result sent to driver
[2021-05-14 11:19:09,763] {docker.py:276} INFO - 21/05/14 14:19:09 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 363) (0d1d3e01326c, executor driver, partition 75, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:09,764] {docker.py:276} INFO - 21/05/14 14:19:09 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 360) in 6515 ms on 0d1d3e01326c (executor driver) (72/200)
[2021-05-14 11:19:09,764] {docker.py:276} INFO - 21/05/14 14:19:09 INFO Executor: Running task 75.0 in stage 4.0 (TID 363)
[2021-05-14 11:19:09,777] {docker.py:276} INFO - 21/05/14 14:19:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:19:09,787] {docker.py:276} INFO - 21/05/14 14:19:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:19:09,787] {docker.py:276} INFO - 21/05/14 14:19:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:09,788] {docker.py:276} INFO - 21/05/14 14:19:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:09,788] {docker.py:276} INFO - 21/05/14 14:19:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437551575715590382444_0004_m_000075_363, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437551575715590382444_0004_m_000075_363}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437551575715590382444_0004}; taskId=attempt_202105141416437551575715590382444_0004_m_000075_363, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bdceae3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:09,788] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416437551575715590382444_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437551575715590382444_0004_m_000075_363
[2021-05-14 11:19:09,792] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Task committer attempt_202105141416437551575715590382444_0004_m_000075_363: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437551575715590382444_0004_m_000075_363 : duration 0:00.004s
[2021-05-14 11:19:09,820] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Starting: Task committer attempt_20210514141643307638316705925910_0004_m_000071_359: needsTaskCommit() Task attempt_20210514141643307638316705925910_0004_m_000071_359
[2021-05-14 11:19:09,820] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Task committer attempt_20210514141643307638316705925910_0004_m_000071_359: needsTaskCommit() Task attempt_20210514141643307638316705925910_0004_m_000071_359: duration 0:00.001s
21/05/14 14:19:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643307638316705925910_0004_m_000071_359
[2021-05-14 11:19:09,822] {docker.py:276} INFO - 21/05/14 14:19:09 INFO Executor: Finished task 71.0 in stage 4.0 (TID 359). 5149 bytes result sent to driver
[2021-05-14 11:19:09,823] {docker.py:276} INFO - 21/05/14 14:19:09 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 364) (0d1d3e01326c, executor driver, partition 76, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:09,823] {docker.py:276} INFO - 21/05/14 14:19:09 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 359) in 7031 ms on 0d1d3e01326c (executor driver) (73/200)
[2021-05-14 11:19:09,824] {docker.py:276} INFO - 21/05/14 14:19:09 INFO Executor: Running task 76.0 in stage 4.0 (TID 364)
[2021-05-14 11:19:09,831] {docker.py:276} INFO - 21/05/14 14:19:09 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:09,839] {docker.py:276} INFO - 21/05/14 14:19:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:09,840] {docker.py:276} INFO - 21/05/14 14:19:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431100945963273459641_0004_m_000076_364, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431100945963273459641_0004_m_000076_364}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431100945963273459641_0004}; taskId=attempt_202105141416431100945963273459641_0004_m_000076_364, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@bce511}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:09,840] {docker.py:276} INFO - 21/05/14 14:19:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:09,841] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416431100945963273459641_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431100945963273459641_0004_m_000076_364
[2021-05-14 11:19:09,843] {docker.py:276} INFO - 21/05/14 14:19:09 INFO StagingCommitter: Task committer attempt_202105141416431100945963273459641_0004_m_000076_364: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431100945963273459641_0004_m_000076_364 : duration 0:00.004s
[2021-05-14 11:19:13,544] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416437553178206739147796_0004_m_000073_361: needsTaskCommit() Task attempt_202105141416437553178206739147796_0004_m_000073_361
21/05/14 14:19:13 INFO StagingCommitter: Task committer attempt_202105141416437553178206739147796_0004_m_000073_361: needsTaskCommit() Task attempt_202105141416437553178206739147796_0004_m_000073_361: duration 0:00.001s
21/05/14 14:19:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437553178206739147796_0004_m_000073_361
[2021-05-14 11:19:13,546] {docker.py:276} INFO - 21/05/14 14:19:13 INFO Executor: Finished task 73.0 in stage 4.0 (TID 361). 5149 bytes result sent to driver
21/05/14 14:19:13 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 365) (0d1d3e01326c, executor driver, partition 77, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:13,549] {docker.py:276} INFO - 21/05/14 14:19:13 INFO Executor: Running task 77.0 in stage 4.0 (TID 365)
21/05/14 14:19:13 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 361) in 7008 ms on 0d1d3e01326c (executor driver) (74/200)
[2021-05-14 11:19:13,556] {docker.py:276} INFO - 21/05/14 14:19:13 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:19:13,567] {docker.py:276} INFO - 21/05/14 14:19:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438593051840910942652_0004_m_000077_365, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438593051840910942652_0004_m_000077_365}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438593051840910942652_0004}; taskId=attempt_202105141416438593051840910942652_0004_m_000077_365, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cd8b36c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416438593051840910942652_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438593051840910942652_0004_m_000077_365
[2021-05-14 11:19:13,572] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Task committer attempt_202105141416438593051840910942652_0004_m_000077_365: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438593051840910942652_0004_m_000077_365 : duration 0:00.004s
[2021-05-14 11:19:13,754] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416437849170604353143281_0004_m_000074_362: needsTaskCommit() Task attempt_202105141416437849170604353143281_0004_m_000074_362
[2021-05-14 11:19:13,755] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Task committer attempt_202105141416437849170604353143281_0004_m_000074_362: needsTaskCommit() Task attempt_202105141416437849170604353143281_0004_m_000074_362: duration 0:00.002s
21/05/14 14:19:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437849170604353143281_0004_m_000074_362
[2021-05-14 11:19:13,757] {docker.py:276} INFO - 21/05/14 14:19:13 INFO Executor: Finished task 74.0 in stage 4.0 (TID 362). 5149 bytes result sent to driver
[2021-05-14 11:19:13,759] {docker.py:276} INFO - 21/05/14 14:19:13 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 366) (0d1d3e01326c, executor driver, partition 78, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:13,760] {docker.py:276} INFO - 21/05/14 14:19:13 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 362) in 7036 ms on 0d1d3e01326c (executor driver) (75/200)
[2021-05-14 11:19:13,761] {docker.py:276} INFO - 21/05/14 14:19:13 INFO Executor: Running task 78.0 in stage 4.0 (TID 366)
[2021-05-14 11:19:13,772] {docker.py:276} INFO - 21/05/14 14:19:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:19:13,773] {docker.py:276} INFO - 21/05/14 14:19:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:19:13,784] {docker.py:276} INFO - 21/05/14 14:19:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:13,785] {docker.py:276} INFO - 21/05/14 14:19:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433034174715176330481_0004_m_000078_366, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433034174715176330481_0004_m_000078_366}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433034174715176330481_0004}; taskId=attempt_202105141416433034174715176330481_0004_m_000078_366, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5ee8d45c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:13,785] {docker.py:276} INFO - 21/05/14 14:19:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:13,786] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416433034174715176330481_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433034174715176330481_0004_m_000078_366
[2021-05-14 11:19:13,790] {docker.py:276} INFO - 21/05/14 14:19:13 INFO StagingCommitter: Task committer attempt_202105141416433034174715176330481_0004_m_000078_366: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433034174715176330481_0004_m_000078_366 : duration 0:00.006s
[2021-05-14 11:19:16,694] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416431100945963273459641_0004_m_000076_364: needsTaskCommit() Task attempt_202105141416431100945963273459641_0004_m_000076_364
[2021-05-14 11:19:16,695] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Task committer attempt_202105141416431100945963273459641_0004_m_000076_364: needsTaskCommit() Task attempt_202105141416431100945963273459641_0004_m_000076_364: duration 0:00.003s
21/05/14 14:19:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431100945963273459641_0004_m_000076_364
[2021-05-14 11:19:16,696] {docker.py:276} INFO - 21/05/14 14:19:16 INFO Executor: Finished task 76.0 in stage 4.0 (TID 364). 5149 bytes result sent to driver
[2021-05-14 11:19:16,697] {docker.py:276} INFO - 21/05/14 14:19:16 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 367) (0d1d3e01326c, executor driver, partition 79, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:16,698] {docker.py:276} INFO - 21/05/14 14:19:16 INFO Executor: Running task 79.0 in stage 4.0 (TID 367)
21/05/14 14:19:16 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 364) in 6848 ms on 0d1d3e01326c (executor driver) (76/200)
[2021-05-14 11:19:16,706] {docker.py:276} INFO - 21/05/14 14:19:16 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:16,715] {docker.py:276} INFO - 21/05/14 14:19:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432215788801834213028_0004_m_000079_367, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432215788801834213028_0004_m_000079_367}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432215788801834213028_0004}; taskId=attempt_202105141416432215788801834213028_0004_m_000079_367, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7863e234}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416432215788801834213028_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432215788801834213028_0004_m_000079_367
[2021-05-14 11:19:16,719] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Task committer attempt_202105141416432215788801834213028_0004_m_000079_367: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432215788801834213028_0004_m_000079_367 : duration 0:00.003s
[2021-05-14 11:19:16,956] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416437551575715590382444_0004_m_000075_363: needsTaskCommit() Task attempt_202105141416437551575715590382444_0004_m_000075_363
[2021-05-14 11:19:16,957] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Task committer attempt_202105141416437551575715590382444_0004_m_000075_363: needsTaskCommit() Task attempt_202105141416437551575715590382444_0004_m_000075_363: duration 0:00.003s
21/05/14 14:19:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437551575715590382444_0004_m_000075_363
[2021-05-14 11:19:16,960] {docker.py:276} INFO - 21/05/14 14:19:16 INFO Executor: Finished task 75.0 in stage 4.0 (TID 363). 5149 bytes result sent to driver
[2021-05-14 11:19:16,961] {docker.py:276} INFO - 21/05/14 14:19:16 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 368) (0d1d3e01326c, executor driver, partition 80, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:16,963] {docker.py:276} INFO - 21/05/14 14:19:16 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 363) in 7173 ms on 0d1d3e01326c (executor driver) (77/200)
[2021-05-14 11:19:16,964] {docker.py:276} INFO - 21/05/14 14:19:16 INFO Executor: Running task 80.0 in stage 4.0 (TID 368)
[2021-05-14 11:19:16,972] {docker.py:276} INFO - 21/05/14 14:19:16 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:16,981] {docker.py:276} INFO - 21/05/14 14:19:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:16,982] {docker.py:276} INFO - 21/05/14 14:19:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435806457489144775377_0004_m_000080_368, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435806457489144775377_0004_m_000080_368}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435806457489144775377_0004}; taskId=attempt_202105141416435806457489144775377_0004_m_000080_368, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d31a0ba}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:16,982] {docker.py:276} INFO - 21/05/14 14:19:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:16,982] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416435806457489144775377_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435806457489144775377_0004_m_000080_368
[2021-05-14 11:19:16,986] {docker.py:276} INFO - 21/05/14 14:19:16 INFO StagingCommitter: Task committer attempt_202105141416435806457489144775377_0004_m_000080_368: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435806457489144775377_0004_m_000080_368 : duration 0:00.004s
[2021-05-14 11:19:20,554] {docker.py:276} INFO - 21/05/14 14:19:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416438593051840910942652_0004_m_000077_365: needsTaskCommit() Task attempt_202105141416438593051840910942652_0004_m_000077_365
21/05/14 14:19:20 INFO StagingCommitter: Task committer attempt_202105141416438593051840910942652_0004_m_000077_365: needsTaskCommit() Task attempt_202105141416438593051840910942652_0004_m_000077_365: duration 0:00.002s
21/05/14 14:19:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438593051840910942652_0004_m_000077_365
[2021-05-14 11:19:20,555] {docker.py:276} INFO - 21/05/14 14:19:20 INFO Executor: Finished task 77.0 in stage 4.0 (TID 365). 5106 bytes result sent to driver
[2021-05-14 11:19:20,556] {docker.py:276} INFO - 21/05/14 14:19:20 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 369) (0d1d3e01326c, executor driver, partition 81, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:20,557] {docker.py:276} INFO - 21/05/14 14:19:20 INFO Executor: Running task 81.0 in stage 4.0 (TID 369)
[2021-05-14 11:19:20,558] {docker.py:276} INFO - 21/05/14 14:19:20 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 365) in 7019 ms on 0d1d3e01326c (executor driver) (78/200)
[2021-05-14 11:19:20,566] {docker.py:276} INFO - 21/05/14 14:19:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:20,574] {docker.py:276} INFO - 21/05/14 14:19:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436033458812618617733_0004_m_000081_369, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436033458812618617733_0004_m_000081_369}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436033458812618617733_0004}; taskId=attempt_202105141416436033458812618617733_0004_m_000081_369, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@628edbd7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:20,575] {docker.py:276} INFO - 21/05/14 14:19:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416436033458812618617733_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436033458812618617733_0004_m_000081_369
[2021-05-14 11:19:20,579] {docker.py:276} INFO - 21/05/14 14:19:20 INFO StagingCommitter: Task committer attempt_202105141416436033458812618617733_0004_m_000081_369: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436033458812618617733_0004_m_000081_369 : duration 0:00.004s
[2021-05-14 11:19:20,763] {docker.py:276} INFO - 21/05/14 14:19:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416433034174715176330481_0004_m_000078_366: needsTaskCommit() Task attempt_202105141416433034174715176330481_0004_m_000078_366
[2021-05-14 11:19:20,763] {docker.py:276} INFO - 21/05/14 14:19:20 INFO StagingCommitter: Task committer attempt_202105141416433034174715176330481_0004_m_000078_366: needsTaskCommit() Task attempt_202105141416433034174715176330481_0004_m_000078_366: duration 0:00.002s
21/05/14 14:19:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433034174715176330481_0004_m_000078_366
[2021-05-14 11:19:20,765] {docker.py:276} INFO - 21/05/14 14:19:20 INFO Executor: Finished task 78.0 in stage 4.0 (TID 366). 5149 bytes result sent to driver
[2021-05-14 11:19:20,766] {docker.py:276} INFO - 21/05/14 14:19:20 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 370) (0d1d3e01326c, executor driver, partition 82, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:20,767] {docker.py:276} INFO - 21/05/14 14:19:20 INFO Executor: Running task 82.0 in stage 4.0 (TID 370)
[2021-05-14 11:19:20,767] {docker.py:276} INFO - 21/05/14 14:19:20 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 366) in 7018 ms on 0d1d3e01326c (executor driver) (79/200)
[2021-05-14 11:19:20,775] {docker.py:276} INFO - 21/05/14 14:19:20 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:20,784] {docker.py:276} INFO - 21/05/14 14:19:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:20,784] {docker.py:276} INFO - 21/05/14 14:19:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434786397818170778640_0004_m_000082_370, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434786397818170778640_0004_m_000082_370}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434786397818170778640_0004}; taskId=attempt_202105141416434786397818170778640_0004_m_000082_370, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@395cce54}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416434786397818170778640_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434786397818170778640_0004_m_000082_370
[2021-05-14 11:19:20,788] {docker.py:276} INFO - 21/05/14 14:19:20 INFO StagingCommitter: Task committer attempt_202105141416434786397818170778640_0004_m_000082_370: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434786397818170778640_0004_m_000082_370 : duration 0:00.004s
[2021-05-14 11:19:23,455] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416432215788801834213028_0004_m_000079_367: needsTaskCommit() Task attempt_202105141416432215788801834213028_0004_m_000079_367
[2021-05-14 11:19:23,456] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Task committer attempt_202105141416432215788801834213028_0004_m_000079_367: needsTaskCommit() Task attempt_202105141416432215788801834213028_0004_m_000079_367: duration 0:00.004s
21/05/14 14:19:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432215788801834213028_0004_m_000079_367
[2021-05-14 11:19:23,458] {docker.py:276} INFO - 21/05/14 14:19:23 INFO Executor: Finished task 79.0 in stage 4.0 (TID 367). 5149 bytes result sent to driver
[2021-05-14 11:19:23,462] {docker.py:276} INFO - 21/05/14 14:19:23 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 371) (0d1d3e01326c, executor driver, partition 83, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:23,463] {docker.py:276} INFO - 21/05/14 14:19:23 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 367) in 6774 ms on 0d1d3e01326c (executor driver) (80/200)
[2021-05-14 11:19:23,464] {docker.py:276} INFO - 21/05/14 14:19:23 INFO Executor: Running task 83.0 in stage 4.0 (TID 371)
[2021-05-14 11:19:23,473] {docker.py:276} INFO - 21/05/14 14:19:23 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:23,482] {docker.py:276} INFO - 21/05/14 14:19:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437604954318455948423_0004_m_000083_371, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437604954318455948423_0004_m_000083_371}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437604954318455948423_0004}; taskId=attempt_202105141416437604954318455948423_0004_m_000083_371, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@304a5187}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:23,482] {docker.py:276} INFO - 21/05/14 14:19:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416437604954318455948423_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437604954318455948423_0004_m_000083_371
[2021-05-14 11:19:23,487] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Task committer attempt_202105141416437604954318455948423_0004_m_000083_371: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437604954318455948423_0004_m_000083_371 : duration 0:00.004s
[2021-05-14 11:19:23,632] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416435806457489144775377_0004_m_000080_368: needsTaskCommit() Task attempt_202105141416435806457489144775377_0004_m_000080_368
[2021-05-14 11:19:23,633] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Task committer attempt_202105141416435806457489144775377_0004_m_000080_368: needsTaskCommit() Task attempt_202105141416435806457489144775377_0004_m_000080_368: duration 0:00.003s
21/05/14 14:19:23 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435806457489144775377_0004_m_000080_368
[2021-05-14 11:19:23,634] {docker.py:276} INFO - 21/05/14 14:19:23 INFO Executor: Finished task 80.0 in stage 4.0 (TID 368). 5149 bytes result sent to driver
[2021-05-14 11:19:23,636] {docker.py:276} INFO - 21/05/14 14:19:23 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 372) (0d1d3e01326c, executor driver, partition 84, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:23,638] {docker.py:276} INFO - 21/05/14 14:19:23 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 368) in 6685 ms on 0d1d3e01326c (executor driver) (81/200)
21/05/14 14:19:23 INFO Executor: Running task 84.0 in stage 4.0 (TID 372)
[2021-05-14 11:19:23,648] {docker.py:276} INFO - 21/05/14 14:19:23 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:23,656] {docker.py:276} INFO - 21/05/14 14:19:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:23 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:23 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432718814938483461714_0004_m_000084_372, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432718814938483461714_0004_m_000084_372}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432718814938483461714_0004}; taskId=attempt_202105141416432718814938483461714_0004_m_000084_372, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6adefa0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:23,657] {docker.py:276} INFO - 21/05/14 14:19:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:23,657] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Starting: Task committer attempt_202105141416432718814938483461714_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432718814938483461714_0004_m_000084_372
[2021-05-14 11:19:23,661] {docker.py:276} INFO - 21/05/14 14:19:23 INFO StagingCommitter: Task committer attempt_202105141416432718814938483461714_0004_m_000084_372: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432718814938483461714_0004_m_000084_372 : duration 0:00.004s
[2021-05-14 11:19:27,326] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416436033458812618617733_0004_m_000081_369: needsTaskCommit() Task attempt_202105141416436033458812618617733_0004_m_000081_369
[2021-05-14 11:19:27,327] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Task committer attempt_202105141416436033458812618617733_0004_m_000081_369: needsTaskCommit() Task attempt_202105141416436033458812618617733_0004_m_000081_369: duration 0:00.001s
21/05/14 14:19:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436033458812618617733_0004_m_000081_369
[2021-05-14 11:19:27,328] {docker.py:276} INFO - 21/05/14 14:19:27 INFO Executor: Finished task 81.0 in stage 4.0 (TID 369). 5149 bytes result sent to driver
[2021-05-14 11:19:27,329] {docker.py:276} INFO - 21/05/14 14:19:27 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 373) (0d1d3e01326c, executor driver, partition 85, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:27,330] {docker.py:276} INFO - 21/05/14 14:19:27 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 369) in 6783 ms on 0d1d3e01326c (executor driver) (82/200)
[2021-05-14 11:19:27,331] {docker.py:276} INFO - 21/05/14 14:19:27 INFO Executor: Running task 85.0 in stage 4.0 (TID 373)
[2021-05-14 11:19:27,340] {docker.py:276} INFO - 21/05/14 14:19:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:19:27,340] {docker.py:276} INFO - 21/05/14 14:19:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:27,349] {docker.py:276} INFO - 21/05/14 14:19:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:27,349] {docker.py:276} INFO - 21/05/14 14:19:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:27,350] {docker.py:276} INFO - 21/05/14 14:19:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643774517126371024452_0004_m_000085_373, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643774517126371024452_0004_m_000085_373}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643774517126371024452_0004}; taskId=attempt_20210514141643774517126371024452_0004_m_000085_373, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ac07f21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:27,350] {docker.py:276} INFO - 21/05/14 14:19:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:27 INFO StagingCommitter: Starting: Task committer attempt_20210514141643774517126371024452_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643774517126371024452_0004_m_000085_373
[2021-05-14 11:19:27,354] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Task committer attempt_20210514141643774517126371024452_0004_m_000085_373: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643774517126371024452_0004_m_000085_373 : duration 0:00.005s
[2021-05-14 11:19:27,699] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416434786397818170778640_0004_m_000082_370: needsTaskCommit() Task attempt_202105141416434786397818170778640_0004_m_000082_370
[2021-05-14 11:19:27,700] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Task committer attempt_202105141416434786397818170778640_0004_m_000082_370: needsTaskCommit() Task attempt_202105141416434786397818170778640_0004_m_000082_370: duration 0:00.002s
[2021-05-14 11:19:27,700] {docker.py:276} INFO - 21/05/14 14:19:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434786397818170778640_0004_m_000082_370
[2021-05-14 11:19:27,702] {docker.py:276} INFO - 21/05/14 14:19:27 INFO Executor: Finished task 82.0 in stage 4.0 (TID 370). 5106 bytes result sent to driver
[2021-05-14 11:19:27,702] {docker.py:276} INFO - 21/05/14 14:19:27 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 374) (0d1d3e01326c, executor driver, partition 86, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:27,703] {docker.py:276} INFO - 21/05/14 14:19:27 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 370) in 6945 ms on 0d1d3e01326c (executor driver) (83/200)
[2021-05-14 11:19:27,704] {docker.py:276} INFO - 21/05/14 14:19:27 INFO Executor: Running task 86.0 in stage 4.0 (TID 374)
[2021-05-14 11:19:27,712] {docker.py:276} INFO - 21/05/14 14:19:27 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:19:27,712] {docker.py:276} INFO - 21/05/14 14:19:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:27,720] {docker.py:276} INFO - 21/05/14 14:19:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438903930556864576798_0004_m_000086_374, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438903930556864576798_0004_m_000086_374}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438903930556864576798_0004}; taskId=attempt_202105141416438903930556864576798_0004_m_000086_374, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4d807f7c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:27,721] {docker.py:276} INFO - 21/05/14 14:19:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:27,721] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416438903930556864576798_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438903930556864576798_0004_m_000086_374
[2021-05-14 11:19:27,726] {docker.py:276} INFO - 21/05/14 14:19:27 INFO StagingCommitter: Task committer attempt_202105141416438903930556864576798_0004_m_000086_374: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438903930556864576798_0004_m_000086_374 : duration 0:00.005s
[2021-05-14 11:19:30,700] {docker.py:276} INFO - 21/05/14 14:19:30 INFO StagingCommitter: Starting: Task committer attempt_202105141416432718814938483461714_0004_m_000084_372: needsTaskCommit() Task attempt_202105141416432718814938483461714_0004_m_000084_372
21/05/14 14:19:30 INFO StagingCommitter: Task committer attempt_202105141416432718814938483461714_0004_m_000084_372: needsTaskCommit() Task attempt_202105141416432718814938483461714_0004_m_000084_372: duration 0:00.001s
21/05/14 14:19:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432718814938483461714_0004_m_000084_372
[2021-05-14 11:19:30,701] {docker.py:276} INFO - 21/05/14 14:19:30 INFO Executor: Finished task 84.0 in stage 4.0 (TID 372). 5149 bytes result sent to driver
[2021-05-14 11:19:30,703] {docker.py:276} INFO - 21/05/14 14:19:30 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 375) (0d1d3e01326c, executor driver, partition 87, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:30,704] {docker.py:276} INFO - 21/05/14 14:19:30 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 372) in 7076 ms on 0d1d3e01326c (executor driver) (84/200)
[2021-05-14 11:19:30,705] {docker.py:276} INFO - 21/05/14 14:19:30 INFO Executor: Running task 87.0 in stage 4.0 (TID 375)
[2021-05-14 11:19:30,716] {docker.py:276} INFO - 21/05/14 14:19:30 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:30,724] {docker.py:276} INFO - 21/05/14 14:19:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:30,724] {docker.py:276} INFO - 21/05/14 14:19:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437752150671369585692_0004_m_000087_375, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437752150671369585692_0004_m_000087_375}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437752150671369585692_0004}; taskId=attempt_202105141416437752150671369585692_0004_m_000087_375, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@570477b6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:30 INFO StagingCommitter: Starting: Task committer attempt_202105141416437752150671369585692_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437752150671369585692_0004_m_000087_375
[2021-05-14 11:19:30,730] {docker.py:276} INFO - 21/05/14 14:19:30 INFO StagingCommitter: Task committer attempt_202105141416437752150671369585692_0004_m_000087_375: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437752150671369585692_0004_m_000087_375 : duration 0:00.006s
[2021-05-14 11:19:30,834] {docker.py:276} INFO - 21/05/14 14:19:30 INFO StagingCommitter: Starting: Task committer attempt_202105141416437604954318455948423_0004_m_000083_371: needsTaskCommit() Task attempt_202105141416437604954318455948423_0004_m_000083_371
[2021-05-14 11:19:30,835] {docker.py:276} INFO - 21/05/14 14:19:30 INFO StagingCommitter: Task committer attempt_202105141416437604954318455948423_0004_m_000083_371: needsTaskCommit() Task attempt_202105141416437604954318455948423_0004_m_000083_371: duration 0:00.003s
[2021-05-14 11:19:30,836] {docker.py:276} INFO - 21/05/14 14:19:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437604954318455948423_0004_m_000083_371
[2021-05-14 11:19:30,838] {docker.py:276} INFO - 21/05/14 14:19:30 INFO Executor: Finished task 83.0 in stage 4.0 (TID 371). 5149 bytes result sent to driver
[2021-05-14 11:19:30,840] {docker.py:276} INFO - 21/05/14 14:19:30 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 376) (0d1d3e01326c, executor driver, partition 88, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:30,841] {docker.py:276} INFO - 21/05/14 14:19:30 INFO Executor: Running task 88.0 in stage 4.0 (TID 376)
[2021-05-14 11:19:30,842] {docker.py:276} INFO - 21/05/14 14:19:30 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 371) in 7387 ms on 0d1d3e01326c (executor driver) (85/200)
[2021-05-14 11:19:30,850] {docker.py:276} INFO - 21/05/14 14:19:30 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:30,861] {docker.py:276} INFO - 21/05/14 14:19:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:30 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:30,862] {docker.py:276} INFO - 21/05/14 14:19:30 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435005852243210964678_0004_m_000088_376, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435005852243210964678_0004_m_000088_376}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435005852243210964678_0004}; taskId=attempt_202105141416435005852243210964678_0004_m_000088_376, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3f80d403}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:30,862] {docker.py:276} INFO - 21/05/14 14:19:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:30 INFO StagingCommitter: Starting: Task committer attempt_202105141416435005852243210964678_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435005852243210964678_0004_m_000088_376
[2021-05-14 11:19:30,866] {docker.py:276} INFO - 21/05/14 14:19:30 INFO StagingCommitter: Task committer attempt_202105141416435005852243210964678_0004_m_000088_376: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435005852243210964678_0004_m_000088_376 : duration 0:00.005s
[2021-05-14 11:19:34,290] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Starting: Task committer attempt_20210514141643774517126371024452_0004_m_000085_373: needsTaskCommit() Task attempt_20210514141643774517126371024452_0004_m_000085_373
[2021-05-14 11:19:34,291] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Task committer attempt_20210514141643774517126371024452_0004_m_000085_373: needsTaskCommit() Task attempt_20210514141643774517126371024452_0004_m_000085_373: duration 0:00.001s
21/05/14 14:19:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643774517126371024452_0004_m_000085_373
[2021-05-14 11:19:34,297] {docker.py:276} INFO - 21/05/14 14:19:34 INFO Executor: Finished task 85.0 in stage 4.0 (TID 373). 5149 bytes result sent to driver
21/05/14 14:19:34 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 377) (0d1d3e01326c, executor driver, partition 89, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:19:34 INFO Executor: Running task 89.0 in stage 4.0 (TID 377)
21/05/14 14:19:34 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 373) in 6976 ms on 0d1d3e01326c (executor driver) (86/200)
[2021-05-14 11:19:34,306] {docker.py:276} INFO - 21/05/14 14:19:34 INFO ShuffleBlockFetcherIterator: Getting 5 (45.0 KiB) non-empty blocks including 5 (45.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:34,315] {docker.py:276} INFO - 21/05/14 14:19:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:34,316] {docker.py:276} INFO - 21/05/14 14:19:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431155150874871489683_0004_m_000089_377, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431155150874871489683_0004_m_000089_377}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431155150874871489683_0004}; taskId=attempt_202105141416431155150874871489683_0004_m_000089_377, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48f92954}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416431155150874871489683_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431155150874871489683_0004_m_000089_377
[2021-05-14 11:19:34,320] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Task committer attempt_202105141416431155150874871489683_0004_m_000089_377: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431155150874871489683_0004_m_000089_377 : duration 0:00.005s
[2021-05-14 11:19:34,556] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416438903930556864576798_0004_m_000086_374: needsTaskCommit() Task attempt_202105141416438903930556864576798_0004_m_000086_374
[2021-05-14 11:19:34,557] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Task committer attempt_202105141416438903930556864576798_0004_m_000086_374: needsTaskCommit() Task attempt_202105141416438903930556864576798_0004_m_000086_374: duration 0:00.003s
21/05/14 14:19:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438903930556864576798_0004_m_000086_374
[2021-05-14 11:19:34,558] {docker.py:276} INFO - 21/05/14 14:19:34 INFO Executor: Finished task 86.0 in stage 4.0 (TID 374). 5149 bytes result sent to driver
[2021-05-14 11:19:34,559] {docker.py:276} INFO - 21/05/14 14:19:34 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 378) (0d1d3e01326c, executor driver, partition 90, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:34,560] {docker.py:276} INFO - 21/05/14 14:19:34 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 374) in 6866 ms on 0d1d3e01326c (executor driver) (87/200)
[2021-05-14 11:19:34,561] {docker.py:276} INFO - 21/05/14 14:19:34 INFO Executor: Running task 90.0 in stage 4.0 (TID 378)
[2021-05-14 11:19:34,570] {docker.py:276} INFO - 21/05/14 14:19:34 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:34,579] {docker.py:276} INFO - 21/05/14 14:19:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437223848588596949319_0004_m_000090_378, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437223848588596949319_0004_m_000090_378}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437223848588596949319_0004}; taskId=attempt_202105141416437223848588596949319_0004_m_000090_378, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@115a14ff}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:34,579] {docker.py:276} INFO - 21/05/14 14:19:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416437223848588596949319_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437223848588596949319_0004_m_000090_378
[2021-05-14 11:19:34,582] {docker.py:276} INFO - 21/05/14 14:19:34 INFO StagingCommitter: Task committer attempt_202105141416437223848588596949319_0004_m_000090_378: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437223848588596949319_0004_m_000090_378 : duration 0:00.004s
[2021-05-14 11:19:37,564] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Starting: Task committer attempt_202105141416437752150671369585692_0004_m_000087_375: needsTaskCommit() Task attempt_202105141416437752150671369585692_0004_m_000087_375
[2021-05-14 11:19:37,564] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Task committer attempt_202105141416437752150671369585692_0004_m_000087_375: needsTaskCommit() Task attempt_202105141416437752150671369585692_0004_m_000087_375: duration 0:00.001s
21/05/14 14:19:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437752150671369585692_0004_m_000087_375
[2021-05-14 11:19:37,566] {docker.py:276} INFO - 21/05/14 14:19:37 INFO Executor: Finished task 87.0 in stage 4.0 (TID 375). 5106 bytes result sent to driver
[2021-05-14 11:19:37,567] {docker.py:276} INFO - 21/05/14 14:19:37 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 379) (0d1d3e01326c, executor driver, partition 91, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:37,569] {docker.py:276} INFO - 21/05/14 14:19:37 INFO Executor: Running task 91.0 in stage 4.0 (TID 379)
[2021-05-14 11:19:37,570] {docker.py:276} INFO - 21/05/14 14:19:37 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 375) in 6875 ms on 0d1d3e01326c (executor driver) (88/200)
[2021-05-14 11:19:37,579] {docker.py:276} INFO - 21/05/14 14:19:37 INFO ShuffleBlockFetcherIterator: Getting 5 (40.1 KiB) non-empty blocks including 5 (40.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:37,588] {docker.py:276} INFO - 21/05/14 14:19:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431961825548405423823_0004_m_000091_379, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431961825548405423823_0004_m_000091_379}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431961825548405423823_0004}; taskId=attempt_202105141416431961825548405423823_0004_m_000091_379, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@777b9337}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:37,588] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Starting: Task committer attempt_202105141416431961825548405423823_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431961825548405423823_0004_m_000091_379
[2021-05-14 11:19:37,592] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Task committer attempt_202105141416431961825548405423823_0004_m_000091_379: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431961825548405423823_0004_m_000091_379 : duration 0:00.004s
[2021-05-14 11:19:37,809] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Starting: Task committer attempt_202105141416435005852243210964678_0004_m_000088_376: needsTaskCommit() Task attempt_202105141416435005852243210964678_0004_m_000088_376
[2021-05-14 11:19:37,809] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Task committer attempt_202105141416435005852243210964678_0004_m_000088_376: needsTaskCommit() Task attempt_202105141416435005852243210964678_0004_m_000088_376: duration 0:00.002s
21/05/14 14:19:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435005852243210964678_0004_m_000088_376
[2021-05-14 11:19:37,810] {docker.py:276} INFO - 21/05/14 14:19:37 INFO Executor: Finished task 88.0 in stage 4.0 (TID 376). 5106 bytes result sent to driver
[2021-05-14 11:19:37,812] {docker.py:276} INFO - 21/05/14 14:19:37 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 380) (0d1d3e01326c, executor driver, partition 92, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:37,813] {docker.py:276} INFO - 21/05/14 14:19:37 INFO Executor: Running task 92.0 in stage 4.0 (TID 380)
21/05/14 14:19:37 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 376) in 6983 ms on 0d1d3e01326c (executor driver) (89/200)
[2021-05-14 11:19:37,823] {docker.py:276} INFO - 21/05/14 14:19:37 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:37,842] {docker.py:276} INFO - 21/05/14 14:19:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:37 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:37 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438984002221283549521_0004_m_000092_380, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438984002221283549521_0004_m_000092_380}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438984002221283549521_0004}; taskId=attempt_202105141416438984002221283549521_0004_m_000092_380, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19504a39}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:37,842] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Starting: Task committer attempt_202105141416438984002221283549521_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438984002221283549521_0004_m_000092_380
[2021-05-14 11:19:37,846] {docker.py:276} INFO - 21/05/14 14:19:37 INFO StagingCommitter: Task committer attempt_202105141416438984002221283549521_0004_m_000092_380: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438984002221283549521_0004_m_000092_380 : duration 0:00.004s
[2021-05-14 11:19:41,052] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416431155150874871489683_0004_m_000089_377: needsTaskCommit() Task attempt_202105141416431155150874871489683_0004_m_000089_377
[2021-05-14 11:19:41,052] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Task committer attempt_202105141416431155150874871489683_0004_m_000089_377: needsTaskCommit() Task attempt_202105141416431155150874871489683_0004_m_000089_377: duration 0:00.002s
21/05/14 14:19:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431155150874871489683_0004_m_000089_377
[2021-05-14 11:19:41,053] {docker.py:276} INFO - 21/05/14 14:19:41 INFO Executor: Finished task 89.0 in stage 4.0 (TID 377). 5149 bytes result sent to driver
[2021-05-14 11:19:41,054] {docker.py:276} INFO - 21/05/14 14:19:41 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 381) (0d1d3e01326c, executor driver, partition 93, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:41,055] {docker.py:276} INFO - 21/05/14 14:19:41 INFO Executor: Running task 93.0 in stage 4.0 (TID 381)
[2021-05-14 11:19:41,056] {docker.py:276} INFO - 21/05/14 14:19:41 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 377) in 6734 ms on 0d1d3e01326c (executor driver) (90/200)
[2021-05-14 11:19:41,064] {docker.py:276} INFO - 21/05/14 14:19:41 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:41,072] {docker.py:276} INFO - 21/05/14 14:19:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:41,073] {docker.py:276} INFO - 21/05/14 14:19:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:41,073] {docker.py:276} INFO - 21/05/14 14:19:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436458571227700678487_0004_m_000093_381, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436458571227700678487_0004_m_000093_381}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436458571227700678487_0004}; taskId=attempt_202105141416436458571227700678487_0004_m_000093_381, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4862090c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:41,074] {docker.py:276} INFO - 21/05/14 14:19:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:41,074] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416436458571227700678487_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436458571227700678487_0004_m_000093_381
[2021-05-14 11:19:41,077] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Task committer attempt_202105141416436458571227700678487_0004_m_000093_381: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436458571227700678487_0004_m_000093_381 : duration 0:00.005s
[2021-05-14 11:19:41,396] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416437223848588596949319_0004_m_000090_378: needsTaskCommit() Task attempt_202105141416437223848588596949319_0004_m_000090_378
[2021-05-14 11:19:41,403] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Task committer attempt_202105141416437223848588596949319_0004_m_000090_378: needsTaskCommit() Task attempt_202105141416437223848588596949319_0004_m_000090_378: duration 0:00.002s
21/05/14 14:19:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437223848588596949319_0004_m_000090_378
[2021-05-14 11:19:41,403] {docker.py:276} INFO - 21/05/14 14:19:41 INFO Executor: Finished task 90.0 in stage 4.0 (TID 378). 5149 bytes result sent to driver
[2021-05-14 11:19:41,404] {docker.py:276} INFO - 21/05/14 14:19:41 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 382) (0d1d3e01326c, executor driver, partition 94, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/05/14 14:19:41 INFO Executor: Running task 94.0 in stage 4.0 (TID 382)
21/05/14 14:19:41 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 378) in 6814 ms on 0d1d3e01326c (executor driver) (91/200)
[2021-05-14 11:19:41,410] {docker.py:276} INFO - 21/05/14 14:19:41 INFO ShuffleBlockFetcherIterator: Getting 5 (43.0 KiB) non-empty blocks including 5 (43.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:41,420] {docker.py:276} INFO - 21/05/14 14:19:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:41,421] {docker.py:276} INFO - 21/05/14 14:19:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437252301059093112287_0004_m_000094_382, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437252301059093112287_0004_m_000094_382}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437252301059093112287_0004}; taskId=attempt_202105141416437252301059093112287_0004_m_000094_382, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@48611229}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416437252301059093112287_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437252301059093112287_0004_m_000094_382
[2021-05-14 11:19:41,425] {docker.py:276} INFO - 21/05/14 14:19:41 INFO StagingCommitter: Task committer attempt_202105141416437252301059093112287_0004_m_000094_382: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437252301059093112287_0004_m_000094_382 : duration 0:00.005s
[2021-05-14 11:19:44,859] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Starting: Task committer attempt_202105141416438984002221283549521_0004_m_000092_380: needsTaskCommit() Task attempt_202105141416438984002221283549521_0004_m_000092_380
[2021-05-14 11:19:44,860] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Task committer attempt_202105141416438984002221283549521_0004_m_000092_380: needsTaskCommit() Task attempt_202105141416438984002221283549521_0004_m_000092_380: duration 0:00.002s
21/05/14 14:19:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438984002221283549521_0004_m_000092_380
[2021-05-14 11:19:44,861] {docker.py:276} INFO - 21/05/14 14:19:44 INFO Executor: Finished task 92.0 in stage 4.0 (TID 380). 5149 bytes result sent to driver
[2021-05-14 11:19:44,862] {docker.py:276} INFO - 21/05/14 14:19:44 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 380) in 7025 ms on 0d1d3e01326c (executor driver) (92/200)
[2021-05-14 11:19:44,863] {docker.py:276} INFO - 21/05/14 14:19:44 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 383) (0d1d3e01326c, executor driver, partition 95, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:44,865] {docker.py:276} INFO - 21/05/14 14:19:44 INFO Executor: Running task 95.0 in stage 4.0 (TID 383)
[2021-05-14 11:19:44,874] {docker.py:276} INFO - 21/05/14 14:19:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:44,884] {docker.py:276} INFO - 21/05/14 14:19:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431359617275168071485_0004_m_000095_383, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431359617275168071485_0004_m_000095_383}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431359617275168071485_0004}; taskId=attempt_202105141416431359617275168071485_0004_m_000095_383, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17a4cced}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:44 INFO StagingCommitter: Starting: Task committer attempt_202105141416431359617275168071485_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431359617275168071485_0004_m_000095_383
[2021-05-14 11:19:44,888] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Task committer attempt_202105141416431359617275168071485_0004_m_000095_383: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431359617275168071485_0004_m_000095_383 : duration 0:00.004s
[2021-05-14 11:19:44,963] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Starting: Task committer attempt_202105141416431961825548405423823_0004_m_000091_379: needsTaskCommit() Task attempt_202105141416431961825548405423823_0004_m_000091_379
[2021-05-14 11:19:44,963] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Task committer attempt_202105141416431961825548405423823_0004_m_000091_379: needsTaskCommit() Task attempt_202105141416431961825548405423823_0004_m_000091_379: duration 0:00.001s
21/05/14 14:19:44 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431961825548405423823_0004_m_000091_379
[2021-05-14 11:19:44,965] {docker.py:276} INFO - 21/05/14 14:19:44 INFO Executor: Finished task 91.0 in stage 4.0 (TID 379). 5149 bytes result sent to driver
[2021-05-14 11:19:44,966] {docker.py:276} INFO - 21/05/14 14:19:44 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 384) (0d1d3e01326c, executor driver, partition 96, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:44,967] {docker.py:276} INFO - 21/05/14 14:19:44 INFO Executor: Running task 96.0 in stage 4.0 (TID 384)
[2021-05-14 11:19:44,967] {docker.py:276} INFO - 21/05/14 14:19:44 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 379) in 7375 ms on 0d1d3e01326c (executor driver) (93/200)
[2021-05-14 11:19:44,975] {docker.py:276} INFO - 21/05/14 14:19:44 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:44,985] {docker.py:276} INFO - 21/05/14 14:19:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:44 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:44 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435190850625675590546_0004_m_000096_384, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435190850625675590546_0004_m_000096_384}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435190850625675590546_0004}; taskId=attempt_202105141416435190850625675590546_0004_m_000096_384, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68860f35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:44 INFO StagingCommitter: Starting: Task committer attempt_202105141416435190850625675590546_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435190850625675590546_0004_m_000096_384
[2021-05-14 11:19:44,990] {docker.py:276} INFO - 21/05/14 14:19:44 INFO StagingCommitter: Task committer attempt_202105141416435190850625675590546_0004_m_000096_384: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435190850625675590546_0004_m_000096_384 : duration 0:00.004s
[2021-05-14 11:19:47,584] {docker.py:276} INFO - 21/05/14 14:19:47 INFO StagingCommitter: Starting: Task committer attempt_202105141416436458571227700678487_0004_m_000093_381: needsTaskCommit() Task attempt_202105141416436458571227700678487_0004_m_000093_381
[2021-05-14 11:19:47,585] {docker.py:276} INFO - 21/05/14 14:19:47 INFO StagingCommitter: Task committer attempt_202105141416436458571227700678487_0004_m_000093_381: needsTaskCommit() Task attempt_202105141416436458571227700678487_0004_m_000093_381: duration 0:00.002s
21/05/14 14:19:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436458571227700678487_0004_m_000093_381
[2021-05-14 11:19:47,587] {docker.py:276} INFO - 21/05/14 14:19:47 INFO Executor: Finished task 93.0 in stage 4.0 (TID 381). 5106 bytes result sent to driver
[2021-05-14 11:19:47,589] {docker.py:276} INFO - 21/05/14 14:19:47 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 385) (0d1d3e01326c, executor driver, partition 97, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:47,590] {docker.py:276} INFO - 21/05/14 14:19:47 INFO Executor: Running task 97.0 in stage 4.0 (TID 385)
21/05/14 14:19:47 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 381) in 6544 ms on 0d1d3e01326c (executor driver) (94/200)
[2021-05-14 11:19:47,611] {docker.py:276} INFO - 21/05/14 14:19:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:19:47,620] {docker.py:276} INFO - 21/05/14 14:19:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435909162422967415136_0004_m_000097_385, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435909162422967415136_0004_m_000097_385}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435909162422967415136_0004}; taskId=attempt_202105141416435909162422967415136_0004_m_000097_385, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5d917e7d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:47,620] {docker.py:276} INFO - 21/05/14 14:19:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:47 INFO StagingCommitter: Starting: Task committer attempt_202105141416435909162422967415136_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435909162422967415136_0004_m_000097_385
[2021-05-14 11:19:47,624] {docker.py:276} INFO - 21/05/14 14:19:47 INFO StagingCommitter: Task committer attempt_202105141416435909162422967415136_0004_m_000097_385: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435909162422967415136_0004_m_000097_385 : duration 0:00.004s
[2021-05-14 11:19:48,353] {docker.py:276} INFO - 21/05/14 14:19:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416437252301059093112287_0004_m_000094_382: needsTaskCommit() Task attempt_202105141416437252301059093112287_0004_m_000094_382
[2021-05-14 11:19:48,353] {docker.py:276} INFO - 21/05/14 14:19:48 INFO StagingCommitter: Task committer attempt_202105141416437252301059093112287_0004_m_000094_382: needsTaskCommit() Task attempt_202105141416437252301059093112287_0004_m_000094_382: duration 0:00.002s
21/05/14 14:19:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437252301059093112287_0004_m_000094_382
[2021-05-14 11:19:48,355] {docker.py:276} INFO - 21/05/14 14:19:48 INFO Executor: Finished task 94.0 in stage 4.0 (TID 382). 5149 bytes result sent to driver
[2021-05-14 11:19:48,356] {docker.py:276} INFO - 21/05/14 14:19:48 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 386) (0d1d3e01326c, executor driver, partition 98, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:48,357] {docker.py:276} INFO - 21/05/14 14:19:48 INFO Executor: Running task 98.0 in stage 4.0 (TID 386)
[2021-05-14 11:19:48,359] {docker.py:276} INFO - 21/05/14 14:19:48 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 382) in 6967 ms on 0d1d3e01326c (executor driver) (95/200)
[2021-05-14 11:19:48,377] {docker.py:276} INFO - 21/05/14 14:19:48 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:48,394] {docker.py:276} INFO - 21/05/14 14:19:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:48,395] {docker.py:276} INFO - 21/05/14 14:19:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437445636111164614755_0004_m_000098_386, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437445636111164614755_0004_m_000098_386}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437445636111164614755_0004}; taskId=attempt_202105141416437445636111164614755_0004_m_000098_386, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e4e3236}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:48,396] {docker.py:276} INFO - 21/05/14 14:19:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416437445636111164614755_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437445636111164614755_0004_m_000098_386
[2021-05-14 11:19:48,401] {docker.py:276} INFO - 21/05/14 14:19:48 INFO StagingCommitter: Task committer attempt_202105141416437445636111164614755_0004_m_000098_386: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437445636111164614755_0004_m_000098_386 : duration 0:00.006s
[2021-05-14 11:19:51,094] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416435190850625675590546_0004_m_000096_384: needsTaskCommit() Task attempt_202105141416435190850625675590546_0004_m_000096_384
[2021-05-14 11:19:51,095] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Task committer attempt_202105141416435190850625675590546_0004_m_000096_384: needsTaskCommit() Task attempt_202105141416435190850625675590546_0004_m_000096_384: duration 0:00.001s
21/05/14 14:19:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435190850625675590546_0004_m_000096_384
[2021-05-14 11:19:51,098] {docker.py:276} INFO - 21/05/14 14:19:51 INFO Executor: Finished task 96.0 in stage 4.0 (TID 384). 5149 bytes result sent to driver
21/05/14 14:19:51 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 387) (0d1d3e01326c, executor driver, partition 99, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:51,098] {docker.py:276} INFO - 21/05/14 14:19:51 INFO Executor: Running task 99.0 in stage 4.0 (TID 387)
[2021-05-14 11:19:51,099] {docker.py:276} INFO - 21/05/14 14:19:51 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 384) in 6140 ms on 0d1d3e01326c (executor driver) (96/200)
[2021-05-14 11:19:51,105] {docker.py:276} INFO - 21/05/14 14:19:51 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:51,114] {docker.py:276} INFO - 21/05/14 14:19:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433746137966554836124_0004_m_000099_387, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433746137966554836124_0004_m_000099_387}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433746137966554836124_0004}; taskId=attempt_202105141416433746137966554836124_0004_m_000099_387, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3fccbba7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:51,114] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416433746137966554836124_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433746137966554836124_0004_m_000099_387
[2021-05-14 11:19:51,118] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Task committer attempt_202105141416433746137966554836124_0004_m_000099_387: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433746137966554836124_0004_m_000099_387 : duration 0:00.003s
[2021-05-14 11:19:51,779] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416431359617275168071485_0004_m_000095_383: needsTaskCommit() Task attempt_202105141416431359617275168071485_0004_m_000095_383
[2021-05-14 11:19:51,780] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Task committer attempt_202105141416431359617275168071485_0004_m_000095_383: needsTaskCommit() Task attempt_202105141416431359617275168071485_0004_m_000095_383: duration 0:00.001s
21/05/14 14:19:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431359617275168071485_0004_m_000095_383
[2021-05-14 11:19:51,780] {docker.py:276} INFO - 21/05/14 14:19:51 INFO Executor: Finished task 95.0 in stage 4.0 (TID 383). 5149 bytes result sent to driver
[2021-05-14 11:19:51,781] {docker.py:276} INFO - 21/05/14 14:19:51 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 388) (0d1d3e01326c, executor driver, partition 100, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:51,782] {docker.py:276} INFO - 21/05/14 14:19:51 INFO Executor: Running task 100.0 in stage 4.0 (TID 388)
[2021-05-14 11:19:51,783] {docker.py:276} INFO - 21/05/14 14:19:51 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 383) in 6928 ms on 0d1d3e01326c (executor driver) (97/200)
[2021-05-14 11:19:51,792] {docker.py:276} INFO - 21/05/14 14:19:51 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:19:51,793] {docker.py:276} INFO - 21/05/14 14:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:51,800] {docker.py:276} INFO - 21/05/14 14:19:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:19:51,801] {docker.py:276} INFO - 21/05/14 14:19:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:51,801] {docker.py:276} INFO - 21/05/14 14:19:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436773364082462351992_0004_m_000100_388, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436773364082462351992_0004_m_000100_388}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436773364082462351992_0004}; taskId=attempt_202105141416436773364082462351992_0004_m_000100_388, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a097dca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:51,801] {docker.py:276} INFO - 21/05/14 14:19:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:51,801] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416436773364082462351992_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436773364082462351992_0004_m_000100_388
[2021-05-14 11:19:51,805] {docker.py:276} INFO - 21/05/14 14:19:51 INFO StagingCommitter: Task committer attempt_202105141416436773364082462351992_0004_m_000100_388: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436773364082462351992_0004_m_000100_388 : duration 0:00.004s
[2021-05-14 11:19:54,340] {docker.py:276} INFO - 21/05/14 14:19:54 INFO StagingCommitter: Starting: Task committer attempt_202105141416435909162422967415136_0004_m_000097_385: needsTaskCommit() Task attempt_202105141416435909162422967415136_0004_m_000097_385
[2021-05-14 11:19:54,342] {docker.py:276} INFO - 21/05/14 14:19:54 INFO StagingCommitter: Task committer attempt_202105141416435909162422967415136_0004_m_000097_385: needsTaskCommit() Task attempt_202105141416435909162422967415136_0004_m_000097_385: duration 0:00.004s
21/05/14 14:19:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435909162422967415136_0004_m_000097_385
[2021-05-14 11:19:54,344] {docker.py:276} INFO - 21/05/14 14:19:54 INFO Executor: Finished task 97.0 in stage 4.0 (TID 385). 5149 bytes result sent to driver
[2021-05-14 11:19:54,345] {docker.py:276} INFO - 21/05/14 14:19:54 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 389) (0d1d3e01326c, executor driver, partition 101, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:54,346] {docker.py:276} INFO - 21/05/14 14:19:54 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 385) in 6764 ms on 0d1d3e01326c (executor driver) (98/200)
[2021-05-14 11:19:54,347] {docker.py:276} INFO - 21/05/14 14:19:54 INFO Executor: Running task 101.0 in stage 4.0 (TID 389)
[2021-05-14 11:19:54,356] {docker.py:276} INFO - 21/05/14 14:19:54 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:54,367] {docker.py:276} INFO - 21/05/14 14:19:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433028222054776244917_0004_m_000101_389, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433028222054776244917_0004_m_000101_389}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433028222054776244917_0004}; taskId=attempt_202105141416433028222054776244917_0004_m_000101_389, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@140e35b0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:54,368] {docker.py:276} INFO - 21/05/14 14:19:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:54 INFO StagingCommitter: Starting: Task committer attempt_202105141416433028222054776244917_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433028222054776244917_0004_m_000101_389
[2021-05-14 11:19:54,375] {docker.py:276} INFO - 21/05/14 14:19:54 INFO StagingCommitter: Task committer attempt_202105141416433028222054776244917_0004_m_000101_389: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433028222054776244917_0004_m_000101_389 : duration 0:00.007s
[2021-05-14 11:19:55,475] {docker.py:276} INFO - 21/05/14 14:19:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416437445636111164614755_0004_m_000098_386: needsTaskCommit() Task attempt_202105141416437445636111164614755_0004_m_000098_386
[2021-05-14 11:19:55,475] {docker.py:276} INFO - 21/05/14 14:19:55 INFO StagingCommitter: Task committer attempt_202105141416437445636111164614755_0004_m_000098_386: needsTaskCommit() Task attempt_202105141416437445636111164614755_0004_m_000098_386: duration 0:00.002s
21/05/14 14:19:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437445636111164614755_0004_m_000098_386
[2021-05-14 11:19:55,479] {docker.py:276} INFO - 21/05/14 14:19:55 INFO Executor: Finished task 98.0 in stage 4.0 (TID 386). 5106 bytes result sent to driver
21/05/14 14:19:55 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 390) (0d1d3e01326c, executor driver, partition 102, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:55,480] {docker.py:276} INFO - 21/05/14 14:19:55 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 386) in 7132 ms on 0d1d3e01326c (executor driver) (99/200)
[2021-05-14 11:19:55,481] {docker.py:276} INFO - 21/05/14 14:19:55 INFO Executor: Running task 102.0 in stage 4.0 (TID 390)
[2021-05-14 11:19:55,490] {docker.py:276} INFO - 21/05/14 14:19:55 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:55,498] {docker.py:276} INFO - 21/05/14 14:19:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438937754081037420804_0004_m_000102_390, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438937754081037420804_0004_m_000102_390}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438937754081037420804_0004}; taskId=attempt_202105141416438937754081037420804_0004_m_000102_390, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@113fdbfb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:55,499] {docker.py:276} INFO - 21/05/14 14:19:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416438937754081037420804_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438937754081037420804_0004_m_000102_390
[2021-05-14 11:19:55,502] {docker.py:276} INFO - 21/05/14 14:19:55 INFO StagingCommitter: Task committer attempt_202105141416438937754081037420804_0004_m_000102_390: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438937754081037420804_0004_m_000102_390 : duration 0:00.004s
[2021-05-14 11:19:58,107] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416433746137966554836124_0004_m_000099_387: needsTaskCommit() Task attempt_202105141416433746137966554836124_0004_m_000099_387
[2021-05-14 11:19:58,108] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Task committer attempt_202105141416433746137966554836124_0004_m_000099_387: needsTaskCommit() Task attempt_202105141416433746137966554836124_0004_m_000099_387: duration 0:00.003s
21/05/14 14:19:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433746137966554836124_0004_m_000099_387
[2021-05-14 11:19:58,112] {docker.py:276} INFO - 21/05/14 14:19:58 INFO Executor: Finished task 99.0 in stage 4.0 (TID 387). 5149 bytes result sent to driver
[2021-05-14 11:19:58,114] {docker.py:276} INFO - 21/05/14 14:19:58 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 391) (0d1d3e01326c, executor driver, partition 103, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:58,114] {docker.py:276} INFO - 21/05/14 14:19:58 INFO Executor: Running task 103.0 in stage 4.0 (TID 391)
[2021-05-14 11:19:58,114] {docker.py:276} INFO - 21/05/14 14:19:58 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 387) in 7025 ms on 0d1d3e01326c (executor driver) (100/200)
[2021-05-14 11:19:58,122] {docker.py:276} INFO - 21/05/14 14:19:58 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:19:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:58,130] {docker.py:276} INFO - 21/05/14 14:19:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:19:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437876607740711557903_0004_m_000103_391, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437876607740711557903_0004_m_000103_391}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437876607740711557903_0004}; taskId=attempt_202105141416437876607740711557903_0004_m_000103_391, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3ebc65c6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:19:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:19:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416437876607740711557903_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437876607740711557903_0004_m_000103_391
[2021-05-14 11:19:58,134] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Task committer attempt_202105141416437876607740711557903_0004_m_000103_391: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437876607740711557903_0004_m_000103_391 : duration 0:00.003s
[2021-05-14 11:19:58,918] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416436773364082462351992_0004_m_000100_388: needsTaskCommit() Task attempt_202105141416436773364082462351992_0004_m_000100_388
[2021-05-14 11:19:58,918] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Task committer attempt_202105141416436773364082462351992_0004_m_000100_388: needsTaskCommit() Task attempt_202105141416436773364082462351992_0004_m_000100_388: duration 0:00.001s
21/05/14 14:19:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436773364082462351992_0004_m_000100_388
[2021-05-14 11:19:58,920] {docker.py:276} INFO - 21/05/14 14:19:58 INFO Executor: Finished task 100.0 in stage 4.0 (TID 388). 5149 bytes result sent to driver
[2021-05-14 11:19:58,923] {docker.py:276} INFO - 21/05/14 14:19:58 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 392) (0d1d3e01326c, executor driver, partition 104, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:19:58,923] {docker.py:276} INFO - 21/05/14 14:19:58 INFO Executor: Running task 104.0 in stage 4.0 (TID 392)
[2021-05-14 11:19:58,924] {docker.py:276} INFO - 21/05/14 14:19:58 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 388) in 7149 ms on 0d1d3e01326c (executor driver) (101/200)
[2021-05-14 11:19:58,932] {docker.py:276} INFO - 21/05/14 14:19:58 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:19:58,932] {docker.py:276} INFO - 21/05/14 14:19:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:19:58,942] {docker.py:276} INFO - 21/05/14 14:19:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:19:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:19:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:58,942] {docker.py:276} INFO - 21/05/14 14:19:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436530602037951233134_0004_m_000104_392, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530602037951233134_0004_m_000104_392}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436530602037951233134_0004}; taskId=attempt_202105141416436530602037951233134_0004_m_000104_392, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ad4cc6b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:19:58,943] {docker.py:276} INFO - 21/05/14 14:19:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:19:58,943] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416436530602037951233134_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530602037951233134_0004_m_000104_392
[2021-05-14 11:19:58,947] {docker.py:276} INFO - 21/05/14 14:19:58 INFO StagingCommitter: Task committer attempt_202105141416436530602037951233134_0004_m_000104_392: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530602037951233134_0004_m_000104_392 : duration 0:00.004s
[2021-05-14 11:20:01,221] {docker.py:276} INFO - 21/05/14 14:20:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416433028222054776244917_0004_m_000101_389: needsTaskCommit() Task attempt_202105141416433028222054776244917_0004_m_000101_389
[2021-05-14 11:20:01,222] {docker.py:276} INFO - 21/05/14 14:20:01 INFO StagingCommitter: Task committer attempt_202105141416433028222054776244917_0004_m_000101_389: needsTaskCommit() Task attempt_202105141416433028222054776244917_0004_m_000101_389: duration 0:00.001s
21/05/14 14:20:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433028222054776244917_0004_m_000101_389
[2021-05-14 11:20:01,223] {docker.py:276} INFO - 21/05/14 14:20:01 INFO Executor: Finished task 101.0 in stage 4.0 (TID 389). 5149 bytes result sent to driver
[2021-05-14 11:20:01,224] {docker.py:276} INFO - 21/05/14 14:20:01 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 393) (0d1d3e01326c, executor driver, partition 105, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:01,225] {docker.py:276} INFO - 21/05/14 14:20:01 INFO Executor: Running task 105.0 in stage 4.0 (TID 393)
[2021-05-14 11:20:01,225] {docker.py:276} INFO - 21/05/14 14:20:01 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 389) in 6888 ms on 0d1d3e01326c (executor driver) (102/200)
[2021-05-14 11:20:01,232] {docker.py:276} INFO - 21/05/14 14:20:01 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:01,240] {docker.py:276} INFO - 21/05/14 14:20:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:01,240] {docker.py:276} INFO - 21/05/14 14:20:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:01,241] {docker.py:276} INFO - 21/05/14 14:20:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437176599220360620468_0004_m_000105_393, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437176599220360620468_0004_m_000105_393}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437176599220360620468_0004}; taskId=attempt_202105141416437176599220360620468_0004_m_000105_393, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@75478dd3}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:01,241] {docker.py:276} INFO - 21/05/14 14:20:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416437176599220360620468_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437176599220360620468_0004_m_000105_393
[2021-05-14 11:20:01,245] {docker.py:276} INFO - 21/05/14 14:20:01 INFO StagingCommitter: Task committer attempt_202105141416437176599220360620468_0004_m_000105_393: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437176599220360620468_0004_m_000105_393 : duration 0:00.005s
[2021-05-14 11:20:02,468] {docker.py:276} INFO - 21/05/14 14:20:02 INFO StagingCommitter: Starting: Task committer attempt_202105141416438937754081037420804_0004_m_000102_390: needsTaskCommit() Task attempt_202105141416438937754081037420804_0004_m_000102_390
[2021-05-14 11:20:02,470] {docker.py:276} INFO - 21/05/14 14:20:02 INFO StagingCommitter: Task committer attempt_202105141416438937754081037420804_0004_m_000102_390: needsTaskCommit() Task attempt_202105141416438937754081037420804_0004_m_000102_390: duration 0:00.001s
21/05/14 14:20:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438937754081037420804_0004_m_000102_390
[2021-05-14 11:20:02,471] {docker.py:276} INFO - 21/05/14 14:20:02 INFO Executor: Finished task 102.0 in stage 4.0 (TID 390). 5149 bytes result sent to driver
[2021-05-14 11:20:02,471] {docker.py:276} INFO - 21/05/14 14:20:02 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 394) (0d1d3e01326c, executor driver, partition 106, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:02,472] {docker.py:276} INFO - 21/05/14 14:20:02 INFO Executor: Running task 106.0 in stage 4.0 (TID 394)
[2021-05-14 11:20:02,474] {docker.py:276} INFO - 21/05/14 14:20:02 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 390) in 7003 ms on 0d1d3e01326c (executor driver) (103/200)
[2021-05-14 11:20:02,482] {docker.py:276} INFO - 21/05/14 14:20:02 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:02,490] {docker.py:276} INFO - 21/05/14 14:20:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643830852553071960901_0004_m_000106_394, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643830852553071960901_0004_m_000106_394}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643830852553071960901_0004}; taskId=attempt_20210514141643830852553071960901_0004_m_000106_394, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53179044}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:02 INFO StagingCommitter: Starting: Task committer attempt_20210514141643830852553071960901_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643830852553071960901_0004_m_000106_394
[2021-05-14 11:20:02,495] {docker.py:276} INFO - 21/05/14 14:20:02 INFO StagingCommitter: Task committer attempt_20210514141643830852553071960901_0004_m_000106_394: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643830852553071960901_0004_m_000106_394 : duration 0:00.005s
[2021-05-14 11:20:05,109] {docker.py:276} INFO - 21/05/14 14:20:05 INFO StagingCommitter: Starting: Task committer attempt_202105141416437876607740711557903_0004_m_000103_391: needsTaskCommit() Task attempt_202105141416437876607740711557903_0004_m_000103_391
[2021-05-14 11:20:05,110] {docker.py:276} INFO - 21/05/14 14:20:05 INFO StagingCommitter: Task committer attempt_202105141416437876607740711557903_0004_m_000103_391: needsTaskCommit() Task attempt_202105141416437876607740711557903_0004_m_000103_391: duration 0:00.002s
21/05/14 14:20:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437876607740711557903_0004_m_000103_391
[2021-05-14 11:20:05,112] {docker.py:276} INFO - 21/05/14 14:20:05 INFO Executor: Finished task 103.0 in stage 4.0 (TID 391). 5106 bytes result sent to driver
[2021-05-14 11:20:05,113] {docker.py:276} INFO - 21/05/14 14:20:05 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 395) (0d1d3e01326c, executor driver, partition 107, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:05,114] {docker.py:276} INFO - 21/05/14 14:20:05 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 391) in 7009 ms on 0d1d3e01326c (executor driver) (104/200)
[2021-05-14 11:20:05,114] {docker.py:276} INFO - 21/05/14 14:20:05 INFO Executor: Running task 107.0 in stage 4.0 (TID 395)
[2021-05-14 11:20:05,123] {docker.py:276} INFO - 21/05/14 14:20:05 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:20:05,124] {docker.py:276} INFO - 21/05/14 14:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:05,132] {docker.py:276} INFO - 21/05/14 14:20:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:20:05,132] {docker.py:276} INFO - 21/05/14 14:20:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:05,133] {docker.py:276} INFO - 21/05/14 14:20:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:05,133] {docker.py:276} INFO - 21/05/14 14:20:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431384564371042158649_0004_m_000107_395, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431384564371042158649_0004_m_000107_395}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431384564371042158649_0004}; taskId=attempt_202105141416431384564371042158649_0004_m_000107_395, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@25f9f1af}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:05,133] {docker.py:276} INFO - 21/05/14 14:20:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:05,134] {docker.py:276} INFO - 21/05/14 14:20:05 INFO StagingCommitter: Starting: Task committer attempt_202105141416431384564371042158649_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431384564371042158649_0004_m_000107_395
[2021-05-14 11:20:05,139] {docker.py:276} INFO - 21/05/14 14:20:05 INFO StagingCommitter: Task committer attempt_202105141416431384564371042158649_0004_m_000107_395: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431384564371042158649_0004_m_000107_395 : duration 0:00.006s
[2021-05-14 11:20:06,036] {docker.py:276} INFO - 21/05/14 14:20:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416436530602037951233134_0004_m_000104_392: needsTaskCommit() Task attempt_202105141416436530602037951233134_0004_m_000104_392
21/05/14 14:20:06 INFO StagingCommitter: Task committer attempt_202105141416436530602037951233134_0004_m_000104_392: needsTaskCommit() Task attempt_202105141416436530602037951233134_0004_m_000104_392: duration 0:00.002s
21/05/14 14:20:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436530602037951233134_0004_m_000104_392
[2021-05-14 11:20:06,039] {docker.py:276} INFO - 21/05/14 14:20:06 INFO Executor: Finished task 104.0 in stage 4.0 (TID 392). 5149 bytes result sent to driver
[2021-05-14 11:20:06,040] {docker.py:276} INFO - 21/05/14 14:20:06 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 396) (0d1d3e01326c, executor driver, partition 108, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:06,041] {docker.py:276} INFO - 21/05/14 14:20:06 INFO Executor: Running task 108.0 in stage 4.0 (TID 396)
21/05/14 14:20:06 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 392) in 7129 ms on 0d1d3e01326c (executor driver) (105/200)
[2021-05-14 11:20:06,052] {docker.py:276} INFO - 21/05/14 14:20:06 INFO ShuffleBlockFetcherIterator: Getting 5 (41.8 KiB) non-empty blocks including 5 (41.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:06,060] {docker.py:276} INFO - 21/05/14 14:20:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:06,061] {docker.py:276} INFO - 21/05/14 14:20:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436509660355535856891_0004_m_000108_396, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436509660355535856891_0004_m_000108_396}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436509660355535856891_0004}; taskId=attempt_202105141416436509660355535856891_0004_m_000108_396, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63adf0b1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416436509660355535856891_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436509660355535856891_0004_m_000108_396
[2021-05-14 11:20:06,065] {docker.py:276} INFO - 21/05/14 14:20:06 INFO StagingCommitter: Task committer attempt_202105141416436509660355535856891_0004_m_000108_396: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436509660355535856891_0004_m_000108_396 : duration 0:00.005s
[2021-05-14 11:20:08,250] {docker.py:276} INFO - 21/05/14 14:20:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416437176599220360620468_0004_m_000105_393: needsTaskCommit() Task attempt_202105141416437176599220360620468_0004_m_000105_393
[2021-05-14 11:20:08,251] {docker.py:276} INFO - 21/05/14 14:20:08 INFO StagingCommitter: Task committer attempt_202105141416437176599220360620468_0004_m_000105_393: needsTaskCommit() Task attempt_202105141416437176599220360620468_0004_m_000105_393: duration 0:00.002s
21/05/14 14:20:08 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437176599220360620468_0004_m_000105_393
[2021-05-14 11:20:08,252] {docker.py:276} INFO - 21/05/14 14:20:08 INFO Executor: Finished task 105.0 in stage 4.0 (TID 393). 5149 bytes result sent to driver
[2021-05-14 11:20:08,253] {docker.py:276} INFO - 21/05/14 14:20:08 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 397) (0d1d3e01326c, executor driver, partition 109, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:08,255] {docker.py:276} INFO - 21/05/14 14:20:08 INFO Executor: Running task 109.0 in stage 4.0 (TID 397)
[2021-05-14 11:20:08,256] {docker.py:276} INFO - 21/05/14 14:20:08 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 393) in 7040 ms on 0d1d3e01326c (executor driver) (106/200)
[2021-05-14 11:20:08,266] {docker.py:276} INFO - 21/05/14 14:20:08 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:08,279] {docker.py:276} INFO - 21/05/14 14:20:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:08,279] {docker.py:276} INFO - 21/05/14 14:20:08 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:08,280] {docker.py:276} INFO - 21/05/14 14:20:08 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439068489547113188374_0004_m_000109_397, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439068489547113188374_0004_m_000109_397}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439068489547113188374_0004}; taskId=attempt_202105141416439068489547113188374_0004_m_000109_397, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4a1f48f1}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:08,280] {docker.py:276} INFO - 21/05/14 14:20:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:08,280] {docker.py:276} INFO - 21/05/14 14:20:08 INFO StagingCommitter: Starting: Task committer attempt_202105141416439068489547113188374_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439068489547113188374_0004_m_000109_397
[2021-05-14 11:20:08,284] {docker.py:276} INFO - 21/05/14 14:20:08 INFO StagingCommitter: Task committer attempt_202105141416439068489547113188374_0004_m_000109_397: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439068489547113188374_0004_m_000109_397 : duration 0:00.004s
[2021-05-14 11:20:09,238] {docker.py:276} INFO - 21/05/14 14:20:09 INFO StagingCommitter: Starting: Task committer attempt_20210514141643830852553071960901_0004_m_000106_394: needsTaskCommit() Task attempt_20210514141643830852553071960901_0004_m_000106_394
[2021-05-14 11:20:09,239] {docker.py:276} INFO - 21/05/14 14:20:09 INFO StagingCommitter: Task committer attempt_20210514141643830852553071960901_0004_m_000106_394: needsTaskCommit() Task attempt_20210514141643830852553071960901_0004_m_000106_394: duration 0:00.002s
21/05/14 14:20:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643830852553071960901_0004_m_000106_394
[2021-05-14 11:20:09,241] {docker.py:276} INFO - 21/05/14 14:20:09 INFO Executor: Finished task 106.0 in stage 4.0 (TID 394). 5149 bytes result sent to driver
[2021-05-14 11:20:09,243] {docker.py:276} INFO - 21/05/14 14:20:09 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 398) (0d1d3e01326c, executor driver, partition 110, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:09,244] {docker.py:276} INFO - 21/05/14 14:20:09 INFO Executor: Running task 110.0 in stage 4.0 (TID 398)
[2021-05-14 11:20:09,244] {docker.py:276} INFO - 21/05/14 14:20:09 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 394) in 6781 ms on 0d1d3e01326c (executor driver) (107/200)
[2021-05-14 11:20:09,253] {docker.py:276} INFO - 21/05/14 14:20:09 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:20:09,254] {docker.py:276} INFO - 21/05/14 14:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:09,262] {docker.py:276} INFO - 21/05/14 14:20:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:09,262] {docker.py:276} INFO - 21/05/14 14:20:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438986702738838237602_0004_m_000110_398, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438986702738838237602_0004_m_000110_398}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438986702738838237602_0004}; taskId=attempt_202105141416438986702738838237602_0004_m_000110_398, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@71efe897}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:09,262] {docker.py:276} INFO - 21/05/14 14:20:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416438986702738838237602_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438986702738838237602_0004_m_000110_398
[2021-05-14 11:20:09,267] {docker.py:276} INFO - 21/05/14 14:20:09 INFO StagingCommitter: Task committer attempt_202105141416438986702738838237602_0004_m_000110_398: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438986702738838237602_0004_m_000110_398 : duration 0:00.005s
[2021-05-14 11:20:12,311] {docker.py:276} INFO - 21/05/14 14:20:12 INFO StagingCommitter: Starting: Task committer attempt_202105141416431384564371042158649_0004_m_000107_395: needsTaskCommit() Task attempt_202105141416431384564371042158649_0004_m_000107_395
[2021-05-14 11:20:12,312] {docker.py:276} INFO - 21/05/14 14:20:12 INFO StagingCommitter: Task committer attempt_202105141416431384564371042158649_0004_m_000107_395: needsTaskCommit() Task attempt_202105141416431384564371042158649_0004_m_000107_395: duration 0:00.002s
21/05/14 14:20:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431384564371042158649_0004_m_000107_395
[2021-05-14 11:20:12,315] {docker.py:276} INFO - 21/05/14 14:20:12 INFO Executor: Finished task 107.0 in stage 4.0 (TID 395). 5149 bytes result sent to driver
21/05/14 14:20:12 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 399) (0d1d3e01326c, executor driver, partition 111, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:12,316] {docker.py:276} INFO - 21/05/14 14:20:12 INFO Executor: Running task 111.0 in stage 4.0 (TID 399)
21/05/14 14:20:12 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 395) in 7177 ms on 0d1d3e01326c (executor driver) (108/200)
[2021-05-14 11:20:12,326] {docker.py:276} INFO - 21/05/14 14:20:12 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:12,334] {docker.py:276} INFO - 21/05/14 14:20:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436697062732247815715_0004_m_000111_399, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436697062732247815715_0004_m_000111_399}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436697062732247815715_0004}; taskId=attempt_202105141416436697062732247815715_0004_m_000111_399, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@13cc93b2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:12 INFO StagingCommitter: Starting: Task committer attempt_202105141416436697062732247815715_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436697062732247815715_0004_m_000111_399
[2021-05-14 11:20:12,338] {docker.py:276} INFO - 21/05/14 14:20:12 INFO StagingCommitter: Task committer attempt_202105141416436697062732247815715_0004_m_000111_399: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436697062732247815715_0004_m_000111_399 : duration 0:00.004s
[2021-05-14 11:20:13,272] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416436509660355535856891_0004_m_000108_396: needsTaskCommit() Task attempt_202105141416436509660355535856891_0004_m_000108_396
[2021-05-14 11:20:13,273] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Task committer attempt_202105141416436509660355535856891_0004_m_000108_396: needsTaskCommit() Task attempt_202105141416436509660355535856891_0004_m_000108_396: duration 0:00.001s
21/05/14 14:20:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436509660355535856891_0004_m_000108_396
[2021-05-14 11:20:13,274] {docker.py:276} INFO - 21/05/14 14:20:13 INFO Executor: Finished task 108.0 in stage 4.0 (TID 396). 5106 bytes result sent to driver
[2021-05-14 11:20:13,275] {docker.py:276} INFO - 21/05/14 14:20:13 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 400) (0d1d3e01326c, executor driver, partition 112, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:13,276] {docker.py:276} INFO - 21/05/14 14:20:13 INFO Executor: Running task 112.0 in stage 4.0 (TID 400)
[2021-05-14 11:20:13,276] {docker.py:276} INFO - 21/05/14 14:20:13 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 396) in 7210 ms on 0d1d3e01326c (executor driver) (109/200)
[2021-05-14 11:20:13,284] {docker.py:276} INFO - 21/05/14 14:20:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:13,293] {docker.py:276} INFO - 21/05/14 14:20:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436892292736082377518_0004_m_000112_400, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436892292736082377518_0004_m_000112_400}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436892292736082377518_0004}; taskId=attempt_202105141416436892292736082377518_0004_m_000112_400, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@19884347}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:13,293] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416436892292736082377518_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436892292736082377518_0004_m_000112_400
[2021-05-14 11:20:13,298] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Task committer attempt_202105141416436892292736082377518_0004_m_000112_400: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436892292736082377518_0004_m_000112_400 : duration 0:00.004s
[2021-05-14 11:20:13,890] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416439068489547113188374_0004_m_000109_397: needsTaskCommit() Task attempt_202105141416439068489547113188374_0004_m_000109_397
[2021-05-14 11:20:13,891] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Task committer attempt_202105141416439068489547113188374_0004_m_000109_397: needsTaskCommit() Task attempt_202105141416439068489547113188374_0004_m_000109_397: duration 0:00.001s
21/05/14 14:20:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439068489547113188374_0004_m_000109_397
[2021-05-14 11:20:13,893] {docker.py:276} INFO - 21/05/14 14:20:13 INFO Executor: Finished task 109.0 in stage 4.0 (TID 397). 5106 bytes result sent to driver
[2021-05-14 11:20:13,893] {docker.py:276} INFO - 21/05/14 14:20:13 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 401) (0d1d3e01326c, executor driver, partition 113, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:13,894] {docker.py:276} INFO - 21/05/14 14:20:13 INFO Executor: Running task 113.0 in stage 4.0 (TID 401)
[2021-05-14 11:20:13,895] {docker.py:276} INFO - 21/05/14 14:20:13 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 397) in 5613 ms on 0d1d3e01326c (executor driver) (110/200)
[2021-05-14 11:20:13,911] {docker.py:276} INFO - 21/05/14 14:20:13 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:13,919] {docker.py:276} INFO - 21/05/14 14:20:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:13 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:13 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432005958380904879623_0004_m_000113_401, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432005958380904879623_0004_m_000113_401}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432005958380904879623_0004}; taskId=attempt_202105141416432005958380904879623_0004_m_000113_401, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5b05a1a0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:13,920] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Starting: Task committer attempt_202105141416432005958380904879623_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432005958380904879623_0004_m_000113_401
[2021-05-14 11:20:13,924] {docker.py:276} INFO - 21/05/14 14:20:13 INFO StagingCommitter: Task committer attempt_202105141416432005958380904879623_0004_m_000113_401: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432005958380904879623_0004_m_000113_401 : duration 0:00.004s
[2021-05-14 11:20:16,093] {docker.py:276} INFO - 21/05/14 14:20:16 INFO StagingCommitter: Starting: Task committer attempt_202105141416438986702738838237602_0004_m_000110_398: needsTaskCommit() Task attempt_202105141416438986702738838237602_0004_m_000110_398
[2021-05-14 11:20:16,093] {docker.py:276} INFO - 21/05/14 14:20:16 INFO StagingCommitter: Task committer attempt_202105141416438986702738838237602_0004_m_000110_398: needsTaskCommit() Task attempt_202105141416438986702738838237602_0004_m_000110_398: duration 0:00.002s
21/05/14 14:20:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438986702738838237602_0004_m_000110_398
[2021-05-14 11:20:16,096] {docker.py:276} INFO - 21/05/14 14:20:16 INFO Executor: Finished task 110.0 in stage 4.0 (TID 398). 5149 bytes result sent to driver
[2021-05-14 11:20:16,097] {docker.py:276} INFO - 21/05/14 14:20:16 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 402) (0d1d3e01326c, executor driver, partition 114, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:16,097] {docker.py:276} INFO - 21/05/14 14:20:16 INFO Executor: Running task 114.0 in stage 4.0 (TID 402)
[2021-05-14 11:20:16,098] {docker.py:276} INFO - 21/05/14 14:20:16 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 398) in 6828 ms on 0d1d3e01326c (executor driver) (111/200)
[2021-05-14 11:20:16,108] {docker.py:276} INFO - 21/05/14 14:20:16 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:16,117] {docker.py:276} INFO - 21/05/14 14:20:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:16 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:16 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643182017156320505174_0004_m_000114_402, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643182017156320505174_0004_m_000114_402}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643182017156320505174_0004}; taskId=attempt_20210514141643182017156320505174_0004_m_000114_402, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@65fe1960}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:16 INFO StagingCommitter: Starting: Task committer attempt_20210514141643182017156320505174_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643182017156320505174_0004_m_000114_402
[2021-05-14 11:20:16,121] {docker.py:276} INFO - 21/05/14 14:20:16 INFO StagingCommitter: Task committer attempt_20210514141643182017156320505174_0004_m_000114_402: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643182017156320505174_0004_m_000114_402 : duration 0:00.004s
[2021-05-14 11:20:19,321] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Starting: Task committer attempt_202105141416436892292736082377518_0004_m_000112_400: needsTaskCommit() Task attempt_202105141416436892292736082377518_0004_m_000112_400
[2021-05-14 11:20:19,322] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Task committer attempt_202105141416436892292736082377518_0004_m_000112_400: needsTaskCommit() Task attempt_202105141416436892292736082377518_0004_m_000112_400: duration 0:00.002s
[2021-05-14 11:20:19,323] {docker.py:276} INFO - 21/05/14 14:20:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436892292736082377518_0004_m_000112_400
[2021-05-14 11:20:19,324] {docker.py:276} INFO - 21/05/14 14:20:19 INFO Executor: Finished task 112.0 in stage 4.0 (TID 400). 5149 bytes result sent to driver
[2021-05-14 11:20:19,326] {docker.py:276} INFO - 21/05/14 14:20:19 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 403) (0d1d3e01326c, executor driver, partition 115, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:19,327] {docker.py:276} INFO - 21/05/14 14:20:19 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 400) in 6059 ms on 0d1d3e01326c (executor driver) (112/200)
[2021-05-14 11:20:19,328] {docker.py:276} INFO - 21/05/14 14:20:19 INFO Executor: Running task 115.0 in stage 4.0 (TID 403)
[2021-05-14 11:20:19,338] {docker.py:276} INFO - 21/05/14 14:20:19 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:19,346] {docker.py:276} INFO - 21/05/14 14:20:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643362251563476980664_0004_m_000115_403, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643362251563476980664_0004_m_000115_403}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643362251563476980664_0004}; taskId=attempt_20210514141643362251563476980664_0004_m_000115_403, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@53bad3d8}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:19 INFO StagingCommitter: Starting: Task committer attempt_20210514141643362251563476980664_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643362251563476980664_0004_m_000115_403
[2021-05-14 11:20:19,350] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Task committer attempt_20210514141643362251563476980664_0004_m_000115_403: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643362251563476980664_0004_m_000115_403 : duration 0:00.003s
[2021-05-14 11:20:19,398] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Starting: Task committer attempt_202105141416436697062732247815715_0004_m_000111_399: needsTaskCommit() Task attempt_202105141416436697062732247815715_0004_m_000111_399
21/05/14 14:20:19 INFO StagingCommitter: Task committer attempt_202105141416436697062732247815715_0004_m_000111_399: needsTaskCommit() Task attempt_202105141416436697062732247815715_0004_m_000111_399: duration 0:00.002s
21/05/14 14:20:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436697062732247815715_0004_m_000111_399
[2021-05-14 11:20:19,402] {docker.py:276} INFO - 21/05/14 14:20:19 INFO Executor: Finished task 111.0 in stage 4.0 (TID 399). 5149 bytes result sent to driver
[2021-05-14 11:20:19,403] {docker.py:276} INFO - 21/05/14 14:20:19 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 404) (0d1d3e01326c, executor driver, partition 116, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:19,404] {docker.py:276} INFO - 21/05/14 14:20:19 INFO Executor: Running task 116.0 in stage 4.0 (TID 404)
[2021-05-14 11:20:19,405] {docker.py:276} INFO - 21/05/14 14:20:19 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 399) in 7099 ms on 0d1d3e01326c (executor driver) (113/200)
[2021-05-14 11:20:19,414] {docker.py:276} INFO - 21/05/14 14:20:19 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:19,422] {docker.py:276} INFO - 21/05/14 14:20:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:19,423] {docker.py:276} INFO - 21/05/14 14:20:19 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:19,423] {docker.py:276} INFO - 21/05/14 14:20:19 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432880416028415585809_0004_m_000116_404, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432880416028415585809_0004_m_000116_404}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432880416028415585809_0004}; taskId=attempt_202105141416432880416028415585809_0004_m_000116_404, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61683997}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:19,423] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Starting: Task committer attempt_202105141416432880416028415585809_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432880416028415585809_0004_m_000116_404
[2021-05-14 11:20:19,427] {docker.py:276} INFO - 21/05/14 14:20:19 INFO StagingCommitter: Task committer attempt_202105141416432880416028415585809_0004_m_000116_404: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432880416028415585809_0004_m_000116_404 : duration 0:00.004s
[2021-05-14 11:20:21,188] {docker.py:276} INFO - 21/05/14 14:20:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416432005958380904879623_0004_m_000113_401: needsTaskCommit() Task attempt_202105141416432005958380904879623_0004_m_000113_401
[2021-05-14 11:20:21,189] {docker.py:276} INFO - 21/05/14 14:20:21 INFO StagingCommitter: Task committer attempt_202105141416432005958380904879623_0004_m_000113_401: needsTaskCommit() Task attempt_202105141416432005958380904879623_0004_m_000113_401: duration 0:00.003s
21/05/14 14:20:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432005958380904879623_0004_m_000113_401
[2021-05-14 11:20:21,190] {docker.py:276} INFO - 21/05/14 14:20:21 INFO Executor: Finished task 113.0 in stage 4.0 (TID 401). 5149 bytes result sent to driver
[2021-05-14 11:20:21,192] {docker.py:276} INFO - 21/05/14 14:20:21 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 405) (0d1d3e01326c, executor driver, partition 117, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:21,193] {docker.py:276} INFO - 21/05/14 14:20:21 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 401) in 7308 ms on 0d1d3e01326c (executor driver) (114/200)
21/05/14 14:20:21 INFO Executor: Running task 117.0 in stage 4.0 (TID 405)
[2021-05-14 11:20:21,203] {docker.py:276} INFO - 21/05/14 14:20:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:21,212] {docker.py:276} INFO - 21/05/14 14:20:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:21,213] {docker.py:276} INFO - 21/05/14 14:20:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438990630165196772871_0004_m_000117_405, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438990630165196772871_0004_m_000117_405}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438990630165196772871_0004}; taskId=attempt_202105141416438990630165196772871_0004_m_000117_405, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@129ab2d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:21,213] {docker.py:276} INFO - 21/05/14 14:20:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416438990630165196772871_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438990630165196772871_0004_m_000117_405
[2021-05-14 11:20:21,219] {docker.py:276} INFO - 21/05/14 14:20:21 INFO StagingCommitter: Task committer attempt_202105141416438990630165196772871_0004_m_000117_405: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438990630165196772871_0004_m_000117_405 : duration 0:00.007s
[2021-05-14 11:20:22,947] {docker.py:276} INFO - 21/05/14 14:20:22 INFO StagingCommitter: Starting: Task committer attempt_20210514141643182017156320505174_0004_m_000114_402: needsTaskCommit() Task attempt_20210514141643182017156320505174_0004_m_000114_402
[2021-05-14 11:20:22,947] {docker.py:276} INFO - 21/05/14 14:20:22 INFO StagingCommitter: Task committer attempt_20210514141643182017156320505174_0004_m_000114_402: needsTaskCommit() Task attempt_20210514141643182017156320505174_0004_m_000114_402: duration 0:00.004s
21/05/14 14:20:22 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643182017156320505174_0004_m_000114_402
[2021-05-14 11:20:22,949] {docker.py:276} INFO - 21/05/14 14:20:22 INFO Executor: Finished task 114.0 in stage 4.0 (TID 402). 5106 bytes result sent to driver
[2021-05-14 11:20:22,952] {docker.py:276} INFO - 21/05/14 14:20:22 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 406) (0d1d3e01326c, executor driver, partition 118, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:22,952] {docker.py:276} INFO - 21/05/14 14:20:22 INFO Executor: Running task 118.0 in stage 4.0 (TID 406)
21/05/14 14:20:22 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 402) in 6864 ms on 0d1d3e01326c (executor driver) (115/200)
[2021-05-14 11:20:22,962] {docker.py:276} INFO - 21/05/14 14:20:22 INFO ShuffleBlockFetcherIterator: Getting 5 (45.7 KiB) non-empty blocks including 5 (45.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:22,970] {docker.py:276} INFO - 21/05/14 14:20:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:22 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:22 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434910873105648347603_0004_m_000118_406, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434910873105648347603_0004_m_000118_406}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434910873105648347603_0004}; taskId=attempt_202105141416434910873105648347603_0004_m_000118_406, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1c06f329}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:22,970] {docker.py:276} INFO - 21/05/14 14:20:22 INFO StagingCommitter: Starting: Task committer attempt_202105141416434910873105648347603_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434910873105648347603_0004_m_000118_406
[2021-05-14 11:20:22,975] {docker.py:276} INFO - 21/05/14 14:20:22 INFO StagingCommitter: Task committer attempt_202105141416434910873105648347603_0004_m_000118_406: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434910873105648347603_0004_m_000118_406 : duration 0:00.005s
[2021-05-14 11:20:26,165] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Starting: Task committer attempt_202105141416432880416028415585809_0004_m_000116_404: needsTaskCommit() Task attempt_202105141416432880416028415585809_0004_m_000116_404
[2021-05-14 11:20:26,165] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Task committer attempt_202105141416432880416028415585809_0004_m_000116_404: needsTaskCommit() Task attempt_202105141416432880416028415585809_0004_m_000116_404: duration 0:00.001s
21/05/14 14:20:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432880416028415585809_0004_m_000116_404
[2021-05-14 11:20:26,168] {docker.py:276} INFO - 21/05/14 14:20:26 INFO Executor: Finished task 116.0 in stage 4.0 (TID 404). 5149 bytes result sent to driver
21/05/14 14:20:26 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 407) (0d1d3e01326c, executor driver, partition 119, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:26,168] {docker.py:276} INFO - 21/05/14 14:20:26 INFO Executor: Running task 119.0 in stage 4.0 (TID 407)
[2021-05-14 11:20:26,169] {docker.py:276} INFO - 21/05/14 14:20:26 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 404) in 6774 ms on 0d1d3e01326c (executor driver) (116/200)
[2021-05-14 11:20:26,179] {docker.py:276} INFO - 21/05/14 14:20:26 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:26,189] {docker.py:276} INFO - 21/05/14 14:20:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:26,189] {docker.py:276} INFO - 21/05/14 14:20:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438581640281272890698_0004_m_000119_407, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438581640281272890698_0004_m_000119_407}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438581640281272890698_0004}; taskId=attempt_202105141416438581640281272890698_0004_m_000119_407, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c58c51d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:26 INFO StagingCommitter: Starting: Task committer attempt_202105141416438581640281272890698_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438581640281272890698_0004_m_000119_407
[2021-05-14 11:20:26,193] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Task committer attempt_202105141416438581640281272890698_0004_m_000119_407: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438581640281272890698_0004_m_000119_407 : duration 0:00.004s
[2021-05-14 11:20:26,225] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Starting: Task committer attempt_20210514141643362251563476980664_0004_m_000115_403: needsTaskCommit() Task attempt_20210514141643362251563476980664_0004_m_000115_403
[2021-05-14 11:20:26,226] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Task committer attempt_20210514141643362251563476980664_0004_m_000115_403: needsTaskCommit() Task attempt_20210514141643362251563476980664_0004_m_000115_403: duration 0:00.002s
21/05/14 14:20:26 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643362251563476980664_0004_m_000115_403
[2021-05-14 11:20:26,228] {docker.py:276} INFO - 21/05/14 14:20:26 INFO Executor: Finished task 115.0 in stage 4.0 (TID 403). 5149 bytes result sent to driver
[2021-05-14 11:20:26,229] {docker.py:276} INFO - 21/05/14 14:20:26 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 408) (0d1d3e01326c, executor driver, partition 120, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:26,230] {docker.py:276} INFO - 21/05/14 14:20:26 INFO Executor: Running task 120.0 in stage 4.0 (TID 408)
[2021-05-14 11:20:26,233] {docker.py:276} INFO - 21/05/14 14:20:26 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 403) in 6916 ms on 0d1d3e01326c (executor driver) (117/200)
[2021-05-14 11:20:26,242] {docker.py:276} INFO - 21/05/14 14:20:26 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:20:26,243] {docker.py:276} INFO - 21/05/14 14:20:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:20:26,251] {docker.py:276} INFO - 21/05/14 14:20:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:20:26,252] {docker.py:276} INFO - 21/05/14 14:20:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:26,252] {docker.py:276} INFO - 21/05/14 14:20:26 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:26,252] {docker.py:276} INFO - 21/05/14 14:20:26 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439185527850363365256_0004_m_000120_408, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439185527850363365256_0004_m_000120_408}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439185527850363365256_0004}; taskId=attempt_202105141416439185527850363365256_0004_m_000120_408, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66c7267e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:26,253] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Starting: Task committer attempt_202105141416439185527850363365256_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439185527850363365256_0004_m_000120_408
[2021-05-14 11:20:26,257] {docker.py:276} INFO - 21/05/14 14:20:26 INFO StagingCommitter: Task committer attempt_202105141416439185527850363365256_0004_m_000120_408: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439185527850363365256_0004_m_000120_408 : duration 0:00.005s
[2021-05-14 11:20:28,457] {docker.py:276} INFO - 21/05/14 14:20:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416438990630165196772871_0004_m_000117_405: needsTaskCommit() Task attempt_202105141416438990630165196772871_0004_m_000117_405
[2021-05-14 11:20:28,460] {docker.py:276} INFO - 21/05/14 14:20:28 INFO StagingCommitter: Task committer attempt_202105141416438990630165196772871_0004_m_000117_405: needsTaskCommit() Task attempt_202105141416438990630165196772871_0004_m_000117_405: duration 0:00.004s
21/05/14 14:20:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438990630165196772871_0004_m_000117_405
[2021-05-14 11:20:28,462] {docker.py:276} INFO - 21/05/14 14:20:28 INFO Executor: Finished task 117.0 in stage 4.0 (TID 405). 5149 bytes result sent to driver
[2021-05-14 11:20:28,463] {docker.py:276} INFO - 21/05/14 14:20:28 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 409) (0d1d3e01326c, executor driver, partition 121, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:28,464] {docker.py:276} INFO - 21/05/14 14:20:28 INFO Executor: Running task 121.0 in stage 4.0 (TID 409)
[2021-05-14 11:20:28,465] {docker.py:276} INFO - 21/05/14 14:20:28 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 405) in 7282 ms on 0d1d3e01326c (executor driver) (118/200)
[2021-05-14 11:20:28,475] {docker.py:276} INFO - 21/05/14 14:20:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:28,483] {docker.py:276} INFO - 21/05/14 14:20:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:28,483] {docker.py:276} INFO - 21/05/14 14:20:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433092980062404809401_0004_m_000121_409, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433092980062404809401_0004_m_000121_409}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433092980062404809401_0004}; taskId=attempt_202105141416433092980062404809401_0004_m_000121_409, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3364a247}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416433092980062404809401_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433092980062404809401_0004_m_000121_409
[2021-05-14 11:20:28,488] {docker.py:276} INFO - 21/05/14 14:20:28 INFO StagingCommitter: Task committer attempt_202105141416433092980062404809401_0004_m_000121_409: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433092980062404809401_0004_m_000121_409 : duration 0:00.004s
[2021-05-14 11:20:29,367] {docker.py:276} INFO - 21/05/14 14:20:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416434910873105648347603_0004_m_000118_406: needsTaskCommit() Task attempt_202105141416434910873105648347603_0004_m_000118_406
[2021-05-14 11:20:29,368] {docker.py:276} INFO - 21/05/14 14:20:29 INFO StagingCommitter: Task committer attempt_202105141416434910873105648347603_0004_m_000118_406: needsTaskCommit() Task attempt_202105141416434910873105648347603_0004_m_000118_406: duration 0:00.002s
21/05/14 14:20:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434910873105648347603_0004_m_000118_406
[2021-05-14 11:20:29,370] {docker.py:276} INFO - 21/05/14 14:20:29 INFO Executor: Finished task 118.0 in stage 4.0 (TID 406). 5149 bytes result sent to driver
[2021-05-14 11:20:29,371] {docker.py:276} INFO - 21/05/14 14:20:29 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 410) (0d1d3e01326c, executor driver, partition 122, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:29,372] {docker.py:276} INFO - 21/05/14 14:20:29 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 406) in 6430 ms on 0d1d3e01326c (executor driver) (119/200)
[2021-05-14 11:20:29,373] {docker.py:276} INFO - 21/05/14 14:20:29 INFO Executor: Running task 122.0 in stage 4.0 (TID 410)
[2021-05-14 11:20:29,383] {docker.py:276} INFO - 21/05/14 14:20:29 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:29,391] {docker.py:276} INFO - 21/05/14 14:20:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:29,391] {docker.py:276} INFO - 21/05/14 14:20:29 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:29,391] {docker.py:276} INFO - 21/05/14 14:20:29 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431579127700642062259_0004_m_000122_410, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431579127700642062259_0004_m_000122_410}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431579127700642062259_0004}; taskId=attempt_202105141416431579127700642062259_0004_m_000122_410, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1306ed58}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:29,392] {docker.py:276} INFO - 21/05/14 14:20:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:29,392] {docker.py:276} INFO - 21/05/14 14:20:29 INFO StagingCommitter: Starting: Task committer attempt_202105141416431579127700642062259_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431579127700642062259_0004_m_000122_410
[2021-05-14 11:20:29,397] {docker.py:276} INFO - 21/05/14 14:20:29 INFO StagingCommitter: Task committer attempt_202105141416431579127700642062259_0004_m_000122_410: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431579127700642062259_0004_m_000122_410 : duration 0:00.006s
[2021-05-14 11:20:32,996] {docker.py:276} INFO - 21/05/14 14:20:33 INFO StagingCommitter: Starting: Task committer attempt_202105141416438581640281272890698_0004_m_000119_407: needsTaskCommit() Task attempt_202105141416438581640281272890698_0004_m_000119_407
[2021-05-14 11:20:32,997] {docker.py:276} INFO - 21/05/14 14:20:33 INFO StagingCommitter: Task committer attempt_202105141416438581640281272890698_0004_m_000119_407: needsTaskCommit() Task attempt_202105141416438581640281272890698_0004_m_000119_407: duration 0:00.003s
21/05/14 14:20:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438581640281272890698_0004_m_000119_407
[2021-05-14 11:20:33,000] {docker.py:276} INFO - 21/05/14 14:20:33 INFO Executor: Finished task 119.0 in stage 4.0 (TID 407). 5106 bytes result sent to driver
[2021-05-14 11:20:33,001] {docker.py:276} INFO - 21/05/14 14:20:33 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 411) (0d1d3e01326c, executor driver, partition 123, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:33,002] {docker.py:276} INFO - 21/05/14 14:20:33 INFO StagingCommitter: Starting: Task committer attempt_202105141416439185527850363365256_0004_m_000120_408: needsTaskCommit() Task attempt_202105141416439185527850363365256_0004_m_000120_408
[2021-05-14 11:20:33,003] {docker.py:276} INFO - 21/05/14 14:20:33 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 407) in 6842 ms on 0d1d3e01326c (executor driver) (120/200)
21/05/14 14:20:33 INFO Executor: Running task 123.0 in stage 4.0 (TID 411)
[2021-05-14 11:20:33,004] {docker.py:276} INFO - 21/05/14 14:20:33 INFO StagingCommitter: Task committer attempt_202105141416439185527850363365256_0004_m_000120_408: needsTaskCommit() Task attempt_202105141416439185527850363365256_0004_m_000120_408: duration 0:00.002s
[2021-05-14 11:20:33,004] {docker.py:276} INFO - 21/05/14 14:20:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439185527850363365256_0004_m_000120_408
[2021-05-14 11:20:33,005] {docker.py:276} INFO - 21/05/14 14:20:33 INFO Executor: Finished task 120.0 in stage 4.0 (TID 408). 5106 bytes result sent to driver
[2021-05-14 11:20:33,006] {docker.py:276} INFO - 21/05/14 14:20:33 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 412) (0d1d3e01326c, executor driver, partition 124, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:33,007] {docker.py:276} INFO - 21/05/14 14:20:33 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 408) in 6785 ms on 0d1d3e01326c (executor driver) (121/200)
[2021-05-14 11:20:33,007] {docker.py:276} INFO - 21/05/14 14:20:33 INFO Executor: Running task 124.0 in stage 4.0 (TID 412)
[2021-05-14 11:20:33,023] {docker.py:276} INFO - 21/05/14 14:20:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:33,023] {docker.py:276} INFO - 21/05/14 14:20:33 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:33,032] {docker.py:276} INFO - 21/05/14 14:20:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643955779653081670389_0004_m_000123_411, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643955779653081670389_0004_m_000123_411}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643955779653081670389_0004}; taskId=attempt_20210514141643955779653081670389_0004_m_000123_411, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@c3ea913}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:33,032] {docker.py:276} INFO - 21/05/14 14:20:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:33,033] {docker.py:276} INFO - 21/05/14 14:20:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:33 INFO StagingCommitter: Starting: Task committer attempt_20210514141643955779653081670389_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643955779653081670389_0004_m_000123_411 
21/05/14 14:20:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:33,033] {docker.py:276} INFO - 21/05/14 14:20:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436023767000228108017_0004_m_000124_412, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436023767000228108017_0004_m_000124_412}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436023767000228108017_0004}; taskId=attempt_202105141416436023767000228108017_0004_m_000124_412, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@377e64d4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:33,034] {docker.py:276} INFO - 21/05/14 14:20:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:33 INFO StagingCommitter: Starting: Task committer attempt_202105141416436023767000228108017_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436023767000228108017_0004_m_000124_412
[2021-05-14 11:20:33,039] {docker.py:276} INFO - 21/05/14 14:20:33 INFO StagingCommitter: Task committer attempt_20210514141643955779653081670389_0004_m_000123_411: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643955779653081670389_0004_m_000123_411 : duration 0:00.005s
21/05/14 14:20:33 INFO StagingCommitter: Task committer attempt_202105141416436023767000228108017_0004_m_000124_412: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436023767000228108017_0004_m_000124_412 : duration 0:00.006s
[2021-05-14 11:20:35,548] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416433092980062404809401_0004_m_000121_409: needsTaskCommit() Task attempt_202105141416433092980062404809401_0004_m_000121_409
[2021-05-14 11:20:35,549] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Task committer attempt_202105141416433092980062404809401_0004_m_000121_409: needsTaskCommit() Task attempt_202105141416433092980062404809401_0004_m_000121_409: duration 0:00.001s
[2021-05-14 11:20:35,549] {docker.py:276} INFO - 21/05/14 14:20:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433092980062404809401_0004_m_000121_409
[2021-05-14 11:20:35,550] {docker.py:276} INFO - 21/05/14 14:20:35 INFO Executor: Finished task 121.0 in stage 4.0 (TID 409). 5149 bytes result sent to driver
[2021-05-14 11:20:35,551] {docker.py:276} INFO - 21/05/14 14:20:35 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 413) (0d1d3e01326c, executor driver, partition 125, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:35,552] {docker.py:276} INFO - 21/05/14 14:20:35 INFO Executor: Running task 125.0 in stage 4.0 (TID 413)
[2021-05-14 11:20:35,553] {docker.py:276} INFO - 21/05/14 14:20:35 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 409) in 7099 ms on 0d1d3e01326c (executor driver) (122/200)
[2021-05-14 11:20:35,560] {docker.py:276} INFO - 21/05/14 14:20:35 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:35,572] {docker.py:276} INFO - 21/05/14 14:20:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432843622510219457266_0004_m_000125_413, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432843622510219457266_0004_m_000125_413}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432843622510219457266_0004}; taskId=attempt_202105141416432843622510219457266_0004_m_000125_413, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1623d234}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416432843622510219457266_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432843622510219457266_0004_m_000125_413
[2021-05-14 11:20:35,575] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Task committer attempt_202105141416432843622510219457266_0004_m_000125_413: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432843622510219457266_0004_m_000125_413 : duration 0:00.003s
[2021-05-14 11:20:35,901] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416431579127700642062259_0004_m_000122_410: needsTaskCommit() Task attempt_202105141416431579127700642062259_0004_m_000122_410
[2021-05-14 11:20:35,902] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Task committer attempt_202105141416431579127700642062259_0004_m_000122_410: needsTaskCommit() Task attempt_202105141416431579127700642062259_0004_m_000122_410: duration 0:00.002s
21/05/14 14:20:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431579127700642062259_0004_m_000122_410
[2021-05-14 11:20:35,904] {docker.py:276} INFO - 21/05/14 14:20:35 INFO Executor: Finished task 122.0 in stage 4.0 (TID 410). 5149 bytes result sent to driver
[2021-05-14 11:20:35,905] {docker.py:276} INFO - 21/05/14 14:20:35 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 414) (0d1d3e01326c, executor driver, partition 126, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:35,906] {docker.py:276} INFO - 21/05/14 14:20:35 INFO Executor: Running task 126.0 in stage 4.0 (TID 414)
21/05/14 14:20:35 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 410) in 6543 ms on 0d1d3e01326c (executor driver) (123/200)
[2021-05-14 11:20:35,918] {docker.py:276} INFO - 21/05/14 14:20:35 INFO ShuffleBlockFetcherIterator: Getting 5 (41.7 KiB) non-empty blocks including 5 (41.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:35,927] {docker.py:276} INFO - 21/05/14 14:20:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:35,927] {docker.py:276} INFO - 21/05/14 14:20:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:35,927] {docker.py:276} INFO - 21/05/14 14:20:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436233387802856909614_0004_m_000126_414, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436233387802856909614_0004_m_000126_414}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436233387802856909614_0004}; taskId=attempt_202105141416436233387802856909614_0004_m_000126_414, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4c2c3ddd}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:35,928] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416436233387802856909614_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436233387802856909614_0004_m_000126_414
[2021-05-14 11:20:35,931] {docker.py:276} INFO - 21/05/14 14:20:35 INFO StagingCommitter: Task committer attempt_202105141416436233387802856909614_0004_m_000126_414: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436233387802856909614_0004_m_000126_414 : duration 0:00.004s
[2021-05-14 11:20:39,717] {docker.py:276} INFO - 21/05/14 14:20:39 INFO StagingCommitter: Starting: Task committer attempt_20210514141643955779653081670389_0004_m_000123_411: needsTaskCommit() Task attempt_20210514141643955779653081670389_0004_m_000123_411
21/05/14 14:20:39 INFO StagingCommitter: Task committer attempt_20210514141643955779653081670389_0004_m_000123_411: needsTaskCommit() Task attempt_20210514141643955779653081670389_0004_m_000123_411: duration 0:00.003s
[2021-05-14 11:20:39,717] {docker.py:276} INFO - 21/05/14 14:20:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643955779653081670389_0004_m_000123_411
[2021-05-14 11:20:39,719] {docker.py:276} INFO - 21/05/14 14:20:39 INFO Executor: Finished task 123.0 in stage 4.0 (TID 411). 5149 bytes result sent to driver
[2021-05-14 11:20:39,720] {docker.py:276} INFO - 21/05/14 14:20:39 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 415) (0d1d3e01326c, executor driver, partition 127, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:39,722] {docker.py:276} INFO - 21/05/14 14:20:39 INFO Executor: Running task 127.0 in stage 4.0 (TID 415)
21/05/14 14:20:39 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 411) in 6729 ms on 0d1d3e01326c (executor driver) (124/200)
[2021-05-14 11:20:39,734] {docker.py:276} INFO - 21/05/14 14:20:39 INFO ShuffleBlockFetcherIterator: Getting 5 (41.9 KiB) non-empty blocks including 5 (41.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:39,742] {docker.py:276} INFO - 21/05/14 14:20:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438832752482604315541_0004_m_000127_415, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438832752482604315541_0004_m_000127_415}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438832752482604315541_0004}; taskId=attempt_202105141416438832752482604315541_0004_m_000127_415, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@177c5c66}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416438832752482604315541_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438832752482604315541_0004_m_000127_415
[2021-05-14 11:20:39,748] {docker.py:276} INFO - 21/05/14 14:20:39 INFO StagingCommitter: Task committer attempt_202105141416438832752482604315541_0004_m_000127_415: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438832752482604315541_0004_m_000127_415 : duration 0:00.005s
[2021-05-14 11:20:39,942] {docker.py:276} INFO - 21/05/14 14:20:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416436023767000228108017_0004_m_000124_412: needsTaskCommit() Task attempt_202105141416436023767000228108017_0004_m_000124_412
[2021-05-14 11:20:39,943] {docker.py:276} INFO - 21/05/14 14:20:39 INFO StagingCommitter: Task committer attempt_202105141416436023767000228108017_0004_m_000124_412: needsTaskCommit() Task attempt_202105141416436023767000228108017_0004_m_000124_412: duration 0:00.002s
21/05/14 14:20:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436023767000228108017_0004_m_000124_412
[2021-05-14 11:20:39,945] {docker.py:276} INFO - 21/05/14 14:20:39 INFO Executor: Finished task 124.0 in stage 4.0 (TID 412). 5149 bytes result sent to driver
[2021-05-14 11:20:39,946] {docker.py:276} INFO - 21/05/14 14:20:39 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 416) (0d1d3e01326c, executor driver, partition 128, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:39,948] {docker.py:276} INFO - 21/05/14 14:20:39 INFO Executor: Running task 128.0 in stage 4.0 (TID 416)
21/05/14 14:20:39 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 412) in 6950 ms on 0d1d3e01326c (executor driver) (125/200)
[2021-05-14 11:20:39,958] {docker.py:276} INFO - 21/05/14 14:20:39 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:39,966] {docker.py:276} INFO - 21/05/14 14:20:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432737510579129705639_0004_m_000128_416, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432737510579129705639_0004_m_000128_416}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432737510579129705639_0004}; taskId=attempt_202105141416432737510579129705639_0004_m_000128_416, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dc70c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:39,967] {docker.py:276} INFO - 21/05/14 14:20:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:40 INFO StagingCommitter: Starting: Task committer attempt_202105141416432737510579129705639_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432737510579129705639_0004_m_000128_416
[2021-05-14 11:20:39,970] {docker.py:276} INFO - 21/05/14 14:20:40 INFO StagingCommitter: Task committer attempt_202105141416432737510579129705639_0004_m_000128_416: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432737510579129705639_0004_m_000128_416 : duration 0:00.004s
[2021-05-14 11:20:41,916] {docker.py:276} INFO - 21/05/14 14:20:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416436233387802856909614_0004_m_000126_414: needsTaskCommit() Task attempt_202105141416436233387802856909614_0004_m_000126_414
[2021-05-14 11:20:41,917] {docker.py:276} INFO - 21/05/14 14:20:41 INFO StagingCommitter: Task committer attempt_202105141416436233387802856909614_0004_m_000126_414: needsTaskCommit() Task attempt_202105141416436233387802856909614_0004_m_000126_414: duration 0:00.003s
21/05/14 14:20:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436233387802856909614_0004_m_000126_414
[2021-05-14 11:20:41,918] {docker.py:276} INFO - 21/05/14 14:20:41 INFO Executor: Finished task 126.0 in stage 4.0 (TID 414). 5149 bytes result sent to driver
[2021-05-14 11:20:41,920] {docker.py:276} INFO - 21/05/14 14:20:41 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 417) (0d1d3e01326c, executor driver, partition 129, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:41,921] {docker.py:276} INFO - 21/05/14 14:20:41 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 414) in 5989 ms on 0d1d3e01326c (executor driver) (126/200)
[2021-05-14 11:20:41,922] {docker.py:276} INFO - 21/05/14 14:20:41 INFO Executor: Running task 129.0 in stage 4.0 (TID 417)
[2021-05-14 11:20:41,934] {docker.py:276} INFO - 21/05/14 14:20:41 INFO ShuffleBlockFetcherIterator: Getting 5 (40.7 KiB) non-empty blocks including 5 (40.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:41,944] {docker.py:276} INFO - 21/05/14 14:20:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432562268170197836969_0004_m_000129_417, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432562268170197836969_0004_m_000129_417}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432562268170197836969_0004}; taskId=attempt_202105141416432562268170197836969_0004_m_000129_417, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f948255}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:41,944] {docker.py:276} INFO - 21/05/14 14:20:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416432562268170197836969_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432562268170197836969_0004_m_000129_417
[2021-05-14 11:20:41,949] {docker.py:276} INFO - 21/05/14 14:20:41 INFO StagingCommitter: Task committer attempt_202105141416432562268170197836969_0004_m_000129_417: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432562268170197836969_0004_m_000129_417 : duration 0:00.005s
[2021-05-14 11:20:42,435] {docker.py:276} INFO - 21/05/14 14:20:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416432843622510219457266_0004_m_000125_413: needsTaskCommit() Task attempt_202105141416432843622510219457266_0004_m_000125_413
[2021-05-14 11:20:42,435] {docker.py:276} INFO - 21/05/14 14:20:42 INFO StagingCommitter: Task committer attempt_202105141416432843622510219457266_0004_m_000125_413: needsTaskCommit() Task attempt_202105141416432843622510219457266_0004_m_000125_413: duration 0:00.002s
21/05/14 14:20:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432843622510219457266_0004_m_000125_413
[2021-05-14 11:20:42,438] {docker.py:276} INFO - 21/05/14 14:20:42 INFO Executor: Finished task 125.0 in stage 4.0 (TID 413). 5149 bytes result sent to driver
21/05/14 14:20:42 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 418) (0d1d3e01326c, executor driver, partition 130, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:42,439] {docker.py:276} INFO - 21/05/14 14:20:42 INFO Executor: Running task 130.0 in stage 4.0 (TID 418)
21/05/14 14:20:42 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 413) in 6860 ms on 0d1d3e01326c (executor driver) (127/200)
[2021-05-14 11:20:42,449] {docker.py:276} INFO - 21/05/14 14:20:42 INFO ShuffleBlockFetcherIterator: Getting 5 (43.9 KiB) non-empty blocks including 5 (43.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:42,459] {docker.py:276} INFO - 21/05/14 14:20:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:42,460] {docker.py:276} INFO - 21/05/14 14:20:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432717591206561369695_0004_m_000130_418, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432717591206561369695_0004_m_000130_418}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432717591206561369695_0004}; taskId=attempt_202105141416432717591206561369695_0004_m_000130_418, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@595cf939}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416432717591206561369695_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432717591206561369695_0004_m_000130_418
[2021-05-14 11:20:42,465] {docker.py:276} INFO - 21/05/14 14:20:42 INFO StagingCommitter: Task committer attempt_202105141416432717591206561369695_0004_m_000130_418: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432717591206561369695_0004_m_000130_418 : duration 0:00.006s
[2021-05-14 11:20:46,540] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416438832752482604315541_0004_m_000127_415: needsTaskCommit() Task attempt_202105141416438832752482604315541_0004_m_000127_415
[2021-05-14 11:20:46,540] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Task committer attempt_202105141416438832752482604315541_0004_m_000127_415: needsTaskCommit() Task attempt_202105141416438832752482604315541_0004_m_000127_415: duration 0:00.002s
[2021-05-14 11:20:46,541] {docker.py:276} INFO - 21/05/14 14:20:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438832752482604315541_0004_m_000127_415
[2021-05-14 11:20:46,542] {docker.py:276} INFO - 21/05/14 14:20:46 INFO Executor: Finished task 127.0 in stage 4.0 (TID 415). 5149 bytes result sent to driver
[2021-05-14 11:20:46,543] {docker.py:276} INFO - 21/05/14 14:20:46 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 419) (0d1d3e01326c, executor driver, partition 131, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:46,544] {docker.py:276} INFO - 21/05/14 14:20:46 INFO Executor: Running task 131.0 in stage 4.0 (TID 419)
[2021-05-14 11:20:46,544] {docker.py:276} INFO - 21/05/14 14:20:46 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 415) in 6797 ms on 0d1d3e01326c (executor driver) (128/200)
[2021-05-14 11:20:46,551] {docker.py:276} INFO - 21/05/14 14:20:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:46,559] {docker.py:276} INFO - 21/05/14 14:20:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439063922029816877155_0004_m_000131_419, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439063922029816877155_0004_m_000131_419}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439063922029816877155_0004}; taskId=attempt_202105141416439063922029816877155_0004_m_000131_419, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@29ae402d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416439063922029816877155_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439063922029816877155_0004_m_000131_419
[2021-05-14 11:20:46,563] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Task committer attempt_202105141416439063922029816877155_0004_m_000131_419: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439063922029816877155_0004_m_000131_419 : duration 0:00.004s
[2021-05-14 11:20:46,810] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416432737510579129705639_0004_m_000128_416: needsTaskCommit() Task attempt_202105141416432737510579129705639_0004_m_000128_416
[2021-05-14 11:20:46,811] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Task committer attempt_202105141416432737510579129705639_0004_m_000128_416: needsTaskCommit() Task attempt_202105141416432737510579129705639_0004_m_000128_416: duration 0:00.003s
21/05/14 14:20:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432737510579129705639_0004_m_000128_416
[2021-05-14 11:20:46,813] {docker.py:276} INFO - 21/05/14 14:20:46 INFO Executor: Finished task 128.0 in stage 4.0 (TID 416). 5149 bytes result sent to driver
[2021-05-14 11:20:46,814] {docker.py:276} INFO - 21/05/14 14:20:46 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 420) (0d1d3e01326c, executor driver, partition 132, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:46,816] {docker.py:276} INFO - 21/05/14 14:20:46 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 416) in 6843 ms on 0d1d3e01326c (executor driver) (129/200)
[2021-05-14 11:20:46,816] {docker.py:276} INFO - 21/05/14 14:20:46 INFO Executor: Running task 132.0 in stage 4.0 (TID 420)
[2021-05-14 11:20:46,825] {docker.py:276} INFO - 21/05/14 14:20:46 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:46,833] {docker.py:276} INFO - 21/05/14 14:20:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:46 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:46 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438486428874215516799_0004_m_000132_420, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438486428874215516799_0004_m_000132_420}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438486428874215516799_0004}; taskId=attempt_202105141416438486428874215516799_0004_m_000132_420, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5fe381f6}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416438486428874215516799_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438486428874215516799_0004_m_000132_420
[2021-05-14 11:20:46,837] {docker.py:276} INFO - 21/05/14 14:20:46 INFO StagingCommitter: Task committer attempt_202105141416438486428874215516799_0004_m_000132_420: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438486428874215516799_0004_m_000132_420 : duration 0:00.004s
[2021-05-14 11:20:48,578] {docker.py:276} INFO - 21/05/14 14:20:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416432562268170197836969_0004_m_000129_417: needsTaskCommit() Task attempt_202105141416432562268170197836969_0004_m_000129_417
[2021-05-14 11:20:48,579] {docker.py:276} INFO - 21/05/14 14:20:48 INFO StagingCommitter: Task committer attempt_202105141416432562268170197836969_0004_m_000129_417: needsTaskCommit() Task attempt_202105141416432562268170197836969_0004_m_000129_417: duration 0:00.002s
[2021-05-14 11:20:48,579] {docker.py:276} INFO - 21/05/14 14:20:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432562268170197836969_0004_m_000129_417
[2021-05-14 11:20:48,581] {docker.py:276} INFO - 21/05/14 14:20:48 INFO Executor: Finished task 129.0 in stage 4.0 (TID 417). 5106 bytes result sent to driver
[2021-05-14 11:20:48,582] {docker.py:276} INFO - 21/05/14 14:20:48 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 421) (0d1d3e01326c, executor driver, partition 133, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:48,584] {docker.py:276} INFO - 21/05/14 14:20:48 INFO Executor: Running task 133.0 in stage 4.0 (TID 421)
[2021-05-14 11:20:48,584] {docker.py:276} INFO - 21/05/14 14:20:48 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 417) in 6671 ms on 0d1d3e01326c (executor driver) (130/200)
[2021-05-14 11:20:48,594] {docker.py:276} INFO - 21/05/14 14:20:48 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:48,601] {docker.py:276} INFO - 21/05/14 14:20:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:48 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:48 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437220432528206755648_0004_m_000133_421, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437220432528206755648_0004_m_000133_421}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437220432528206755648_0004}; taskId=attempt_202105141416437220432528206755648_0004_m_000133_421, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6f14bc25}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:48 INFO StagingCommitter: Starting: Task committer attempt_202105141416437220432528206755648_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437220432528206755648_0004_m_000133_421
[2021-05-14 11:20:48,605] {docker.py:276} INFO - 21/05/14 14:20:48 INFO StagingCommitter: Task committer attempt_202105141416437220432528206755648_0004_m_000133_421: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437220432528206755648_0004_m_000133_421 : duration 0:00.004s
[2021-05-14 11:20:49,521] {docker.py:276} INFO - 21/05/14 14:20:49 INFO StagingCommitter: Starting: Task committer attempt_202105141416432717591206561369695_0004_m_000130_418: needsTaskCommit() Task attempt_202105141416432717591206561369695_0004_m_000130_418
[2021-05-14 11:20:49,522] {docker.py:276} INFO - 21/05/14 14:20:49 INFO StagingCommitter: Task committer attempt_202105141416432717591206561369695_0004_m_000130_418: needsTaskCommit() Task attempt_202105141416432717591206561369695_0004_m_000130_418: duration 0:00.002s
21/05/14 14:20:49 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432717591206561369695_0004_m_000130_418
[2021-05-14 11:20:49,524] {docker.py:276} INFO - 21/05/14 14:20:49 INFO Executor: Finished task 130.0 in stage 4.0 (TID 418). 5106 bytes result sent to driver
[2021-05-14 11:20:49,526] {docker.py:276} INFO - 21/05/14 14:20:49 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 422) (0d1d3e01326c, executor driver, partition 134, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:49,527] {docker.py:276} INFO - 21/05/14 14:20:49 INFO Executor: Running task 134.0 in stage 4.0 (TID 422)
21/05/14 14:20:49 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 418) in 7098 ms on 0d1d3e01326c (executor driver) (131/200)
[2021-05-14 11:20:49,536] {docker.py:276} INFO - 21/05/14 14:20:49 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:49,556] {docker.py:276} INFO - 21/05/14 14:20:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:20:49,557] {docker.py:276} INFO - 21/05/14 14:20:49 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:49,557] {docker.py:276} INFO - 21/05/14 14:20:49 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436530838689660601097_0004_m_000134_422, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530838689660601097_0004_m_000134_422}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436530838689660601097_0004}; taskId=attempt_202105141416436530838689660601097_0004_m_000134_422, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@507a41b7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:49,557] {docker.py:276} INFO - 21/05/14 14:20:49 INFO StagingCommitter: Starting: Task committer attempt_202105141416436530838689660601097_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530838689660601097_0004_m_000134_422
[2021-05-14 11:20:49,562] {docker.py:276} INFO - 21/05/14 14:20:49 INFO StagingCommitter: Task committer attempt_202105141416436530838689660601097_0004_m_000134_422: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436530838689660601097_0004_m_000134_422 : duration 0:00.005s
[2021-05-14 11:20:53,453] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416439063922029816877155_0004_m_000131_419: needsTaskCommit() Task attempt_202105141416439063922029816877155_0004_m_000131_419
[2021-05-14 11:20:53,456] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Task committer attempt_202105141416439063922029816877155_0004_m_000131_419: needsTaskCommit() Task attempt_202105141416439063922029816877155_0004_m_000131_419: duration 0:00.003s
21/05/14 14:20:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439063922029816877155_0004_m_000131_419
[2021-05-14 11:20:53,457] {docker.py:276} INFO - 21/05/14 14:20:53 INFO Executor: Finished task 131.0 in stage 4.0 (TID 419). 5149 bytes result sent to driver
[2021-05-14 11:20:53,457] {docker.py:276} INFO - 21/05/14 14:20:53 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 423) (0d1d3e01326c, executor driver, partition 135, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:53,458] {docker.py:276} INFO - 21/05/14 14:20:53 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 419) in 6924 ms on 0d1d3e01326c (executor driver) (132/200)
[2021-05-14 11:20:53,460] {docker.py:276} INFO - 21/05/14 14:20:53 INFO Executor: Running task 135.0 in stage 4.0 (TID 423)
[2021-05-14 11:20:53,469] {docker.py:276} INFO - 21/05/14 14:20:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:53,477] {docker.py:276} INFO - 21/05/14 14:20:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:53,478] {docker.py:276} INFO - 21/05/14 14:20:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431617484194249018795_0004_m_000135_423, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431617484194249018795_0004_m_000135_423}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431617484194249018795_0004}; taskId=attempt_202105141416431617484194249018795_0004_m_000135_423, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c42969d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:53,478] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416431617484194249018795_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431617484194249018795_0004_m_000135_423
[2021-05-14 11:20:53,482] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Task committer attempt_202105141416431617484194249018795_0004_m_000135_423: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431617484194249018795_0004_m_000135_423 : duration 0:00.005s
[2021-05-14 11:20:53,821] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416438486428874215516799_0004_m_000132_420: needsTaskCommit() Task attempt_202105141416438486428874215516799_0004_m_000132_420
[2021-05-14 11:20:53,822] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Task committer attempt_202105141416438486428874215516799_0004_m_000132_420: needsTaskCommit() Task attempt_202105141416438486428874215516799_0004_m_000132_420: duration 0:00.004s
21/05/14 14:20:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438486428874215516799_0004_m_000132_420
[2021-05-14 11:20:53,824] {docker.py:276} INFO - 21/05/14 14:20:53 INFO Executor: Finished task 132.0 in stage 4.0 (TID 420). 5149 bytes result sent to driver
[2021-05-14 11:20:53,826] {docker.py:276} INFO - 21/05/14 14:20:53 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 424) (0d1d3e01326c, executor driver, partition 136, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:53,827] {docker.py:276} INFO - 21/05/14 14:20:53 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 420) in 7022 ms on 0d1d3e01326c (executor driver) (133/200)
[2021-05-14 11:20:53,828] {docker.py:276} INFO - 21/05/14 14:20:53 INFO Executor: Running task 136.0 in stage 4.0 (TID 424)
[2021-05-14 11:20:53,837] {docker.py:276} INFO - 21/05/14 14:20:53 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:20:53,838] {docker.py:276} INFO - 21/05/14 14:20:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:53,846] {docker.py:276} INFO - 21/05/14 14:20:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:53 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:53 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432067224360507219538_0004_m_000136_424, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432067224360507219538_0004_m_000136_424}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432067224360507219538_0004}; taskId=attempt_202105141416432067224360507219538_0004_m_000136_424, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@38225edf}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:53,847] {docker.py:276} INFO - 21/05/14 14:20:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:53,847] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Starting: Task committer attempt_202105141416432067224360507219538_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432067224360507219538_0004_m_000136_424
[2021-05-14 11:20:53,852] {docker.py:276} INFO - 21/05/14 14:20:53 INFO StagingCommitter: Task committer attempt_202105141416432067224360507219538_0004_m_000136_424: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432067224360507219538_0004_m_000136_424 : duration 0:00.005s
[2021-05-14 11:20:55,927] {docker.py:276} INFO - 21/05/14 14:20:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416437220432528206755648_0004_m_000133_421: needsTaskCommit() Task attempt_202105141416437220432528206755648_0004_m_000133_421
[2021-05-14 11:20:55,928] {docker.py:276} INFO - 21/05/14 14:20:55 INFO StagingCommitter: Task committer attempt_202105141416437220432528206755648_0004_m_000133_421: needsTaskCommit() Task attempt_202105141416437220432528206755648_0004_m_000133_421: duration 0:00.002s
21/05/14 14:20:55 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437220432528206755648_0004_m_000133_421
[2021-05-14 11:20:55,930] {docker.py:276} INFO - 21/05/14 14:20:55 INFO Executor: Finished task 133.0 in stage 4.0 (TID 421). 5149 bytes result sent to driver
[2021-05-14 11:20:55,930] {docker.py:276} INFO - 21/05/14 14:20:55 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 425) (0d1d3e01326c, executor driver, partition 137, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:55,932] {docker.py:276} INFO - 21/05/14 14:20:55 INFO Executor: Running task 137.0 in stage 4.0 (TID 425)
21/05/14 14:20:55 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 421) in 7359 ms on 0d1d3e01326c (executor driver) (134/200)
[2021-05-14 11:20:55,943] {docker.py:276} INFO - 21/05/14 14:20:55 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:55,951] {docker.py:276} INFO - 21/05/14 14:20:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:55 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:55 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436818626548007909771_0004_m_000137_425, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436818626548007909771_0004_m_000137_425}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436818626548007909771_0004}; taskId=attempt_202105141416436818626548007909771_0004_m_000137_425, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@467a3666}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:20:55,952] {docker.py:276} INFO - 21/05/14 14:20:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:20:55 INFO StagingCommitter: Starting: Task committer attempt_202105141416436818626548007909771_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436818626548007909771_0004_m_000137_425
[2021-05-14 11:20:55,956] {docker.py:276} INFO - 21/05/14 14:20:55 INFO StagingCommitter: Task committer attempt_202105141416436818626548007909771_0004_m_000137_425: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436818626548007909771_0004_m_000137_425 : duration 0:00.004s
[2021-05-14 11:20:56,504] {docker.py:276} INFO - 21/05/14 14:20:56 INFO StagingCommitter: Starting: Task committer attempt_202105141416436530838689660601097_0004_m_000134_422: needsTaskCommit() Task attempt_202105141416436530838689660601097_0004_m_000134_422
[2021-05-14 11:20:56,505] {docker.py:276} INFO - 21/05/14 14:20:56 INFO StagingCommitter: Task committer attempt_202105141416436530838689660601097_0004_m_000134_422: needsTaskCommit() Task attempt_202105141416436530838689660601097_0004_m_000134_422: duration 0:00.002s
21/05/14 14:20:56 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436530838689660601097_0004_m_000134_422
[2021-05-14 11:20:56,506] {docker.py:276} INFO - 21/05/14 14:20:56 INFO Executor: Finished task 134.0 in stage 4.0 (TID 422). 5149 bytes result sent to driver
[2021-05-14 11:20:56,508] {docker.py:276} INFO - 21/05/14 14:20:56 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 426) (0d1d3e01326c, executor driver, partition 138, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:20:56,508] {docker.py:276} INFO - 21/05/14 14:20:56 INFO Executor: Running task 138.0 in stage 4.0 (TID 426)
[2021-05-14 11:20:56,509] {docker.py:276} INFO - 21/05/14 14:20:56 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 422) in 6992 ms on 0d1d3e01326c (executor driver) (135/200)
[2021-05-14 11:20:56,516] {docker.py:276} INFO - 21/05/14 14:20:56 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:20:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:20:56,524] {docker.py:276} INFO - 21/05/14 14:20:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:20:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:20:56 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:20:56 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437627461964476292472_0004_m_000138_426, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437627461964476292472_0004_m_000138_426}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437627461964476292472_0004}; taskId=attempt_202105141416437627461964476292472_0004_m_000138_426, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@55d2c4c2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:20:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:20:56,525] {docker.py:276} INFO - 21/05/14 14:20:56 INFO StagingCommitter: Starting: Task committer attempt_202105141416437627461964476292472_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437627461964476292472_0004_m_000138_426
[2021-05-14 11:20:56,528] {docker.py:276} INFO - 21/05/14 14:20:56 INFO StagingCommitter: Task committer attempt_202105141416437627461964476292472_0004_m_000138_426: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437627461964476292472_0004_m_000138_426 : duration 0:00.003s
[2021-05-14 11:21:00,331] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Starting: Task committer attempt_202105141416431617484194249018795_0004_m_000135_423: needsTaskCommit() Task attempt_202105141416431617484194249018795_0004_m_000135_423
[2021-05-14 11:21:00,332] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Task committer attempt_202105141416431617484194249018795_0004_m_000135_423: needsTaskCommit() Task attempt_202105141416431617484194249018795_0004_m_000135_423: duration 0:00.003s
21/05/14 14:21:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431617484194249018795_0004_m_000135_423
[2021-05-14 11:21:00,333] {docker.py:276} INFO - 21/05/14 14:21:00 INFO Executor: Finished task 135.0 in stage 4.0 (TID 423). 5149 bytes result sent to driver
[2021-05-14 11:21:00,335] {docker.py:276} INFO - 21/05/14 14:21:00 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 427) (0d1d3e01326c, executor driver, partition 139, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:00,336] {docker.py:276} INFO - 21/05/14 14:21:00 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 423) in 6887 ms on 0d1d3e01326c (executor driver) (136/200)
21/05/14 14:21:00 INFO Executor: Running task 139.0 in stage 4.0 (TID 427)
[2021-05-14 11:21:00,346] {docker.py:276} INFO - 21/05/14 14:21:00 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:00,354] {docker.py:276} INFO - 21/05/14 14:21:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435233763041523234768_0004_m_000139_427, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435233763041523234768_0004_m_000139_427}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435233763041523234768_0004}; taskId=attempt_202105141416435233763041523234768_0004_m_000139_427, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@696e81ac}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:00 INFO StagingCommitter: Starting: Task committer attempt_202105141416435233763041523234768_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435233763041523234768_0004_m_000139_427
[2021-05-14 11:21:00,358] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Task committer attempt_202105141416435233763041523234768_0004_m_000139_427: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435233763041523234768_0004_m_000139_427 : duration 0:00.004s
[2021-05-14 11:21:00,746] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Starting: Task committer attempt_202105141416432067224360507219538_0004_m_000136_424: needsTaskCommit() Task attempt_202105141416432067224360507219538_0004_m_000136_424
[2021-05-14 11:21:00,747] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Task committer attempt_202105141416432067224360507219538_0004_m_000136_424: needsTaskCommit() Task attempt_202105141416432067224360507219538_0004_m_000136_424: duration 0:00.002s
[2021-05-14 11:21:00,747] {docker.py:276} INFO - 21/05/14 14:21:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432067224360507219538_0004_m_000136_424
[2021-05-14 11:21:00,748] {docker.py:276} INFO - 21/05/14 14:21:00 INFO Executor: Finished task 136.0 in stage 4.0 (TID 424). 5149 bytes result sent to driver
[2021-05-14 11:21:00,750] {docker.py:276} INFO - 21/05/14 14:21:00 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 428) (0d1d3e01326c, executor driver, partition 140, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:00,752] {docker.py:276} INFO - 21/05/14 14:21:00 INFO Executor: Running task 140.0 in stage 4.0 (TID 428)
[2021-05-14 11:21:00,752] {docker.py:276} INFO - 21/05/14 14:21:00 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 424) in 6934 ms on 0d1d3e01326c (executor driver) (137/200)
[2021-05-14 11:21:00,760] {docker.py:276} INFO - 21/05/14 14:21:00 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:00,767] {docker.py:276} INFO - 21/05/14 14:21:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:00 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:00,768] {docker.py:276} INFO - 21/05/14 14:21:00 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438661816038029236375_0004_m_000140_428, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438661816038029236375_0004_m_000140_428}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438661816038029236375_0004}; taskId=attempt_202105141416438661816038029236375_0004_m_000140_428, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1e65261c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:00 INFO StagingCommitter: Starting: Task committer attempt_202105141416438661816038029236375_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438661816038029236375_0004_m_000140_428
[2021-05-14 11:21:00,772] {docker.py:276} INFO - 21/05/14 14:21:00 INFO StagingCommitter: Task committer attempt_202105141416438661816038029236375_0004_m_000140_428: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438661816038029236375_0004_m_000140_428 : duration 0:00.005s
[2021-05-14 11:21:02,856] {docker.py:276} INFO - 21/05/14 14:21:02 INFO StagingCommitter: Starting: Task committer attempt_202105141416436818626548007909771_0004_m_000137_425: needsTaskCommit() Task attempt_202105141416436818626548007909771_0004_m_000137_425
21/05/14 14:21:02 INFO StagingCommitter: Task committer attempt_202105141416436818626548007909771_0004_m_000137_425: needsTaskCommit() Task attempt_202105141416436818626548007909771_0004_m_000137_425: duration 0:00.002s
21/05/14 14:21:02 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436818626548007909771_0004_m_000137_425
[2021-05-14 11:21:02,857] {docker.py:276} INFO - 21/05/14 14:21:02 INFO Executor: Finished task 137.0 in stage 4.0 (TID 425). 5149 bytes result sent to driver
[2021-05-14 11:21:02,859] {docker.py:276} INFO - 21/05/14 14:21:02 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 429) (0d1d3e01326c, executor driver, partition 141, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:02,860] {docker.py:276} INFO - 21/05/14 14:21:02 INFO Executor: Running task 141.0 in stage 4.0 (TID 429)
21/05/14 14:21:02 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 425) in 6936 ms on 0d1d3e01326c (executor driver) (138/200)
[2021-05-14 11:21:02,869] {docker.py:276} INFO - 21/05/14 14:21:02 INFO ShuffleBlockFetcherIterator: Getting 5 (41.6 KiB) non-empty blocks including 5 (41.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:02,880] {docker.py:276} INFO - 21/05/14 14:21:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:02 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:02 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433715529733455824158_0004_m_000141_429, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433715529733455824158_0004_m_000141_429}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433715529733455824158_0004}; taskId=attempt_202105141416433715529733455824158_0004_m_000141_429, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@291417da}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:02,880] {docker.py:276} INFO - 21/05/14 14:21:02 INFO StagingCommitter: Starting: Task committer attempt_202105141416433715529733455824158_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433715529733455824158_0004_m_000141_429
[2021-05-14 11:21:02,884] {docker.py:276} INFO - 21/05/14 14:21:02 INFO StagingCommitter: Task committer attempt_202105141416433715529733455824158_0004_m_000141_429: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433715529733455824158_0004_m_000141_429 : duration 0:00.004s
[2021-05-14 11:21:03,046] {docker.py:276} INFO - 21/05/14 14:21:03 INFO StagingCommitter: Starting: Task committer attempt_202105141416437627461964476292472_0004_m_000138_426: needsTaskCommit() Task attempt_202105141416437627461964476292472_0004_m_000138_426
[2021-05-14 11:21:03,047] {docker.py:276} INFO - 21/05/14 14:21:03 INFO StagingCommitter: Task committer attempt_202105141416437627461964476292472_0004_m_000138_426: needsTaskCommit() Task attempt_202105141416437627461964476292472_0004_m_000138_426: duration 0:00.002s
[2021-05-14 11:21:03,048] {docker.py:276} INFO - 21/05/14 14:21:03 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437627461964476292472_0004_m_000138_426
[2021-05-14 11:21:03,049] {docker.py:276} INFO - 21/05/14 14:21:03 INFO Executor: Finished task 138.0 in stage 4.0 (TID 426). 5149 bytes result sent to driver
[2021-05-14 11:21:03,050] {docker.py:276} INFO - 21/05/14 14:21:03 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 430) (0d1d3e01326c, executor driver, partition 142, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:03,051] {docker.py:276} INFO - 21/05/14 14:21:03 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 426) in 6551 ms on 0d1d3e01326c (executor driver) (139/200)
[2021-05-14 11:21:03,051] {docker.py:276} INFO - 21/05/14 14:21:03 INFO Executor: Running task 142.0 in stage 4.0 (TID 430)
[2021-05-14 11:21:03,062] {docker.py:276} INFO - 21/05/14 14:21:03 INFO ShuffleBlockFetcherIterator: Getting 5 (46.3 KiB) non-empty blocks including 5 (46.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:03,070] {docker.py:276} INFO - 21/05/14 14:21:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:03 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:03,071] {docker.py:276} INFO - 21/05/14 14:21:03 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643778953588224852604_0004_m_000142_430, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643778953588224852604_0004_m_000142_430}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643778953588224852604_0004}; taskId=attempt_20210514141643778953588224852604_0004_m_000142_430, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43bb26d9}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:03,071] {docker.py:276} INFO - 21/05/14 14:21:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:03,071] {docker.py:276} INFO - 21/05/14 14:21:03 INFO StagingCommitter: Starting: Task committer attempt_20210514141643778953588224852604_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643778953588224852604_0004_m_000142_430
[2021-05-14 11:21:03,075] {docker.py:276} INFO - 21/05/14 14:21:03 INFO StagingCommitter: Task committer attempt_20210514141643778953588224852604_0004_m_000142_430: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643778953588224852604_0004_m_000142_430 : duration 0:00.004s
[2021-05-14 11:21:07,217] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416435233763041523234768_0004_m_000139_427: needsTaskCommit() Task attempt_202105141416435233763041523234768_0004_m_000139_427
[2021-05-14 11:21:07,221] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Task committer attempt_202105141416435233763041523234768_0004_m_000139_427: needsTaskCommit() Task attempt_202105141416435233763041523234768_0004_m_000139_427: duration 0:00.001s
21/05/14 14:21:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435233763041523234768_0004_m_000139_427
[2021-05-14 11:21:07,221] {docker.py:276} INFO - 21/05/14 14:21:07 INFO Executor: Finished task 139.0 in stage 4.0 (TID 427). 5106 bytes result sent to driver
[2021-05-14 11:21:07,222] {docker.py:276} INFO - 21/05/14 14:21:07 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 431) (0d1d3e01326c, executor driver, partition 143, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:07,222] {docker.py:276} INFO - 21/05/14 14:21:07 INFO Executor: Running task 143.0 in stage 4.0 (TID 431)
21/05/14 14:21:07 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 427) in 6895 ms on 0d1d3e01326c (executor driver) (140/200)
[2021-05-14 11:21:07,230] {docker.py:276} INFO - 21/05/14 14:21:07 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:07,237] {docker.py:276} INFO - 21/05/14 14:21:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432025022136278288668_0004_m_000143_431, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432025022136278288668_0004_m_000143_431}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432025022136278288668_0004}; taskId=attempt_202105141416432025022136278288668_0004_m_000143_431, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@241d69ec}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416432025022136278288668_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432025022136278288668_0004_m_000143_431
[2021-05-14 11:21:07,241] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Task committer attempt_202105141416432025022136278288668_0004_m_000143_431: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432025022136278288668_0004_m_000143_431 : duration 0:00.003s
[2021-05-14 11:21:07,481] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416438661816038029236375_0004_m_000140_428: needsTaskCommit() Task attempt_202105141416438661816038029236375_0004_m_000140_428
[2021-05-14 11:21:07,482] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Task committer attempt_202105141416438661816038029236375_0004_m_000140_428: needsTaskCommit() Task attempt_202105141416438661816038029236375_0004_m_000140_428: duration 0:00.003s
21/05/14 14:21:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438661816038029236375_0004_m_000140_428
[2021-05-14 11:21:07,484] {docker.py:276} INFO - 21/05/14 14:21:07 INFO Executor: Finished task 140.0 in stage 4.0 (TID 428). 5106 bytes result sent to driver
[2021-05-14 11:21:07,485] {docker.py:276} INFO - 21/05/14 14:21:07 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 432) (0d1d3e01326c, executor driver, partition 144, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:07,486] {docker.py:276} INFO - 21/05/14 14:21:07 INFO Executor: Running task 144.0 in stage 4.0 (TID 432)
[2021-05-14 11:21:07,487] {docker.py:276} INFO - 21/05/14 14:21:07 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 428) in 6745 ms on 0d1d3e01326c (executor driver) (141/200)
[2021-05-14 11:21:07,495] {docker.py:276} INFO - 21/05/14 14:21:07 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:07,502] {docker.py:276} INFO - 21/05/14 14:21:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:07,503] {docker.py:276} INFO - 21/05/14 14:21:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437742528752590077968_0004_m_000144_432, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437742528752590077968_0004_m_000144_432}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437742528752590077968_0004}; taskId=attempt_202105141416437742528752590077968_0004_m_000144_432, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4b8a7e32}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416437742528752590077968_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437742528752590077968_0004_m_000144_432
[2021-05-14 11:21:07,506] {docker.py:276} INFO - 21/05/14 14:21:07 INFO StagingCommitter: Task committer attempt_202105141416437742528752590077968_0004_m_000144_432: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437742528752590077968_0004_m_000144_432 : duration 0:00.004s
[2021-05-14 11:21:09,888] {docker.py:276} INFO - 21/05/14 14:21:09 INFO StagingCommitter: Starting: Task committer attempt_20210514141643778953588224852604_0004_m_000142_430: needsTaskCommit() Task attempt_20210514141643778953588224852604_0004_m_000142_430
[2021-05-14 11:21:09,889] {docker.py:276} INFO - 21/05/14 14:21:09 INFO StagingCommitter: Task committer attempt_20210514141643778953588224852604_0004_m_000142_430: needsTaskCommit() Task attempt_20210514141643778953588224852604_0004_m_000142_430: duration 0:00.001s
21/05/14 14:21:09 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643778953588224852604_0004_m_000142_430
[2021-05-14 11:21:09,891] {docker.py:276} INFO - 21/05/14 14:21:09 INFO Executor: Finished task 142.0 in stage 4.0 (TID 430). 5149 bytes result sent to driver
[2021-05-14 11:21:09,891] {docker.py:276} INFO - 21/05/14 14:21:09 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 433) (0d1d3e01326c, executor driver, partition 145, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:09,892] {docker.py:276} INFO - 21/05/14 14:21:09 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 430) in 6850 ms on 0d1d3e01326c (executor driver) (142/200)
[2021-05-14 11:21:09,892] {docker.py:276} INFO - 21/05/14 14:21:09 INFO Executor: Running task 145.0 in stage 4.0 (TID 433)
[2021-05-14 11:21:09,900] {docker.py:276} INFO - 21/05/14 14:21:09 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:09,908] {docker.py:276} INFO - 21/05/14 14:21:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:09 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:09 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434855343661795367037_0004_m_000145_433, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434855343661795367037_0004_m_000145_433}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434855343661795367037_0004}; taskId=attempt_202105141416434855343661795367037_0004_m_000145_433, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3a8c0c34}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:09 INFO StagingCommitter: Starting: Task committer attempt_202105141416434855343661795367037_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434855343661795367037_0004_m_000145_433
[2021-05-14 11:21:09,912] {docker.py:276} INFO - 21/05/14 14:21:09 INFO StagingCommitter: Task committer attempt_202105141416434855343661795367037_0004_m_000145_433: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434855343661795367037_0004_m_000145_433 : duration 0:00.004s
[2021-05-14 11:21:10,184] {docker.py:276} INFO - 21/05/14 14:21:10 INFO StagingCommitter: Starting: Task committer attempt_202105141416433715529733455824158_0004_m_000141_429: needsTaskCommit() Task attempt_202105141416433715529733455824158_0004_m_000141_429
[2021-05-14 11:21:10,184] {docker.py:276} INFO - 21/05/14 14:21:10 INFO StagingCommitter: Task committer attempt_202105141416433715529733455824158_0004_m_000141_429: needsTaskCommit() Task attempt_202105141416433715529733455824158_0004_m_000141_429: duration 0:00.003s
[2021-05-14 11:21:10,185] {docker.py:276} INFO - 21/05/14 14:21:10 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433715529733455824158_0004_m_000141_429
[2021-05-14 11:21:10,187] {docker.py:276} INFO - 21/05/14 14:21:10 INFO Executor: Finished task 141.0 in stage 4.0 (TID 429). 5149 bytes result sent to driver
[2021-05-14 11:21:10,188] {docker.py:276} INFO - 21/05/14 14:21:10 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 434) (0d1d3e01326c, executor driver, partition 146, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:10,189] {docker.py:276} INFO - 21/05/14 14:21:10 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 429) in 7307 ms on 0d1d3e01326c (executor driver) (143/200)
[2021-05-14 11:21:10,190] {docker.py:276} INFO - 21/05/14 14:21:10 INFO Executor: Running task 146.0 in stage 4.0 (TID 434)
[2021-05-14 11:21:10,199] {docker.py:276} INFO - 21/05/14 14:21:10 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:10,207] {docker.py:276} INFO - 21/05/14 14:21:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:10 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:10 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431643785445663117317_0004_m_000146_434, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431643785445663117317_0004_m_000146_434}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431643785445663117317_0004}; taskId=attempt_202105141416431643785445663117317_0004_m_000146_434, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1674958a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:10,207] {docker.py:276} INFO - 21/05/14 14:21:10 INFO StagingCommitter: Starting: Task committer attempt_202105141416431643785445663117317_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431643785445663117317_0004_m_000146_434
[2021-05-14 11:21:10,211] {docker.py:276} INFO - 21/05/14 14:21:10 INFO StagingCommitter: Task committer attempt_202105141416431643785445663117317_0004_m_000146_434: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431643785445663117317_0004_m_000146_434 : duration 0:00.004s
[2021-05-14 11:21:14,100] {docker.py:276} INFO - 21/05/14 14:21:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416432025022136278288668_0004_m_000143_431: needsTaskCommit() Task attempt_202105141416432025022136278288668_0004_m_000143_431
21/05/14 14:21:14 INFO StagingCommitter: Task committer attempt_202105141416432025022136278288668_0004_m_000143_431: needsTaskCommit() Task attempt_202105141416432025022136278288668_0004_m_000143_431: duration 0:00.001s
[2021-05-14 11:21:14,100] {docker.py:276} INFO - 21/05/14 14:21:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432025022136278288668_0004_m_000143_431
[2021-05-14 11:21:14,101] {docker.py:276} INFO - 21/05/14 14:21:14 INFO Executor: Finished task 143.0 in stage 4.0 (TID 431). 5149 bytes result sent to driver
[2021-05-14 11:21:14,102] {docker.py:276} INFO - 21/05/14 14:21:14 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 435) (0d1d3e01326c, executor driver, partition 147, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:14,103] {docker.py:276} INFO - 21/05/14 14:21:14 INFO Executor: Running task 147.0 in stage 4.0 (TID 435)
[2021-05-14 11:21:14,103] {docker.py:276} INFO - 21/05/14 14:21:14 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 431) in 6857 ms on 0d1d3e01326c (executor driver) (144/200)
[2021-05-14 11:21:14,112] {docker.py:276} INFO - 21/05/14 14:21:14 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:14,120] {docker.py:276} INFO - 21/05/14 14:21:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433689791519451487524_0004_m_000147_435, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433689791519451487524_0004_m_000147_435}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433689791519451487524_0004}; taskId=attempt_202105141416433689791519451487524_0004_m_000147_435, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3c095190}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416433689791519451487524_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433689791519451487524_0004_m_000147_435
[2021-05-14 11:21:14,124] {docker.py:276} INFO - 21/05/14 14:21:14 INFO StagingCommitter: Task committer attempt_202105141416433689791519451487524_0004_m_000147_435: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433689791519451487524_0004_m_000147_435 : duration 0:00.004s
[2021-05-14 11:21:14,566] {docker.py:276} INFO - 21/05/14 14:21:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416437742528752590077968_0004_m_000144_432: needsTaskCommit() Task attempt_202105141416437742528752590077968_0004_m_000144_432
21/05/14 14:21:14 INFO StagingCommitter: Task committer attempt_202105141416437742528752590077968_0004_m_000144_432: needsTaskCommit() Task attempt_202105141416437742528752590077968_0004_m_000144_432: duration 0:00.001s
21/05/14 14:21:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437742528752590077968_0004_m_000144_432
[2021-05-14 11:21:14,568] {docker.py:276} INFO - 21/05/14 14:21:14 INFO Executor: Finished task 144.0 in stage 4.0 (TID 432). 5149 bytes result sent to driver
[2021-05-14 11:21:14,569] {docker.py:276} INFO - 21/05/14 14:21:14 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 436) (0d1d3e01326c, executor driver, partition 148, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:14,570] {docker.py:276} INFO - 21/05/14 14:21:14 INFO Executor: Running task 148.0 in stage 4.0 (TID 436)
[2021-05-14 11:21:14,570] {docker.py:276} INFO - 21/05/14 14:21:14 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 432) in 7058 ms on 0d1d3e01326c (executor driver) (145/200)
[2021-05-14 11:21:14,578] {docker.py:276} INFO - 21/05/14 14:21:14 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:14,586] {docker.py:276} INFO - 21/05/14 14:21:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435864427467455250960_0004_m_000148_436, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435864427467455250960_0004_m_000148_436}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435864427467455250960_0004}; taskId=attempt_202105141416435864427467455250960_0004_m_000148_436, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fb91b33}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416435864427467455250960_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435864427467455250960_0004_m_000148_436
[2021-05-14 11:21:14,590] {docker.py:276} INFO - 21/05/14 14:21:14 INFO StagingCommitter: Task committer attempt_202105141416435864427467455250960_0004_m_000148_436: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435864427467455250960_0004_m_000148_436 : duration 0:00.004s
[2021-05-14 11:21:17,163] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Starting: Task committer attempt_202105141416434855343661795367037_0004_m_000145_433: needsTaskCommit() Task attempt_202105141416434855343661795367037_0004_m_000145_433
[2021-05-14 11:21:17,163] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Task committer attempt_202105141416434855343661795367037_0004_m_000145_433: needsTaskCommit() Task attempt_202105141416434855343661795367037_0004_m_000145_433: duration 0:00.002s
[2021-05-14 11:21:17,164] {docker.py:276} INFO - 21/05/14 14:21:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434855343661795367037_0004_m_000145_433
[2021-05-14 11:21:17,165] {docker.py:276} INFO - 21/05/14 14:21:17 INFO Executor: Finished task 145.0 in stage 4.0 (TID 433). 5106 bytes result sent to driver
[2021-05-14 11:21:17,166] {docker.py:276} INFO - 21/05/14 14:21:17 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 437) (0d1d3e01326c, executor driver, partition 149, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:17,167] {docker.py:276} INFO - 21/05/14 14:21:17 INFO Executor: Running task 149.0 in stage 4.0 (TID 437)
[2021-05-14 11:21:17,168] {docker.py:276} INFO - 21/05/14 14:21:17 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 433) in 7251 ms on 0d1d3e01326c (executor driver) (146/200)
[2021-05-14 11:21:17,177] {docker.py:276} INFO - 21/05/14 14:21:17 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:17,185] {docker.py:276} INFO - 21/05/14 14:21:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:17,186] {docker.py:276} INFO - 21/05/14 14:21:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433374856344170301801_0004_m_000149_437, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433374856344170301801_0004_m_000149_437}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433374856344170301801_0004}; taskId=attempt_202105141416433374856344170301801_0004_m_000149_437, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4f70d98a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:17,186] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Starting: Task committer attempt_202105141416433374856344170301801_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433374856344170301801_0004_m_000149_437
[2021-05-14 11:21:17,190] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Task committer attempt_202105141416433374856344170301801_0004_m_000149_437: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433374856344170301801_0004_m_000149_437 : duration 0:00.004s
[2021-05-14 11:21:17,223] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Starting: Task committer attempt_202105141416431643785445663117317_0004_m_000146_434: needsTaskCommit() Task attempt_202105141416431643785445663117317_0004_m_000146_434
[2021-05-14 11:21:17,224] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Task committer attempt_202105141416431643785445663117317_0004_m_000146_434: needsTaskCommit() Task attempt_202105141416431643785445663117317_0004_m_000146_434: duration 0:00.003s
21/05/14 14:21:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431643785445663117317_0004_m_000146_434
[2021-05-14 11:21:17,227] {docker.py:276} INFO - 21/05/14 14:21:17 INFO Executor: Finished task 146.0 in stage 4.0 (TID 434). 5106 bytes result sent to driver
[2021-05-14 11:21:17,243] {docker.py:276} INFO - 21/05/14 14:21:17 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 438) (0d1d3e01326c, executor driver, partition 150, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:17,244] {docker.py:276} INFO - 21/05/14 14:21:17 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 434) in 7064 ms on 0d1d3e01326c (executor driver) (147/200)
21/05/14 14:21:17 INFO Executor: Running task 150.0 in stage 4.0 (TID 438)
[2021-05-14 11:21:17,252] {docker.py:276} INFO - 21/05/14 14:21:17 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:17,260] {docker.py:276} INFO - 21/05/14 14:21:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:17 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:17,260] {docker.py:276} INFO - 21/05/14 14:21:17 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438944667909628252579_0004_m_000150_438, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438944667909628252579_0004_m_000150_438}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438944667909628252579_0004}; taskId=attempt_202105141416438944667909628252579_0004_m_000150_438, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6379b70b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:17,261] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Starting: Task committer attempt_202105141416438944667909628252579_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438944667909628252579_0004_m_000150_438
[2021-05-14 11:21:17,264] {docker.py:276} INFO - 21/05/14 14:21:17 INFO StagingCommitter: Task committer attempt_202105141416438944667909628252579_0004_m_000150_438: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438944667909628252579_0004_m_000150_438 : duration 0:00.004s
[2021-05-14 11:21:20,671] {docker.py:276} INFO - 21/05/14 14:21:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416435864427467455250960_0004_m_000148_436: needsTaskCommit() Task attempt_202105141416435864427467455250960_0004_m_000148_436
21/05/14 14:21:20 INFO StagingCommitter: Task committer attempt_202105141416435864427467455250960_0004_m_000148_436: needsTaskCommit() Task attempt_202105141416435864427467455250960_0004_m_000148_436: duration 0:00.003s
21/05/14 14:21:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435864427467455250960_0004_m_000148_436
[2021-05-14 11:21:20,672] {docker.py:276} INFO - 21/05/14 14:21:20 INFO Executor: Finished task 148.0 in stage 4.0 (TID 436). 5149 bytes result sent to driver
[2021-05-14 11:21:20,672] {docker.py:276} INFO - 21/05/14 14:21:20 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 439) (0d1d3e01326c, executor driver, partition 151, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:20,673] {docker.py:276} INFO - 21/05/14 14:21:20 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 436) in 6112 ms on 0d1d3e01326c (executor driver) (148/200)
[2021-05-14 11:21:20,674] {docker.py:276} INFO - 21/05/14 14:21:20 INFO Executor: Running task 151.0 in stage 4.0 (TID 439)
[2021-05-14 11:21:20,683] {docker.py:276} INFO - 21/05/14 14:21:20 INFO ShuffleBlockFetcherIterator: Getting 5 (44.2 KiB) non-empty blocks including 5 (44.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:20,691] {docker.py:276} INFO - 21/05/14 14:21:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436955559655722849721_0004_m_000151_439, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436955559655722849721_0004_m_000151_439}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436955559655722849721_0004}; taskId=attempt_202105141416436955559655722849721_0004_m_000151_439, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c58e388}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416436955559655722849721_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436955559655722849721_0004_m_000151_439
[2021-05-14 11:21:20,695] {docker.py:276} INFO - 21/05/14 14:21:20 INFO StagingCommitter: Task committer attempt_202105141416436955559655722849721_0004_m_000151_439: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436955559655722849721_0004_m_000151_439 : duration 0:00.004s
[2021-05-14 11:21:21,448] {docker.py:276} INFO - 21/05/14 14:21:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416433689791519451487524_0004_m_000147_435: needsTaskCommit() Task attempt_202105141416433689791519451487524_0004_m_000147_435
[2021-05-14 11:21:21,448] {docker.py:276} INFO - 21/05/14 14:21:21 INFO StagingCommitter: Task committer attempt_202105141416433689791519451487524_0004_m_000147_435: needsTaskCommit() Task attempt_202105141416433689791519451487524_0004_m_000147_435: duration 0:00.005s
21/05/14 14:21:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433689791519451487524_0004_m_000147_435
[2021-05-14 11:21:21,450] {docker.py:276} INFO - 21/05/14 14:21:21 INFO Executor: Finished task 147.0 in stage 4.0 (TID 435). 5149 bytes result sent to driver
[2021-05-14 11:21:21,452] {docker.py:276} INFO - 21/05/14 14:21:21 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 440) (0d1d3e01326c, executor driver, partition 152, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:21,453] {docker.py:276} INFO - 21/05/14 14:21:21 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 435) in 7359 ms on 0d1d3e01326c (executor driver) (149/200)
[2021-05-14 11:21:21,454] {docker.py:276} INFO - 21/05/14 14:21:21 INFO Executor: Running task 152.0 in stage 4.0 (TID 440)
[2021-05-14 11:21:21,463] {docker.py:276} INFO - 21/05/14 14:21:21 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:21,472] {docker.py:276} INFO - 21/05/14 14:21:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:21,472] {docker.py:276} INFO - 21/05/14 14:21:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435170714242314675497_0004_m_000152_440, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435170714242314675497_0004_m_000152_440}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435170714242314675497_0004}; taskId=attempt_202105141416435170714242314675497_0004_m_000152_440, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b976e74}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416435170714242314675497_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435170714242314675497_0004_m_000152_440
[2021-05-14 11:21:21,476] {docker.py:276} INFO - 21/05/14 14:21:21 INFO StagingCommitter: Task committer attempt_202105141416435170714242314675497_0004_m_000152_440: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435170714242314675497_0004_m_000152_440 : duration 0:00.004s
[2021-05-14 11:21:24,374] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Starting: Task committer attempt_202105141416433374856344170301801_0004_m_000149_437: needsTaskCommit() Task attempt_202105141416433374856344170301801_0004_m_000149_437
[2021-05-14 11:21:24,375] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Task committer attempt_202105141416433374856344170301801_0004_m_000149_437: needsTaskCommit() Task attempt_202105141416433374856344170301801_0004_m_000149_437: duration 0:00.004s
21/05/14 14:21:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433374856344170301801_0004_m_000149_437
[2021-05-14 11:21:24,376] {docker.py:276} INFO - 21/05/14 14:21:24 INFO Executor: Finished task 149.0 in stage 4.0 (TID 437). 5149 bytes result sent to driver
[2021-05-14 11:21:24,377] {docker.py:276} INFO - 21/05/14 14:21:24 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 441) (0d1d3e01326c, executor driver, partition 153, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:24,378] {docker.py:276} INFO - 21/05/14 14:21:24 INFO Executor: Running task 153.0 in stage 4.0 (TID 441)
[2021-05-14 11:21:24,379] {docker.py:276} INFO - 21/05/14 14:21:24 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 437) in 7221 ms on 0d1d3e01326c (executor driver) (150/200)
[2021-05-14 11:21:24,387] {docker.py:276} INFO - 21/05/14 14:21:24 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:24,394] {docker.py:276} INFO - 21/05/14 14:21:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:24,395] {docker.py:276} INFO - 21/05/14 14:21:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431044981657355874081_0004_m_000153_441, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431044981657355874081_0004_m_000153_441}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431044981657355874081_0004}; taskId=attempt_202105141416431044981657355874081_0004_m_000153_441, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4524f985}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:24,395] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Starting: Task committer attempt_202105141416431044981657355874081_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431044981657355874081_0004_m_000153_441
[2021-05-14 11:21:24,398] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Task committer attempt_202105141416431044981657355874081_0004_m_000153_441: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431044981657355874081_0004_m_000153_441 : duration 0:00.003s
[2021-05-14 11:21:24,570] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Starting: Task committer attempt_202105141416438944667909628252579_0004_m_000150_438: needsTaskCommit() Task attempt_202105141416438944667909628252579_0004_m_000150_438
[2021-05-14 11:21:24,571] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Task committer attempt_202105141416438944667909628252579_0004_m_000150_438: needsTaskCommit() Task attempt_202105141416438944667909628252579_0004_m_000150_438: duration 0:00.002s
21/05/14 14:21:24 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438944667909628252579_0004_m_000150_438
[2021-05-14 11:21:24,573] {docker.py:276} INFO - 21/05/14 14:21:24 INFO Executor: Finished task 150.0 in stage 4.0 (TID 438). 5106 bytes result sent to driver
[2021-05-14 11:21:24,574] {docker.py:276} INFO - 21/05/14 14:21:24 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 442) (0d1d3e01326c, executor driver, partition 154, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:24,575] {docker.py:276} INFO - 21/05/14 14:21:24 INFO Executor: Running task 154.0 in stage 4.0 (TID 442)
[2021-05-14 11:21:24,576] {docker.py:276} INFO - 21/05/14 14:21:24 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 438) in 7341 ms on 0d1d3e01326c (executor driver) (151/200)
[2021-05-14 11:21:24,585] {docker.py:276} INFO - 21/05/14 14:21:24 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:21:24,585] {docker.py:276} INFO - 21/05/14 14:21:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:24,593] {docker.py:276} INFO - 21/05/14 14:21:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:24,593] {docker.py:276} INFO - 21/05/14 14:21:24 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:24 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434964638794425254576_0004_m_000154_442, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434964638794425254576_0004_m_000154_442}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434964638794425254576_0004}; taskId=attempt_202105141416434964638794425254576_0004_m_000154_442, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2bfd3d18}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:24,594] {docker.py:276} INFO - 21/05/14 14:21:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:24,594] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Starting: Task committer attempt_202105141416434964638794425254576_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434964638794425254576_0004_m_000154_442
[2021-05-14 11:21:24,598] {docker.py:276} INFO - 21/05/14 14:21:24 INFO StagingCommitter: Task committer attempt_202105141416434964638794425254576_0004_m_000154_442: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434964638794425254576_0004_m_000154_442 : duration 0:00.005s
[2021-05-14 11:21:28,137] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416436955559655722849721_0004_m_000151_439: needsTaskCommit() Task attempt_202105141416436955559655722849721_0004_m_000151_439
[2021-05-14 11:21:28,138] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Task committer attempt_202105141416436955559655722849721_0004_m_000151_439: needsTaskCommit() Task attempt_202105141416436955559655722849721_0004_m_000151_439: duration 0:00.002s
21/05/14 14:21:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436955559655722849721_0004_m_000151_439
[2021-05-14 11:21:28,140] {docker.py:276} INFO - 21/05/14 14:21:28 INFO Executor: Finished task 151.0 in stage 4.0 (TID 439). 5149 bytes result sent to driver
[2021-05-14 11:21:28,141] {docker.py:276} INFO - 21/05/14 14:21:28 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 443) (0d1d3e01326c, executor driver, partition 155, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:28,142] {docker.py:276} INFO - 21/05/14 14:21:28 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 439) in 7479 ms on 0d1d3e01326c (executor driver) (152/200)
[2021-05-14 11:21:28,143] {docker.py:276} INFO - 21/05/14 14:21:28 INFO Executor: Running task 155.0 in stage 4.0 (TID 443)
[2021-05-14 11:21:28,152] {docker.py:276} INFO - 21/05/14 14:21:28 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:28,160] {docker.py:276} INFO - 21/05/14 14:21:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:28,160] {docker.py:276} INFO - 21/05/14 14:21:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436154443844596550096_0004_m_000155_443, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436154443844596550096_0004_m_000155_443}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436154443844596550096_0004}; taskId=attempt_202105141416436154443844596550096_0004_m_000155_443, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@550b2033}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416436154443844596550096_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436154443844596550096_0004_m_000155_443
[2021-05-14 11:21:28,165] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Task committer attempt_202105141416436154443844596550096_0004_m_000155_443: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436154443844596550096_0004_m_000155_443 : duration 0:00.004s
[2021-05-14 11:21:28,919] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416435170714242314675497_0004_m_000152_440: needsTaskCommit() Task attempt_202105141416435170714242314675497_0004_m_000152_440
[2021-05-14 11:21:28,920] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Task committer attempt_202105141416435170714242314675497_0004_m_000152_440: needsTaskCommit() Task attempt_202105141416435170714242314675497_0004_m_000152_440: duration 0:00.001s
21/05/14 14:21:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435170714242314675497_0004_m_000152_440
[2021-05-14 11:21:28,921] {docker.py:276} INFO - 21/05/14 14:21:28 INFO Executor: Finished task 152.0 in stage 4.0 (TID 440). 5149 bytes result sent to driver
[2021-05-14 11:21:28,922] {docker.py:276} INFO - 21/05/14 14:21:28 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 444) (0d1d3e01326c, executor driver, partition 156, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:28,923] {docker.py:276} INFO - 21/05/14 14:21:28 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 440) in 7482 ms on 0d1d3e01326c (executor driver) (153/200)
21/05/14 14:21:28 INFO Executor: Running task 156.0 in stage 4.0 (TID 444)
[2021-05-14 11:21:28,932] {docker.py:276} INFO - 21/05/14 14:21:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:28,940] {docker.py:276} INFO - 21/05/14 14:21:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:28,940] {docker.py:276} INFO - 21/05/14 14:21:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434769043510906323612_0004_m_000156_444, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434769043510906323612_0004_m_000156_444}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434769043510906323612_0004}; taskId=attempt_202105141416434769043510906323612_0004_m_000156_444, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52dc4e4d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416434769043510906323612_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434769043510906323612_0004_m_000156_444
[2021-05-14 11:21:28,944] {docker.py:276} INFO - 21/05/14 14:21:28 INFO StagingCommitter: Task committer attempt_202105141416434769043510906323612_0004_m_000156_444: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434769043510906323612_0004_m_000156_444 : duration 0:00.004s
[2021-05-14 11:21:31,419] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Starting: Task committer attempt_202105141416431044981657355874081_0004_m_000153_441: needsTaskCommit() Task attempt_202105141416431044981657355874081_0004_m_000153_441
[2021-05-14 11:21:31,420] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Task committer attempt_202105141416431044981657355874081_0004_m_000153_441: needsTaskCommit() Task attempt_202105141416431044981657355874081_0004_m_000153_441: duration 0:00.003s
[2021-05-14 11:21:31,420] {docker.py:276} INFO - 21/05/14 14:21:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431044981657355874081_0004_m_000153_441
[2021-05-14 11:21:31,422] {docker.py:276} INFO - 21/05/14 14:21:31 INFO Executor: Finished task 153.0 in stage 4.0 (TID 441). 5149 bytes result sent to driver
[2021-05-14 11:21:31,424] {docker.py:276} INFO - 21/05/14 14:21:31 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 445) (0d1d3e01326c, executor driver, partition 157, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:31,425] {docker.py:276} INFO - 21/05/14 14:21:31 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 441) in 7055 ms on 0d1d3e01326c (executor driver) (154/200)
21/05/14 14:21:31 INFO Executor: Running task 157.0 in stage 4.0 (TID 445)
[2021-05-14 11:21:31,435] {docker.py:276} INFO - 21/05/14 14:21:31 INFO ShuffleBlockFetcherIterator: Getting 5 (41.1 KiB) non-empty blocks including 5 (41.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:31,442] {docker.py:276} INFO - 21/05/14 14:21:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:31,442] {docker.py:276} INFO - 21/05/14 14:21:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416439114727535652763359_0004_m_000157_445, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439114727535652763359_0004_m_000157_445}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416439114727535652763359_0004}; taskId=attempt_202105141416439114727535652763359_0004_m_000157_445, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@76f36e15}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:31 INFO StagingCommitter: Starting: Task committer attempt_202105141416439114727535652763359_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439114727535652763359_0004_m_000157_445
[2021-05-14 11:21:31,445] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Task committer attempt_202105141416439114727535652763359_0004_m_000157_445: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416439114727535652763359_0004_m_000157_445 : duration 0:00.003s
[2021-05-14 11:21:31,789] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Starting: Task committer attempt_202105141416434964638794425254576_0004_m_000154_442: needsTaskCommit() Task attempt_202105141416434964638794425254576_0004_m_000154_442
[2021-05-14 11:21:31,790] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Task committer attempt_202105141416434964638794425254576_0004_m_000154_442: needsTaskCommit() Task attempt_202105141416434964638794425254576_0004_m_000154_442: duration 0:00.002s
[2021-05-14 11:21:31,790] {docker.py:276} INFO - 21/05/14 14:21:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434964638794425254576_0004_m_000154_442
[2021-05-14 11:21:31,792] {docker.py:276} INFO - 21/05/14 14:21:31 INFO Executor: Finished task 154.0 in stage 4.0 (TID 442). 5149 bytes result sent to driver
[2021-05-14 11:21:31,793] {docker.py:276} INFO - 21/05/14 14:21:31 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 446) (0d1d3e01326c, executor driver, partition 158, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:31,794] {docker.py:276} INFO - 21/05/14 14:21:31 INFO Executor: Running task 158.0 in stage 4.0 (TID 446)
[2021-05-14 11:21:31,795] {docker.py:276} INFO - 21/05/14 14:21:31 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 442) in 7229 ms on 0d1d3e01326c (executor driver) (155/200)
[2021-05-14 11:21:31,802] {docker.py:276} INFO - 21/05/14 14:21:31 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:31,811] {docker.py:276} INFO - 21/05/14 14:21:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:31 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:31,811] {docker.py:276} INFO - 21/05/14 14:21:31 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437332617825124921155_0004_m_000158_446, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437332617825124921155_0004_m_000158_446}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437332617825124921155_0004}; taskId=attempt_202105141416437332617825124921155_0004_m_000158_446, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@39f75ced}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:31,811] {docker.py:276} INFO - 21/05/14 14:21:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:31,812] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Starting: Task committer attempt_202105141416437332617825124921155_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437332617825124921155_0004_m_000158_446
[2021-05-14 11:21:31,816] {docker.py:276} INFO - 21/05/14 14:21:31 INFO StagingCommitter: Task committer attempt_202105141416437332617825124921155_0004_m_000158_446: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437332617825124921155_0004_m_000158_446 : duration 0:00.004s
[2021-05-14 11:21:34,493] {docker.py:276} INFO - 21/05/14 14:21:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416436154443844596550096_0004_m_000155_443: needsTaskCommit() Task attempt_202105141416436154443844596550096_0004_m_000155_443
[2021-05-14 11:21:34,494] {docker.py:276} INFO - 21/05/14 14:21:34 INFO StagingCommitter: Task committer attempt_202105141416436154443844596550096_0004_m_000155_443: needsTaskCommit() Task attempt_202105141416436154443844596550096_0004_m_000155_443: duration 0:00.004s
[2021-05-14 11:21:34,495] {docker.py:276} INFO - 21/05/14 14:21:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436154443844596550096_0004_m_000155_443
[2021-05-14 11:21:34,497] {docker.py:276} INFO - 21/05/14 14:21:34 INFO Executor: Finished task 155.0 in stage 4.0 (TID 443). 5106 bytes result sent to driver
[2021-05-14 11:21:34,499] {docker.py:276} INFO - 21/05/14 14:21:34 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 447) (0d1d3e01326c, executor driver, partition 159, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:34,499] {docker.py:276} INFO - 21/05/14 14:21:34 INFO Executor: Running task 159.0 in stage 4.0 (TID 447)
[2021-05-14 11:21:34,500] {docker.py:276} INFO - 21/05/14 14:21:34 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 443) in 6366 ms on 0d1d3e01326c (executor driver) (156/200)
[2021-05-14 11:21:34,509] {docker.py:276} INFO - 21/05/14 14:21:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:34,517] {docker.py:276} INFO - 21/05/14 14:21:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643120764126154748865_0004_m_000159_447, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643120764126154748865_0004_m_000159_447}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643120764126154748865_0004}; taskId=attempt_20210514141643120764126154748865_0004_m_000159_447, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7fe8a37e}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:34 INFO StagingCommitter: Starting: Task committer attempt_20210514141643120764126154748865_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643120764126154748865_0004_m_000159_447
[2021-05-14 11:21:34,521] {docker.py:276} INFO - 21/05/14 14:21:34 INFO StagingCommitter: Task committer attempt_20210514141643120764126154748865_0004_m_000159_447: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643120764126154748865_0004_m_000159_447 : duration 0:00.004s
[2021-05-14 11:21:35,615] {docker.py:276} INFO - 21/05/14 14:21:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416434769043510906323612_0004_m_000156_444: needsTaskCommit() Task attempt_202105141416434769043510906323612_0004_m_000156_444
21/05/14 14:21:35 INFO StagingCommitter: Task committer attempt_202105141416434769043510906323612_0004_m_000156_444: needsTaskCommit() Task attempt_202105141416434769043510906323612_0004_m_000156_444: duration 0:00.001s
21/05/14 14:21:35 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434769043510906323612_0004_m_000156_444
[2021-05-14 11:21:35,616] {docker.py:276} INFO - 21/05/14 14:21:35 INFO Executor: Finished task 156.0 in stage 4.0 (TID 444). 5106 bytes result sent to driver
[2021-05-14 11:21:35,617] {docker.py:276} INFO - 21/05/14 14:21:35 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 448) (0d1d3e01326c, executor driver, partition 160, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:35,618] {docker.py:276} INFO - 21/05/14 14:21:35 INFO Executor: Running task 160.0 in stage 4.0 (TID 448)
21/05/14 14:21:35 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 444) in 6704 ms on 0d1d3e01326c (executor driver) (157/200)
[2021-05-14 11:21:35,627] {docker.py:276} INFO - 21/05/14 14:21:35 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:35,643] {docker.py:276} INFO - 21/05/14 14:21:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:35 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:35 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436699715562342563_0004_m_000160_448, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699715562342563_0004_m_000160_448}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436699715562342563_0004}; taskId=attempt_202105141416436699715562342563_0004_m_000160_448, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@779e9c19}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:35,643] {docker.py:276} INFO - 21/05/14 14:21:35 INFO StagingCommitter: Starting: Task committer attempt_202105141416436699715562342563_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699715562342563_0004_m_000160_448
[2021-05-14 11:21:35,647] {docker.py:276} INFO - 21/05/14 14:21:35 INFO StagingCommitter: Task committer attempt_202105141416436699715562342563_0004_m_000160_448: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436699715562342563_0004_m_000160_448 : duration 0:00.004s
[2021-05-14 11:21:38,397] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416437332617825124921155_0004_m_000158_446: needsTaskCommit() Task attempt_202105141416437332617825124921155_0004_m_000158_446
[2021-05-14 11:21:38,397] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Task committer attempt_202105141416437332617825124921155_0004_m_000158_446: needsTaskCommit() Task attempt_202105141416437332617825124921155_0004_m_000158_446: duration 0:00.001s
21/05/14 14:21:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437332617825124921155_0004_m_000158_446
[2021-05-14 11:21:38,398] {docker.py:276} INFO - 21/05/14 14:21:38 INFO Executor: Finished task 158.0 in stage 4.0 (TID 446). 5149 bytes result sent to driver
[2021-05-14 11:21:38,399] {docker.py:276} INFO - 21/05/14 14:21:38 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 449) (0d1d3e01326c, executor driver, partition 161, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:38,400] {docker.py:276} INFO - 21/05/14 14:21:38 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 446) in 6615 ms on 0d1d3e01326c (executor driver) (158/200)
21/05/14 14:21:38 INFO Executor: Running task 161.0 in stage 4.0 (TID 449)
[2021-05-14 11:21:38,408] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416439114727535652763359_0004_m_000157_445: needsTaskCommit() Task attempt_202105141416439114727535652763359_0004_m_000157_445
[2021-05-14 11:21:38,409] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Task committer attempt_202105141416439114727535652763359_0004_m_000157_445: needsTaskCommit() Task attempt_202105141416439114727535652763359_0004_m_000157_445: duration 0:00.001s
21/05/14 14:21:38 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416439114727535652763359_0004_m_000157_445
[2021-05-14 11:21:38,410] {docker.py:276} INFO - 21/05/14 14:21:38 INFO Executor: Finished task 157.0 in stage 4.0 (TID 445). 5149 bytes result sent to driver
[2021-05-14 11:21:38,411] {docker.py:276} INFO - 21/05/14 14:21:38 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 450) (0d1d3e01326c, executor driver, partition 162, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:38,412] {docker.py:276} INFO - 21/05/14 14:21:38 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 445) in 6997 ms on 0d1d3e01326c (executor driver) (159/200)
21/05/14 14:21:38 INFO Executor: Running task 162.0 in stage 4.0 (TID 450)
[2021-05-14 11:21:38,413] {docker.py:276} INFO - 21/05/14 14:21:38 INFO ShuffleBlockFetcherIterator: Getting 5 (40.8 KiB) non-empty blocks including 5 (40.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:21:38,420] {docker.py:276} INFO - 21/05/14 14:21:38 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:38,421] {docker.py:276} INFO - 21/05/14 14:21:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:38,422] {docker.py:276} INFO - 21/05/14 14:21:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435194562866406189947_0004_m_000161_449, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435194562866406189947_0004_m_000161_449}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435194562866406189947_0004}; taskId=attempt_202105141416435194562866406189947_0004_m_000161_449, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@63ef96ca}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:38,422] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416435194562866406189947_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435194562866406189947_0004_m_000161_449
[2021-05-14 11:21:38,426] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Task committer attempt_202105141416435194562866406189947_0004_m_000161_449: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435194562866406189947_0004_m_000161_449 : duration 0:00.004s
[2021-05-14 11:21:38,430] {docker.py:276} INFO - 21/05/14 14:21:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:38 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:38,431] {docker.py:276} INFO - 21/05/14 14:21:38 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438152900926787378974_0004_m_000162_450, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438152900926787378974_0004_m_000162_450}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438152900926787378974_0004}; taskId=attempt_202105141416438152900926787378974_0004_m_000162_450, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@384091df}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:38 INFO StagingCommitter: Starting: Task committer attempt_202105141416438152900926787378974_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438152900926787378974_0004_m_000162_450
[2021-05-14 11:21:38,435] {docker.py:276} INFO - 21/05/14 14:21:38 INFO StagingCommitter: Task committer attempt_202105141416438152900926787378974_0004_m_000162_450: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438152900926787378974_0004_m_000162_450 : duration 0:00.003s
[2021-05-14 11:21:41,169] {docker.py:276} INFO - 21/05/14 14:21:41 INFO StagingCommitter: Starting: Task committer attempt_20210514141643120764126154748865_0004_m_000159_447: needsTaskCommit() Task attempt_20210514141643120764126154748865_0004_m_000159_447
[2021-05-14 11:21:41,171] {docker.py:276} INFO - 21/05/14 14:21:41 INFO StagingCommitter: Task committer attempt_20210514141643120764126154748865_0004_m_000159_447: needsTaskCommit() Task attempt_20210514141643120764126154748865_0004_m_000159_447: duration 0:00.002s
21/05/14 14:21:41 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643120764126154748865_0004_m_000159_447
[2021-05-14 11:21:41,172] {docker.py:276} INFO - 21/05/14 14:21:41 INFO Executor: Finished task 159.0 in stage 4.0 (TID 447). 5149 bytes result sent to driver
[2021-05-14 11:21:41,173] {docker.py:276} INFO - 21/05/14 14:21:41 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 451) (0d1d3e01326c, executor driver, partition 163, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:41,174] {docker.py:276} INFO - 21/05/14 14:21:41 INFO Executor: Running task 163.0 in stage 4.0 (TID 451)
[2021-05-14 11:21:41,175] {docker.py:276} INFO - 21/05/14 14:21:41 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 447) in 6649 ms on 0d1d3e01326c (executor driver) (160/200)
[2021-05-14 11:21:41,185] {docker.py:276} INFO - 21/05/14 14:21:41 INFO ShuffleBlockFetcherIterator: Getting 5 (41.3 KiB) non-empty blocks including 5 (41.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:41,192] {docker.py:276} INFO - 21/05/14 14:21:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436588238653633570684_0004_m_000163_451, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436588238653633570684_0004_m_000163_451}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436588238653633570684_0004}; taskId=attempt_202105141416436588238653633570684_0004_m_000163_451, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@144bee5d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:41 INFO StagingCommitter: Starting: Task committer attempt_202105141416436588238653633570684_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436588238653633570684_0004_m_000163_451
[2021-05-14 11:21:41,196] {docker.py:276} INFO - 21/05/14 14:21:41 INFO StagingCommitter: Task committer attempt_202105141416436588238653633570684_0004_m_000163_451: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436588238653633570684_0004_m_000163_451 : duration 0:00.004s
[2021-05-14 11:21:42,672] {docker.py:276} INFO - 21/05/14 14:21:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416436699715562342563_0004_m_000160_448: needsTaskCommit() Task attempt_202105141416436699715562342563_0004_m_000160_448
[2021-05-14 11:21:42,673] {docker.py:276} INFO - 21/05/14 14:21:42 INFO StagingCommitter: Task committer attempt_202105141416436699715562342563_0004_m_000160_448: needsTaskCommit() Task attempt_202105141416436699715562342563_0004_m_000160_448: duration 0:00.002s
21/05/14 14:21:42 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436699715562342563_0004_m_000160_448
[2021-05-14 11:21:42,674] {docker.py:276} INFO - 21/05/14 14:21:42 INFO Executor: Finished task 160.0 in stage 4.0 (TID 448). 5149 bytes result sent to driver
[2021-05-14 11:21:42,675] {docker.py:276} INFO - 21/05/14 14:21:42 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 452) (0d1d3e01326c, executor driver, partition 164, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:42,677] {docker.py:276} INFO - 21/05/14 14:21:42 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 448) in 7033 ms on 0d1d3e01326c (executor driver) (161/200)
[2021-05-14 11:21:42,678] {docker.py:276} INFO - 21/05/14 14:21:42 INFO Executor: Running task 164.0 in stage 4.0 (TID 452)
[2021-05-14 11:21:42,688] {docker.py:276} INFO - 21/05/14 14:21:42 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:42,696] {docker.py:276} INFO - 21/05/14 14:21:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:42 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:42 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431292939222942835211_0004_m_000164_452, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431292939222942835211_0004_m_000164_452}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431292939222942835211_0004}; taskId=attempt_202105141416431292939222942835211_0004_m_000164_452, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1794d564}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:42,696] {docker.py:276} INFO - 21/05/14 14:21:42 INFO StagingCommitter: Starting: Task committer attempt_202105141416431292939222942835211_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431292939222942835211_0004_m_000164_452
[2021-05-14 11:21:42,700] {docker.py:276} INFO - 21/05/14 14:21:42 INFO StagingCommitter: Task committer attempt_202105141416431292939222942835211_0004_m_000164_452: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431292939222942835211_0004_m_000164_452 : duration 0:00.004s
[2021-05-14 11:21:45,311] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Starting: Task committer attempt_202105141416435194562866406189947_0004_m_000161_449: needsTaskCommit() Task attempt_202105141416435194562866406189947_0004_m_000161_449
[2021-05-14 11:21:45,312] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Task committer attempt_202105141416435194562866406189947_0004_m_000161_449: needsTaskCommit() Task attempt_202105141416435194562866406189947_0004_m_000161_449: duration 0:00.001s
21/05/14 14:21:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435194562866406189947_0004_m_000161_449
[2021-05-14 11:21:45,314] {docker.py:276} INFO - 21/05/14 14:21:45 INFO Executor: Finished task 161.0 in stage 4.0 (TID 449). 5106 bytes result sent to driver
21/05/14 14:21:45 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 453) (0d1d3e01326c, executor driver, partition 165, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:45,315] {docker.py:276} INFO - 21/05/14 14:21:45 INFO Executor: Running task 165.0 in stage 4.0 (TID 453)
21/05/14 14:21:45 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 449) in 6888 ms on 0d1d3e01326c (executor driver) (162/200)
[2021-05-14 11:21:45,334] {docker.py:276} INFO - 21/05/14 14:21:45 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:21:45,334] {docker.py:276} INFO - 21/05/14 14:21:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:45,342] {docker.py:276} INFO - 21/05/14 14:21:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:45,342] {docker.py:276} INFO - 21/05/14 14:21:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:45,342] {docker.py:276} INFO - 21/05/14 14:21:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435935419781104219255_0004_m_000165_453, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435935419781104219255_0004_m_000165_453}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435935419781104219255_0004}; taskId=attempt_202105141416435935419781104219255_0004_m_000165_453, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5649a995}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:45,343] {docker.py:276} INFO - 21/05/14 14:21:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:45,343] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Starting: Task committer attempt_202105141416435935419781104219255_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435935419781104219255_0004_m_000165_453
[2021-05-14 11:21:45,347] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Task committer attempt_202105141416435935419781104219255_0004_m_000165_453: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435935419781104219255_0004_m_000165_453 : duration 0:00.004s
[2021-05-14 11:21:45,727] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Starting: Task committer attempt_202105141416438152900926787378974_0004_m_000162_450: needsTaskCommit() Task attempt_202105141416438152900926787378974_0004_m_000162_450
[2021-05-14 11:21:45,727] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Task committer attempt_202105141416438152900926787378974_0004_m_000162_450: needsTaskCommit() Task attempt_202105141416438152900926787378974_0004_m_000162_450: duration 0:00.001s
21/05/14 14:21:45 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438152900926787378974_0004_m_000162_450
[2021-05-14 11:21:45,729] {docker.py:276} INFO - 21/05/14 14:21:45 INFO Executor: Finished task 162.0 in stage 4.0 (TID 450). 5149 bytes result sent to driver
[2021-05-14 11:21:45,730] {docker.py:276} INFO - 21/05/14 14:21:45 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 454) (0d1d3e01326c, executor driver, partition 166, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:45,731] {docker.py:276} INFO - 21/05/14 14:21:45 INFO Executor: Running task 166.0 in stage 4.0 (TID 454)
21/05/14 14:21:45 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 450) in 7293 ms on 0d1d3e01326c (executor driver) (163/200)
[2021-05-14 11:21:45,738] {docker.py:276} INFO - 21/05/14 14:21:45 INFO ShuffleBlockFetcherIterator: Getting 5 (43.5 KiB) non-empty blocks including 5 (43.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:45,747] {docker.py:276} INFO - 21/05/14 14:21:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:45 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:45 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643536908146510069370_0004_m_000166_454, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643536908146510069370_0004_m_000166_454}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643536908146510069370_0004}; taskId=attempt_20210514141643536908146510069370_0004_m_000166_454, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@321c44ce}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:45 INFO StagingCommitter: Starting: Task committer attempt_20210514141643536908146510069370_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643536908146510069370_0004_m_000166_454
[2021-05-14 11:21:45,751] {docker.py:276} INFO - 21/05/14 14:21:45 INFO StagingCommitter: Task committer attempt_20210514141643536908146510069370_0004_m_000166_454: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643536908146510069370_0004_m_000166_454 : duration 0:00.004s
[2021-05-14 11:21:47,485] {docker.py:276} INFO - 21/05/14 14:21:47 INFO StagingCommitter: Starting: Task committer attempt_202105141416436588238653633570684_0004_m_000163_451: needsTaskCommit() Task attempt_202105141416436588238653633570684_0004_m_000163_451
[2021-05-14 11:21:47,486] {docker.py:276} INFO - 21/05/14 14:21:47 INFO StagingCommitter: Task committer attempt_202105141416436588238653633570684_0004_m_000163_451: needsTaskCommit() Task attempt_202105141416436588238653633570684_0004_m_000163_451: duration 0:00.001s
21/05/14 14:21:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436588238653633570684_0004_m_000163_451
[2021-05-14 11:21:47,487] {docker.py:276} INFO - 21/05/14 14:21:47 INFO Executor: Finished task 163.0 in stage 4.0 (TID 451). 5149 bytes result sent to driver
[2021-05-14 11:21:47,488] {docker.py:276} INFO - 21/05/14 14:21:47 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 455) (0d1d3e01326c, executor driver, partition 167, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:47,489] {docker.py:276} INFO - 21/05/14 14:21:47 INFO Executor: Running task 167.0 in stage 4.0 (TID 455)
[2021-05-14 11:21:47,489] {docker.py:276} INFO - 21/05/14 14:21:47 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 451) in 6324 ms on 0d1d3e01326c (executor driver) (164/200)
[2021-05-14 11:21:47,496] {docker.py:276} INFO - 21/05/14 14:21:47 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:47,503] {docker.py:276} INFO - 21/05/14 14:21:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:47 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:47 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416433521747725249316606_0004_m_000167_455, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433521747725249316606_0004_m_000167_455}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416433521747725249316606_0004}; taskId=attempt_202105141416433521747725249316606_0004_m_000167_455, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@576f485a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:47 INFO StagingCommitter: Starting: Task committer attempt_202105141416433521747725249316606_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433521747725249316606_0004_m_000167_455
[2021-05-14 11:21:47,507] {docker.py:276} INFO - 21/05/14 14:21:47 INFO StagingCommitter: Task committer attempt_202105141416433521747725249316606_0004_m_000167_455: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416433521747725249316606_0004_m_000167_455 : duration 0:00.004s
[2021-05-14 11:21:50,361] {docker.py:276} INFO - 21/05/14 14:21:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416431292939222942835211_0004_m_000164_452: needsTaskCommit() Task attempt_202105141416431292939222942835211_0004_m_000164_452
[2021-05-14 11:21:50,362] {docker.py:276} INFO - 21/05/14 14:21:50 INFO StagingCommitter: Task committer attempt_202105141416431292939222942835211_0004_m_000164_452: needsTaskCommit() Task attempt_202105141416431292939222942835211_0004_m_000164_452: duration 0:00.003s
21/05/14 14:21:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431292939222942835211_0004_m_000164_452
[2021-05-14 11:21:50,365] {docker.py:276} INFO - 21/05/14 14:21:50 INFO Executor: Finished task 164.0 in stage 4.0 (TID 452). 5149 bytes result sent to driver
[2021-05-14 11:21:50,366] {docker.py:276} INFO - 21/05/14 14:21:50 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 456) (0d1d3e01326c, executor driver, partition 168, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:50,367] {docker.py:276} INFO - 21/05/14 14:21:50 INFO Executor: Running task 168.0 in stage 4.0 (TID 456)
[2021-05-14 11:21:50,367] {docker.py:276} INFO - 21/05/14 14:21:50 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 452) in 7700 ms on 0d1d3e01326c (executor driver) (165/200)
[2021-05-14 11:21:50,378] {docker.py:276} INFO - 21/05/14 14:21:50 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:50,386] {docker.py:276} INFO - 21/05/14 14:21:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:50 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:50 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435078854021762381125_0004_m_000168_456, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435078854021762381125_0004_m_000168_456}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435078854021762381125_0004}; taskId=attempt_202105141416435078854021762381125_0004_m_000168_456, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@d6bd378}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:50,386] {docker.py:276} INFO - 21/05/14 14:21:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:50 INFO StagingCommitter: Starting: Task committer attempt_202105141416435078854021762381125_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435078854021762381125_0004_m_000168_456
[2021-05-14 11:21:50,389] {docker.py:276} INFO - 21/05/14 14:21:50 INFO StagingCommitter: Task committer attempt_202105141416435078854021762381125_0004_m_000168_456: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435078854021762381125_0004_m_000168_456 : duration 0:00.003s
[2021-05-14 11:21:51,587] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416435935419781104219255_0004_m_000165_453: needsTaskCommit() Task attempt_202105141416435935419781104219255_0004_m_000165_453
[2021-05-14 11:21:51,589] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Task committer attempt_202105141416435935419781104219255_0004_m_000165_453: needsTaskCommit() Task attempt_202105141416435935419781104219255_0004_m_000165_453: duration 0:00.002s
[2021-05-14 11:21:51,590] {docker.py:276} INFO - 21/05/14 14:21:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435935419781104219255_0004_m_000165_453
[2021-05-14 11:21:51,590] {docker.py:276} INFO - 21/05/14 14:21:51 INFO Executor: Finished task 165.0 in stage 4.0 (TID 453). 5149 bytes result sent to driver
[2021-05-14 11:21:51,591] {docker.py:276} INFO - 21/05/14 14:21:51 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 457) (0d1d3e01326c, executor driver, partition 169, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:51,592] {docker.py:276} INFO - 21/05/14 14:21:51 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 453) in 6286 ms on 0d1d3e01326c (executor driver) (166/200)
[2021-05-14 11:21:51,592] {docker.py:276} INFO - 21/05/14 14:21:51 INFO Executor: Running task 169.0 in stage 4.0 (TID 457)
[2021-05-14 11:21:51,600] {docker.py:276} INFO - 21/05/14 14:21:51 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2021-05-14 11:21:51,612] {docker.py:276} INFO - 21/05/14 14:21:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:51,612] {docker.py:276} INFO - 21/05/14 14:21:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:51,613] {docker.py:276} INFO - 21/05/14 14:21:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434281665257348228624_0004_m_000169_457, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434281665257348228624_0004_m_000169_457}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434281665257348228624_0004}; taskId=attempt_202105141416434281665257348228624_0004_m_000169_457, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6d3c6f1b}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416434281665257348228624_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434281665257348228624_0004_m_000169_457
[2021-05-14 11:21:51,616] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Task committer attempt_202105141416434281665257348228624_0004_m_000169_457: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434281665257348228624_0004_m_000169_457 : duration 0:00.005s
[2021-05-14 11:21:51,758] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Starting: Task committer attempt_20210514141643536908146510069370_0004_m_000166_454: needsTaskCommit() Task attempt_20210514141643536908146510069370_0004_m_000166_454
[2021-05-14 11:21:51,759] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Task committer attempt_20210514141643536908146510069370_0004_m_000166_454: needsTaskCommit() Task attempt_20210514141643536908146510069370_0004_m_000166_454: duration 0:00.002s
21/05/14 14:21:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643536908146510069370_0004_m_000166_454
[2021-05-14 11:21:51,759] {docker.py:276} INFO - 21/05/14 14:21:51 INFO Executor: Finished task 166.0 in stage 4.0 (TID 454). 5106 bytes result sent to driver
[2021-05-14 11:21:51,760] {docker.py:276} INFO - 21/05/14 14:21:51 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 458) (0d1d3e01326c, executor driver, partition 170, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:51,761] {docker.py:276} INFO - 21/05/14 14:21:51 INFO Executor: Running task 170.0 in stage 4.0 (TID 458)
21/05/14 14:21:51 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 454) in 6038 ms on 0d1d3e01326c (executor driver) (167/200)
[2021-05-14 11:21:51,770] {docker.py:276} INFO - 21/05/14 14:21:51 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:51,780] {docker.py:276} INFO - 21/05/14 14:21:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:51,780] {docker.py:276} INFO - 21/05/14 14:21:51 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:51 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432126676383530396292_0004_m_000170_458, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432126676383530396292_0004_m_000170_458}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432126676383530396292_0004}; taskId=attempt_202105141416432126676383530396292_0004_m_000170_458, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@17bd7370}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:51,781] {docker.py:276} INFO - 21/05/14 14:21:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:51 INFO StagingCommitter: Starting: Task committer attempt_202105141416432126676383530396292_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432126676383530396292_0004_m_000170_458
[2021-05-14 11:21:51,785] {docker.py:276} INFO - 21/05/14 14:21:51 INFO StagingCommitter: Task committer attempt_202105141416432126676383530396292_0004_m_000170_458: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432126676383530396292_0004_m_000170_458 : duration 0:00.004s
[2021-05-14 11:21:54,514] {docker.py:276} INFO - 21/05/14 14:21:54 INFO StagingCommitter: Starting: Task committer attempt_202105141416433521747725249316606_0004_m_000167_455: needsTaskCommit() Task attempt_202105141416433521747725249316606_0004_m_000167_455
[2021-05-14 11:21:54,515] {docker.py:276} INFO - 21/05/14 14:21:54 INFO StagingCommitter: Task committer attempt_202105141416433521747725249316606_0004_m_000167_455: needsTaskCommit() Task attempt_202105141416433521747725249316606_0004_m_000167_455: duration 0:00.003s
21/05/14 14:21:54 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416433521747725249316606_0004_m_000167_455
[2021-05-14 11:21:54,518] {docker.py:276} INFO - 21/05/14 14:21:54 INFO Executor: Finished task 167.0 in stage 4.0 (TID 455). 5149 bytes result sent to driver
[2021-05-14 11:21:54,518] {docker.py:276} INFO - 21/05/14 14:21:54 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 459) (0d1d3e01326c, executor driver, partition 171, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:54,519] {docker.py:276} INFO - 21/05/14 14:21:54 INFO Executor: Running task 171.0 in stage 4.0 (TID 459)
[2021-05-14 11:21:54,520] {docker.py:276} INFO - 21/05/14 14:21:54 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 455) in 7040 ms on 0d1d3e01326c (executor driver) (168/200)
[2021-05-14 11:21:54,529] {docker.py:276} INFO - 21/05/14 14:21:54 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:21:54,530] {docker.py:276} INFO - 21/05/14 14:21:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:54,538] {docker.py:276} INFO - 21/05/14 14:21:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:54,539] {docker.py:276} INFO - 21/05/14 14:21:54 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:54,539] {docker.py:276} INFO - 21/05/14 14:21:54 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432854998281839659118_0004_m_000171_459, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432854998281839659118_0004_m_000171_459}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432854998281839659118_0004}; taskId=attempt_202105141416432854998281839659118_0004_m_000171_459, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bccd4a5}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:54,539] {docker.py:276} INFO - 21/05/14 14:21:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:54,539] {docker.py:276} INFO - 21/05/14 14:21:54 INFO StagingCommitter: Starting: Task committer attempt_202105141416432854998281839659118_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432854998281839659118_0004_m_000171_459
[2021-05-14 11:21:54,544] {docker.py:276} INFO - 21/05/14 14:21:54 INFO StagingCommitter: Task committer attempt_202105141416432854998281839659118_0004_m_000171_459: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432854998281839659118_0004_m_000171_459 : duration 0:00.005s
[2021-05-14 11:21:57,358] {docker.py:276} INFO - 21/05/14 14:21:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416435078854021762381125_0004_m_000168_456: needsTaskCommit() Task attempt_202105141416435078854021762381125_0004_m_000168_456
[2021-05-14 11:21:57,359] {docker.py:276} INFO - 21/05/14 14:21:57 INFO StagingCommitter: Task committer attempt_202105141416435078854021762381125_0004_m_000168_456: needsTaskCommit() Task attempt_202105141416435078854021762381125_0004_m_000168_456: duration 0:00.003s
21/05/14 14:21:57 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435078854021762381125_0004_m_000168_456
[2021-05-14 11:21:57,361] {docker.py:276} INFO - 21/05/14 14:21:57 INFO Executor: Finished task 168.0 in stage 4.0 (TID 456). 5149 bytes result sent to driver
[2021-05-14 11:21:57,363] {docker.py:276} INFO - 21/05/14 14:21:57 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 460) (0d1d3e01326c, executor driver, partition 172, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:57,364] {docker.py:276} INFO - 21/05/14 14:21:57 INFO Executor: Running task 172.0 in stage 4.0 (TID 460)
21/05/14 14:21:57 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 456) in 7007 ms on 0d1d3e01326c (executor driver) (169/200)
[2021-05-14 11:21:57,376] {docker.py:276} INFO - 21/05/14 14:21:57 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:57,386] {docker.py:276} INFO - 21/05/14 14:21:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:57 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:57 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437145878829659844414_0004_m_000172_460, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437145878829659844414_0004_m_000172_460}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437145878829659844414_0004}; taskId=attempt_202105141416437145878829659844414_0004_m_000172_460, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c78e49d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:57,386] {docker.py:276} INFO - 21/05/14 14:21:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:57 INFO StagingCommitter: Starting: Task committer attempt_202105141416437145878829659844414_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437145878829659844414_0004_m_000172_460
[2021-05-14 11:21:57,390] {docker.py:276} INFO - 21/05/14 14:21:57 INFO StagingCommitter: Task committer attempt_202105141416437145878829659844414_0004_m_000172_460: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437145878829659844414_0004_m_000172_460 : duration 0:00.004s
[2021-05-14 11:21:58,450] {docker.py:276} INFO - 21/05/14 14:21:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416432126676383530396292_0004_m_000170_458: needsTaskCommit() Task attempt_202105141416432126676383530396292_0004_m_000170_458
[2021-05-14 11:21:58,451] {docker.py:276} INFO - 21/05/14 14:21:58 INFO StagingCommitter: Task committer attempt_202105141416432126676383530396292_0004_m_000170_458: needsTaskCommit() Task attempt_202105141416432126676383530396292_0004_m_000170_458: duration 0:00.002s
21/05/14 14:21:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432126676383530396292_0004_m_000170_458
[2021-05-14 11:21:58,452] {docker.py:276} INFO - 21/05/14 14:21:58 INFO Executor: Finished task 170.0 in stage 4.0 (TID 458). 5149 bytes result sent to driver
[2021-05-14 11:21:58,454] {docker.py:276} INFO - 21/05/14 14:21:58 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 461) (0d1d3e01326c, executor driver, partition 173, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:58,454] {docker.py:276} INFO - 21/05/14 14:21:58 INFO Executor: Running task 173.0 in stage 4.0 (TID 461)
[2021-05-14 11:21:58,455] {docker.py:276} INFO - 21/05/14 14:21:58 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 458) in 6703 ms on 0d1d3e01326c (executor driver) (170/200)
[2021-05-14 11:21:58,462] {docker.py:276} INFO - 21/05/14 14:21:58 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:21:58,463] {docker.py:276} INFO - 21/05/14 14:21:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:58,470] {docker.py:276} INFO - 21/05/14 14:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:21:58,471] {docker.py:276} INFO - 21/05/14 14:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:21:58,471] {docker.py:276} INFO - 21/05/14 14:21:58 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:21:58,471] {docker.py:276} INFO - 21/05/14 14:21:58 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435228480737100704288_0004_m_000173_461, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435228480737100704288_0004_m_000173_461}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435228480737100704288_0004}; taskId=attempt_202105141416435228480737100704288_0004_m_000173_461, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3dab404a}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:21:58,471] {docker.py:276} INFO - 21/05/14 14:21:58 INFO StagingCommitter: Starting: Task committer attempt_202105141416435228480737100704288_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435228480737100704288_0004_m_000173_461
[2021-05-14 11:21:58,475] {docker.py:276} INFO - 21/05/14 14:21:58 INFO StagingCommitter: Task committer attempt_202105141416435228480737100704288_0004_m_000173_461: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435228480737100704288_0004_m_000173_461 : duration 0:00.005s
[2021-05-14 11:21:59,018] {docker.py:276} INFO - 21/05/14 14:21:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416434281665257348228624_0004_m_000169_457: needsTaskCommit() Task attempt_202105141416434281665257348228624_0004_m_000169_457
[2021-05-14 11:21:59,019] {docker.py:276} INFO - 21/05/14 14:21:59 INFO StagingCommitter: Task committer attempt_202105141416434281665257348228624_0004_m_000169_457: needsTaskCommit() Task attempt_202105141416434281665257348228624_0004_m_000169_457: duration 0:00.003s
[2021-05-14 11:21:59,020] {docker.py:276} INFO - 21/05/14 14:21:59 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434281665257348228624_0004_m_000169_457
[2021-05-14 11:21:59,022] {docker.py:276} INFO - 21/05/14 14:21:59 INFO Executor: Finished task 169.0 in stage 4.0 (TID 457). 5149 bytes result sent to driver
[2021-05-14 11:21:59,023] {docker.py:276} INFO - 21/05/14 14:21:59 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 462) (0d1d3e01326c, executor driver, partition 174, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:21:59,025] {docker.py:276} INFO - 21/05/14 14:21:59 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 457) in 7443 ms on 0d1d3e01326c (executor driver) (171/200)
[2021-05-14 11:21:59,026] {docker.py:276} INFO - 21/05/14 14:21:59 INFO Executor: Running task 174.0 in stage 4.0 (TID 462)
[2021-05-14 11:21:59,035] {docker.py:276} INFO - 21/05/14 14:21:59 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:21:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:21:59,043] {docker.py:276} INFO - 21/05/14 14:21:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:21:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:21:59 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:21:59 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435246308160303129917_0004_m_000174_462, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435246308160303129917_0004_m_000174_462}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435246308160303129917_0004}; taskId=attempt_202105141416435246308160303129917_0004_m_000174_462, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1ea96203}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:21:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:21:59 INFO StagingCommitter: Starting: Task committer attempt_202105141416435246308160303129917_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435246308160303129917_0004_m_000174_462
[2021-05-14 11:21:59,048] {docker.py:276} INFO - 21/05/14 14:21:59 INFO StagingCommitter: Task committer attempt_202105141416435246308160303129917_0004_m_000174_462: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435246308160303129917_0004_m_000174_462 : duration 0:00.005s
[2021-05-14 11:22:01,508] {docker.py:276} INFO - 21/05/14 14:22:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416432854998281839659118_0004_m_000171_459: needsTaskCommit() Task attempt_202105141416432854998281839659118_0004_m_000171_459
[2021-05-14 11:22:01,509] {docker.py:276} INFO - 21/05/14 14:22:01 INFO StagingCommitter: Task committer attempt_202105141416432854998281839659118_0004_m_000171_459: needsTaskCommit() Task attempt_202105141416432854998281839659118_0004_m_000171_459: duration 0:00.003s
21/05/14 14:22:01 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432854998281839659118_0004_m_000171_459
[2021-05-14 11:22:01,511] {docker.py:276} INFO - 21/05/14 14:22:01 INFO Executor: Finished task 171.0 in stage 4.0 (TID 459). 5106 bytes result sent to driver
[2021-05-14 11:22:01,512] {docker.py:276} INFO - 21/05/14 14:22:01 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 463) (0d1d3e01326c, executor driver, partition 175, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:01,513] {docker.py:276} INFO - 21/05/14 14:22:01 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 459) in 7003 ms on 0d1d3e01326c (executor driver) (172/200)
[2021-05-14 11:22:01,513] {docker.py:276} INFO - 21/05/14 14:22:01 INFO Executor: Running task 175.0 in stage 4.0 (TID 463)
[2021-05-14 11:22:01,521] {docker.py:276} INFO - 21/05/14 14:22:01 INFO ShuffleBlockFetcherIterator: Getting 5 (42.5 KiB) non-empty blocks including 5 (42.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:01,530] {docker.py:276} INFO - 21/05/14 14:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:01 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:01,530] {docker.py:276} INFO - 21/05/14 14:22:01 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437986729923890435719_0004_m_000175_463, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437986729923890435719_0004_m_000175_463}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437986729923890435719_0004}; taskId=attempt_202105141416437986729923890435719_0004_m_000175_463, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@283b0130}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:01 INFO StagingCommitter: Starting: Task committer attempt_202105141416437986729923890435719_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437986729923890435719_0004_m_000175_463
[2021-05-14 11:22:01,534] {docker.py:276} INFO - 21/05/14 14:22:01 INFO StagingCommitter: Task committer attempt_202105141416437986729923890435719_0004_m_000175_463: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437986729923890435719_0004_m_000175_463 : duration 0:00.004s
[2021-05-14 11:22:04,233] {docker.py:276} INFO - 21/05/14 14:22:04 INFO StagingCommitter: Starting: Task committer attempt_202105141416437145878829659844414_0004_m_000172_460: needsTaskCommit() Task attempt_202105141416437145878829659844414_0004_m_000172_460
[2021-05-14 11:22:04,235] {docker.py:276} INFO - 21/05/14 14:22:04 INFO StagingCommitter: Task committer attempt_202105141416437145878829659844414_0004_m_000172_460: needsTaskCommit() Task attempt_202105141416437145878829659844414_0004_m_000172_460: duration 0:00.002s
21/05/14 14:22:04 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437145878829659844414_0004_m_000172_460
[2021-05-14 11:22:04,238] {docker.py:276} INFO - 21/05/14 14:22:04 INFO Executor: Finished task 172.0 in stage 4.0 (TID 460). 5149 bytes result sent to driver
21/05/14 14:22:04 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 464) (0d1d3e01326c, executor driver, partition 176, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:04,239] {docker.py:276} INFO - 21/05/14 14:22:04 INFO Executor: Running task 176.0 in stage 4.0 (TID 464)
[2021-05-14 11:22:04,240] {docker.py:276} INFO - 21/05/14 14:22:04 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 460) in 6886 ms on 0d1d3e01326c (executor driver) (173/200)
[2021-05-14 11:22:04,253] {docker.py:276} INFO - 21/05/14 14:22:04 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:04,260] {docker.py:276} INFO - 21/05/14 14:22:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:04 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:04 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643198798425077566057_0004_m_000176_464, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643198798425077566057_0004_m_000176_464}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643198798425077566057_0004}; taskId=attempt_20210514141643198798425077566057_0004_m_000176_464, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b8585d}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:04,264] {docker.py:276} INFO - 21/05/14 14:22:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:04 INFO StagingCommitter: Starting: Task committer attempt_20210514141643198798425077566057_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643198798425077566057_0004_m_000176_464
[2021-05-14 11:22:04,267] {docker.py:276} INFO - 21/05/14 14:22:04 INFO StagingCommitter: Task committer attempt_20210514141643198798425077566057_0004_m_000176_464: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643198798425077566057_0004_m_000176_464 : duration 0:00.005s
[2021-05-14 11:22:05,497] {docker.py:276} INFO - 21/05/14 14:22:05 INFO StagingCommitter: Starting: Task committer attempt_202105141416435228480737100704288_0004_m_000173_461: needsTaskCommit() Task attempt_202105141416435228480737100704288_0004_m_000173_461
[2021-05-14 11:22:05,498] {docker.py:276} INFO - 21/05/14 14:22:05 INFO StagingCommitter: Task committer attempt_202105141416435228480737100704288_0004_m_000173_461: needsTaskCommit() Task attempt_202105141416435228480737100704288_0004_m_000173_461: duration 0:00.003s
21/05/14 14:22:05 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435228480737100704288_0004_m_000173_461
[2021-05-14 11:22:05,500] {docker.py:276} INFO - 21/05/14 14:22:05 INFO Executor: Finished task 173.0 in stage 4.0 (TID 461). 5149 bytes result sent to driver
[2021-05-14 11:22:05,501] {docker.py:276} INFO - 21/05/14 14:22:05 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 465) (0d1d3e01326c, executor driver, partition 177, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:05,501] {docker.py:276} INFO - 21/05/14 14:22:05 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 461) in 7054 ms on 0d1d3e01326c (executor driver) (174/200)
[2021-05-14 11:22:05,502] {docker.py:276} INFO - 21/05/14 14:22:05 INFO Executor: Running task 177.0 in stage 4.0 (TID 465)
[2021-05-14 11:22:05,511] {docker.py:276} INFO - 21/05/14 14:22:05 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:05,519] {docker.py:276} INFO - 21/05/14 14:22:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:05 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:05 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431016413148622353308_0004_m_000177_465, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431016413148622353308_0004_m_000177_465}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431016413148622353308_0004}; taskId=attempt_202105141416431016413148622353308_0004_m_000177_465, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ae3cc35}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:05 INFO StagingCommitter: Starting: Task committer attempt_202105141416431016413148622353308_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431016413148622353308_0004_m_000177_465
[2021-05-14 11:22:05,523] {docker.py:276} INFO - 21/05/14 14:22:05 INFO StagingCommitter: Task committer attempt_202105141416431016413148622353308_0004_m_000177_465: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431016413148622353308_0004_m_000177_465 : duration 0:00.004s
[2021-05-14 11:22:06,590] {docker.py:276} INFO - 21/05/14 14:22:06 INFO StagingCommitter: Starting: Task committer attempt_202105141416435246308160303129917_0004_m_000174_462: needsTaskCommit() Task attempt_202105141416435246308160303129917_0004_m_000174_462
[2021-05-14 11:22:06,591] {docker.py:276} INFO - 21/05/14 14:22:06 INFO StagingCommitter: Task committer attempt_202105141416435246308160303129917_0004_m_000174_462: needsTaskCommit() Task attempt_202105141416435246308160303129917_0004_m_000174_462: duration 0:00.003s
21/05/14 14:22:06 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435246308160303129917_0004_m_000174_462
[2021-05-14 11:22:06,592] {docker.py:276} INFO - 21/05/14 14:22:06 INFO Executor: Finished task 174.0 in stage 4.0 (TID 462). 5149 bytes result sent to driver
[2021-05-14 11:22:06,593] {docker.py:276} INFO - 21/05/14 14:22:06 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 466) (0d1d3e01326c, executor driver, partition 178, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:06,595] {docker.py:276} INFO - 21/05/14 14:22:06 INFO Executor: Running task 178.0 in stage 4.0 (TID 466)
[2021-05-14 11:22:06,595] {docker.py:276} INFO - 21/05/14 14:22:06 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 462) in 7581 ms on 0d1d3e01326c (executor driver) (175/200)
[2021-05-14 11:22:06,605] {docker.py:276} INFO - 21/05/14 14:22:06 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:06,613] {docker.py:276} INFO - 21/05/14 14:22:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:06 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:06,614] {docker.py:276} INFO - 21/05/14 14:22:06 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643679004319206846913_0004_m_000178_466, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643679004319206846913_0004_m_000178_466}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643679004319206846913_0004}; taskId=attempt_20210514141643679004319206846913_0004_m_000178_466, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@62392366}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:06,614] {docker.py:276} INFO - 21/05/14 14:22:06 INFO StagingCommitter: Starting: Task committer attempt_20210514141643679004319206846913_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643679004319206846913_0004_m_000178_466
[2021-05-14 11:22:06,619] {docker.py:276} INFO - 21/05/14 14:22:06 INFO StagingCommitter: Task committer attempt_20210514141643679004319206846913_0004_m_000178_466: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643679004319206846913_0004_m_000178_466 : duration 0:00.005s
[2021-05-14 11:22:07,451] {docker.py:276} INFO - 21/05/14 14:22:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416437986729923890435719_0004_m_000175_463: needsTaskCommit() Task attempt_202105141416437986729923890435719_0004_m_000175_463
[2021-05-14 11:22:07,451] {docker.py:276} INFO - 21/05/14 14:22:07 INFO StagingCommitter: Task committer attempt_202105141416437986729923890435719_0004_m_000175_463: needsTaskCommit() Task attempt_202105141416437986729923890435719_0004_m_000175_463: duration 0:00.003s
21/05/14 14:22:07 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437986729923890435719_0004_m_000175_463
[2021-05-14 11:22:07,452] {docker.py:276} INFO - 21/05/14 14:22:07 INFO Executor: Finished task 175.0 in stage 4.0 (TID 463). 5149 bytes result sent to driver
[2021-05-14 11:22:07,454] {docker.py:276} INFO - 21/05/14 14:22:07 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 467) (0d1d3e01326c, executor driver, partition 179, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:07,455] {docker.py:276} INFO - 21/05/14 14:22:07 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 463) in 5950 ms on 0d1d3e01326c (executor driver) (176/200)
[2021-05-14 11:22:07,456] {docker.py:276} INFO - 21/05/14 14:22:07 INFO Executor: Running task 179.0 in stage 4.0 (TID 467)
[2021-05-14 11:22:07,466] {docker.py:276} INFO - 21/05/14 14:22:07 INFO ShuffleBlockFetcherIterator: Getting 5 (42.4 KiB) non-empty blocks including 5 (42.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:07,467] {docker.py:276} INFO - 21/05/14 14:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:07,474] {docker.py:276} INFO - 21/05/14 14:22:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:07 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:07,475] {docker.py:276} INFO - 21/05/14 14:22:07 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434543264254841046282_0004_m_000179_467, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434543264254841046282_0004_m_000179_467}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434543264254841046282_0004}; taskId=attempt_202105141416434543264254841046282_0004_m_000179_467, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@139ba458}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:07,476] {docker.py:276} INFO - 21/05/14 14:22:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:07 INFO StagingCommitter: Starting: Task committer attempt_202105141416434543264254841046282_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434543264254841046282_0004_m_000179_467
[2021-05-14 11:22:07,480] {docker.py:276} INFO - 21/05/14 14:22:07 INFO StagingCommitter: Task committer attempt_202105141416434543264254841046282_0004_m_000179_467: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434543264254841046282_0004_m_000179_467 : duration 0:00.005s
[2021-05-14 11:22:11,234] {docker.py:276} INFO - 21/05/14 14:22:11 INFO StagingCommitter: Starting: Task committer attempt_20210514141643198798425077566057_0004_m_000176_464: needsTaskCommit() Task attempt_20210514141643198798425077566057_0004_m_000176_464
[2021-05-14 11:22:11,235] {docker.py:276} INFO - 21/05/14 14:22:11 INFO StagingCommitter: Task committer attempt_20210514141643198798425077566057_0004_m_000176_464: needsTaskCommit() Task attempt_20210514141643198798425077566057_0004_m_000176_464: duration 0:00.003s
21/05/14 14:22:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643198798425077566057_0004_m_000176_464
[2021-05-14 11:22:11,237] {docker.py:276} INFO - 21/05/14 14:22:11 INFO Executor: Finished task 176.0 in stage 4.0 (TID 464). 5106 bytes result sent to driver
[2021-05-14 11:22:11,238] {docker.py:276} INFO - 21/05/14 14:22:11 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 468) (0d1d3e01326c, executor driver, partition 180, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:11,239] {docker.py:276} INFO - 21/05/14 14:22:11 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 464) in 6973 ms on 0d1d3e01326c (executor driver) (177/200)
[2021-05-14 11:22:11,239] {docker.py:276} INFO - 21/05/14 14:22:11 INFO Executor: Running task 180.0 in stage 4.0 (TID 468)
[2021-05-14 11:22:11,249] {docker.py:276} INFO - 21/05/14 14:22:11 INFO ShuffleBlockFetcherIterator: Getting 5 (42.6 KiB) non-empty blocks including 5 (42.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:11,257] {docker.py:276} INFO - 21/05/14 14:22:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:11 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:11 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432216992845460517014_0004_m_000180_468, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432216992845460517014_0004_m_000180_468}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432216992845460517014_0004}; taskId=attempt_202105141416432216992845460517014_0004_m_000180_468, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4bebf2c}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:11,258] {docker.py:276} INFO - 21/05/14 14:22:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:11 INFO StagingCommitter: Starting: Task committer attempt_202105141416432216992845460517014_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432216992845460517014_0004_m_000180_468
[2021-05-14 11:22:11,261] {docker.py:276} INFO - 21/05/14 14:22:11 INFO StagingCommitter: Task committer attempt_202105141416432216992845460517014_0004_m_000180_468: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432216992845460517014_0004_m_000180_468 : duration 0:00.004s
[2021-05-14 11:22:11,975] {docker.py:276} INFO - 21/05/14 14:22:11 INFO StagingCommitter: Starting: Task committer attempt_202105141416431016413148622353308_0004_m_000177_465: needsTaskCommit() Task attempt_202105141416431016413148622353308_0004_m_000177_465
[2021-05-14 11:22:11,976] {docker.py:276} INFO - 21/05/14 14:22:11 INFO StagingCommitter: Task committer attempt_202105141416431016413148622353308_0004_m_000177_465: needsTaskCommit() Task attempt_202105141416431016413148622353308_0004_m_000177_465: duration 0:00.001s
21/05/14 14:22:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431016413148622353308_0004_m_000177_465
[2021-05-14 11:22:11,977] {docker.py:276} INFO - 21/05/14 14:22:11 INFO Executor: Finished task 177.0 in stage 4.0 (TID 465). 5106 bytes result sent to driver
[2021-05-14 11:22:11,978] {docker.py:276} INFO - 21/05/14 14:22:11 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 469) (0d1d3e01326c, executor driver, partition 181, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:11,979] {docker.py:276} INFO - 21/05/14 14:22:11 INFO Executor: Running task 181.0 in stage 4.0 (TID 469)
[2021-05-14 11:22:11,981] {docker.py:276} INFO - 21/05/14 14:22:11 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 465) in 6455 ms on 0d1d3e01326c (executor driver) (178/200)
[2021-05-14 11:22:11,998] {docker.py:276} INFO - 21/05/14 14:22:12 INFO ShuffleBlockFetcherIterator: Getting 5 (43.1 KiB) non-empty blocks including 5 (43.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:12,005] {docker.py:276} INFO - 21/05/14 14:22:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:12 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:12 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436473134070359727959_0004_m_000181_469, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436473134070359727959_0004_m_000181_469}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436473134070359727959_0004}; taskId=attempt_202105141416436473134070359727959_0004_m_000181_469, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35bc00fc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:12,006] {docker.py:276} INFO - 21/05/14 14:22:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:12 INFO StagingCommitter: Starting: Task committer attempt_202105141416436473134070359727959_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436473134070359727959_0004_m_000181_469
[2021-05-14 11:22:12,010] {docker.py:276} INFO - 21/05/14 14:22:12 INFO StagingCommitter: Task committer attempt_202105141416436473134070359727959_0004_m_000181_469: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436473134070359727959_0004_m_000181_469 : duration 0:00.005s
[2021-05-14 11:22:14,031] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Starting: Task committer attempt_20210514141643679004319206846913_0004_m_000178_466: needsTaskCommit() Task attempt_20210514141643679004319206846913_0004_m_000178_466
[2021-05-14 11:22:14,032] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Task committer attempt_20210514141643679004319206846913_0004_m_000178_466: needsTaskCommit() Task attempt_20210514141643679004319206846913_0004_m_000178_466: duration 0:00.003s
21/05/14 14:22:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643679004319206846913_0004_m_000178_466
[2021-05-14 11:22:14,034] {docker.py:276} INFO - 21/05/14 14:22:14 INFO Executor: Finished task 178.0 in stage 4.0 (TID 466). 5149 bytes result sent to driver
[2021-05-14 11:22:14,035] {docker.py:276} INFO - 21/05/14 14:22:14 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 470) (0d1d3e01326c, executor driver, partition 182, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:14,037] {docker.py:276} INFO - 21/05/14 14:22:14 INFO Executor: Running task 182.0 in stage 4.0 (TID 470)
[2021-05-14 11:22:14,037] {docker.py:276} INFO - 21/05/14 14:22:14 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 466) in 7418 ms on 0d1d3e01326c (executor driver) (179/200)
[2021-05-14 11:22:14,047] {docker.py:276} INFO - 21/05/14 14:22:14 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:14,054] {docker.py:276} INFO - 21/05/14 14:22:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431264473932863713731_0004_m_000182_470, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431264473932863713731_0004_m_000182_470}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431264473932863713731_0004}; taskId=attempt_202105141416431264473932863713731_0004_m_000182_470, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@70f5d8ae}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:14,055] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416431264473932863713731_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431264473932863713731_0004_m_000182_470
[2021-05-14 11:22:14,059] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Task committer attempt_202105141416431264473932863713731_0004_m_000182_470: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431264473932863713731_0004_m_000182_470 : duration 0:00.004s
[2021-05-14 11:22:14,478] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416434543264254841046282_0004_m_000179_467: needsTaskCommit() Task attempt_202105141416434543264254841046282_0004_m_000179_467
[2021-05-14 11:22:14,478] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Task committer attempt_202105141416434543264254841046282_0004_m_000179_467: needsTaskCommit() Task attempt_202105141416434543264254841046282_0004_m_000179_467: duration 0:00.002s
[2021-05-14 11:22:14,479] {docker.py:276} INFO - 21/05/14 14:22:14 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434543264254841046282_0004_m_000179_467
[2021-05-14 11:22:14,481] {docker.py:276} INFO - 21/05/14 14:22:14 INFO Executor: Finished task 179.0 in stage 4.0 (TID 467). 5149 bytes result sent to driver
[2021-05-14 11:22:14,482] {docker.py:276} INFO - 21/05/14 14:22:14 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 471) (0d1d3e01326c, executor driver, partition 183, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:14,483] {docker.py:276} INFO - 21/05/14 14:22:14 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 467) in 7003 ms on 0d1d3e01326c (executor driver) (180/200)
[2021-05-14 11:22:14,483] {docker.py:276} INFO - 21/05/14 14:22:14 INFO Executor: Running task 183.0 in stage 4.0 (TID 471)
[2021-05-14 11:22:14,492] {docker.py:276} INFO - 21/05/14 14:22:14 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:14,500] {docker.py:276} INFO - 21/05/14 14:22:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:14 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:14 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437836296246898880085_0004_m_000183_471, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437836296246898880085_0004_m_000183_471}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437836296246898880085_0004}; taskId=attempt_202105141416437836296246898880085_0004_m_000183_471, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4791bbd7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:14,501] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Starting: Task committer attempt_202105141416437836296246898880085_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437836296246898880085_0004_m_000183_471
[2021-05-14 11:22:14,505] {docker.py:276} INFO - 21/05/14 14:22:14 INFO StagingCommitter: Task committer attempt_202105141416437836296246898880085_0004_m_000183_471: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437836296246898880085_0004_m_000183_471 : duration 0:00.004s
[2021-05-14 11:22:18,385] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416432216992845460517014_0004_m_000180_468: needsTaskCommit() Task attempt_202105141416432216992845460517014_0004_m_000180_468
[2021-05-14 11:22:18,385] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Task committer attempt_202105141416432216992845460517014_0004_m_000180_468: needsTaskCommit() Task attempt_202105141416432216992845460517014_0004_m_000180_468: duration 0:00.001s
21/05/14 14:22:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432216992845460517014_0004_m_000180_468
[2021-05-14 11:22:18,387] {docker.py:276} INFO - 21/05/14 14:22:18 INFO Executor: Finished task 180.0 in stage 4.0 (TID 468). 5149 bytes result sent to driver
[2021-05-14 11:22:18,388] {docker.py:276} INFO - 21/05/14 14:22:18 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 472) (0d1d3e01326c, executor driver, partition 184, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:18,389] {docker.py:276} INFO - 21/05/14 14:22:18 INFO Executor: Running task 184.0 in stage 4.0 (TID 472)
21/05/14 14:22:18 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 468) in 7161 ms on 0d1d3e01326c (executor driver) (181/200)
[2021-05-14 11:22:18,397] {docker.py:276} INFO - 21/05/14 14:22:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.4 KiB) non-empty blocks including 5 (43.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:18,406] {docker.py:276} INFO - 21/05/14 14:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437556294206140629982_0004_m_000184_472, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437556294206140629982_0004_m_000184_472}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437556294206140629982_0004}; taskId=attempt_202105141416437556294206140629982_0004_m_000184_472, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6b6c9df4}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416437556294206140629982_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437556294206140629982_0004_m_000184_472
[2021-05-14 11:22:18,409] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Task committer attempt_202105141416437556294206140629982_0004_m_000184_472: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437556294206140629982_0004_m_000184_472 : duration 0:00.004s
[2021-05-14 11:22:18,811] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416436473134070359727959_0004_m_000181_469: needsTaskCommit() Task attempt_202105141416436473134070359727959_0004_m_000181_469
[2021-05-14 11:22:18,812] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Task committer attempt_202105141416436473134070359727959_0004_m_000181_469: needsTaskCommit() Task attempt_202105141416436473134070359727959_0004_m_000181_469: duration 0:00.002s
[2021-05-14 11:22:18,814] {docker.py:276} INFO - 21/05/14 14:22:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436473134070359727959_0004_m_000181_469
[2021-05-14 11:22:18,816] {docker.py:276} INFO - 21/05/14 14:22:18 INFO Executor: Finished task 181.0 in stage 4.0 (TID 469). 5149 bytes result sent to driver
[2021-05-14 11:22:18,817] {docker.py:276} INFO - 21/05/14 14:22:18 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 473) (0d1d3e01326c, executor driver, partition 185, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:18,818] {docker.py:276} INFO - 21/05/14 14:22:18 INFO Executor: Running task 185.0 in stage 4.0 (TID 473)
[2021-05-14 11:22:18,818] {docker.py:276} INFO - 21/05/14 14:22:18 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 469) in 6849 ms on 0d1d3e01326c (executor driver) (182/200)
[2021-05-14 11:22:18,827] {docker.py:276} INFO - 21/05/14 14:22:18 INFO ShuffleBlockFetcherIterator: Getting 5 (43.3 KiB) non-empty blocks including 5 (43.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:18,827] {docker.py:276} INFO - 21/05/14 14:22:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:18,836] {docker.py:276} INFO - 21/05/14 14:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:22:18,837] {docker.py:276} INFO - 21/05/14 14:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:22:18,837] {docker.py:276} INFO - 21/05/14 14:22:18 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:18,837] {docker.py:276} INFO - 21/05/14 14:22:18 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438445745041922155935_0004_m_000185_473, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438445745041922155935_0004_m_000185_473}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438445745041922155935_0004}; taskId=attempt_202105141416438445745041922155935_0004_m_000185_473, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7ebf912}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:18,838] {docker.py:276} INFO - 21/05/14 14:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:18,838] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Starting: Task committer attempt_202105141416438445745041922155935_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438445745041922155935_0004_m_000185_473
[2021-05-14 11:22:18,841] {docker.py:276} INFO - 21/05/14 14:22:18 INFO StagingCommitter: Task committer attempt_202105141416438445745041922155935_0004_m_000185_473: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438445745041922155935_0004_m_000185_473 : duration 0:00.004s
[2021-05-14 11:22:20,858] {docker.py:276} INFO - 21/05/14 14:22:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416431264473932863713731_0004_m_000182_470: needsTaskCommit() Task attempt_202105141416431264473932863713731_0004_m_000182_470
[2021-05-14 11:22:20,859] {docker.py:276} INFO - 21/05/14 14:22:20 INFO StagingCommitter: Task committer attempt_202105141416431264473932863713731_0004_m_000182_470: needsTaskCommit() Task attempt_202105141416431264473932863713731_0004_m_000182_470: duration 0:00.004s
21/05/14 14:22:20 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431264473932863713731_0004_m_000182_470
[2021-05-14 11:22:20,860] {docker.py:276} INFO - 21/05/14 14:22:20 INFO Executor: Finished task 182.0 in stage 4.0 (TID 470). 5106 bytes result sent to driver
[2021-05-14 11:22:20,862] {docker.py:276} INFO - 21/05/14 14:22:20 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 474) (0d1d3e01326c, executor driver, partition 186, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:20,863] {docker.py:276} INFO - 21/05/14 14:22:20 INFO Executor: Running task 186.0 in stage 4.0 (TID 474)
21/05/14 14:22:20 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 470) in 6836 ms on 0d1d3e01326c (executor driver) (183/200)
[2021-05-14 11:22:20,873] {docker.py:276} INFO - 21/05/14 14:22:20 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:20,873] {docker.py:276} INFO - 21/05/14 14:22:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:20,892] {docker.py:276} INFO - 21/05/14 14:22:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:22:20,892] {docker.py:276} INFO - 21/05/14 14:22:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:22:20,893] {docker.py:276} INFO - 21/05/14 14:22:20 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:20 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416435940789304226485340_0004_m_000186_474, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435940789304226485340_0004_m_000186_474}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416435940789304226485340_0004}; taskId=attempt_202105141416435940789304226485340_0004_m_000186_474, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e173ec7}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:20,893] {docker.py:276} INFO - 21/05/14 14:22:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:20 INFO StagingCommitter: Starting: Task committer attempt_202105141416435940789304226485340_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435940789304226485340_0004_m_000186_474
[2021-05-14 11:22:20,897] {docker.py:276} INFO - 21/05/14 14:22:20 INFO StagingCommitter: Task committer attempt_202105141416435940789304226485340_0004_m_000186_474: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416435940789304226485340_0004_m_000186_474 : duration 0:00.004s
[2021-05-14 11:22:21,009] {docker.py:276} INFO - 21/05/14 14:22:21 INFO StagingCommitter: Starting: Task committer attempt_202105141416437836296246898880085_0004_m_000183_471: needsTaskCommit() Task attempt_202105141416437836296246898880085_0004_m_000183_471
[2021-05-14 11:22:21,010] {docker.py:276} INFO - 21/05/14 14:22:21 INFO StagingCommitter: Task committer attempt_202105141416437836296246898880085_0004_m_000183_471: needsTaskCommit() Task attempt_202105141416437836296246898880085_0004_m_000183_471: duration 0:00.002s
21/05/14 14:22:21 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437836296246898880085_0004_m_000183_471
[2021-05-14 11:22:21,011] {docker.py:276} INFO - 21/05/14 14:22:21 INFO Executor: Finished task 183.0 in stage 4.0 (TID 471). 5149 bytes result sent to driver
[2021-05-14 11:22:21,012] {docker.py:276} INFO - 21/05/14 14:22:21 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 475) (0d1d3e01326c, executor driver, partition 187, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:21,014] {docker.py:276} INFO - 21/05/14 14:22:21 INFO Executor: Running task 187.0 in stage 4.0 (TID 475)
21/05/14 14:22:21 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 471) in 6539 ms on 0d1d3e01326c (executor driver) (184/200)
[2021-05-14 11:22:21,024] {docker.py:276} INFO - 21/05/14 14:22:21 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:21,032] {docker.py:276} INFO - 21/05/14 14:22:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:21 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:21 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643665748441146867934_0004_m_000187_475, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643665748441146867934_0004_m_000187_475}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643665748441146867934_0004}; taskId=attempt_20210514141643665748441146867934_0004_m_000187_475, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1cf534f2}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:21,033] {docker.py:276} INFO - 21/05/14 14:22:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:21 INFO StagingCommitter: Starting: Task committer attempt_20210514141643665748441146867934_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643665748441146867934_0004_m_000187_475
[2021-05-14 11:22:21,037] {docker.py:276} INFO - 21/05/14 14:22:21 INFO StagingCommitter: Task committer attempt_20210514141643665748441146867934_0004_m_000187_475: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643665748441146867934_0004_m_000187_475 : duration 0:00.005s
[2021-05-14 11:22:25,767] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416438445745041922155935_0004_m_000185_473: needsTaskCommit() Task attempt_202105141416438445745041922155935_0004_m_000185_473
[2021-05-14 11:22:25,768] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Task committer attempt_202105141416438445745041922155935_0004_m_000185_473: needsTaskCommit() Task attempt_202105141416438445745041922155935_0004_m_000185_473: duration 0:00.003s
21/05/14 14:22:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438445745041922155935_0004_m_000185_473
[2021-05-14 11:22:25,771] {docker.py:276} INFO - 21/05/14 14:22:25 INFO Executor: Finished task 185.0 in stage 4.0 (TID 473). 5149 bytes result sent to driver
[2021-05-14 11:22:25,772] {docker.py:276} INFO - 21/05/14 14:22:25 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 476) (0d1d3e01326c, executor driver, partition 188, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:25,773] {docker.py:276} INFO - 21/05/14 14:22:25 INFO Executor: Running task 188.0 in stage 4.0 (TID 476)
[2021-05-14 11:22:25,773] {docker.py:276} INFO - 21/05/14 14:22:25 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 473) in 6965 ms on 0d1d3e01326c (executor driver) (185/200)
[2021-05-14 11:22:25,783] {docker.py:276} INFO - 21/05/14 14:22:25 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:25,783] {docker.py:276} INFO - 21/05/14 14:22:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:25,791] {docker.py:276} INFO - 21/05/14 14:22:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2021-05-14 11:22:25,791] {docker.py:276} INFO - 21/05/14 14:22:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:22:25,792] {docker.py:276} INFO - 21/05/14 14:22:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:25,792] {docker.py:276} INFO - 21/05/14 14:22:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438178235572334545389_0004_m_000188_476, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438178235572334545389_0004_m_000188_476}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438178235572334545389_0004}; taskId=attempt_202105141416438178235572334545389_0004_m_000188_476, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@66ee9192}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:25,792] {docker.py:276} INFO - 21/05/14 14:22:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:25,792] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416438178235572334545389_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438178235572334545389_0004_m_000188_476
[2021-05-14 11:22:25,796] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Task committer attempt_202105141416438178235572334545389_0004_m_000188_476: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438178235572334545389_0004_m_000188_476 : duration 0:00.004s
[2021-05-14 11:22:25,891] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Starting: Task committer attempt_202105141416437556294206140629982_0004_m_000184_472: needsTaskCommit() Task attempt_202105141416437556294206140629982_0004_m_000184_472
21/05/14 14:22:25 INFO StagingCommitter: Task committer attempt_202105141416437556294206140629982_0004_m_000184_472: needsTaskCommit() Task attempt_202105141416437556294206140629982_0004_m_000184_472: duration 0:00.001s
21/05/14 14:22:25 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437556294206140629982_0004_m_000184_472
[2021-05-14 11:22:25,892] {docker.py:276} INFO - 21/05/14 14:22:25 INFO Executor: Finished task 184.0 in stage 4.0 (TID 472). 5149 bytes result sent to driver
[2021-05-14 11:22:25,892] {docker.py:276} INFO - 21/05/14 14:22:25 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 477) (0d1d3e01326c, executor driver, partition 189, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:25,894] {docker.py:276} INFO - 21/05/14 14:22:25 INFO Executor: Running task 189.0 in stage 4.0 (TID 477)
[2021-05-14 11:22:25,894] {docker.py:276} INFO - 21/05/14 14:22:25 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 472) in 7514 ms on 0d1d3e01326c (executor driver) (186/200)
[2021-05-14 11:22:25,903] {docker.py:276} INFO - 21/05/14 14:22:25 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:25,904] {docker.py:276} INFO - 21/05/14 14:22:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:25,911] {docker.py:276} INFO - 21/05/14 14:22:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:25 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:25 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643613639034870293213_0004_m_000189_477, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643613639034870293213_0004_m_000189_477}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643613639034870293213_0004}; taskId=attempt_20210514141643613639034870293213_0004_m_000189_477, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@276d8a21}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:25 INFO StagingCommitter: Starting: Task committer attempt_20210514141643613639034870293213_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643613639034870293213_0004_m_000189_477
[2021-05-14 11:22:25,916] {docker.py:276} INFO - 21/05/14 14:22:25 INFO StagingCommitter: Task committer attempt_20210514141643613639034870293213_0004_m_000189_477: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643613639034870293213_0004_m_000189_477 : duration 0:00.005s
[2021-05-14 11:22:27,211] {docker.py:276} INFO - 21/05/14 14:22:27 INFO StagingCommitter: Starting: Task committer attempt_20210514141643665748441146867934_0004_m_000187_475: needsTaskCommit() Task attempt_20210514141643665748441146867934_0004_m_000187_475
21/05/14 14:22:27 INFO StagingCommitter: Task committer attempt_20210514141643665748441146867934_0004_m_000187_475: needsTaskCommit() Task attempt_20210514141643665748441146867934_0004_m_000187_475: duration 0:00.002s
21/05/14 14:22:27 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643665748441146867934_0004_m_000187_475
[2021-05-14 11:22:27,213] {docker.py:276} INFO - 21/05/14 14:22:27 INFO Executor: Finished task 187.0 in stage 4.0 (TID 475). 5106 bytes result sent to driver
[2021-05-14 11:22:27,215] {docker.py:276} INFO - 21/05/14 14:22:27 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 478) (0d1d3e01326c, executor driver, partition 190, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:27,215] {docker.py:276} INFO - 21/05/14 14:22:27 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 475) in 6210 ms on 0d1d3e01326c (executor driver) (187/200)
[2021-05-14 11:22:27,216] {docker.py:276} INFO - 21/05/14 14:22:27 INFO Executor: Running task 190.0 in stage 4.0 (TID 478)
[2021-05-14 11:22:27,224] {docker.py:276} INFO - 21/05/14 14:22:27 INFO ShuffleBlockFetcherIterator: Getting 5 (44.8 KiB) non-empty blocks including 5 (44.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:27,233] {docker.py:276} INFO - 21/05/14 14:22:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:22:27,234] {docker.py:276} INFO - 21/05/14 14:22:27 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:27,234] {docker.py:276} INFO - 21/05/14 14:22:27 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431081370787291605984_0004_m_000190_478, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431081370787291605984_0004_m_000190_478}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431081370787291605984_0004}; taskId=attempt_202105141416431081370787291605984_0004_m_000190_478, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@12d56320}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:27,234] {docker.py:276} INFO - 21/05/14 14:22:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:27 INFO StagingCommitter: Starting: Task committer attempt_202105141416431081370787291605984_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431081370787291605984_0004_m_000190_478
[2021-05-14 11:22:27,238] {docker.py:276} INFO - 21/05/14 14:22:27 INFO StagingCommitter: Task committer attempt_202105141416431081370787291605984_0004_m_000190_478: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431081370787291605984_0004_m_000190_478 : duration 0:00.005s
[2021-05-14 11:22:28,071] {docker.py:276} INFO - 21/05/14 14:22:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416435940789304226485340_0004_m_000186_474: needsTaskCommit() Task attempt_202105141416435940789304226485340_0004_m_000186_474
[2021-05-14 11:22:28,072] {docker.py:276} INFO - 21/05/14 14:22:28 INFO StagingCommitter: Task committer attempt_202105141416435940789304226485340_0004_m_000186_474: needsTaskCommit() Task attempt_202105141416435940789304226485340_0004_m_000186_474: duration 0:00.002s
[2021-05-14 11:22:28,073] {docker.py:276} INFO - 21/05/14 14:22:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416435940789304226485340_0004_m_000186_474
[2021-05-14 11:22:28,075] {docker.py:276} INFO - 21/05/14 14:22:28 INFO Executor: Finished task 186.0 in stage 4.0 (TID 474). 5149 bytes result sent to driver
[2021-05-14 11:22:28,077] {docker.py:276} INFO - 21/05/14 14:22:28 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 479) (0d1d3e01326c, executor driver, partition 191, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:28,078] {docker.py:276} INFO - 21/05/14 14:22:28 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 474) in 7223 ms on 0d1d3e01326c (executor driver) (188/200)
[2021-05-14 11:22:28,078] {docker.py:276} INFO - 21/05/14 14:22:28 INFO Executor: Running task 191.0 in stage 4.0 (TID 479)
[2021-05-14 11:22:28,091] {docker.py:276} INFO - 21/05/14 14:22:28 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[2021-05-14 11:22:28,092] {docker.py:276} INFO - 21/05/14 14:22:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:28,103] {docker.py:276} INFO - 21/05/14 14:22:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2021-05-14 11:22:28,104] {docker.py:276} INFO - 21/05/14 14:22:28 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:28 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416434805338379766782596_0004_m_000191_479, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434805338379766782596_0004_m_000191_479}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416434805338379766782596_0004}; taskId=attempt_202105141416434805338379766782596_0004_m_000191_479, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45acd077}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:28 INFO StagingCommitter: Starting: Task committer attempt_202105141416434805338379766782596_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434805338379766782596_0004_m_000191_479
[2021-05-14 11:22:28,110] {docker.py:276} INFO - 21/05/14 14:22:28 INFO StagingCommitter: Task committer attempt_202105141416434805338379766782596_0004_m_000191_479: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416434805338379766782596_0004_m_000191_479 : duration 0:00.006s
[2021-05-14 11:22:32,485] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416438178235572334545389_0004_m_000188_476: needsTaskCommit() Task attempt_202105141416438178235572334545389_0004_m_000188_476
[2021-05-14 11:22:32,486] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Task committer attempt_202105141416438178235572334545389_0004_m_000188_476: needsTaskCommit() Task attempt_202105141416438178235572334545389_0004_m_000188_476: duration 0:00.004s
21/05/14 14:22:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438178235572334545389_0004_m_000188_476
[2021-05-14 11:22:32,488] {docker.py:276} INFO - 21/05/14 14:22:32 INFO Executor: Finished task 188.0 in stage 4.0 (TID 476). 5149 bytes result sent to driver
[2021-05-14 11:22:32,489] {docker.py:276} INFO - 21/05/14 14:22:32 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 480) (0d1d3e01326c, executor driver, partition 192, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:32,490] {docker.py:276} INFO - 21/05/14 14:22:32 INFO Executor: Running task 192.0 in stage 4.0 (TID 480)
[2021-05-14 11:22:32,491] {docker.py:276} INFO - 21/05/14 14:22:32 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 476) in 6726 ms on 0d1d3e01326c (executor driver) (189/200)
[2021-05-14 11:22:32,501] {docker.py:276} INFO - 21/05/14 14:22:32 INFO ShuffleBlockFetcherIterator: Getting 5 (42.9 KiB) non-empty blocks including 5 (42.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:32,509] {docker.py:276} INFO - 21/05/14 14:22:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416437646232085660711459_0004_m_000192_480, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437646232085660711459_0004_m_000192_480}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416437646232085660711459_0004}; taskId=attempt_202105141416437646232085660711459_0004_m_000192_480, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@fd810c0}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:32,510] {docker.py:276} INFO - 21/05/14 14:22:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416437646232085660711459_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437646232085660711459_0004_m_000192_480
[2021-05-14 11:22:32,514] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Task committer attempt_202105141416437646232085660711459_0004_m_000192_480: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416437646232085660711459_0004_m_000192_480 : duration 0:00.005s
[2021-05-14 11:22:32,938] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Starting: Task committer attempt_20210514141643613639034870293213_0004_m_000189_477: needsTaskCommit() Task attempt_20210514141643613639034870293213_0004_m_000189_477
[2021-05-14 11:22:32,939] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Task committer attempt_20210514141643613639034870293213_0004_m_000189_477: needsTaskCommit() Task attempt_20210514141643613639034870293213_0004_m_000189_477: duration 0:00.003s
21/05/14 14:22:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643613639034870293213_0004_m_000189_477
[2021-05-14 11:22:32,941] {docker.py:276} INFO - 21/05/14 14:22:32 INFO Executor: Finished task 189.0 in stage 4.0 (TID 477). 5149 bytes result sent to driver
[2021-05-14 11:22:32,942] {docker.py:276} INFO - 21/05/14 14:22:32 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 481) (0d1d3e01326c, executor driver, partition 193, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:32,943] {docker.py:276} INFO - 21/05/14 14:22:32 INFO Executor: Running task 193.0 in stage 4.0 (TID 481)
[2021-05-14 11:22:32,943] {docker.py:276} INFO - 21/05/14 14:22:32 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 477) in 7059 ms on 0d1d3e01326c (executor driver) (190/200)
[2021-05-14 11:22:32,953] {docker.py:276} INFO - 21/05/14 14:22:32 INFO ShuffleBlockFetcherIterator: Getting 5 (44.4 KiB) non-empty blocks including 5 (44.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:32,961] {docker.py:276} INFO - 21/05/14 14:22:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:32 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:32 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416438821085928091131176_0004_m_000193_481, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438821085928091131176_0004_m_000193_481}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416438821085928091131176_0004}; taskId=attempt_202105141416438821085928091131176_0004_m_000193_481, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@41728abb}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:32,962] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Starting: Task committer attempt_202105141416438821085928091131176_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438821085928091131176_0004_m_000193_481
[2021-05-14 11:22:32,965] {docker.py:276} INFO - 21/05/14 14:22:32 INFO StagingCommitter: Task committer attempt_202105141416438821085928091131176_0004_m_000193_481: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416438821085928091131176_0004_m_000193_481 : duration 0:00.004s
[2021-05-14 11:22:33,926] {docker.py:276} INFO - 21/05/14 14:22:33 INFO StagingCommitter: Starting: Task committer attempt_202105141416434805338379766782596_0004_m_000191_479: needsTaskCommit() Task attempt_202105141416434805338379766782596_0004_m_000191_479
[2021-05-14 11:22:33,927] {docker.py:276} INFO - 21/05/14 14:22:33 INFO StagingCommitter: Task committer attempt_202105141416434805338379766782596_0004_m_000191_479: needsTaskCommit() Task attempt_202105141416434805338379766782596_0004_m_000191_479: duration 0:00.003s
21/05/14 14:22:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416434805338379766782596_0004_m_000191_479
[2021-05-14 11:22:33,929] {docker.py:276} INFO - 21/05/14 14:22:33 INFO Executor: Finished task 191.0 in stage 4.0 (TID 479). 5149 bytes result sent to driver
[2021-05-14 11:22:33,930] {docker.py:276} INFO - 21/05/14 14:22:33 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 482) (0d1d3e01326c, executor driver, partition 194, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:33,931] {docker.py:276} INFO - 21/05/14 14:22:33 INFO Executor: Running task 194.0 in stage 4.0 (TID 482)
[2021-05-14 11:22:33,932] {docker.py:276} INFO - 21/05/14 14:22:33 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 479) in 5863 ms on 0d1d3e01326c (executor driver) (191/200)
[2021-05-14 11:22:33,942] {docker.py:276} INFO - 21/05/14 14:22:33 INFO ShuffleBlockFetcherIterator: Getting 5 (44.0 KiB) non-empty blocks including 5 (44.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:33,950] {docker.py:276} INFO - 21/05/14 14:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:33 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:33 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416432858686638942508699_0004_m_000194_482, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432858686638942508699_0004_m_000194_482}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416432858686638942508699_0004}; taskId=attempt_202105141416432858686638942508699_0004_m_000194_482, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@20e91a24}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:33,951] {docker.py:276} INFO - 21/05/14 14:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:33,951] {docker.py:276} INFO - 21/05/14 14:22:33 INFO StagingCommitter: Starting: Task committer attempt_202105141416432858686638942508699_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432858686638942508699_0004_m_000194_482
[2021-05-14 11:22:33,954] {docker.py:276} INFO - 21/05/14 14:22:33 INFO StagingCommitter: Task committer attempt_202105141416432858686638942508699_0004_m_000194_482: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416432858686638942508699_0004_m_000194_482 : duration 0:00.004s
[2021-05-14 11:22:34,038] {docker.py:276} INFO - 21/05/14 14:22:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416431081370787291605984_0004_m_000190_478: needsTaskCommit() Task attempt_202105141416431081370787291605984_0004_m_000190_478
[2021-05-14 11:22:34,040] {docker.py:276} INFO - 21/05/14 14:22:34 INFO StagingCommitter: Task committer attempt_202105141416431081370787291605984_0004_m_000190_478: needsTaskCommit() Task attempt_202105141416431081370787291605984_0004_m_000190_478: duration 0:00.002s
21/05/14 14:22:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431081370787291605984_0004_m_000190_478
[2021-05-14 11:22:34,041] {docker.py:276} INFO - 21/05/14 14:22:34 INFO Executor: Finished task 190.0 in stage 4.0 (TID 478). 5149 bytes result sent to driver
[2021-05-14 11:22:34,043] {docker.py:276} INFO - 21/05/14 14:22:34 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 483) (0d1d3e01326c, executor driver, partition 195, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:34,044] {docker.py:276} INFO - 21/05/14 14:22:34 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 478) in 6838 ms on 0d1d3e01326c (executor driver) (192/200)
[2021-05-14 11:22:34,044] {docker.py:276} INFO - 21/05/14 14:22:34 INFO Executor: Running task 195.0 in stage 4.0 (TID 483)
[2021-05-14 11:22:34,054] {docker.py:276} INFO - 21/05/14 14:22:34 INFO ShuffleBlockFetcherIterator: Getting 5 (42.1 KiB) non-empty blocks including 5 (42.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:34,061] {docker.py:276} INFO - 21/05/14 14:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:34 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:34,062] {docker.py:276} INFO - 21/05/14 14:22:34 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431279318892950533825_0004_m_000195_483, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431279318892950533825_0004_m_000195_483}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431279318892950533825_0004}; taskId=attempt_202105141416431279318892950533825_0004_m_000195_483, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6a2dba89}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:34,062] {docker.py:276} INFO - 21/05/14 14:22:34 INFO StagingCommitter: Starting: Task committer attempt_202105141416431279318892950533825_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431279318892950533825_0004_m_000195_483
[2021-05-14 11:22:34,066] {docker.py:276} INFO - 21/05/14 14:22:34 INFO StagingCommitter: Task committer attempt_202105141416431279318892950533825_0004_m_000195_483: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431279318892950533825_0004_m_000195_483 : duration 0:00.004s
[2021-05-14 11:22:39,730] {docker.py:276} INFO - 21/05/14 14:22:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416437646232085660711459_0004_m_000192_480: needsTaskCommit() Task attempt_202105141416437646232085660711459_0004_m_000192_480
[2021-05-14 11:22:39,731] {docker.py:276} INFO - 21/05/14 14:22:39 INFO StagingCommitter: Task committer attempt_202105141416437646232085660711459_0004_m_000192_480: needsTaskCommit() Task attempt_202105141416437646232085660711459_0004_m_000192_480: duration 0:00.002s
21/05/14 14:22:39 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416437646232085660711459_0004_m_000192_480
[2021-05-14 11:22:39,734] {docker.py:276} INFO - 21/05/14 14:22:39 INFO Executor: Finished task 192.0 in stage 4.0 (TID 480). 5106 bytes result sent to driver
[2021-05-14 11:22:39,735] {docker.py:276} INFO - 21/05/14 14:22:39 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 484) (0d1d3e01326c, executor driver, partition 196, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:39,736] {docker.py:276} INFO - 21/05/14 14:22:39 INFO Executor: Running task 196.0 in stage 4.0 (TID 484)
[2021-05-14 11:22:39,737] {docker.py:276} INFO - 21/05/14 14:22:39 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 480) in 7256 ms on 0d1d3e01326c (executor driver) (193/200)
[2021-05-14 11:22:39,746] {docker.py:276} INFO - 21/05/14 14:22:39 INFO ShuffleBlockFetcherIterator: Getting 5 (44.3 KiB) non-empty blocks including 5 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:39,755] {docker.py:276} INFO - 21/05/14 14:22:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:39 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:39 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416431130289348676679845_0004_m_000196_484, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431130289348676679845_0004_m_000196_484}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416431130289348676679845_0004}; taskId=attempt_202105141416431130289348676679845_0004_m_000196_484, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f052c14}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:39 INFO StagingCommitter: Starting: Task committer attempt_202105141416431130289348676679845_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431130289348676679845_0004_m_000196_484
[2021-05-14 11:22:39,769] {docker.py:276} INFO - 21/05/14 14:22:39 INFO StagingCommitter: Task committer attempt_202105141416431130289348676679845_0004_m_000196_484: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416431130289348676679845_0004_m_000196_484 : duration 0:00.014s
[2021-05-14 11:22:40,082] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Starting: Task committer attempt_202105141416438821085928091131176_0004_m_000193_481: needsTaskCommit() Task attempt_202105141416438821085928091131176_0004_m_000193_481
[2021-05-14 11:22:40,084] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Task committer attempt_202105141416438821085928091131176_0004_m_000193_481: needsTaskCommit() Task attempt_202105141416438821085928091131176_0004_m_000193_481: duration 0:00.003s
[2021-05-14 11:22:40,084] {docker.py:276} INFO - 21/05/14 14:22:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416438821085928091131176_0004_m_000193_481
[2021-05-14 11:22:40,087] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Finished task 193.0 in stage 4.0 (TID 481). 5149 bytes result sent to driver
[2021-05-14 11:22:40,088] {docker.py:276} INFO - 21/05/14 14:22:40 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 485) (0d1d3e01326c, executor driver, partition 197, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:40,090] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Running task 197.0 in stage 4.0 (TID 485)
21/05/14 14:22:40 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 481) in 7121 ms on 0d1d3e01326c (executor driver) (194/200)
[2021-05-14 11:22:40,099] {docker.py:276} INFO - 21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Getting 5 (42.0 KiB) non-empty blocks including 5 (42.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:40,107] {docker.py:276} INFO - 21/05/14 14:22:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_202105141416436786929872809848880_0004_m_000197_485, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436786929872809848880_0004_m_000197_485}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202105141416436786929872809848880_0004}; taskId=attempt_202105141416436786929872809848880_0004_m_000197_485, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68cd46dc}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
[2021-05-14 11:22:40,107] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Starting: Task committer attempt_202105141416436786929872809848880_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436786929872809848880_0004_m_000197_485
[2021-05-14 11:22:40,110] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Task committer attempt_202105141416436786929872809848880_0004_m_000197_485: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_202105141416436786929872809848880_0004_m_000197_485 : duration 0:00.004s
[2021-05-14 11:22:40,835] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Starting: Task committer attempt_202105141416432858686638942508699_0004_m_000194_482: needsTaskCommit() Task attempt_202105141416432858686638942508699_0004_m_000194_482
[2021-05-14 11:22:40,836] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Task committer attempt_202105141416432858686638942508699_0004_m_000194_482: needsTaskCommit() Task attempt_202105141416432858686638942508699_0004_m_000194_482: duration 0:00.003s
[2021-05-14 11:22:40,837] {docker.py:276} INFO - 21/05/14 14:22:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416432858686638942508699_0004_m_000194_482
[2021-05-14 11:22:40,838] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Finished task 194.0 in stage 4.0 (TID 482). 5149 bytes result sent to driver
[2021-05-14 11:22:40,839] {docker.py:276} INFO - 21/05/14 14:22:40 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 486) (0d1d3e01326c, executor driver, partition 198, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:40,840] {docker.py:276} INFO - 21/05/14 14:22:40 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 482) in 6884 ms on 0d1d3e01326c (executor driver) (195/200)
[2021-05-14 11:22:40,841] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Running task 198.0 in stage 4.0 (TID 486)
[2021-05-14 11:22:40,851] {docker.py:276} INFO - 21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:40,859] {docker.py:276} INFO - 21/05/14 14:22:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:40 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:40 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643273389356335095237_0004_m_000198_486, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643273389356335095237_0004_m_000198_486}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643273389356335095237_0004}; taskId=attempt_20210514141643273389356335095237_0004_m_000198_486, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6c477408}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:40 INFO StagingCommitter: Starting: Task committer attempt_20210514141643273389356335095237_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643273389356335095237_0004_m_000198_486
[2021-05-14 11:22:40,863] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Task committer attempt_20210514141643273389356335095237_0004_m_000198_486: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643273389356335095237_0004_m_000198_486 : duration 0:00.004s
[2021-05-14 11:22:40,983] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Starting: Task committer attempt_202105141416431279318892950533825_0004_m_000195_483: needsTaskCommit() Task attempt_202105141416431279318892950533825_0004_m_000195_483
[2021-05-14 11:22:40,984] {docker.py:276} INFO - 21/05/14 14:22:40 INFO StagingCommitter: Task committer attempt_202105141416431279318892950533825_0004_m_000195_483: needsTaskCommit() Task attempt_202105141416431279318892950533825_0004_m_000195_483: duration 0:00.003s
21/05/14 14:22:40 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431279318892950533825_0004_m_000195_483
[2021-05-14 11:22:40,986] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Finished task 195.0 in stage 4.0 (TID 483). 5149 bytes result sent to driver
[2021-05-14 11:22:40,988] {docker.py:276} INFO - 21/05/14 14:22:40 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 487) (0d1d3e01326c, executor driver, partition 199, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2021-05-14 11:22:40,988] {docker.py:276} INFO - 21/05/14 14:22:40 INFO Executor: Running task 199.0 in stage 4.0 (TID 487)
[2021-05-14 11:22:40,989] {docker.py:276} INFO - 21/05/14 14:22:40 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 483) in 6919 ms on 0d1d3e01326c (executor driver) (196/200)
[2021-05-14 11:22:40,998] {docker.py:276} INFO - 21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Getting 5 (43.8 KiB) non-empty blocks including 5 (43.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/05/14 14:22:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2021-05-14 11:22:41,007] {docker.py:276} INFO - 21/05/14 14:22:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/05/14 14:22:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/05/14 14:22:41 INFO AbstractS3ACommitterFactory: Using committer partitioned to output data to s3a://udac-forex-project/consolidated_data
21/05/14 14:22:41 INFO AbstractS3ACommitterFactory: Using Commmitter PartitionedStagingCommitter{StagingCommitter{AbstractS3ACommitter{role=Task committer attempt_20210514141643295176142122293943_0004_m_000199_487, name=partitioned, outputPath=s3a://udac-forex-project/consolidated_data, workPath=file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643295176142122293943_0004_m_000199_487}, conflictResolution=REPLACE, wrappedCommitter=FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20210514141643295176142122293943_0004}; taskId=attempt_20210514141643295176142122293943_0004_m_000199_487, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@26f0ceed}; outputPath=file:/home/jovyan/tmp/staging/jovyan/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/staging-uploads, workPath=null, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false}}} for s3a://udac-forex-project/consolidated_data
21/05/14 14:22:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter
21/05/14 14:22:41 INFO StagingCommitter: Starting: Task committer attempt_20210514141643295176142122293943_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643295176142122293943_0004_m_000199_487
[2021-05-14 11:22:41,011] {docker.py:276} INFO - 21/05/14 14:22:41 INFO StagingCommitter: Task committer attempt_20210514141643295176142122293943_0004_m_000199_487: setup task attempt path file:/tmp/hadoop-jovyan/s3a/1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4/_temporary/0/_temporary/attempt_20210514141643295176142122293943_0004_m_000199_487 : duration 0:00.004s
[2021-05-14 11:22:46,712] {docker.py:276} INFO - 21/05/14 14:22:46 INFO StagingCommitter: Starting: Task committer attempt_202105141416436786929872809848880_0004_m_000197_485: needsTaskCommit() Task attempt_202105141416436786929872809848880_0004_m_000197_485
[2021-05-14 11:22:46,713] {docker.py:276} INFO - 21/05/14 14:22:46 INFO StagingCommitter: Task committer attempt_202105141416436786929872809848880_0004_m_000197_485: needsTaskCommit() Task attempt_202105141416436786929872809848880_0004_m_000197_485: duration 0:00.001s
21/05/14 14:22:46 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416436786929872809848880_0004_m_000197_485
[2021-05-14 11:22:46,714] {docker.py:276} INFO - 21/05/14 14:22:46 INFO Executor: Finished task 197.0 in stage 4.0 (TID 485). 5106 bytes result sent to driver
[2021-05-14 11:22:46,715] {docker.py:276} INFO - 21/05/14 14:22:46 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 485) in 6636 ms on 0d1d3e01326c (executor driver) (197/200)
[2021-05-14 11:22:47,105] {docker.py:276} INFO - 21/05/14 14:22:47 INFO StagingCommitter: Starting: Task committer attempt_202105141416431130289348676679845_0004_m_000196_484: needsTaskCommit() Task attempt_202105141416431130289348676679845_0004_m_000196_484
[2021-05-14 11:22:47,106] {docker.py:276} INFO - 21/05/14 14:22:47 INFO StagingCommitter: Task committer attempt_202105141416431130289348676679845_0004_m_000196_484: needsTaskCommit() Task attempt_202105141416431130289348676679845_0004_m_000196_484: duration 0:00.000s
[2021-05-14 11:22:47,106] {docker.py:276} INFO - 21/05/14 14:22:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202105141416431130289348676679845_0004_m_000196_484
[2021-05-14 11:22:47,108] {docker.py:276} INFO - 21/05/14 14:22:47 INFO Executor: Finished task 196.0 in stage 4.0 (TID 484). 5149 bytes result sent to driver
[2021-05-14 11:22:47,109] {docker.py:276} INFO - 21/05/14 14:22:47 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 484) in 7349 ms on 0d1d3e01326c (executor driver) (198/200)
[2021-05-14 11:22:47,196] {docker.py:276} INFO - 21/05/14 14:22:47 INFO StagingCommitter: Starting: Task committer attempt_20210514141643273389356335095237_0004_m_000198_486: needsTaskCommit() Task attempt_20210514141643273389356335095237_0004_m_000198_486
21/05/14 14:22:47 INFO StagingCommitter: Task committer attempt_20210514141643273389356335095237_0004_m_000198_486: needsTaskCommit() Task attempt_20210514141643273389356335095237_0004_m_000198_486: duration 0:00.000s
21/05/14 14:22:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643273389356335095237_0004_m_000198_486
[2021-05-14 11:22:47,197] {docker.py:276} INFO - 21/05/14 14:22:47 INFO Executor: Finished task 198.0 in stage 4.0 (TID 486). 5106 bytes result sent to driver
[2021-05-14 11:22:47,198] {docker.py:276} INFO - 21/05/14 14:22:47 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 486) in 6367 ms on 0d1d3e01326c (executor driver) (199/200)
[2021-05-14 11:22:47,584] {docker.py:276} INFO - 21/05/14 14:22:47 INFO StagingCommitter: Starting: Task committer attempt_20210514141643295176142122293943_0004_m_000199_487: needsTaskCommit() Task attempt_20210514141643295176142122293943_0004_m_000199_487
[2021-05-14 11:22:47,585] {docker.py:276} INFO - 21/05/14 14:22:47 INFO StagingCommitter: Task committer attempt_20210514141643295176142122293943_0004_m_000199_487: needsTaskCommit() Task attempt_20210514141643295176142122293943_0004_m_000199_487: duration 0:00.001s
21/05/14 14:22:47 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210514141643295176142122293943_0004_m_000199_487
[2021-05-14 11:22:47,586] {docker.py:276} INFO - 21/05/14 14:22:47 INFO Executor: Finished task 199.0 in stage 4.0 (TID 487). 5106 bytes result sent to driver
[2021-05-14 11:22:47,588] {docker.py:276} INFO - 21/05/14 14:22:47 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 487) in 6609 ms on 0d1d3e01326c (executor driver) (200/200)
21/05/14 14:22:47 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2021-05-14 11:22:47,588] {docker.py:276} INFO - 21/05/14 14:22:47 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 346.189 s
[2021-05-14 11:22:47,589] {docker.py:276} INFO - 21/05/14 14:22:47 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/05/14 14:22:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2021-05-14 11:22:47,589] {docker.py:276} INFO - 21/05/14 14:22:47 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 364.454107 s
[2021-05-14 11:22:47,592] {docker.py:276} INFO - 21/05/14 14:22:47 INFO AbstractS3ACommitter: Starting: Task committer attempt_202105141416434402549272011988574_0000_m_000000_0: commitJob((no job ID))
[2021-05-14 11:22:47,613] {docker.py:276} INFO - 21/05/14 14:22:47 WARN AbstractS3ACommitter: Task committer attempt_202105141416434402549272011988574_0000_m_000000_0: No pending uploads to commit
[2021-05-14 11:22:49,272] {docker.py:276} INFO - 21/05/14 14:22:49 INFO AbstractS3ACommitter: Starting: Cleanup job (no job ID)
21/05/14 14:22:49 INFO AbstractS3ACommitter: Starting: Aborting all pending commits under s3a://udac-forex-project/consolidated_data
[2021-05-14 11:22:49,452] {docker.py:276} INFO - 21/05/14 14:22:49 INFO AbstractS3ACommitter: Aborting all pending commits under s3a://udac-forex-project/consolidated_data: duration 0:00.182s
21/05/14 14:22:49 INFO AbstractS3ACommitter: Cleanup job (no job ID): duration 0:00.182s
21/05/14 14:22:49 INFO AbstractS3ACommitter: Task committer attempt_202105141416434402549272011988574_0000_m_000000_0: commitJob((no job ID)): duration 0:01.862s
[2021-05-14 11:22:49,956] {docker.py:276} INFO - 21/05/14 14:22:49 INFO FileFormatWriter: Write Job 1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4 committed.
[2021-05-14 11:22:49,965] {docker.py:276} INFO - 21/05/14 14:22:49 INFO FileFormatWriter: Finished processing stats for write job 1d9983de-ab53-4557-9f9e-a6f5f6c8e3f4.
[2021-05-14 11:22:50,067] {docker.py:276} INFO - 21/05/14 14:22:50 INFO SparkContext: Invoking stop() from shutdown hook
[2021-05-14 11:22:50,081] {docker.py:276} INFO - 21/05/14 14:22:50 INFO SparkUI: Stopped Spark web UI at http://0d1d3e01326c:4040
[2021-05-14 11:22:50,102] {docker.py:276} INFO - 21/05/14 14:22:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-05-14 11:22:50,120] {docker.py:276} INFO - 21/05/14 14:22:50 INFO MemoryStore: MemoryStore cleared
[2021-05-14 11:22:50,120] {docker.py:276} INFO - 21/05/14 14:22:50 INFO BlockManager: BlockManager stopped
[2021-05-14 11:22:50,124] {docker.py:276} INFO - 21/05/14 14:22:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-05-14 11:22:50,128] {docker.py:276} INFO - 21/05/14 14:22:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-05-14 11:22:50,136] {docker.py:276} INFO - 21/05/14 14:22:50 INFO SparkContext: Successfully stopped SparkContext
[2021-05-14 11:22:50,137] {docker.py:276} INFO - 21/05/14 14:22:50 INFO ShutdownHookManager: Shutdown hook called
[2021-05-14 11:22:50,138] {docker.py:276} INFO - 21/05/14 14:22:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-e2413e96-de74-4e9f-99f2-0a76141b96df/pyspark-a68b0f4d-1517-4185-bd93-e0f9a2faf83b
[2021-05-14 11:22:50,141] {docker.py:276} INFO - 21/05/14 14:22:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-e2413e96-de74-4e9f-99f2-0a76141b96df
[2021-05-14 11:22:50,143] {docker.py:276} INFO - 21/05/14 14:22:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-023498e4-6bcd-4c74-95b0-dc411b022761
[2021-05-14 11:22:50,149] {docker.py:276} INFO - 21/05/14 14:22:50 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2021-05-14 11:22:50,150] {docker.py:276} INFO - 21/05/14 14:22:50 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2021-05-14 11:22:50,151] {docker.py:276} INFO - 21/05/14 14:22:50 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2021-05-14 11:22:50,390] {xcom.py:238} ERROR - Could not serialize the XCom value into JSON. If you are using pickles instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2021-05-14 11:22:50,391] {taskinstance.py:1482} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1344, in _execute_task
    self.xcom_push(key=XCOM_RETURN_KEY, value=result)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1928, in xcom_push
    session=session,
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/xcom.py", line 88, in set
    value = XCom.serialize_value(value)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/airflow/models/xcom.py", line 235, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type bytes is not JSON serializable
[2021-05-14 11:22:50,401] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=etl, task_id=run_spark_job, execution_date=20210514T141505, start_date=20210514T141600, end_date=20210514T142250
[2021-05-14 11:22:50,462] {local_task_job.py:146} INFO - Task exited with return code 1
